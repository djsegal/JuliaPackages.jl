data parallel training flux modern scale deep learning models increased size parameters substaintially aims provide tools mechanisms multiple gpus supports task based process parallelism former suited single node latter multi handled design locally installed gpu clusters using setting dataset toml repo example datasets package imagenet local set default update path system available future requirement lifted favour api configure code basic usage start julia environment activated currently threads finally via instantiate simple demo fluxdistributed metalhead cuda optimisers classes model resnet key blobtree tree train solutions loc solution csv val opt momentum float setup buffer prepare devices optimizer batchsize epochs loss losses logitcrossentropy generic function method sched identity describes table accessed purposes published alongside images look allow access validation test sets typical neural network current supervised support semi coming soon information found documentation bin directory driver initial configuration level designed loop notice takes arguments keyword sensible defaults worth mentioning ones command logical cores starts workers implying happen tweaked according written heterogenous file loading mind trained filsystem hosted remotely amazon aws bucket rcs updated grads channel remotechannel child processes send gradients forth synchronise perform syncgrads main continually monitors manual synchrnisation sends ultimately trains sent optimise logging hook favourite mlops backend various backends trace metrics printed console replace infrastructure logger wandb unofficial bindings platform wandblogger project ddp name config dict saveweights false cycles step increment