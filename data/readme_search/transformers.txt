julia implementation transformer based models flux installation repl add transformers using gpu install build cuda run model example pretrained bert basic pretrain env datadeps accept true wordpiece tokenizer uncased vocab vocabulary text peter piper picked peck pickled peppers fuzzy wuzzy bear cls sep assert pick led zzy token indices segment fill length sample tok embedding embed feature tensors folder complete huggingface support loading hub hgf base cased forquestionanswering warning hgfbertforquestionanswering doesn field repo gsoc src fields aren initialized loaded outputs current finished information package document series blog posts wrote jsoc tag chengchingwen slack discourse questions create issue github roadmap transformerdecoder data positionembedding positionwise handling input docstring functions runable examples pretrains apis tutorials benchmarks