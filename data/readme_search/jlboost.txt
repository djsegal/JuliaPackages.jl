jlboost julia implementation gradient boosting regresssion trees gbrt based heavily algorithms published xgboost lightgbm catboost papers referred decision tree gbdt limitations currently union missing feature type supported planned single valued models multivariate target support numeric boolean features categorical weights provided records objectives featured batteries included regression library play nice ecosystem tables dataframes categoricalarrays fit data easy manipulate fitting pruning adjustments deploy quick start model dataframe binary classification predicting iris species specify column default columns using rdatasets dataset datasets setosa setdiff names details xgtreemodel jlboosttrees jlboosttreemodel abstractjlboostt ree eta weight petallength logitlogloss returned contains vector loss function typeof array abstractjlboosttree symbol control parameters max depth nrounds sepallength convenience predict score pred plus vcat element float functions computing auc gini adjust multiplying factor unique importances obtain importance row quality gain coverage frequency integration compatible tabular structure accessible table advised define following methods generic package efficient nrow returns rows ncol view cols defines replace lossfunctions supervisedloss choose leaast squares called distloss rand warm fill lpdistloss save load jlb testing jdf jdffile enabling larger ram sometimes convert format functionalities interface jlbosst key advantage loaded memory time hence enable trained computer savejdf irisdisk disk xgtree clean force true recursive mlj available via jlboostmlj notes cpu algorithm described original paper trying implement mentioned ligthgbm port gpus similar project juml