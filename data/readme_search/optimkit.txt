optimkit blisfully ignorant julia package gradient optimization tries assume little descent conjugate bfgs method implemented starts defining algorithm creating instance gradientdescent params conjugategradient flavor lbfgs int parameters namely maxiter iterations defaults typemax essentially unbounded gradtol convergence criterion stop norm default value linesearch currently choice hagerzhanglinesearch verbosity level amount information printed single stdout output line summary iteration step furthermore takes positional argument previous steps account construction approximate inverse hessian keyword acceptfirst determines guess alpha search accepted true typically leads function evaluations otherwise required despite erratic additional following hagerzhang real hestenesstiefel polakribiere daiyuan parameter wolfe condition paremeter accept fluctation bisection performed expansion initial bracket interval independent flag control equal equivalent applied calling numfg normgradhistory optimize kwargs objective specified fval gval returns assumed type including tuples named user specify functions via arguments precondition apply preconditioner current tangent vector position finalize numiter completion allows modify corresponding printing statistics note happens computing directions modified risk conditions satisfied retract direction gradients starting length local path informally inner compute product similar objects dependence useful manifolds represents metric particular symmetric valued scale possibly return negative add overwriting transport retraction receives final computed contain data recomputed satisfy utility optimtest facilitate testing compatibility relation requires require five values provided algorithms standard array linearalgebra dot axpy rmul finally isometrictransport bool indicate vectors preserves false unless robust theoretically proven isometric complement rotation parallel called locking huang gallivan absil approach described section