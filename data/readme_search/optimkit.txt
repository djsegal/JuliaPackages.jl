optimkit blisfully ignorant julia package gradient optimization tries assume little descent conjugate bfgs method implemented starts defining algorithm creating instance gradientdescent params conjugategradient flavor lbfgs int parameters namely maxiter iterations defaults typemax essentially unbounded gradtol convergence criterion stop norm default value linesearch currently choice hagerzhanglinesearch verbosity level amount information printed single stdout output line summary iteration step furthermore takes positional argument previous steps account construction approximate inverse hessian additional keyword following hagerzhang real hestenesstiefel polakribierepolyak daiyuan parameter wolfe condition paremeter accept fluctation function bisection determines performed expansion initial bracket interval independent flag control equal equivalent applied calling normgradhistory optimize kwargs objective specified fval gval returns assumed type including tuples named user specify functions via arguments retract direction gradients starting length local tangent path position informally inner compute product similar objects dependence useful manifolds represents metric particular symmetric valued scale possibly return negative add overwriting transport vector retraction receives final computed contain data recomputed note satisfy utility optimtest facilitate testing compatibility relation requires require five values provided algorithms standard array linearalgebra dot axpy rmul finally isometrictransport bool indicate vectors preserves false unless robust theoretically proven isometric complement rotation parallel called locking huang gallivan absil approach described section