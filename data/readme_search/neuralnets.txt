neuralnets implentation artificial neural networks julia neat features include poised deliver cutting edge synergy business housecat real time twitter ready box hal skynet proof low calorie vegan homeopathic friendly excellent source vitamin exciting flexible network topology combination activation function layer support common node functions addition arbitrary automatic differentiation broad range training algorithms chose hope develop library encompass modern types namely deep belief usage currently multi perceptrons instantiated using mlp genf sizes act constructor describe initialisation procedure follows initialise weights commonly rand randn vector int element input nodes output intermediary elements hidden corresponding actd derivatives respective provided seen dictionary derivs example relu logis ident relud logisd identd returns layers comprised lack specify easily ensure convergence behaviour target classification initialised trained predictions prop command column inputs course defined arrays inputting array data native following define derivative calculated automatically forwarddiff package natively supported bit twice fast evaluate compared identify logistic sigmoid exp logissafe safe doesn collapse evaluating values rectified linear units log tanh hyperbolic tangent methods type constructed train trainx valx traint valt method relies calling external optim default gradient descent algorithm setting parameter selected levenberg marquardt momentum nelder mead accepts sets set outputs validation matrix occuring optional parameters maxiter iterations giving tol threshold affect marquard iterl performance evaluated iter slightly iteration takes verbose true print information gdmtrain implemented list losses batch size randomly subset extremely feature stochastic learning rate larger converge faster result typically happening infinity getting lots nans suggested start value increase improves amount apply try eval store trace false returned entire adatrain lmtrain