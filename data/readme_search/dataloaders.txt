dataloaders parallel iterator machine learning datasets don fit memory inspired pytorch dataloader class threadpools process batches keeping primary thread free utilizing learnbase data access pattern containers box custom easily supported implementing getobs nobs usage options dataset batchsize kwargs arguments container supporting integer samples batch keyword shuffle bool true observations iterating numworkers max threads nthreads workers spawn load kept transformfn function applied individual batching collatefn collates multiple default behavior collate droplast false drop divisible ensures size dropped simple example stuff note fits toy using images import struct imagedataset files abstractvector abstractstring interface idx length image jpg set julia num environment variable starting session