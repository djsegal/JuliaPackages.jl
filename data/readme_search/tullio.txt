tullio flexible einsum macro understands array operations written index notation example sum create log write add values ind exp product writes ordinary nested loops difference parse expressions convolution worse multi threading via threads spawn recursive tiling arrays operates various packages provided loaded called loopvectorization avx speed disable false day match openblas matrix multiplication kernelabstractions kernel gpu version cuda somewhat experimental fast tries provide gradient tracker zygote grad nograd default takes symbolic derivative hand expression using tensor input otherwise simply fills reductions min max option dual instead forwarddiff differentiate allows complicated line inbounds begin mat mod axes infer range output indices explicitly writing existing ranges assignment attempt assumes bounds safe able fine pipe operators indicate functions performed outside lse vec dims verbose true cause print derivatives notices unable mentioned simple pkg test rand sqrt loop broadcasting mult transpose int concatenation permutedims cat abs reduce dropdims maximum dbl examples downsample allowed terms intersect shifts calculated automatic shift offsetarrays convolve filter offsetarray implies extrema wrapped padded wraps clamp repeats pad zeros fftw fft finalisers applied equivalent map norm eachcol anon func reduction function ifelse findmin init typemax access fields eachindex fill evaluated nameddims axiskeys dimension names plus pretty printing row col slow straightforward real tends depending size computer comparison mine race useful diagnostic isn goal little avoiding blas libraries precisely optimised weird contractions nnlib benchmarktools batched matmul batch bmm rev randn mul btime permuted mkl complex aren handled slower chained doesn algorithm makes multiplying sequentially slightly obviously broadcast avoid allocations speeding handling tiled memory opp kib mib result extra checks iteration edges conv flux reshape crosscor stride dsp ones cudakernels defined res larger cyclic kern means won summed trunc disables prevents inferred stencil offsets vector tuples read applying sin cos tan rowmap agrees options setting fastmath base sets threshold divide replacing guess inserts unroll calculation switches tensoroperations removes list note prints calculations absolutely nameddimsarray update initial value sensible defaults zero implicit appear run intersection shifted start unless visible calling module gradients switched sufficiently syntax hooks attached reversediff kernels constructed passed cpu testing extras specify fix constant prevent preferred include constants indexing maps lie inference similarly extends inserting nan padding implementation left underscore whatever based printgrad macros evaluate provides definitions previously behaviour suitable variant changes allow running tests internals following ein omeinsum tensorcast expanding roughly promote type eltype undef acc similar taking tanh expands act abstractarray final isnothing check rhs core compiler return fallback threader eval division dispatch methods generated cuarray times tiles dividing axis half repeatedly divides except parallel finally exists friends attach looks adjoint fwd safely broken filled notice forward pass free saved fail method handle plain eps partials scalar returns variable elsewhere relatives available gaius paddedmatrices build gpuifyloops generate compatible threadsx threaded else strided front near lookalikes seamless replacement identify patterns call basic tensorrules differentiable tensorgrad tensortrack earlier attempts expresses julia treats special lazy tortilla exist publicly nice talk arraymeta tokamak readme