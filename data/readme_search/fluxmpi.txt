fluxmpi distributed data parallel training neural networks installation stable release add development version main quick start using cuda lux optimisers random zygote init allowscalar false model chain dense tanh rng default seed local rank setup gpu synchronize root rand opt distributedoptimizer adam loss sum abs gradient update time epoch global pullback println run code mpiexecjl julia project filename examples deep equilibrium models implicit infinite odes imagenet style guide follow contributions adhere changelog dropped support mpi disable cudampi instead localpreferences toml file clean print functions learnbase aka dataloaders distributeddatacontainer compatible mlutils distributedoptimiser name changed introduces api synchronization don wrap line allreduce gradients namedtuple internal mpiextensions renamed bcast reduce unaware bug resolved luxdl aware true temporarily added dependencies mldatautils ensure future averages values summed processes averaging divide total workers rrule frule defined safely inside current initialized calling doesn lead segfault throw error flux removed dispatch available params users manually broadcasting function broadcast parameters synchronizes lot trainable tied essentially deal