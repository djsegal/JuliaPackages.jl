covariancefunctions primary goal efficient computations kernel matrices called gramian package implements lazy type solve linear algebraic arising methods efficiently running memory code automatically recognizes structures exploits computational efficiency particular fast matrix vector multiplications mvms feature highlights include representation footprint parallelized gradient observations exact gps contrast complexity hessian toeplitz structure log solves isotropic uniform grid kronecker separable product kernels cartesian dimensions barnes hut algorithm approximate sparsification specified tolerance subsequent zeros basic usage example construct using function multiplication mul linearalgebra maternp mat generating data samples randn vectors time instantiating allocation bytes size zero multiplying allocates little allocations kib hand densely consumes gib costs popularly covariance stationary list implementations found src exponentiatedquadratic rbf rationalquadratic exponential exp gammaexponential matern real valued parameters integer cosinekernel cos spectralmixture following mercer dot functions polynomial poly exponentialdot brownian motion finitebasis corresponding finite set basis neuralnetwork mckay neural network combining transformations combinations smooth rational quadratic line combination assigns resulting evaluated base classes true custom simple sum abs implementation advantage specialized aware algorithms prudent input trait typeof isotropicinput options dotproductinput stationarylinearfunctionalinput enable output inference extend eltype bottom union inputs expected promote regular dimension exhibits special detects multiplies direct range recognized toeplitzmatrices symmetrictoeplitz float mib slower expensive conjunction iterative solvers solver levinson whereas vely magnitude julia naive notably results equal machine precision taking rise dimensional constructed grids mvm inversion allows faster improvement exploit note lazily represent lazygrid separableproduct length fill subsequently calling represents kroneckerproduct implemented kroneckerproducts isa factorizations cholesky factorizing solving takes fraction despite system million variables constituent represented required instantiate keeping instantiates combined yield quasi ski framework conditioning gaussian processes information roos noted sparse proposed method low regime automatic derivation engine including complex composite mackay spectral mixture permitting operations contains generic fallback currently gradientkernel impossible structured computes covariances includes value valuegradientkernel via ldiv minimum residual iterations converge extremely conditioned highlight scalability compare gpytorch provided skip plot times exponentiated scales linearly pre processing quadratically dominates total runtime restricted seperable rank accuracy compares dense mathematically support supported provides scalable accurate class extends achieved computing differentiation exemplifies quad component hook costum specifying traits amenable specializations constrast main files containing algebra addition similar reducing comprehensive hessiankernel moderate approach amount gathered observation constants optimized feel free reach benefit improved track depending distribution associated exponentially decaying approximately particularly average distance uniformaly distributed unit hyper cube grows sparsearrays guaranteeing user defined element wise guarnatee positive definiteness care thank david bindel bottleneck computation nearestneighbors inrange principle nearest neighbors search based ball trees operation intrinsic consequently manifold brute force unstructured accelerate parallelization looking highly nnz entries multiply break quickly origins accelerating gravity simulutations variant applied allowing barneshutfactorization constructor applies transform whenever relies summarize interactions clusters guaranteed increases constructing tree parameter trades speed digits norm increase accurary expense setting instead