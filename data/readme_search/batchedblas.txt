nvidia provides batched versions blas gemv gemm trsm functions support floating alpha beta scalars batchedblas extends arrays currently providing dot symv spmv ger syr spr abstractfloats integers scaling coefficients vectors addition type flexibility performance benefit symmetric packed matrices execution times faster equivalent benchmarks follow dashed lines transposed version upper triangle lower example usage julia using cuda symmetricformats matrix size batch dimension reshape unitrange int eltype cuarray float undef packedsize symmetricpacked view tri mem devicebuffer array transpose