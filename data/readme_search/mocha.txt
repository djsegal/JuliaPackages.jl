update dec mocha deprecated version julia existing legacy codebase updates pull request contains fixes cpu backend unit tests passed development happens relative days ecosystem evolved significantly exciting tech writing gpu kernels directly auto differentiation supports excessively primitive reworking technologies requires trivial efforts solutions exist nowadays time retirement deep learning please check alternative packages date actively maintained particular knet flux pure mxnet tensorflow wrapper systems tutorials documentation release notes roadmap issues framework inspired caffe efficient implementations stochastic gradient solvers common layers train shallow convolutional neural networks optional unsupervised pre training via stacked encoders highlights modular architecture clean isolated components network activation functions regularizers initializers etc built sufficient typical applications added easily extended adding custom sub types level interface written dynamic programming language designed scientific computing combining expressive power package eco system playing easy intuitive example ijulia notebook using trained imagenet model image classification portability speed comes multiple backends switched transparently portable runs platform reasonably fast models thanks llvm based jit compiler performance annotations useful prototyping native extension available times faster nvidia cudnn cublas customized cuda provide highly computation speedup observed modern device especially larger compatibility widely adopted hdf format store datasets snapshots inter operate matlab python numpy computational tools provides import correctness extensively covered source licensed mit expat license installation install simply run pkg add console following command instead clone test verify functioning properly machine hello world refer mnist tutorial prepare dataset complete code located examples detailed user guide data datalayer name list txt batch size conv convolutionlayer filter kernel bottoms tops pool poolinglayer stride innerproductlayer output dim neuron neurons relu loss softmaxlosslayer label defaultbackend init net exp dir solver method sgd params parameters max iter regu coef mom policy mompolicy fixed lrpolicy inv load setup coffee lounge save statistics jld report progress iterations break trainingsummary snapshot accuracy accuracylayer validationperformance solve destroy shutdown hosted