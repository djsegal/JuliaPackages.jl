yaad automatic differentiation package julia installation press pkg mode repl type add introduction check blog post implement day project aims provide similar interface pytorch autograd keeping simple core implementation contains straight forward line highly inspired operation directly return cachednode constructs computation graph dynamically using global tape note self moment related experimental coding don guarantee consistency stability versions version compatible practical usage suggest try flux tracker zygote performance aimed projects mark differentiate variable value grad time backward evaluate gradient accumulated linearalgebra rand tracked propagation print goes register define create node sin abstractnode typeof output cos license apache