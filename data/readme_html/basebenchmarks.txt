<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-basebenchmarksjl" class="anchor" aria-hidden="true" href="#basebenchmarksjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BaseBenchmarks.jl</h1>
<p><a href="https://travis-ci.org/JuliaCI/BaseBenchmarks.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/b211617f494f6abcd3b7808fd4d3f55eb324da9b/68747470733a2f2f7472617669732d63692e6f72672f4a756c696143492f4261736542656e63686d61726b732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaCI/BaseBenchmarks.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package is a collection of Julia benchmarks using to track the performance of <a href="https://github.com/JuliaLang/julia">the Julia language</a>.</p>
<p>BaseBenchmarks is written using the <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools</a> package. I highly suggest at least skimming the <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md">BenchmarkTools manual</a> before using BaseBenchmarks locally.</p>
<h4><a id="user-content-loading-and-running-benchmarks" class="anchor" aria-hidden="true" href="#loading-and-running-benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Loading and running benchmarks</h4>
<p>BaseBenchmarks contains a large amount of code, not all of which is suitable for precompilation. Loading all of this code at once can take an annoyingly long time if you only need to run one or two benchmarks. To solve this problem, BaseBenchmarks allows you to dynamically load benchmark suites when you need them:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BaseBenchmarks

<span class="pl-c"><span class="pl-c">#</span> This is the top-level BenchmarkGroup. It's empty until you load child groups into it.</span>
julia<span class="pl-k">&gt;</span> BaseBenchmarks<span class="pl-k">.</span>SUITE
<span class="pl-c1">0</span><span class="pl-k">-</span>element BenchmarkTools<span class="pl-k">.</span>BenchmarkGroup<span class="pl-k">:</span>
  tags<span class="pl-k">:</span> []

<span class="pl-c"><span class="pl-c">#</span> Here's an example of how to load the "linalg" group into BaseBenchmarks.SUITE. You can</span>
<span class="pl-c"><span class="pl-c">#</span> optionally pass in a different BenchmarkGroup as the first argument to load "linalg"</span>
<span class="pl-c"><span class="pl-c">#</span> into it.</span>
julia<span class="pl-k">&gt;</span> BaseBenchmarks<span class="pl-k">.</span><span class="pl-c1">load!</span>(<span class="pl-s"><span class="pl-pds">"</span>linalg<span class="pl-pds">"</span></span>)
  <span class="pl-c1">1</span><span class="pl-k">-</span>element BenchmarkTools<span class="pl-k">.</span>BenchmarkGroup<span class="pl-k">:</span>
    tags<span class="pl-k">:</span> []
    <span class="pl-s"><span class="pl-pds">"</span>linalg<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">3</span><span class="pl-k">-</span>element <span class="pl-c1">BenchmarkGroup</span>([<span class="pl-s"><span class="pl-pds">"</span>array<span class="pl-pds">"</span></span>])

<span class="pl-c"><span class="pl-c">#</span> Here's an example of how to load all the benchmarks into BaseBenchmarks.SUITE. Once again,</span>
<span class="pl-c"><span class="pl-c">#</span> you can pass in a different BenchmarkGroup as the first argument to load the benchmarks</span>
<span class="pl-c"><span class="pl-c">#</span> there instead.</span>
julia<span class="pl-k">&gt;</span> BaseBenchmarks<span class="pl-k">.</span><span class="pl-c1">loadall!</span>();
loading group <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">0.379868963</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>linalg<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">5.4598628</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>parallel<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">0.086358304</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>tuple<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">0.651417342</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>micro<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">0.377109301</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>io<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">0.068647882</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>scalar<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">16.922505539</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>sparse<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">3.750095955</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>simd<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">2.542815776</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>problem<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">2.002920499</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>array<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">6.072152907</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>sort<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">3.308745574</span> seconds)
loading group <span class="pl-s"><span class="pl-pds">"</span>shootout<span class="pl-pds">"</span></span><span class="pl-k">...</span>done (took <span class="pl-c1">0.72022176</span> seconds)</pre></div>
<p>Now that the benchmarks are loaded, you can run them just like any other <code>BenchmarkTools.BenchmarkGroup</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> run benchmarks matching a tag query</span>
<span class="pl-c1">run</span>(BaseBenchmarks<span class="pl-k">.</span>SUITE[<span class="pl-c1">@tagged</span> (<span class="pl-s"><span class="pl-pds">"</span>array<span class="pl-pds">"</span></span> <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">"</span>linalg<span class="pl-pds">"</span></span>) <span class="pl-k">&amp;&amp;</span> <span class="pl-k">!</span>(<span class="pl-s"><span class="pl-pds">"</span>simd<span class="pl-pds">"</span></span>)]);

<span class="pl-c"><span class="pl-c">#</span> run a specific benchmark group</span>
<span class="pl-c1">run</span>(BaseBenchmarks<span class="pl-k">.</span>SUITE[<span class="pl-s"><span class="pl-pds">"</span>linalg<span class="pl-pds">"</span></span>][<span class="pl-s"><span class="pl-pds">"</span>arithmetic<span class="pl-pds">"</span></span>]);

<span class="pl-c"><span class="pl-c">#</span> run a single benchmark</span>
<span class="pl-c1">run</span>(BaseBenchmarks<span class="pl-k">.</span>SUITE[<span class="pl-s"><span class="pl-pds">"</span>scalar<span class="pl-pds">"</span></span>][<span class="pl-s"><span class="pl-pds">"</span>fastmath<span class="pl-pds">"</span></span>][<span class="pl-s"><span class="pl-pds">"</span>add<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Complex{Float64}<span class="pl-pds">"</span></span>])

<span class="pl-c"><span class="pl-c">#</span> equivalent to the above, but this form makes it</span>
<span class="pl-c"><span class="pl-c">#</span> easy to copy and paste IDs from benchmark reports</span>
<span class="pl-c1">run</span>(BaseBenchmarks<span class="pl-k">.</span>SUITE[[<span class="pl-s"><span class="pl-pds">"</span>scalar<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>fastmath<span class="pl-pds">"</span></span>, (<span class="pl-s"><span class="pl-pds">"</span>add<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Complex{Float64}<span class="pl-pds">"</span></span>)]]);</pre></div>
<p>See the <code>BenchmarkTools</code> repository for documentation of <code>BenchmarkTools.BenchmarkGroup</code> features (e.g. regression classification and filtering, parameter tuning, leaf iteration, higher order mapping/filtering, etc.).</p>
<h4><a id="user-content-recipe-for-testing-a-julia-pr-locally" class="anchor" aria-hidden="true" href="#recipe-for-testing-a-julia-pr-locally"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Recipe for testing a Julia PR locally</h4>
<p>If you're a collaborator, <a href="https://github.com/JuliaCI/Nanosoldier.jl">you can trigger Julia's @nanosoldier
bot</a> to automatically test the performance of
your PR vs. Julia's master branch. However, this bot's purpose isn't to have the final
say on performance matters, but rather to identify areas which require local performance
testing. Here's a procedure for testing your Julia PR locally:</p>
<ol>
<li>Run benchmarks and save results using master Julia build</li>
<li>Run benchmarks and save results using PR Julia build</li>
<li>Load and compare the results, looking for regressions</li>
<li>Profile any regressions to find opportunities for performance improvements</li>
</ol>
<p>For steps 1 and 2, first build Julia on the appropriate branch. Then, you can run the
following code to execute all benchmarks and save the results (replacing <code>filename</code> with
an actual unique file name):</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> BenchmarkTools, BaseBenchmarks
BaseBenchmarks<span class="pl-k">.</span><span class="pl-c1">loadall!</span>() <span class="pl-c"><span class="pl-c">#</span> load all benchmarks</span>
results <span class="pl-k">=</span> <span class="pl-c1">run</span>(BaseBenchmarks<span class="pl-k">.</span>SUITE; verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>) <span class="pl-c"><span class="pl-c">#</span> run all benchmarks</span>
BenchmarkTools<span class="pl-k">.</span><span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>filename.json<span class="pl-pds">"</span></span>, results) <span class="pl-c"><span class="pl-c">#</span> save results to JSON file</span></pre></div>
<p>Next, you can load the results and check for regressions (once again replacing the JSON file
names used here with the actual file names):</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> BenchmarkTools, BaseBenchmarks
master <span class="pl-k">=</span> BenchmarkTools<span class="pl-k">.</span><span class="pl-c1">load</span>(<span class="pl-s"><span class="pl-pds">"</span>master.json<span class="pl-pds">"</span></span>)[<span class="pl-c1">1</span>]
pr <span class="pl-k">=</span> BenchmarkTools<span class="pl-k">.</span><span class="pl-c1">load</span>(<span class="pl-s"><span class="pl-pds">"</span>pr.json<span class="pl-pds">"</span></span>)[<span class="pl-c1">1</span>]
regs <span class="pl-k">=</span> <span class="pl-c1">regressions</span>(<span class="pl-c1">judge</span>(<span class="pl-c1">minimum</span>(pr), <span class="pl-c1">minimum</span>(master))) <span class="pl-c"><span class="pl-c">#</span> a BenchmarkGroup containing the regressions</span>
pairs <span class="pl-k">=</span> <span class="pl-c1">leaves</span>(regs) <span class="pl-c"><span class="pl-c">#</span> an array of (ID, `TrialJudgement`) pairs</span></pre></div>
<p>This will show which tests resulted in regressions and to what magnitude. Here's an
example showing what <code>pairs</code> might look like:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">2</span><span class="pl-k">-</span>element Array{Any,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 (Any[<span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>join<span class="pl-pds">"</span></span>],BenchmarkTools<span class="pl-k">.</span>TrialJudgement<span class="pl-k">:</span>
  time<span class="pl-k">:</span>   <span class="pl-k">+</span><span class="pl-c1">41.13</span><span class="pl-k">%</span> <span class="pl-k">=&gt;</span> regression (<span class="pl-c1">1.00</span><span class="pl-k">%</span> tolerance)
  memory<span class="pl-k">:</span> <span class="pl-k">+</span><span class="pl-c1">0.00</span><span class="pl-k">%</span> <span class="pl-k">=&gt;</span> invariant (<span class="pl-c1">1.00</span><span class="pl-k">%</span> tolerance))
 (Any[<span class="pl-s"><span class="pl-pds">"</span>io<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>read<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>readstring<span class="pl-pds">"</span></span>],BenchmarkTools<span class="pl-k">.</span>TrialJudgement<span class="pl-k">:</span>
  time<span class="pl-k">:</span>   <span class="pl-k">+</span><span class="pl-c1">13.85</span><span class="pl-k">%</span> <span class="pl-k">=&gt;</span> regression (<span class="pl-c1">3.00</span><span class="pl-k">%</span> tolerance)
  memory<span class="pl-k">:</span> <span class="pl-k">+</span><span class="pl-c1">0.00</span><span class="pl-k">%</span> <span class="pl-k">=&gt;</span> invariant (<span class="pl-c1">1.00</span><span class="pl-k">%</span> tolerance))</pre></div>
<p>Each pair above is structured as <code>(benchmark ID, TrialJudgement for benchmark)</code>. You can
now examine these benchmarks in detail and try to fix the regressions. Let's use the
<code>["io","read","readstring"]</code> ID shown above as an example.</p>
<p>To examine this benchmark on your currently-built branch, first make sure you've loaded
the benchmark's parent group (the first element in the ID, <code>"io"</code>):</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BenchmarkTools, BaseBenchmarks

julia<span class="pl-k">&gt;</span> <span class="pl-c1">showall</span>(BaseBenchmarks<span class="pl-k">.</span><span class="pl-c1">load!</span>(<span class="pl-s"><span class="pl-pds">"</span>io<span class="pl-pds">"</span></span>))
<span class="pl-c1">1</span><span class="pl-k">-</span>element BenchmarkTools<span class="pl-k">.</span>BenchmarkGroup<span class="pl-k">:</span>
  tags<span class="pl-k">:</span> []
  <span class="pl-s"><span class="pl-pds">"</span>io<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">-</span>element BenchmarkTools<span class="pl-k">.</span>BenchmarkGroup<span class="pl-k">:</span>
	  tags<span class="pl-k">:</span> []
	  <span class="pl-s"><span class="pl-pds">"</span>read<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span><span class="pl-k">-</span>element BenchmarkTools<span class="pl-k">.</span>BenchmarkGroup<span class="pl-k">:</span>
		  tags<span class="pl-k">:</span> [<span class="pl-s"><span class="pl-pds">"</span>buffer<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>stream<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>string<span class="pl-pds">"</span></span>]
		  <span class="pl-s"><span class="pl-pds">"</span>readstring<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> BenchmarkTools<span class="pl-k">.</span>Benchmark<span class="pl-k">...</span>
		  <span class="pl-s"><span class="pl-pds">"</span>read<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> BenchmarkTools<span class="pl-k">.</span>Benchmark<span class="pl-k">...</span></pre></div>
<p>You can now run the benchmark by calling
<code>run(BaseBenchmarks.SUITE[["io","read","readstring"]])</code>, or profile it using <code>@profile</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@profile</span> <span class="pl-c1">run</span>(BaseBenchmarks<span class="pl-k">.</span>SUITE[[<span class="pl-s"><span class="pl-pds">"</span>io<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>read<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>readstring<span class="pl-pds">"</span></span>]])</pre></div>
<p>After profiling the benchmark, you can use <code>Profile.print()</code> or <code>ProfileView.view()</code> to
analyze the bottlenecks that led to that regression.</p>
<h4><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h4>
<p>Our performance tracker could always benefit from more benchmarks! If you have a benchmark that depends only on <code>Base</code> Julia code, it is welcome here - just open a PR against the master branch.</p>
<p>Here are some contribution tips and guidelines:</p>
<ul>
<li>All benchmarks should only depend on base Julia.</li>
<li>You'll need to use <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools</a> to write the benchmarks (feel free to open a WIP PR if you'd like help with this).</li>
<li>Newly defined functions whose calls are measured should have <code>perf_</code> prepended to their name. This makes it easier to find a given benchmark's "entry point" in the code.</li>
<li>Try to reuse existing tags when possible. Tags should be lowercase and singular.</li>
<li>If your benchmark requires a significant amount of code, wrap it in a module.</li>
</ul>
<h4><a id="user-content-which-version-of-basebenchmarks-is-being-used-in-ci" class="anchor" aria-hidden="true" href="#which-version-of-basebenchmarks-is-being-used-in-ci"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Which version of BaseBenchmarks is being used in CI?</h4>
<p>New benchmarks added to BaseBenchmarks won't be present via CI right away, as their execution parameters must be <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md#caching-parameters">tuned and cached</a> on <a href="https://github.com/JuliaCI/Nanosoldier.jl">Nanosoldier</a> (our benchmark cluster) before they are suitable for running. This process is performed periodically and upon request, after which the <code>master</code> branch is merged into the <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/tree/nanosoldier"><code>nanosoldier</code></a> branch. Nanosoldier pulls down the <code>nanosoldier</code> branch before running every benchmark job, so whatever's currently on the <code>nanosoldier</code> branch is what's being used in CI.</p>
</article></div>