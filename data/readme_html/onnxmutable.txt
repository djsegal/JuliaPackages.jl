<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-onnxmutable" class="anchor" aria-hidden="true" href="#onnxmutable"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ONNXmutable</h1>
<p><a href="https://github.com/DrChainsaw/ONNXmutable.jl/actions"><img src="https://github.com/DrChainsaw/ONNXmutable.jl/workflows/CI/badge.svg?branch=master" alt="Build status" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/DrChainsaw/ONNXmutable-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/94a377100437e217aee770526da6fbd602417c952d4d73f0a5c5d90e340413db/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f4472436861696e7361772f4f4e4e586d757461626c652e6a6c3f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/DrChainsaw/ONNXmutable.jl?svg=true" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/DrChainsaw/ONNXmutable.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/aff2f7e46e2043ae36c19d54bfa53931bbfb4a84486e307cf1ccea386b0289ab/68747470733a2f2f636f6465636f762e696f2f67682f4472436861696e7361772f4f4e4e586d757461626c652e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/DrChainsaw/ONNXmutable.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p><a href="https://onnx.ai" rel="nofollow">ONNX</a> import and export for <a href="https://github.com/FluxML/Flux.jl">Flux</a>.</p>
<p>Models are imported as <a href="https://github.com/DrChainsaw/NaiveNASflux.jl">NaiveNASflux</a> graphs, meaning that things like removing/inserting layers and pruning pre-trained models is a breeze.</p>
<p>Model export does not require the model to have any particular format. Almost any julia function can be exported as long as the primitives are recognized by ONNXmutable.</p>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic usage</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Pkg.add(url=&quot;https://github.com/DrChainsaw/ONNXmutable.jl&quot;)
"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(url<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>https://github.com/DrChainsaw/ONNXmutable.jl<span class="pl-pds">"</span></span>)</pre></div>
<p>Exporting is done using the <code>onnx</code> function which accepts a filename <code>String</code> or an <code>IO</code> as first argument:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Save model as model.onnx where inputshapes are tuples with sizes of input.
onnx(&quot;model.onnx&quot;, model, inputshapes...)

# Load model as a CompGraph
graph = CompGraph(&quot;model.onnx&quot;, inputshapes...)
"><pre><span class="pl-c"><span class="pl-c">#</span> Save model as model.onnx where inputshapes are tuples with sizes of input.</span>
<span class="pl-c1">onnx</span>(<span class="pl-s"><span class="pl-pds">"</span>model.onnx<span class="pl-pds">"</span></span>, model, inputshapes<span class="pl-k">...</span>)

<span class="pl-c"><span class="pl-c">#</span> Load model as a CompGraph</span>
graph <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(<span class="pl-s"><span class="pl-pds">"</span>model.onnx<span class="pl-pds">"</span></span>, inputshapes<span class="pl-k">...</span>)</pre></div>
<p>Input shapes can be omitted in which case an attempt to infer the shapes will be made. If supplied, one tuple with size as the dimensions of the corresponding input array (including batch dimension) is expected.</p>
<p>Elements of input shape tuples can have one of the following types:</p>
<ul>
<li><code>Integer</code>: The size of the corresponding dimension</li>
<li><code>Missing</code>: No shape info will be recorded for this dimension</li>
<li><code>Symbol</code> : Use the provided symbol as a variable name in the exported ONNX model</li>
</ul>
<p>Names can be attached to inputs by providing a <code>Pair</code> where the first element is the name as a string, for example <code>"imageinput" =&gt; (:W, :H, 3, missing)</code>. Note that non-integer input sizes will be ignored when loading a model.</p>
<p>More elaborate example with a model defined as a plain Julia function:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using ONNXmutable, Test, Statistics

l1 = Conv((3,3), 2=&gt;3, relu)
l2 = Dense(3, 4, elu)

f = function(x,y)
    x = l1(x)
    # Home-brewed global average pool
    x = dropdims(mean(x, dims=(1,2)), dims=(1,2))
    x = l2(x)
    return x + y
end

# Serialize f. Use a string to save to file instead
io = PipeBuffer()
x_shape = (:W, :H, 2, :Batch)
y_shape = (4, :Batch)
onnx(io, f, x_shape, y_shape)

# Deserialize as a NaiveNASflux CompGraph
g = CompGraph(io)

x = ones(Float32, 5,4,2,3)
y = ones(Float32, 4, 3)
@test g(x,y) ≈ f(x,y)

# Serialization of CompGraphs does not require input shapes to be provided as they can be inferred.
io = PipeBuffer()
onnx(io, g)

g = CompGraph(io)
@test g(x,y) ≈ f(x,y)
"><pre><span class="pl-k">using</span> ONNXmutable, Test, Statistics

l1 <span class="pl-k">=</span> <span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), <span class="pl-c1">2</span><span class="pl-k">=&gt;</span><span class="pl-c1">3</span>, relu)
l2 <span class="pl-k">=</span> <span class="pl-c1">Dense</span>(<span class="pl-c1">3</span>, <span class="pl-c1">4</span>, elu)

f <span class="pl-k">=</span> <span class="pl-k">function</span>(x,y)
    x <span class="pl-k">=</span> <span class="pl-c1">l1</span>(x)
    <span class="pl-c"><span class="pl-c">#</span> Home-brewed global average pool</span>
    x <span class="pl-k">=</span> <span class="pl-c1">dropdims</span>(<span class="pl-c1">mean</span>(x, dims<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">2</span>)), dims<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">2</span>))
    x <span class="pl-k">=</span> <span class="pl-c1">l2</span>(x)
    <span class="pl-k">return</span> x <span class="pl-k">+</span> y
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Serialize f. Use a string to save to file instead</span>
io <span class="pl-k">=</span> <span class="pl-c1">PipeBuffer</span>()
x_shape <span class="pl-k">=</span> (<span class="pl-c1">:W</span>, <span class="pl-c1">:H</span>, <span class="pl-c1">2</span>, <span class="pl-c1">:Batch</span>)
y_shape <span class="pl-k">=</span> (<span class="pl-c1">4</span>, <span class="pl-c1">:Batch</span>)
<span class="pl-c1">onnx</span>(io, f, x_shape, y_shape)

<span class="pl-c"><span class="pl-c">#</span> Deserialize as a NaiveNASflux CompGraph</span>
g <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(io)

x <span class="pl-k">=</span> <span class="pl-c1">ones</span>(Float32, <span class="pl-c1">5</span>,<span class="pl-c1">4</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)
y <span class="pl-k">=</span> <span class="pl-c1">ones</span>(Float32, <span class="pl-c1">4</span>, <span class="pl-c1">3</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">g</span>(x,y) <span class="pl-k">≈</span> <span class="pl-c1">f</span>(x,y)

<span class="pl-c"><span class="pl-c">#</span> Serialization of CompGraphs does not require input shapes to be provided as they can be inferred.</span>
io <span class="pl-k">=</span> <span class="pl-c1">PipeBuffer</span>()
<span class="pl-c1">onnx</span>(io, g)

g <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(io)
<span class="pl-c1">@test</span> <span class="pl-c1">g</span>(x,y) <span class="pl-k">≈</span> <span class="pl-c1">f</span>(x,y)</pre></div>
<h2><a id="user-content-supported-operations" class="anchor" aria-hidden="true" href="#supported-operations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supported Operations</h2>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Add
AveragePool
BatchNormalization
Concat
Constant
Conv
Dropout
Elu
Flatten
Gemm
GlobalAveragePool
GlobalMaxPool
LSTM
MaxPool
Mul
RNN
ReduceMean
Relu
Reshape
Selu
Softmax
Squeeze
Tanh
"><pre><code>Add
AveragePool
BatchNormalization
Concat
Constant
Conv
Dropout
Elu
Flatten
Gemm
GlobalAveragePool
GlobalMaxPool
LSTM
MaxPool
Mul
RNN
ReduceMean
Relu
Reshape
Selu
Softmax
Squeeze
Tanh
</code></pre></div>
<h2><a id="user-content-adding-operations" class="anchor" aria-hidden="true" href="#adding-operations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding Operations</h2>
<p>While the list of supported operations is currently quite meager, it is relatively straightforward to add support for more.</p>
<p>Serialization uses a lightweight graph tracing mechanism where <code>AbstractProbe</code>s are sent through the function to collect all ONNX operations they encounter.</p>
<p>To map the function <code>myfun(args::SomeType....)</code> to an ONNX operation one just defines a method <code>myfun(args::AbstractProbe...)</code> which</p>
<ol>
<li>Adds ONNX information to one of the inputs (does not matter which one)</li>
<li>Returns at least one <code>AbstractProbe</code> with information for the next function</li>
</ol>
<p>This function typically looks something like this:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="import ONNXmutable: AbstractProbe, recursename, nextname, newfrom, add!, name
function myfun(probes::AbstractProbe...)
    p = probes[1] # select any probe
    optype = &quot;MyOpType&quot;
    # Naming strategy (e.g. how to avoid duplicate names) is provided by the probe
    # Not strictly needed, but the onnx model is basically corrupt if duplicates exist
    nodename = recursename(optype, nextname(p))

    # Add ONNX node info
    add!(p, ONNX.NodeProto(
    # Names of input is provided by probes. This is why new probes need to be provided as output
    input = collect(name.(probes)),
    # Name of output from this node
    output = [nodename],
    op_type = optype))

    # Probes can procreate like this
    return newfrom(p, nodename, s -&gt; s)
end
"><pre><span class="pl-k">import</span> ONNXmutable<span class="pl-k">:</span> AbstractProbe, recursename, nextname, newfrom, add!, name
<span class="pl-k">function</span> <span class="pl-en">myfun</span>(probes<span class="pl-k">::</span><span class="pl-c1">AbstractProbe...</span>)
    p <span class="pl-k">=</span> probes[<span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span> select any probe</span>
    optype <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>MyOpType<span class="pl-pds">"</span></span>
    <span class="pl-c"><span class="pl-c">#</span> Naming strategy (e.g. how to avoid duplicate names) is provided by the probe</span>
    <span class="pl-c"><span class="pl-c">#</span> Not strictly needed, but the onnx model is basically corrupt if duplicates exist</span>
    nodename <span class="pl-k">=</span> <span class="pl-c1">recursename</span>(optype, <span class="pl-c1">nextname</span>(p))

    <span class="pl-c"><span class="pl-c">#</span> Add ONNX node info</span>
    <span class="pl-c1">add!</span>(p, ONNX<span class="pl-k">.</span><span class="pl-c1">NodeProto</span>(
    <span class="pl-c"><span class="pl-c">#</span> Names of input is provided by probes. This is why new probes need to be provided as output</span>
    input <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">name</span>.(probes)),
    <span class="pl-c"><span class="pl-c">#</span> Name of output from this node</span>
    output <span class="pl-k">=</span> [nodename],
    op_type <span class="pl-k">=</span> optype))

    <span class="pl-c"><span class="pl-c">#</span> Probes can procreate like this</span>
    <span class="pl-k">return</span> <span class="pl-c1">newfrom</span>(p, nodename, s <span class="pl-k">-&gt;</span> s)
<span class="pl-k">end</span></pre></div>
<p>See <a href="src/serialize/serialize.jl">serialize.jl</a> for existing operations.</p>
<p>Deserialization is done by simply mapping operation types to functions in a dictionary. This allows for both easy extension as well as overwriting of existing mappings with own implementations:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="import ONNXmutable: actfuns

# All inputs which are not output from another node in the graph are provided in the method call
actfuns[:SomeOp] = (params, α, β) -&gt; x -&gt; x^α + β
# Params contains a Dict with attributes.
actfuns[:AnotherOp] = function(params)
    α = get(params, :alpha, 1)
    return x -&gt; α / x
end
ONNXmutable.refresh()
"><pre><span class="pl-k">import</span> ONNXmutable<span class="pl-k">:</span> actfuns

<span class="pl-c"><span class="pl-c">#</span> All inputs which are not output from another node in the graph are provided in the method call</span>
actfuns[<span class="pl-c1">:SomeOp</span>] <span class="pl-k">=</span> (params, α, β) <span class="pl-k">-&gt;</span> x <span class="pl-k">-&gt;</span> x<span class="pl-k">^</span>α <span class="pl-k">+</span> β
<span class="pl-c"><span class="pl-c">#</span> Params contains a Dict with attributes.</span>
actfuns[<span class="pl-c1">:AnotherOp</span>] <span class="pl-k">=</span> <span class="pl-k">function</span>(params)
    α <span class="pl-k">=</span> <span class="pl-c1">get</span>(params, <span class="pl-c1">:alpha</span>, <span class="pl-c1">1</span>)
    <span class="pl-k">return</span> x <span class="pl-k">-&gt;</span> α <span class="pl-k">/</span> x
<span class="pl-k">end</span>
ONNXmutable<span class="pl-k">.</span><span class="pl-c1">refresh</span>()</pre></div>
<p>Note: After adding/changing an operation mapping one needs to call <code>ONNXmutable.refresh()</code> for it to take effect.
See <a href="src/deserialize/ops.jl">ops.jl</a> for existing operations.</p>
<h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<p>All contributions are welcome. Please file an issue before creating a PR.</p>
</article></div>