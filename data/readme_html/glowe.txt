<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-glowe" class="anchor" aria-hidden="true" href="#glowe"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Glowe</h1>
<p>Julia interface to <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow">GloVe</a>.</p>
<p><a href="LICENSE.md"><img src="https://camo.githubusercontent.com/bbf49a2eb96e6f718803f2493bd7aa3baae61abb09b7f8fc185a94e08c504dc6/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d627269676874677265656e2e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" style="max-width:100%;"></a>
<a href="https://travis-ci.org/zgornel/Glowe.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ae0af9c3ead085b81d82642f8a99f2a9b6c4c78d088fffd1d1d84ba605441d51/68747470733a2f2f7472617669732d63692e6f72672f7a676f726e656c2f476c6f77652e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/zgornel/Glowe.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/zgornel/Glowe.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/60636d430159d6c2105a43512268286ecb71765de74a05d91cf7c07b0bddd873/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7a676f726e656c2f476c6f77652e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/zgornel/Glowe.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package provides functionality for generating and working with GloVe word embeddings. The training is done using the original C code from the <a href="https://github.com/stanfordnlp/GloVe">GloVe github repository</a>.</p>
<p>Note that there is also a package called <a href="https://github.com/domluna/Glove.jl">Glove.jl</a> that provides a pure Julia implementation of the algorithm.</p>
<ul>
<li><a href="https://github.com/zgornel/Glowe.jl/blob/master/NEWS.md">Release Notes</a></li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Pkg.clone(&quot;https://github.com/zgornel/Glowe.jl&quot;)
"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/zgornel/Glowe.jl<span class="pl-pds">"</span></span>)</pre></div>
<p>for the latest <code>master</code> or</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Pkg.add(&quot;Glowe&quot;)
"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Glowe<span class="pl-pds">"</span></span>)</pre></div>
<p>for the stable versions.</p>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h2>
<p>Most of the documentation is provided in Julia's native docsystem.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<p>Following Word2Vec.jl's example, considering the corpus from <a href="http://mattmahoney.net/dc/text8.zip" rel="nofollow">http://mattmahoney.net/dc/text8.zip</a> extracted as text file <code>text8</code> in the current working directory, the GloVe model can be obtained with:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; # Training (may take a while)
       vocab_count(&quot;text8&quot;, &quot;vocab.txt&quot;, min_count=5, verbose=1);
       cooccur(&quot;text8&quot;, &quot;vocab.txt&quot;, &quot;cooccurrence.bin&quot;, memory=8.0, verbose=1);
       shuffle(&quot;cooccurrence.bin&quot;, &quot;cooccurrence.shuf.bin&quot;, memory=8.0, verbose=1);
       glove(&quot;cooccurrence.shuf.bin&quot;, &quot;vocab.txt&quot;, &quot;text8-vec&quot;, threads=8,
             x_max=10.0, iter=15, vector_size=300, binary=0, write_header=1,
             verbose=1);
# BUILDING VOCABULARY
# Truncating vocabulary at min count 5.
# Using vocabulary of size 71290.
#
# COUNTING COOCCURRENCES
# window size: 15
# context: symmetric
# Merging cooccurrence files: processed 60666468 lines.
#
# SHUFFLING COOCCURRENCES
# array size: 510027366
# Merging temp files: processed 60666468 lines.
#
# TRAINING MODEL
# Read 60666468 lines.
# vector size: 300
# vocab size: 71290
# x_max: 10.000000
# alpha: 0.750000
# 12/11/18 - 12:58.58AM, iter: 001, cost: 0.070201
# 12/11/18 - 01:00.33AM, iter: 002, cost: 0.052521
# ...
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Training (may take a while)</span>
       <span class="pl-c1">vocab_count</span>(<span class="pl-s"><span class="pl-pds">"</span>text8<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>vocab.txt<span class="pl-pds">"</span></span>, min_count<span class="pl-k">=</span><span class="pl-c1">5</span>, verbose<span class="pl-k">=</span><span class="pl-c1">1</span>);
       <span class="pl-c1">cooccur</span>(<span class="pl-s"><span class="pl-pds">"</span>text8<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>vocab.txt<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>cooccurrence.bin<span class="pl-pds">"</span></span>, memory<span class="pl-k">=</span><span class="pl-c1">8.0</span>, verbose<span class="pl-k">=</span><span class="pl-c1">1</span>);
       <span class="pl-c1">shuffle</span>(<span class="pl-s"><span class="pl-pds">"</span>cooccurrence.bin<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>cooccurrence.shuf.bin<span class="pl-pds">"</span></span>, memory<span class="pl-k">=</span><span class="pl-c1">8.0</span>, verbose<span class="pl-k">=</span><span class="pl-c1">1</span>);
       <span class="pl-c1">glove</span>(<span class="pl-s"><span class="pl-pds">"</span>cooccurrence.shuf.bin<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>vocab.txt<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>text8-vec<span class="pl-pds">"</span></span>, threads<span class="pl-k">=</span><span class="pl-c1">8</span>,
             x_max<span class="pl-k">=</span><span class="pl-c1">10.0</span>, iter<span class="pl-k">=</span><span class="pl-c1">15</span>, vector_size<span class="pl-k">=</span><span class="pl-c1">300</span>, binary<span class="pl-k">=</span><span class="pl-c1">0</span>, write_header<span class="pl-k">=</span><span class="pl-c1">1</span>,
             verbose<span class="pl-k">=</span><span class="pl-c1">1</span>);
<span class="pl-c"><span class="pl-c">#</span> BUILDING VOCABULARY</span>
<span class="pl-c"><span class="pl-c">#</span> Truncating vocabulary at min count 5.</span>
<span class="pl-c"><span class="pl-c">#</span> Using vocabulary of size 71290.</span>
<span class="pl-c"><span class="pl-c">#</span></span>
<span class="pl-c"><span class="pl-c">#</span> COUNTING COOCCURRENCES</span>
<span class="pl-c"><span class="pl-c">#</span> window size: 15</span>
<span class="pl-c"><span class="pl-c">#</span> context: symmetric</span>
<span class="pl-c"><span class="pl-c">#</span> Merging cooccurrence files: processed 60666468 lines.</span>
<span class="pl-c"><span class="pl-c">#</span></span>
<span class="pl-c"><span class="pl-c">#</span> SHUFFLING COOCCURRENCES</span>
<span class="pl-c"><span class="pl-c">#</span> array size: 510027366</span>
<span class="pl-c"><span class="pl-c">#</span> Merging temp files: processed 60666468 lines.</span>
<span class="pl-c"><span class="pl-c">#</span></span>
<span class="pl-c"><span class="pl-c">#</span> TRAINING MODEL</span>
<span class="pl-c"><span class="pl-c">#</span> Read 60666468 lines.</span>
<span class="pl-c"><span class="pl-c">#</span> vector size: 300</span>
<span class="pl-c"><span class="pl-c">#</span> vocab size: 71290</span>
<span class="pl-c"><span class="pl-c">#</span> x_max: 10.000000</span>
<span class="pl-c"><span class="pl-c">#</span> alpha: 0.750000</span>
<span class="pl-c"><span class="pl-c">#</span> 12/11/18 - 12:58.58AM, iter: 001, cost: 0.070201</span>
<span class="pl-c"><span class="pl-c">#</span> 12/11/18 - 01:00.33AM, iter: 002, cost: 0.052521</span>
<span class="pl-c"><span class="pl-c">#</span> ...</span></pre></div>
<p>The model can be imported with</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model = wordvectors(&quot;text8-vec.txt&quot;, Float32, header=true, kind=:text)
# WordVectors 71291 words, 300-element Float32 vectors
"><pre>model <span class="pl-k">=</span> <span class="pl-c1">wordvectors</span>(<span class="pl-s"><span class="pl-pds">"</span>text8-vec.txt<span class="pl-pds">"</span></span>, Float32, header<span class="pl-k">=</span><span class="pl-c1">true</span>, kind<span class="pl-k">=</span><span class="pl-c1">:text</span>)
<span class="pl-c"><span class="pl-c">#</span> WordVectors 71291 words, 300-element Float32 vectors</span></pre></div>
<p>The vector representation of a word can be obtained using <code>get_vector</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; get_vector(model, &quot;book&quot;)
# 300-element Array{Float32,1}:
#   0.006189716
#   0.04822071
#   0.017121462
#   ...
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">get_vector</span>(model, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">#</span> 300-element Array{Float32,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>   0.006189716</span>
<span class="pl-c"><span class="pl-c">#</span>   0.04822071</span>
<span class="pl-c"><span class="pl-c">#</span>   0.017121462</span>
<span class="pl-c"><span class="pl-c">#</span>   ...</span></pre></div>
<p>The cosine similarity of <code>book</code>, for example, can be computed using <code>cosine_similar_words</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; cosine_similar_words(model, &quot;book&quot;)
# 10-element Array{String,1}:
#  &quot;book&quot;
#  &quot;books&quot;
#  &quot;published&quot;
#  &quot;domesday&quot;
#  &quot;novel&quot;
#  &quot;comic&quot;
#  &quot;written&quot;
#  &quot;bible&quot;
#  &quot;urantia&quot;
#  &quot;work&quot;
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">cosine_similar_words</span>(model, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">#</span> 10-element Array{String,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  "book"</span>
<span class="pl-c"><span class="pl-c">#</span>  "books"</span>
<span class="pl-c"><span class="pl-c">#</span>  "published"</span>
<span class="pl-c"><span class="pl-c">#</span>  "domesday"</span>
<span class="pl-c"><span class="pl-c">#</span>  "novel"</span>
<span class="pl-c"><span class="pl-c">#</span>  "comic"</span>
<span class="pl-c"><span class="pl-c">#</span>  "written"</span>
<span class="pl-c"><span class="pl-c">#</span>  "bible"</span>
<span class="pl-c"><span class="pl-c">#</span>  "urantia"</span>
<span class="pl-c"><span class="pl-c">#</span>  "work"</span></pre></div>
<p>Word vectors have many interesting properties. For example,
<code>vector("king") - vector("man") + vector("woman")</code> is close to <code>vector("queen")</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; analogy_words(model, [&quot;king&quot;, &quot;woman&quot;], [&quot;man&quot;])
# 5-element Array{String,1}:
#  &quot;queen&quot;
#  &quot;daughter&quot;
#  &quot;children&quot;
#  &quot;wife&quot;
#  &quot;son&quot;
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">analogy_words</span>(model, [<span class="pl-s"><span class="pl-pds">"</span>king<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>woman<span class="pl-pds">"</span></span>], [<span class="pl-s"><span class="pl-pds">"</span>man<span class="pl-pds">"</span></span>])
<span class="pl-c"><span class="pl-c">#</span> 5-element Array{String,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  "queen"</span>
<span class="pl-c"><span class="pl-c">#</span>  "daughter"</span>
<span class="pl-c"><span class="pl-c">#</span>  "children"</span>
<span class="pl-c"><span class="pl-c">#</span>  "wife"</span>
<span class="pl-c"><span class="pl-c">#</span>  "son"</span></pre></div>
<h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2>
<p>This code has an MIT license and therefore it is free.
GloVe is released under an Apache License v2.0.</p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<p>[1] <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow">GloVe: Global Vectors for Word Representation</a></p>
<p>[2] <a href="https://github.com/domluna/Glove.jl">Glove.jl - native Julia implementation</a></p>
<h2><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgements</h2>
<p>The design of the package relies on design concepts from <a href="https://github.com/zgornel/Word2Vec.jl">the word2vec Julia interface, Word2Vec.jl</a>.</p>
<h2><a id="user-content-reporting-bugs" class="anchor" aria-hidden="true" href="#reporting-bugs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reporting Bugs</h2>
<p>Please <a href="https://github.com/zgornel/Glowe.jl/issues/new">file an issue</a> to report a bug or request a feature.</p>
</article></div>