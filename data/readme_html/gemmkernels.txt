<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gemmkernels" class="anchor" aria-hidden="true" href="#gemmkernels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GemmKernels</h1>
<p dir="auto"><em>Flexible and performant GEMM kernels in Julia</em></p>
<p dir="auto"><a href="https://codecov.io/gh/JuliaGPU/GemmKernels.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/bc20f86f83d9bba6fd0e40a3a0e8faa7794bb9028800874ec598acb063c5f0ca/68747470733a2f2f636f6465636f762e696f2f67682f4a756c69614750552f47656d6d4b65726e656c732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/JuliaGPU/GemmKernels.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<table>
<thead>
<tr>
<th>Julia</th>
<th>CI</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.6</td>
<td><a href="https://buildkite.com/julialang/gemmkernels-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/48d8842dc717d32e5ee2f59f560c932c1e6d426aa3b623fca9179ba4416b2f01/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f39326632656164393638626166633531366166613335343537366363636237616232663562343261323732643963623066302e7376673f6272616e63683d6d617374657226737465703d4a756c6961253230312e36" alt="Continuous Integration" data-canonical-src="https://badge.buildkite.com/92f2ead968bafc516afa354576cccb7ab2f5b42a272d9cb0f0.svg?branch=master&amp;step=Julia%201.6" style="max-width: 100%;"></a></td>
</tr>
<tr>
<td>1.7</td>
<td><a href="https://buildkite.com/julialang/gemmkernels-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e59d723ae9b2317162eae65b1510011957e98e5db7a10708e0ffd458a638864e/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f39326632656164393638626166633531366166613335343537366363636237616232663562343261323732643963623066302e7376673f6272616e63683d6d617374657226737465703d4a756c6961253230312e37" alt="Continuous Integration" data-canonical-src="https://badge.buildkite.com/92f2ead968bafc516afa354576cccb7ab2f5b42a272d9cb0f0.svg?branch=master&amp;step=Julia%201.7" style="max-width: 100%;"></a></td>
</tr>
<tr>
<td>nightly</td>
<td><a href="https://buildkite.com/julialang/gemmkernels-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/0cdac0eb58dd56435249d29f0fd7ea54c86c8c26caa74d65661ad7025478b91d/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f39326632656164393638626166633531366166613335343537366363636237616232663562343261323732643963623066302e7376673f6272616e63683d6d617374657226737465703d4a756c69612532306e696768746c79" alt="Continuous Integration" data-canonical-src="https://badge.buildkite.com/92f2ead968bafc516afa354576cccb7ab2f5b42a272d9cb0f0.svg?branch=master&amp;step=Julia%20nightly" style="max-width: 100%;"></a></td>
</tr>
</tbody>
</table>
<p dir="auto">This package contains a framework to instantiate flexible, performant GEMM (General Matrix Multiplication) kernels.</p>
<p dir="auto">It decomposes GEMM kernels into orthogonal components:</p>
<ul dir="auto">
<li><em>Params</em> determine the tiling size and launch configuration of the GEMM kernel. The tiling sizes are specified in <em>logical</em> coordinates, i.e. with a meaning specified by the user.</li>
<li><em>Layouts</em> convert the logical coordinates of tiles to physical offsets in memory.</li>
<li><em>Transforms</em> are used to apply any arbitrary Julia functor to the GEMM's inputs or outputs. They are applied after every load, and before every store.</li>
<li><em>Operators</em> are responsible to perform the matrix multiplication itself. They load tiles from shared memory, perform the matrix multiplication, and store the resultant tile back to shared memory.</li>
<li><em>Epilogues</em> copy tiles of the resultant matrix to global memory, and can be used to implement arbitrary post-processing, such as adding a bias vector to the resultant matrix.</li>
</ul>
<p dir="auto">Each of these components corresponds to a set of functions with a predetermined interface.
These functions can be customised by the user through Julia's multiple dispatch functionality.</p>
<p dir="auto">The package includes 2 user-facing interfaces: a fully-featured interface (see e.g. <code>test/matmul.jl</code>) and a BLAS-like interface (see e.g. <code>test/blas.jl</code>).</p>
<p dir="auto">The documentation is still a WIP, but you can get an idea of the usage of this package using the examples in <code>test/</code> and <code>benchmark/</code>.</p>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto">The package can be installed using Julia's built-in package manager.
Open the Julia REPL, type <code>]</code> to enter Pkg-mode, and run:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="pkg&gt; add GemmKernels"><pre class="notranslate"><code>pkg&gt; add GemmKernels
</code></pre></div>
<p dir="auto">Note that you need a sufficiently recent NVIDIA GPU (Volta or later) that contains Tensor Cores to use this package.</p>
<h2 dir="auto"><a id="user-content-project-status" class="anchor" aria-hidden="true" href="#project-status"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Project Status</h2>
<p dir="auto">At the moment, the package only contains GEMM kernels for CUDA-enabled NVIDIA GPUs and targets Tensor Cores exclusively.</p>
<p dir="auto">It contains the necessary components for mixed-precision GEMMs using WMMA, GEMMs exploiting operation fusion with elementwise operations or bias vectors, diagonal matrices, and matrices of complex/dual numbers.</p>
<h2 dir="auto"><a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="media/performance-wmma-gemm.png"><img src="media/performance-wmma-gemm.png" alt="Performance Graph" style="max-width: 100%;"></a></p>
<p dir="auto">The above figure shows the performance of a mixed-precision multiplication of two FP16 matrices, resulting in an FP32 resultant matrix, for different memory layouts.
We compare our kernels with the state-of-the-art libraries cuBLAS and CUTLASS on an RTX 2080 Ti.</p>
<h2 dir="auto"><a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<p dir="auto">For more details on the implementation and performance results, please see our accompanying <a href="https://ieeexplore.ieee.org/document/9655458" rel="nofollow">paper</a> (pre-print available on <a href="https://arxiv.org/abs/2009.12263" rel="nofollow">arXiv</a>).
The <a href="CITATION.bib"><code>CITATION.bib</code></a> file in the root of this repository contains a citation in BibTeX format.</p>
</article></div>