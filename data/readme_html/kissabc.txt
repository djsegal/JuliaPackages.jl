<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-kissabc" class="anchor" aria-hidden="true" href="#kissabc"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>KissABC</h1>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/francescoalemanno/KissABC.jl/workflows/CI/badge.svg?branch=master"><img src="https://github.com/francescoalemanno/KissABC.jl/workflows/CI/badge.svg?branch=master" alt="CI" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/francescoalemanno/KissABC.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4e455ca2bcd0dc79ab42b6b2fa26e0dee7a986c5/68747470733a2f2f636f6465636f762e696f2f67682f6672616e636573636f616c656d616e6e6f2f4b6973734142432e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/francescoalemanno/KissABC.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a>
<a href="https://francescoalemanno.github.io/KissABC.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3e353c26ddfe819150acbc732248f4f2a37f5175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://francescoalemanno.github.io/KissABC.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a></p>
<h1><a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Table of Contents</h1>
<ul>
<li><a href="#usage-guide">Beginners Usage Guide</a></li>
<li><a href="#details">Details</a></li>
</ul>
<h2><a id="user-content-usage-guide" class="anchor" aria-hidden="true" href="#usage-guide"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage guide</h2>
<p>The ingredients you need to use Approximate Bayesian Computation:</p>
<ol>
<li>A simulation which depends on some parameters, able to generate datasets similar to your target dataset if parameters are tuned</li>
<li>A prior distribution over such parameters</li>
<li>A distance function to compare generated dataset to the true dataset</li>
</ol>
<p>We will start with a simple example, we have a dataset generated according to an Normal distribution whose parameters are unknown</p>
<div class="highlight highlight-source-julia"><pre>tdata<span class="pl-k">=</span><span class="pl-c1">randn</span>(<span class="pl-c1">1000</span>)<span class="pl-k">.*</span><span class="pl-c1">0.04</span><span class="pl-k">.+</span><span class="pl-c1">2</span></pre></div>
<p>we are ofcourse able to simulate normal random numbers, so this constitutes our simulation</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">sim</span>((μ,σ), param) <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">1000</span>) <span class="pl-k">.*</span> σ <span class="pl-k">.+</span> μ</pre></div>
<p>The second ingredient is a prior over the parameters μ and σ</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributions
<span class="pl-k">using</span> KissABC
prior<span class="pl-k">=</span><span class="pl-c1">Factored</span>(<span class="pl-c1">Uniform</span>(<span class="pl-c1">1</span>,<span class="pl-c1">3</span>), <span class="pl-c1">Truncated</span>(<span class="pl-c1">Normal</span>(<span class="pl-c1">0</span>,<span class="pl-c1">0.1</span>), <span class="pl-c1">0</span>, <span class="pl-c1">100</span>))</pre></div>
<p>we have chosen a uniform distribution over the interval [1,3] for μ and a normal distribution truncated over ℝ⁺ for σ.</p>
<p>Now all that we need is a distance function to compare the true dataset to the simulated dataset, for this purpose a Kolmogorov-Smirnoff distance is good</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> StatsBase
<span class="pl-k">function</span> <span class="pl-en">ksdist</span>(x,y)
    p1<span class="pl-k">=</span><span class="pl-c1">ecdf</span>(x)
    p2<span class="pl-k">=</span><span class="pl-c1">ecdf</span>(y)
    r<span class="pl-k">=</span>[x;y]
    <span class="pl-c1">maximum</span>(<span class="pl-c1">abs</span>.(<span class="pl-c1">p1</span>.(r)<span class="pl-k">-</span><span class="pl-c1">p2</span>.(r)))
<span class="pl-k">end</span></pre></div>
<p>Now we are all set, first we define an <code>ABCplan</code> via</p>
<div class="highlight highlight-source-julia"><pre>plan <span class="pl-k">=</span> <span class="pl-c1">ABCplan</span>(prior, sim, tdata, ksdist)</pre></div>
<p>where ofcourse the four parameters are the ingredients we defined earlier in the previous steps, and then
we can use <code>ABCSMCPR</code> which is sequential Monte Carlo algorithm to simulate the posterior distribution for this model</p>
<div class="highlight highlight-source-julia"><pre>res,_ <span class="pl-k">=</span> <span class="pl-c1">ABCSMCPR</span>(plan, <span class="pl-c1">0.1</span>, nparticles<span class="pl-k">=</span><span class="pl-c1">200</span>,parallel<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<p>Or, we can use <code>ABCDE</code> which is still an SMC algorithm, but with an adaptive proposal, which is much more efficient</p>
<div class="highlight highlight-source-julia"><pre>res,_ <span class="pl-k">=</span> <span class="pl-c1">ABCDE</span>(plan, <span class="pl-c1">0.1</span>, nparticles<span class="pl-k">=</span><span class="pl-c1">200</span>,parallel<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<p>In any case we chose a tolerance on distances equal to <code>0.1</code>, a number of simulated particles equal to <code>200</code>, we enabled Threaded parallelism, and the simulated posterior results are in <code>res</code>, we are ignoring all the other information returned via <code>_</code>.
We can now extract the results:</p>
<div class="highlight highlight-source-julia"><pre>prsample<span class="pl-k">=</span>[<span class="pl-c1">rand</span>(prior) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5000</span>] <span class="pl-c"><span class="pl-c">#</span>some samples from the prior for comparison</span>
μ_pr<span class="pl-k">=</span><span class="pl-c1">getindex</span>.(prsample,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> μ samples from the prior</span>
σ_pr<span class="pl-k">=</span><span class="pl-c1">getindex</span>.(prsample,<span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> σ samples from the prior</span>
μ_p<span class="pl-k">=</span><span class="pl-c1">getindex</span>.(res,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> μ samples from the posterior</span>
σ_p<span class="pl-k">=</span><span class="pl-c1">getindex</span>.(res,<span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> σ samples from the posterior</span></pre></div>
<p>and plotting prior and posterior side by side we get:</p>
<p><a target="_blank" rel="noopener noreferrer" href="images/inf_normaldist.png"><img src="images/inf_normaldist.png" alt="plots of the Inference Results" title="Inference Results" style="max-width:100%;"></a>
we can see that the algorithm has correctly inferred both parameters, this exact recipe will work for much more complicated models and simulations, with some tuning.</p>
<h2><a id="user-content-details" class="anchor" aria-hidden="true" href="#details"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Details</h2>
<p>This package currently implements 4 algorithms whose details can be found in Docs</p>
<ol>
<li><code>ABC</code> this is a standard rejection algorithm, you can find examples in <code>test/runtests.jl</code></li>
<li><code>ABCSMCPR</code> this is the sequential monte carlo algorithm by Drovandi et al. 2011, you can find an examples in <code>test/runtests.jl</code></li>
<li><code>ABCDE</code> this is the population monte carlo algorithm based on differential evolution by B.M. Turner 2012, you can find an examples in <code>test/runtests.jl</code></li>
<li><code>KABCDE</code> this is the kernel sequential monte carlo algorithm based on differential evolution by B.M. Turner 2012, you can find an examples in <code>test/runtests.jl</code></li>
</ol>
</article></div>