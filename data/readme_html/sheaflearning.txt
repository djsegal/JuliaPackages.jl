<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-sheaflearningjl" class="anchor" aria-hidden="true" href="#sheaflearningjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SheafLearning.jl</h1>
<p>This is a Julia package containing routines for solving the optimization problems in [1]
for learning sheaf Laplacians and matrix-weighted graph Laplacians from smooth signals.
It also has the ability to compute projections onto the cone of sheaf Laplacians.</p>
<p>The package is currently largely untested and no guarantees are made regarding its abilities or performance.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>SheafLearning.jl is not currently in the General registry. To install it, run the following in the Julia REPL:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;https://github.com/hansenjakob/SheafLearning.jl&quot;)
"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/hansenjakob/SheafLearning.jl<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p>Import SheafLearning.jl as usual. The main functions are <code>recover_sheaf_Laplacian</code>, <code>recover_mw_Laplacian</code>, and <code>project_to_sheaf_Laplacian</code>.</p>
<p>The two sheaf recovery functions minimize the objective</p>
<p><code>tr(LM) - alpha * Î£_v log(tr(L_vv)) + beta * ||offdiag(L)||_F</code></p>
<p>where <code>L</code> is restricted to the cone of sheaf Laplacians and the cone of matrix-weighted graph Laplacians, respectively. The structure of the Laplacian is restricted by the assumption that the graph has <code>Nv</code> vertices and that vertex stalks are <code>dv</code>-dimensional. (<code>dv</code> may also be an array of integers of length <code>Nv</code>, giving the dimension of each stalk) <code>M</code> here is the matrix of the outer products of the smooth signals: if columns of <code>X</code> are the signals, we have <code>M = XX^T</code>. This can also be seen (assuming mean-centered data) as a data covariance matrix.</p>
<p>The signatures are</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="recover_sheaf_Laplacian(M,alpha,beta,Nv,dv,tol=1e-7,maxouter=20,tscale=25,verbose=false,backend=&quot;scs&quot;)
recover_mw_Laplacian(M,alpha,beta,Nv,dv,tol=1e-7,maxouter=20,tscale=25,verbose=false,backend=&quot;direct&quot;)
"><pre><span class="pl-c1">recover_sheaf_Laplacian</span>(M,alpha,beta,Nv,dv,tol<span class="pl-k">=</span><span class="pl-c1">1e-7</span>,maxouter<span class="pl-k">=</span><span class="pl-c1">20</span>,tscale<span class="pl-k">=</span><span class="pl-c1">25</span>,verbose<span class="pl-k">=</span><span class="pl-c1">false</span>,backend<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>scs<span class="pl-pds">"</span></span>)
<span class="pl-c1">recover_mw_Laplacian</span>(M,alpha,beta,Nv,dv,tol<span class="pl-k">=</span><span class="pl-c1">1e-7</span>,maxouter<span class="pl-k">=</span><span class="pl-c1">20</span>,tscale<span class="pl-k">=</span><span class="pl-c1">25</span>,verbose<span class="pl-k">=</span><span class="pl-c1">false</span>,backend<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>direct<span class="pl-pds">"</span></span>)</pre></div>
<p>The functions return tuples <code>(Le,obj)</code> and <code>(We,obj)</code>, respectively, where <code>Le</code> is a 3-dimensional array whose slices are edge contribution matrices of dimension <code>2dv</code> by <code>2dv</code>, <code>We</code> is an array whose slices are edge weight matrices of dimension <code>dv</code> by <code>dv</code>, and <code>obj</code> is the objective function value at the minimizer. To convert these arrays into actual Laplacians, use the functions</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="edge_matrices_to_Laplacian(Le,Nv,dv)
edge_weights_to_Laplacian(We,Nv,dv)
"><pre><span class="pl-c1">edge_matrices_to_Laplacian</span>(Le,Nv,dv)
<span class="pl-c1">edge_weights_to_Laplacian</span>(We,Nv,dv)</pre></div>
<p>There are three backends for these two functions: the conic solvers <a href="https://github.com/JuliaOpt/SCS.jl">SCS</a> (<code>"scs"</code>) and <a href="https://www.mosek.com" rel="nofollow">Mosek</a> (<code>"mosek"</code>), and a hand-written direct solver (<code>"direct"</code>, not recommended). The direct backend will likely be removed in the future.</p>
<p>Mosek is a commercial package, but has free academic licenses available, while SCS is open source. The Mosek solver is currently by far the fastes and most robust.</p>
<p>The function <code>project_to_sheaf_Laplacian</code> minimizes</p>
<p><code>||L - M||_F^2</code></p>
<p>for <code>L</code> in the cone of sheaf Laplacians over graphs with <code>Nv</code> vertices and <code>dv</code>-dimensional vertex stalks. The signature is</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="project_to_sheaf_Laplacian(M,Nv,dv;backend=&quot;scs&quot;,verbose=false)
"><pre><span class="pl-c1">project_to_sheaf_Laplacian</span>(M,Nv,dv;backend<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>scs<span class="pl-pds">"</span></span>,verbose<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p>This problem is translated to a conic program and solved using either SCS or Mosek.</p>
<p>More information about the functions can be found using Julia's built-in help. Type <code>?[function_name]</code> into the REPL.</p>
<h2><a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contact</h2>
<p>Feel free to contact me at jakob at jakobhansen.org with questions or suggestions. More resources on cellular sheaves and their Laplacians are available at <a href="http://www.jakobhansen.org" rel="nofollow">jakobhansen.org</a>.</p>
<p>[1] Hansen, Jakob and Ghrist, Robert. <a href="https://www.math.upenn.edu/~jhansen/publications/learningsheaves.pdf" rel="nofollow">Learning Sheaf Laplacians from Smooth Signals</a>. <em>Proceedings of the International Conference on Acoustics, Speech, and Signal Processing</em>, 2019.</p>
</article></div>