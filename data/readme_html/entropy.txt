<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-entropy" class="anchor" aria-hidden="true" href="#entropy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Entropy</h1>
<p dir="auto">This package contains functionality for computing binless estimates of entropy for continuous distributions. Based on 1.	Victor, J. D. Binless strategies for estimation of information from neural data. Phys Rev E Stat Nonlin Soft Matter Phys 66, 051903 (2002).</p>
<p dir="auto"><a href="https://travis-ci.org/roger.herikstad@gmail.com/Entropy.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4697dbf0dae53cb96cc4c3540f657cd4e3cd48467453deae054e7aae129c661a/68747470733a2f2f7472617669732d63692e6f72672f726f6765722e686572696b7374616440676d61696c2e636f6d2f456e74726f70792e6a6c2e706e67" alt="Build Status" data-canonical-src="https://travis-ci.org/roger.herikstad@gmail.com/Entropy.jl.png" style="max-width: 100%;"></a></p>
</article></div>