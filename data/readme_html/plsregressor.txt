<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-plsregressorjl" class="anchor" aria-hidden="true" href="#plsregressorjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PLSRegressor.jl</h1>
<p>A Partial Least Squares Regressor package. Contains PLS1, PLS2 and Kernel PLS2 NIPALS algorithms.
Can be used mainly for regression. However, for classification task, binarizing targets and then obtaining multiple targets, you can apply KPLS.</p>
<table>
<thead>
<tr>
<th align="center"><strong>PackageEvaluator</strong></th>
<th align="center"><strong>Build Status</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://pkg.julialang.org/?pkg=PLSRegressor&amp;ver=0.6" rel="nofollow"><img src="https://camo.githubusercontent.com/4f8f73ffb1655b32bae9e29a0d9d6f2fde52a402/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f504c53526567726573736f725f302e362e737667" alt="" data-canonical-src="http://pkg.julialang.org/badges/PLSRegressor_0.6.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://travis-ci.org/lalvim/PLSRegressor.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c6745c7cef73f6fb521a61caf37757e4cdac365e/68747470733a2f2f7472617669732d63692e6f72672f6c616c76696d2f504c53526567726573736f722e6a6c2e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://travis-ci.org/lalvim/PLSRegressor.jl.svg?branch=master" style="max-width:100%;"></a> <a href="http://codecov.io/github/lalvim/PLSRegressor.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/227047f29a996718f912a23df8983f85353b1217/687474703a2f2f636f6465636f762e696f2f6769746875622f6c616c76696d2f504c53526567726573736f722e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="http://codecov.io/github/lalvim/PLSRegressor.jl/coverage.svg?branch=master" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h1><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h1>
<pre><code>Pkg.add("PLSRegressor")
</code></pre>
<h1><a id="user-content-using" class="anchor" aria-hidden="true" href="#using"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using</h1>
<pre><code>using PLSRegressor
</code></pre>
<h1><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h1>
<pre><code>using PLSRegressor

# learning a single target
X_train        = [1 2; 2 4; 4 6.0]
Y_train        = [4; 6; 8.0]
X_test         = [6 8; 8 10; 10 12.0]
Y_test         = [10; 12; 14.0]

model          = PLSRegressor.fit(X_train,Y_train,nfactors=2)
Y_pred         = PLSRegressor.predict(model,X_test)

print("[PLS1] mae error : $(mean(abs.(Y_test .- Y_pred)))")


# learning multiple targets
X_train        = [1 2; 2 4; 4 6.0]
Y_train        = [2 4;4 6;6 8.0]
X_test         = [6 8; 8 10; 10 12.0]
Y_test         = [8 10; 10 12; 12 14.0]

model          = PLSRegressor.fit(X_train,Y_train,nfactors=2)
Y_pred         = PLSRegressor.predict(model,X_test)

print("[PLS2] mae error : $(mean(abs.(Y_test .- Y_pred)))")

# nonlinear learning with multiple targets
model          = PLSRegressor.fit(X_train,Y_train,nfactors=2,kernel="rbf",width=0.1)
Y_pred         = PLSRegressor.predict(model,X_test)

print("[KPLS] mae error : $(mean(abs.(Y_test .- Y_pred)))")


# if you want to save your model
PLSRegressor.save(model,filename=joinpath(homedir(),"pls_model.jld"))

# if you want to load back your model
model = PLSRegressor.load(filename=joinpath(homedir(),"pls_model.jld"))
</code></pre>
<h1><a id="user-content-what-is-implemented" class="anchor" aria-hidden="true" href="#what-is-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is Implemented</h1>
<ul>
<li>A fast linear algorithm for single targets (PLS1 - NIPALS)</li>
<li>A linear algorithm for multiple targets (PLS2 - NIPALS)</li>
<li>A non linear algorithm for multiple targets (Kernel PLS2 - NIPALS)</li>
</ul>
<h1><a id="user-content-what-is-upcoming" class="anchor" aria-hidden="true" href="#what-is-upcoming"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is Upcoming</h1>
<ul>
<li>Bagging for Kernel PLS</li>
<li>An automatic validation inside fit function</li>
</ul>
<h1><a id="user-content-method-description" class="anchor" aria-hidden="true" href="#method-description"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Method Description</h1>
<ul>
<li>
<p>PLSRegressor.fit - learns from input data and its related single target</p>
<ul>
<li>X::Matrix{:&lt;AbstractFloat} - A matrix that columns are the features and rows are the samples</li>
<li>Y::Vector{:&lt;AbstractFloat} - A vector with float values.</li>
<li>nfactors::Int = 10 - The number of latent variables to explain the data.</li>
<li>copydata::Bool = true - If you want to use the same input matrix or a copy.</li>
<li>centralize::Bool = true - If you want to z-score columns. Recommended if not z-scored yet.</li>
<li>kernel::AbstractString = "rbf" - use a non linear kernel.</li>
<li>width::AbstractFloat   = 1.0 - If you want to z-score columns. Recommended if not z-scored yet.</li>
</ul>
</li>
<li>
<p>PLSRegressor.transform - predicts using the learnt model extracted from fit.</p>
<ul>
<li>model::PLSRegressor.Model - A PLS model learnt from fit.</li>
<li>X::Matrix{:&lt;AbstractFloat} - A matrix that columns are the features and rows are the samples.</li>
<li>copydata::Bool = true - If you want to use the same input matrix or a copy.</li>
</ul>
</li>
</ul>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h1>
<ul>
<li>
<p>PLS1 and PLS2 based on</p>
<ul>
<li>Bob Collins Slides, LPAC Group. <a href="http://vision.cse.psu.edu/seminars/talks/PLSpresentation.pdf" rel="nofollow">http://vision.cse.psu.edu/seminars/talks/PLSpresentation.pdf</a></li>
</ul>
</li>
<li>
<p>A Kernel PLS2 based on</p>
<ul>
<li>Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space" by Roman Rosipal and Leonard J Trejo. Journal of Machine Learning Research 2 (2001) 97-123 <a href="http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf" rel="nofollow">http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf</a></li>
</ul>
</li>
<li>
<p>NIPALS: Nonlinear Iterative Partial Least Squares</p>
<ul>
<li>Wold, H. (1966). Estimation of principal components and related models
by iterative least squares. In P.R. Krishnaiaah (Ed.). Multivariate Analysis.
(pp.391-420) New York: Academic Press.</li>
</ul>
</li>
<li>
<p>SIMPLS: more efficient, optimal result</p>
<ul>
<li>Supports multivariate Y</li>
<li>De Jong, S., 1993. SIMPLS: an alternative approach to partial least squares
regression. Chemometrics and Intelligent Laboratory Systems, 18: 251â€“
263</li>
</ul>
</li>
</ul>
<h1><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h1>
<p>The PLSRegressor.jl is free software: you can redistribute it and/or modify it under the terms of the MIT "Expat"
License. A copy of this license is provided in <code>LICENSE.md</code></p>
</article></div>