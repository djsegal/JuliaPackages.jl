<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-naivegaflux" class="anchor" aria-hidden="true" href="#naivegaflux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NaiveGAflux</h1>
<p><a href="https://github.com/DrChainsaw/NaiveGAflux.jl/actions"><img src="https://github.com/DrChainsaw/NaiveGAflux.jl/workflows/CI/badge.svg?branch=master" alt="Build status" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/DrChainsaw/NaiveGAflux-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/0e7c47af5b18439a0c0edf4365612ab8bc059ea81a0baaa822edc3220cadc92e/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f4472436861696e7361772f4e616976654741666c75782e6a6c3f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/DrChainsaw/NaiveGAflux.jl?svg=true" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/DrChainsaw/NaiveGAflux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f727519edf3fdb11569fc86e9873dfceb2896a5e37e001d90ac79346c1b11775/68747470733a2f2f636f6465636f762e696f2f67682f4472436861696e7361772f4e616976654741666c75782e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/DrChainsaw/NaiveGAflux.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>Neural architecture search for <a href="https://github.com/FluxML/Flux.jl">Flux</a> models using genetic algorithms.</p>
<p>A marketing person might describe it as "practical proxyless NAS using an unrestricted search space".</p>
<p>The more honest purpose is to serve as a pipe cleaner and example for <a href="https://github.com/DrChainsaw/NaiveNASflux.jl">NaiveNASflux</a> which is doing most of the heavy lifting.</p>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic Usage</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="]add NaiveGAflux
"><pre>]add NaiveGAflux</pre></div>
<p>The basic idea is to create not just one model, but a population of several candidate models with different hyperparameters. The whole population is then evolved while the models are being trained.</p>
<table>
<thead>
<tr>
<th align="center">MNIST</th>
<th align="center">CIFAR10</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a target="_blank" rel="noopener noreferrer" href="gif/MNIST.gif"><img src="gif/MNIST.gif" width="500" style="max-width:100%;"></a></td>
<td align="center"><a target="_blank" rel="noopener noreferrer" href="gif/CIFAR10.gif"><img src="gif/CIFAR10.gif" width="500" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>More concretely, this means train each model for a number of iterations, evaluate the fitness of each model, select the ones with highest fitness, apply random mutations (e.g. add/remove neurons/layers) to some of them and repeat until a model with the desired fitness has been produced.</p>
<p>By controlling the number of training iterations before evolving the population, it is possible tune the compromise between fully training each model at the cost of longer time to evolve versus the risk of discarding a model just because it trains slower than the other members.</p>
<p>Like any self-respecting AutoML-type library, NaiveGAflux provides an application with a deceivingly simple API:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveGAflux.AutoFlux, MLDatasets

models = fit(CIFAR10.traindata())
"><pre><span class="pl-k">using</span> NaiveGAflux<span class="pl-k">.</span>AutoFlux, MLDatasets

models <span class="pl-k">=</span> <span class="pl-c1">fit</span>(CIFAR10<span class="pl-k">.</span><span class="pl-c1">traindata</span>())</pre></div>
<p>It is possible (and strongly recommended) to supply a callback function which will receive the whole population of models as input after fitness for each generation has been calculated. A few useful functions are provided:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveGAflux, Plots
# Persist the whole population in directory models/CIFAR10 so that optimization can be resumed if aborted:
models = fit(CIFAR10.traindata(), cb=persist, mdir=&quot;models/CIFAR10&quot;)

# Plot best and average fitness for each generation
plotfitness = PlotFitness(plot, &quot;models/CIFAR10&quot;);
# Plot data will be serialized in a subdir of &quot;models/CIFAR10&quot; for later postprocessing and for resuming optimization.
models = fit(CIFAR10.traindata(), cb=plotfitness, mdir=&quot;models/CIFAR10&quot;)


# Scatter plots from examples above:
scatterpop = ScatterPop(scatter, &quot;models/CIFAR10&quot;);
scatteropt = ScatterOpt(scatter, &quot;models/CIFAR10&quot;);

# Combine multiple plots in one figure:
multiplot = MultiPlot(display ∘ plot, plotfitness, scatterpop, scatteropt)

# Combine multiple callbacks in one function:
callbacks = CbAll(persist, multiplot)

models = fit(CIFAR10.traindata(), cb=callbacks, mdir=&quot;models/CIFAR10&quot;)
"><pre><span class="pl-k">using</span> NaiveGAflux, Plots
<span class="pl-c"><span class="pl-c">#</span> Persist the whole population in directory models/CIFAR10 so that optimization can be resumed if aborted:</span>
models <span class="pl-k">=</span> <span class="pl-c1">fit</span>(CIFAR10<span class="pl-k">.</span><span class="pl-c1">traindata</span>(), cb<span class="pl-k">=</span>persist, mdir<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>models/CIFAR10<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> Plot best and average fitness for each generation</span>
plotfitness <span class="pl-k">=</span> <span class="pl-c1">PlotFitness</span>(plot, <span class="pl-s"><span class="pl-pds">"</span>models/CIFAR10<span class="pl-pds">"</span></span>);
<span class="pl-c"><span class="pl-c">#</span> Plot data will be serialized in a subdir of "models/CIFAR10" for later postprocessing and for resuming optimization.</span>
models <span class="pl-k">=</span> <span class="pl-c1">fit</span>(CIFAR10<span class="pl-k">.</span><span class="pl-c1">traindata</span>(), cb<span class="pl-k">=</span>plotfitness, mdir<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>models/CIFAR10<span class="pl-pds">"</span></span>)


<span class="pl-c"><span class="pl-c">#</span> Scatter plots from examples above:</span>
scatterpop <span class="pl-k">=</span> <span class="pl-c1">ScatterPop</span>(scatter, <span class="pl-s"><span class="pl-pds">"</span>models/CIFAR10<span class="pl-pds">"</span></span>);
scatteropt <span class="pl-k">=</span> <span class="pl-c1">ScatterOpt</span>(scatter, <span class="pl-s"><span class="pl-pds">"</span>models/CIFAR10<span class="pl-pds">"</span></span>);

<span class="pl-c"><span class="pl-c">#</span> Combine multiple plots in one figure:</span>
multiplot <span class="pl-k">=</span> <span class="pl-c1">MultiPlot</span>(display <span class="pl-k">∘</span> plot, plotfitness, scatterpop, scatteropt)

<span class="pl-c"><span class="pl-c">#</span> Combine multiple callbacks in one function:</span>
callbacks <span class="pl-k">=</span> <span class="pl-c1">CbAll</span>(persist, multiplot)

models <span class="pl-k">=</span> <span class="pl-c1">fit</span>(CIFAR10<span class="pl-k">.</span><span class="pl-c1">traindata</span>(), cb<span class="pl-k">=</span>callbacks, mdir<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>models/CIFAR10<span class="pl-pds">"</span></span>)</pre></div>
<p>However, most non-toy uses cases will probably require a dedicated application. NaiveGAflux provides the components to make building it easy and fun!</p>
<p>Tired of tuning hyperparameters? Once you've felt the rush from reasoning about hyper-hyperparameters there is no going back!</p>
<p>This package has the following main components:</p>
<ol>
<li><a href="#search-spaces">Search spaces</a></li>
<li><a href="#mutation">Mutation operations</a></li>
<li><a href="#crossover">Crossover operations</a></li>
<li><a href="#fitness-functions">Fitness functions</a></li>
<li><a href="#candidate-utilities">Candidate utilities</a></li>
<li><a href="#evolution-strategies">Evolution strategies</a></li>
<li><a href="#iterators">Iterators</a></li>
</ol>
<p>Each component is described more in detail below.</p>
<p>Here is a very basic example just to get a feeling for the package:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveGAflux, Random
Random.seed!(NaiveGAflux.rng_default, 0)

nlabels = 3
ninputs = 5

# Step 1: Create initial models
# Search space: 2-4 dense layers of width 3-10
layerspace = VertexSpace(DenseSpace(3:10, [identity, relu, elu, selu]))
initial_hidden = RepeatArchSpace(layerspace, 1:3)
# Output layer has fixed size and is shielded from mutation
outlayer = VertexSpace(Shielded(), DenseSpace(nlabels, identity))
initial_searchspace = ArchSpaceChain(initial_hidden, outlayer)

# Sample 5 models from the initial search space and make an initial population
model(invertex) = CompGraph(invertex, initial_searchspace(invertex))
models = [model(inputvertex(&quot;input&quot;, ninputs, FluxDense())) for _ in 1:5]
@test nv.(models) == [4, 3, 4, 5, 3]

population = Population(CandidateModel.(models))
@test generation(population) == 1

# Step 2: Set up fitness function:
# Train model for one epoch using datasettrain, then measure accuracy on datasetvalidate
# Some dummy data just to make stuff run
onehot(y) = Flux.onehotbatch(y, 1:nlabels)
batchsize = 4
datasettrain    = [(randn(ninputs, batchsize), onehot(rand(1:nlabels, batchsize)))]
datasetvalidate = [(randn(ninputs, batchsize), onehot(rand(1:nlabels, batchsize)))]

fitnessfunction = TrainThenFitness(;
    dataiter = datasettrain,
    defaultloss = Flux.logitcrossentropy, # Will be used if not provided by the candidate
    defaultopt = ADAM(), # Same as above. State is wiped after training to prevent memory leaks
    fitstrat = AccuracyFitness(datasetvalidate) # This is what creates our fitness value after training
)

# Step 3: Define how to search for new candidates
# We choose to evolve the existing ones through mutation

# VertexMutation selects valid vertices from the graph to mutate
# MutationProbability applies mutation m with a probability of p
# Lets shorten that a bit:
mp(m, p) = VertexMutation(MutationProbability(m, p))
# Add a layer (40% chance) and/or remove a layer (40% chance)
# You might want to use lower probabilities than this
addlayer = mp(AddVertexMutation(layerspace), 0.4)
remlayer = mp(RemoveVertexMutation(), 0.4)
mutation = MutationChain(remlayer, addlayer)

# Selection:
# The two best models are not changed
elites = EliteSelection(2)
# Three new candidates are produced by selecting three canidates from the whole population and mutating them
mutate = SusSelection(3, EvolveCandidates(evolvemodel(mutation)))
selection = CombinedEvolution(elites, mutate)

# Step 4: Run evolution
newpopulation = evolve(selection, fitnessfunction, population)
@test newpopulation != population
@test generation(newpopulation) == 2
# Repeat step 4 until a model with the desired fitness is found.
newnewpopulation = evolve(selection, fitnessfunction, newpopulation)
@test newnewpopulation != newpopulation
@test generation(newnewpopulation) == 3
# Maybe in a loop :)
"><pre><span class="pl-k">using</span> NaiveGAflux, Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(NaiveGAflux<span class="pl-k">.</span>rng_default, <span class="pl-c1">0</span>)

nlabels <span class="pl-k">=</span> <span class="pl-c1">3</span>
ninputs <span class="pl-k">=</span> <span class="pl-c1">5</span>

<span class="pl-c"><span class="pl-c">#</span> Step 1: Create initial models</span>
<span class="pl-c"><span class="pl-c">#</span> Search space: 2-4 dense layers of width 3-10</span>
layerspace <span class="pl-k">=</span> <span class="pl-c1">VertexSpace</span>(<span class="pl-c1">DenseSpace</span>(<span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">10</span>, [identity, relu, elu, selu]))
initial_hidden <span class="pl-k">=</span> <span class="pl-c1">RepeatArchSpace</span>(layerspace, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>)
<span class="pl-c"><span class="pl-c">#</span> Output layer has fixed size and is shielded from mutation</span>
outlayer <span class="pl-k">=</span> <span class="pl-c1">VertexSpace</span>(<span class="pl-c1">Shielded</span>(), <span class="pl-c1">DenseSpace</span>(nlabels, identity))
initial_searchspace <span class="pl-k">=</span> <span class="pl-c1">ArchSpaceChain</span>(initial_hidden, outlayer)

<span class="pl-c"><span class="pl-c">#</span> Sample 5 models from the initial search space and make an initial population</span>
<span class="pl-en">model</span>(invertex) <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, <span class="pl-c1">initial_searchspace</span>(invertex))
models <span class="pl-k">=</span> [<span class="pl-c1">model</span>(<span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span>, ninputs, <span class="pl-c1">FluxDense</span>())) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5</span>]
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>.(models) <span class="pl-k">==</span> [<span class="pl-c1">4</span>, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>, <span class="pl-c1">5</span>, <span class="pl-c1">3</span>]

population <span class="pl-k">=</span> <span class="pl-c1">Population</span>(<span class="pl-c1">CandidateModel</span>.(models))
<span class="pl-c1">@test</span> <span class="pl-c1">generation</span>(population) <span class="pl-k">==</span> <span class="pl-c1">1</span>

<span class="pl-c"><span class="pl-c">#</span> Step 2: Set up fitness function:</span>
<span class="pl-c"><span class="pl-c">#</span> Train model for one epoch using datasettrain, then measure accuracy on datasetvalidate</span>
<span class="pl-c"><span class="pl-c">#</span> Some dummy data just to make stuff run</span>
<span class="pl-en">onehot</span>(y) <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">onehotbatch</span>(y, <span class="pl-c1">1</span><span class="pl-k">:</span>nlabels)
batchsize <span class="pl-k">=</span> <span class="pl-c1">4</span>
datasettrain    <span class="pl-k">=</span> [(<span class="pl-c1">randn</span>(ninputs, batchsize), <span class="pl-c1">onehot</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>nlabels, batchsize)))]
datasetvalidate <span class="pl-k">=</span> [(<span class="pl-c1">randn</span>(ninputs, batchsize), <span class="pl-c1">onehot</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>nlabels, batchsize)))]

fitnessfunction <span class="pl-k">=</span> <span class="pl-c1">TrainThenFitness</span>(;
    dataiter <span class="pl-k">=</span> datasettrain,
    defaultloss <span class="pl-k">=</span> Flux<span class="pl-k">.</span>logitcrossentropy, <span class="pl-c"><span class="pl-c">#</span> Will be used if not provided by the candidate</span>
    defaultopt <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(), <span class="pl-c"><span class="pl-c">#</span> Same as above. State is wiped after training to prevent memory leaks</span>
    fitstrat <span class="pl-k">=</span> <span class="pl-c1">AccuracyFitness</span>(datasetvalidate) <span class="pl-c"><span class="pl-c">#</span> This is what creates our fitness value after training</span>
)

<span class="pl-c"><span class="pl-c">#</span> Step 3: Define how to search for new candidates</span>
<span class="pl-c"><span class="pl-c">#</span> We choose to evolve the existing ones through mutation</span>

<span class="pl-c"><span class="pl-c">#</span> VertexMutation selects valid vertices from the graph to mutate</span>
<span class="pl-c"><span class="pl-c">#</span> MutationProbability applies mutation m with a probability of p</span>
<span class="pl-c"><span class="pl-c">#</span> Lets shorten that a bit:</span>
<span class="pl-en">mp</span>(m, p) <span class="pl-k">=</span> <span class="pl-c1">VertexMutation</span>(<span class="pl-c1">MutationProbability</span>(m, p))
<span class="pl-c"><span class="pl-c">#</span> Add a layer (40% chance) and/or remove a layer (40% chance)</span>
<span class="pl-c"><span class="pl-c">#</span> You might want to use lower probabilities than this</span>
addlayer <span class="pl-k">=</span> <span class="pl-c1">mp</span>(<span class="pl-c1">AddVertexMutation</span>(layerspace), <span class="pl-c1">0.4</span>)
remlayer <span class="pl-k">=</span> <span class="pl-c1">mp</span>(<span class="pl-c1">RemoveVertexMutation</span>(), <span class="pl-c1">0.4</span>)
mutation <span class="pl-k">=</span> <span class="pl-c1">MutationChain</span>(remlayer, addlayer)

<span class="pl-c"><span class="pl-c">#</span> Selection:</span>
<span class="pl-c"><span class="pl-c">#</span> The two best models are not changed</span>
elites <span class="pl-k">=</span> <span class="pl-c1">EliteSelection</span>(<span class="pl-c1">2</span>)
<span class="pl-c"><span class="pl-c">#</span> Three new candidates are produced by selecting three canidates from the whole population and mutating them</span>
mutate <span class="pl-k">=</span> <span class="pl-c1">SusSelection</span>(<span class="pl-c1">3</span>, <span class="pl-c1">EvolveCandidates</span>(<span class="pl-c1">evolvemodel</span>(mutation)))
selection <span class="pl-k">=</span> <span class="pl-c1">CombinedEvolution</span>(elites, mutate)

<span class="pl-c"><span class="pl-c">#</span> Step 4: Run evolution</span>
newpopulation <span class="pl-k">=</span> <span class="pl-c1">evolve</span>(selection, fitnessfunction, population)
<span class="pl-c1">@test</span> newpopulation <span class="pl-k">!=</span> population
<span class="pl-c1">@test</span> <span class="pl-c1">generation</span>(newpopulation) <span class="pl-k">==</span> <span class="pl-c1">2</span>
<span class="pl-c"><span class="pl-c">#</span> Repeat step 4 until a model with the desired fitness is found.</span>
newnewpopulation <span class="pl-k">=</span> <span class="pl-c1">evolve</span>(selection, fitnessfunction, newpopulation)
<span class="pl-c1">@test</span> newnewpopulation <span class="pl-k">!=</span> newpopulation
<span class="pl-c1">@test</span> <span class="pl-c1">generation</span>(newnewpopulation) <span class="pl-k">==</span> <span class="pl-c1">3</span>
<span class="pl-c"><span class="pl-c">#</span> Maybe in a loop :)</span></pre></div>
<h3><a id="user-content-search-spaces" class="anchor" aria-hidden="true" href="#search-spaces"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Search Spaces</h3>
<p>The search space is a set of possible architectures which the search policy may use to create initial candidates or to extend existing candidates. Search spaces are constructed from simple components which can be combined in multiple ways, giving a lot of flexibility.</p>
<p>Lets start with the most simple search space, a <code>ParSpace</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Set seed of default random number generator for reproducible results
using NaiveGAflux, Random
Random.seed!(NaiveGAflux.rng_default, 1)

ps1d = ParSpace([2,4,6,10])

# Draw from the search space
@test ps1d() == 6
@test ps1d() == 10

# Possible to supply another rng than the default one
@test ps1d(MersenneTwister(0)) == 4

# Can be of any dimension and type
ps2d = ParSpace([&quot;1&quot;,&quot;2&quot;,&quot;3&quot;], [&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;])

@test typeof(ps1d) == ParSpace{1, Int}
@test typeof(ps2d) == ParSpace{2, String}

@test ps2d() == (&quot;1&quot;, &quot;4&quot;)
"><pre><span class="pl-c"><span class="pl-c">#</span> Set seed of default random number generator for reproducible results</span>
<span class="pl-k">using</span> NaiveGAflux, Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(NaiveGAflux<span class="pl-k">.</span>rng_default, <span class="pl-c1">1</span>)

ps1d <span class="pl-k">=</span> <span class="pl-c1">ParSpace</span>([<span class="pl-c1">2</span>,<span class="pl-c1">4</span>,<span class="pl-c1">6</span>,<span class="pl-c1">10</span>])

<span class="pl-c"><span class="pl-c">#</span> Draw from the search space</span>
<span class="pl-c1">@test</span> <span class="pl-c1">ps1d</span>() <span class="pl-k">==</span> <span class="pl-c1">6</span>
<span class="pl-c1">@test</span> <span class="pl-c1">ps1d</span>() <span class="pl-k">==</span> <span class="pl-c1">10</span>

<span class="pl-c"><span class="pl-c">#</span> Possible to supply another rng than the default one</span>
<span class="pl-c1">@test</span> <span class="pl-c1">ps1d</span>(<span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">0</span>)) <span class="pl-k">==</span> <span class="pl-c1">4</span>

<span class="pl-c"><span class="pl-c">#</span> Can be of any dimension and type</span>
ps2d <span class="pl-k">=</span> <span class="pl-c1">ParSpace</span>([<span class="pl-s"><span class="pl-pds">"</span>1<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>2<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>3<span class="pl-pds">"</span></span>], [<span class="pl-s"><span class="pl-pds">"</span>4<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>5<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>6<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>7<span class="pl-pds">"</span></span>])

<span class="pl-c1">@test</span> <span class="pl-c1">typeof</span>(ps1d) <span class="pl-k">==</span> ParSpace{<span class="pl-c1">1</span>, Int}
<span class="pl-c1">@test</span> <span class="pl-c1">typeof</span>(ps2d) <span class="pl-k">==</span> ParSpace{<span class="pl-c1">2</span>, String}

<span class="pl-c1">@test</span> <span class="pl-c1">ps2d</span>() <span class="pl-k">==</span> (<span class="pl-s"><span class="pl-pds">"</span>1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>4<span class="pl-pds">"</span></span>)</pre></div>
<p>Lets have a look at an example of a search space for convolutional layers:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Random.seed!(NaiveGAflux.rng_default, 1)

cs = ConvSpace{2}(outsizes=4:32, activations=[relu, elu, selu], kernelsizes=3:9)

inputsize = 16
convlayer = cs(inputsize)

@test string(convlayer) == &quot;Conv((8, 3), 16=&gt;22, relu)&quot;
"><pre>Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(NaiveGAflux<span class="pl-k">.</span>rng_default, <span class="pl-c1">1</span>)

cs <span class="pl-k">=</span> <span class="pl-c1">ConvSpace</span><span class="pl-c1">{2}</span>(outsizes<span class="pl-k">=</span><span class="pl-c1">4</span><span class="pl-k">:</span><span class="pl-c1">32</span>, activations<span class="pl-k">=</span>[relu, elu, selu], kernelsizes<span class="pl-k">=</span><span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">9</span>)

inputsize <span class="pl-k">=</span> <span class="pl-c1">16</span>
convlayer <span class="pl-k">=</span> <span class="pl-c1">cs</span>(inputsize)

<span class="pl-c1">@test</span> <span class="pl-c1">string</span>(convlayer) <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>Conv((8, 3), 16=&gt;22, relu)<span class="pl-pds">"</span></span></pre></div>
<p>Lastly, lets look at how to construct a complex search space:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Random.seed!(NaiveGAflux.rng_default, 0)

# VertexSpace creates a MutableVertex of layers generated by the wrapped search space
cs = VertexSpace(ConvSpace{2}(outsizes=8:256, activations=[identity, relu, elu], kernelsizes=3:5))
bs = VertexSpace(BatchNormSpace([identity, relu]))

# Block of conv-&gt;bn and bn-&gt;conv respectively.
# Need to make sure there is always at least one SizeAbsorb layer to make fork and res below play nice
csbs = ArchSpaceChain(cs ,bs)
bscs = ArchSpaceChain(bs, cs)

# Randomly generates either conv or conv-&gt;bn or bn-&gt;conv:
cblock = ArchSpace(ParSpace1D(cs, csbs, bscs))

# Generates between 1 and 5 layers from csbs
rep = RepeatArchSpace(cblock, 1:5)

# Generates between 2 and 4 parallel paths joined by concatenation (inception like-blocks) from rep
fork = ForkArchSpace(rep, 2:4)

# Generates a residual connection around what is generated by rep
res = ResidualArchSpace(rep)

# ... and a residual fork
resfork = ResidualArchSpace(fork)

# Pick one of the above randomly...
repforkres = ArchSpace(ParSpace1D(rep, fork, res, resfork))

# ...1 to 3 times
blocks = RepeatArchSpace(repforkres, 1:3)

# End each block with subsamping through maxpooling
ms = VertexSpace(PoolSpace{2}(windowsizes=2, strides=2, poolfuns=MaxPool))
reduction = ArchSpaceChain(blocks, ms)

# And lets do 2 to 4 reductions
featureextract = RepeatArchSpace(reduction, 2:4)

# Adds 1 to 3 dense layers as outputs
dense = VertexSpace(DenseSpace(16:512, [relu, selu]))
drep = RepeatArchSpace(dense, 0:2)
# Last layer has fixed output size (number of labels)
dout=VertexSpace(Shielded(), DenseSpace(10, identity))
output = ArchSpaceChain(drep, dout)

# Aaaand lets glue it together: Feature extracting conv+bn layers -&gt; global pooling -&gt; dense layers
archspace = ArchSpaceChain(featureextract, GlobalPoolSpace(), output)

# Input is 3 channel image
inputshape = inputvertex(&quot;input&quot;, 3, FluxConv{2}())

# Sample one architecture from the search space
graph1 = CompGraph(inputshape, archspace(inputshape))
@test nv(graph1) == 79

# And one more...
graph2 = CompGraph(inputshape, archspace(inputshape))
@test nv(graph2) == 128
"><pre>Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(NaiveGAflux<span class="pl-k">.</span>rng_default, <span class="pl-c1">0</span>)

<span class="pl-c"><span class="pl-c">#</span> VertexSpace creates a MutableVertex of layers generated by the wrapped search space</span>
cs <span class="pl-k">=</span> <span class="pl-c1">VertexSpace</span>(<span class="pl-c1">ConvSpace</span><span class="pl-c1">{2}</span>(outsizes<span class="pl-k">=</span><span class="pl-c1">8</span><span class="pl-k">:</span><span class="pl-c1">256</span>, activations<span class="pl-k">=</span>[identity, relu, elu], kernelsizes<span class="pl-k">=</span><span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">5</span>))
bs <span class="pl-k">=</span> <span class="pl-c1">VertexSpace</span>(<span class="pl-c1">BatchNormSpace</span>([identity, relu]))

<span class="pl-c"><span class="pl-c">#</span> Block of conv-&gt;bn and bn-&gt;conv respectively.</span>
<span class="pl-c"><span class="pl-c">#</span> Need to make sure there is always at least one SizeAbsorb layer to make fork and res below play nice</span>
csbs <span class="pl-k">=</span> <span class="pl-c1">ArchSpaceChain</span>(cs ,bs)
bscs <span class="pl-k">=</span> <span class="pl-c1">ArchSpaceChain</span>(bs, cs)

<span class="pl-c"><span class="pl-c">#</span> Randomly generates either conv or conv-&gt;bn or bn-&gt;conv:</span>
cblock <span class="pl-k">=</span> <span class="pl-c1">ArchSpace</span>(<span class="pl-c1">ParSpace1D</span>(cs, csbs, bscs))

<span class="pl-c"><span class="pl-c">#</span> Generates between 1 and 5 layers from csbs</span>
rep <span class="pl-k">=</span> <span class="pl-c1">RepeatArchSpace</span>(cblock, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5</span>)

<span class="pl-c"><span class="pl-c">#</span> Generates between 2 and 4 parallel paths joined by concatenation (inception like-blocks) from rep</span>
fork <span class="pl-k">=</span> <span class="pl-c1">ForkArchSpace</span>(rep, <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">4</span>)

<span class="pl-c"><span class="pl-c">#</span> Generates a residual connection around what is generated by rep</span>
res <span class="pl-k">=</span> <span class="pl-c1">ResidualArchSpace</span>(rep)

<span class="pl-c"><span class="pl-c">#</span> ... and a residual fork</span>
resfork <span class="pl-k">=</span> <span class="pl-c1">ResidualArchSpace</span>(fork)

<span class="pl-c"><span class="pl-c">#</span> Pick one of the above randomly...</span>
repforkres <span class="pl-k">=</span> <span class="pl-c1">ArchSpace</span>(<span class="pl-c1">ParSpace1D</span>(rep, fork, res, resfork))

<span class="pl-c"><span class="pl-c">#</span> ...1 to 3 times</span>
blocks <span class="pl-k">=</span> <span class="pl-c1">RepeatArchSpace</span>(repforkres, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>)

<span class="pl-c"><span class="pl-c">#</span> End each block with subsamping through maxpooling</span>
ms <span class="pl-k">=</span> <span class="pl-c1">VertexSpace</span>(<span class="pl-c1">PoolSpace</span><span class="pl-c1">{2}</span>(windowsizes<span class="pl-k">=</span><span class="pl-c1">2</span>, strides<span class="pl-k">=</span><span class="pl-c1">2</span>, poolfuns<span class="pl-k">=</span>MaxPool))
reduction <span class="pl-k">=</span> <span class="pl-c1">ArchSpaceChain</span>(blocks, ms)

<span class="pl-c"><span class="pl-c">#</span> And lets do 2 to 4 reductions</span>
featureextract <span class="pl-k">=</span> <span class="pl-c1">RepeatArchSpace</span>(reduction, <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">4</span>)

<span class="pl-c"><span class="pl-c">#</span> Adds 1 to 3 dense layers as outputs</span>
dense <span class="pl-k">=</span> <span class="pl-c1">VertexSpace</span>(<span class="pl-c1">DenseSpace</span>(<span class="pl-c1">16</span><span class="pl-k">:</span><span class="pl-c1">512</span>, [relu, selu]))
drep <span class="pl-k">=</span> <span class="pl-c1">RepeatArchSpace</span>(dense, <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">2</span>)
<span class="pl-c"><span class="pl-c">#</span> Last layer has fixed output size (number of labels)</span>
dout<span class="pl-k">=</span><span class="pl-c1">VertexSpace</span>(<span class="pl-c1">Shielded</span>(), <span class="pl-c1">DenseSpace</span>(<span class="pl-c1">10</span>, identity))
output <span class="pl-k">=</span> <span class="pl-c1">ArchSpaceChain</span>(drep, dout)

<span class="pl-c"><span class="pl-c">#</span> Aaaand lets glue it together: Feature extracting conv+bn layers -&gt; global pooling -&gt; dense layers</span>
archspace <span class="pl-k">=</span> <span class="pl-c1">ArchSpaceChain</span>(featureextract, <span class="pl-c1">GlobalPoolSpace</span>(), output)

<span class="pl-c"><span class="pl-c">#</span> Input is 3 channel image</span>
inputshape <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span>, <span class="pl-c1">3</span>, <span class="pl-c1">FluxConv</span><span class="pl-c1">{2}</span>())

<span class="pl-c"><span class="pl-c">#</span> Sample one architecture from the search space</span>
graph1 <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(inputshape, <span class="pl-c1">archspace</span>(inputshape))
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph1) <span class="pl-k">==</span> <span class="pl-c1">79</span>

<span class="pl-c"><span class="pl-c">#</span> And one more...</span>
graph2 <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(inputshape, <span class="pl-c1">archspace</span>(inputshape))
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph2) <span class="pl-k">==</span> <span class="pl-c1">128</span></pre></div>
<h3><a id="user-content-mutation" class="anchor" aria-hidden="true" href="#mutation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Mutation</h3>
<p>Mutation is the way one candidate is transformed to a slightly different candidate. NaiveGAflux supports doing this while preserving parameters and alignment between layers, thus reducing the impact of mutating an already trained candidate.</p>
<p>The following basic mutation operations are currently supported:</p>
<ol>
<li>Change the output size of vertices using <code>NoutMutation</code>.</li>
<li>Remove vertices using <code>RemoveVertexMutation</code>.</li>
<li>Add vertices using <code>AddVertexMutation</code>.</li>
<li>Remove edges between vertices using <code>RemoveEdgeMutation</code>.</li>
<li>Add edges between vertices using <code>AddEdgeMutation</code>.</li>
<li>Mutation of kernel size for conv layers using <code>KernelSizeMutation</code>.</li>
<li>Change of activation function using <code>ActivationFunctionMutation</code>.</li>
<li>Change the type of optimizer using <code>OptimizerMutation</code>.</li>
<li>Add an optimizer using <code>AddOptimizerMutation</code>.</li>
</ol>
<p>In addition to the basic mutation operations, there are numerous utilities for adding behaviour and convenience. Here are a few examples:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveGAflux, Random
Random.seed!(NaiveGAflux.rng_default, 0)

invertex = inputvertex(&quot;in&quot;, 3, FluxDense())
layer1 = mutable(Dense(nout(invertex), 4), invertex)
layer2 = mutable(Dense(nout(layer1), 5), layer1)
graph = CompGraph(invertex, layer2)

mutation = NoutMutation(-0.5, 0.5)

@test nout(layer2) == 5

mutation(layer2)

@test nout(layer2) == 6

# VertexMutation applies the wrapped mutation to all vertices in a CompGraph
mutation = VertexMutation(mutation)

@test nout.(vertices(graph)) == [3,4,6]

mutation(graph)

# Input vertex is never mutated
@test nout.(vertices(graph)) == [3,5,8]

# Use the MutationShield trait to protect vertices from mutation
outlayer = mutable(Dense(nout(layer2), 10), layer2, traitfun = MutationShield)
graph = CompGraph(invertex, outlayer)

mutation(graph)

@test nout.(vertices(graph)) == [3,6,5,10]

# In most cases it makes sense to mutate with a certain probability
mutation = VertexMutation(MutationProbability(NoutMutation(-0.5, 0.5), 0.5))

mutation(graph)

@test nout.(vertices(graph)) == [3,7,5,10]

# Or just chose to either mutate the whole graph or don't do anything
mutation = MutationProbability(VertexMutation(NoutMutation(-0.5, 0.5)), 0.5)

mutation(graph)

@test nout.(vertices(graph)) == [3,10,6,10]

# Up until now, size changes have only been kept track of, but not actually applied
@test nout_org.(vertices(graph)) == [3,4,5,10]

Δoutputs(graph, v -&gt; ones(nout_org(v)))
apply_mutation(graph)

@test nout.(vertices(graph)) == nout_org.(vertices(graph)) == [3,10,6,10]
@test size(graph(ones(3,1))) == (10, 1)

# NeuronSelectMutation keeps track of changed vertices and performs the above steps when invoked
mutation = VertexMutation(NeuronSelectMutation(NoutMutation(-0.5,0.5)))

mutation(graph)

@test nout.(vertices(graph)) == [3,11,7,10]
@test nout_org.(vertices(graph)) == [3,10,6,10]

select(mutation.m)

@test nout_org.(vertices(graph)) == [3,11,7,10]
@test size(graph(ones(3,1))) == (10, 1)

# Mutation can also be conditioned:
mutation = VertexMutation(MutationFilter(v -&gt; nout(v) &lt; 8, RemoveVertexMutation()))

mutation(graph)

@test nout.(vertices(graph)) == [3,11,10]

# When adding vertices it is probably a good idea to try to initialize them as identity mappings
addmut = AddVertexMutation(VertexSpace(DenseSpace(5, identity)), IdentityWeightInit())

# Chaining mutations is also useful:
noutmut = NeuronSelectMutation(NoutMutation(-0.8, 0.8))
mutation = VertexMutation(MutationChain(addmut, noutmut))
# For deeply composed blobs like this, it can be cumbersome to &quot;dig up&quot; the NeuronSelectMutation.
# neuronselect helps finding NeuronSelectMutations in the compositional hierarchy

# PostMutation lets us add actions to perform after a mutation is done (such as neuronselect)
logselect(m, g) = @info &quot;Selecting parameters...&quot;
mutation = PostMutation(mutation, logselect, neuronselect)

@test_logs (:info, &quot;Selecting parameters...&quot;) mutation(graph)

@test nout.(vertices(graph)) == nout_org.(vertices(graph)) == [3,8,11,10]
"><pre><span class="pl-k">using</span> NaiveGAflux, Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(NaiveGAflux<span class="pl-k">.</span>rng_default, <span class="pl-c1">0</span>)

invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>in<span class="pl-pds">"</span></span>, <span class="pl-c1">3</span>, <span class="pl-c1">FluxDense</span>())
layer1 <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(invertex), <span class="pl-c1">4</span>), invertex)
layer2 <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(layer1), <span class="pl-c1">5</span>), layer1)
graph <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, layer2)

mutation <span class="pl-k">=</span> <span class="pl-c1">NoutMutation</span>(<span class="pl-k">-</span><span class="pl-c1">0.5</span>, <span class="pl-c1">0.5</span>)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(layer2) <span class="pl-k">==</span> <span class="pl-c1">5</span>

<span class="pl-c1">mutation</span>(layer2)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(layer2) <span class="pl-k">==</span> <span class="pl-c1">6</span>

<span class="pl-c"><span class="pl-c">#</span> VertexMutation applies the wrapped mutation to all vertices in a CompGraph</span>
mutation <span class="pl-k">=</span> <span class="pl-c1">VertexMutation</span>(mutation)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">4</span>,<span class="pl-c1">6</span>]

<span class="pl-c1">mutation</span>(graph)

<span class="pl-c"><span class="pl-c">#</span> Input vertex is never mutated</span>
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">5</span>,<span class="pl-c1">8</span>]

<span class="pl-c"><span class="pl-c">#</span> Use the MutationShield trait to protect vertices from mutation</span>
outlayer <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(layer2), <span class="pl-c1">10</span>), layer2, traitfun <span class="pl-k">=</span> MutationShield)
graph <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, outlayer)

<span class="pl-c1">mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">6</span>,<span class="pl-c1">5</span>,<span class="pl-c1">10</span>]

<span class="pl-c"><span class="pl-c">#</span> In most cases it makes sense to mutate with a certain probability</span>
mutation <span class="pl-k">=</span> <span class="pl-c1">VertexMutation</span>(<span class="pl-c1">MutationProbability</span>(<span class="pl-c1">NoutMutation</span>(<span class="pl-k">-</span><span class="pl-c1">0.5</span>, <span class="pl-c1">0.5</span>), <span class="pl-c1">0.5</span>))

<span class="pl-c1">mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">7</span>,<span class="pl-c1">5</span>,<span class="pl-c1">10</span>]

<span class="pl-c"><span class="pl-c">#</span> Or just chose to either mutate the whole graph or don't do anything</span>
mutation <span class="pl-k">=</span> <span class="pl-c1">MutationProbability</span>(<span class="pl-c1">VertexMutation</span>(<span class="pl-c1">NoutMutation</span>(<span class="pl-k">-</span><span class="pl-c1">0.5</span>, <span class="pl-c1">0.5</span>)), <span class="pl-c1">0.5</span>)

<span class="pl-c1">mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">10</span>,<span class="pl-c1">6</span>,<span class="pl-c1">10</span>]

<span class="pl-c"><span class="pl-c">#</span> Up until now, size changes have only been kept track of, but not actually applied</span>
<span class="pl-c1">@test</span> <span class="pl-c1">nout_org</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">4</span>,<span class="pl-c1">5</span>,<span class="pl-c1">10</span>]

<span class="pl-c1">Δoutputs</span>(graph, v <span class="pl-k">-&gt;</span> <span class="pl-c1">ones</span>(<span class="pl-c1">nout_org</span>(v)))
<span class="pl-c1">apply_mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> <span class="pl-c1">nout_org</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">10</span>,<span class="pl-c1">6</span>,<span class="pl-c1">10</span>]
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graph</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">3</span>,<span class="pl-c1">1</span>))) <span class="pl-k">==</span> (<span class="pl-c1">10</span>, <span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> NeuronSelectMutation keeps track of changed vertices and performs the above steps when invoked</span>
mutation <span class="pl-k">=</span> <span class="pl-c1">VertexMutation</span>(<span class="pl-c1">NeuronSelectMutation</span>(<span class="pl-c1">NoutMutation</span>(<span class="pl-k">-</span><span class="pl-c1">0.5</span>,<span class="pl-c1">0.5</span>)))

<span class="pl-c1">mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">11</span>,<span class="pl-c1">7</span>,<span class="pl-c1">10</span>]
<span class="pl-c1">@test</span> <span class="pl-c1">nout_org</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">10</span>,<span class="pl-c1">6</span>,<span class="pl-c1">10</span>]

<span class="pl-c1">select</span>(mutation<span class="pl-k">.</span>m)

<span class="pl-c1">@test</span> <span class="pl-c1">nout_org</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">11</span>,<span class="pl-c1">7</span>,<span class="pl-c1">10</span>]
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graph</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">3</span>,<span class="pl-c1">1</span>))) <span class="pl-k">==</span> (<span class="pl-c1">10</span>, <span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> Mutation can also be conditioned:</span>
mutation <span class="pl-k">=</span> <span class="pl-c1">VertexMutation</span>(<span class="pl-c1">MutationFilter</span>(v <span class="pl-k">-&gt;</span> <span class="pl-c1">nout</span>(v) <span class="pl-k">&lt;</span> <span class="pl-c1">8</span>, <span class="pl-c1">RemoveVertexMutation</span>()))

<span class="pl-c1">mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">11</span>,<span class="pl-c1">10</span>]

<span class="pl-c"><span class="pl-c">#</span> When adding vertices it is probably a good idea to try to initialize them as identity mappings</span>
addmut <span class="pl-k">=</span> <span class="pl-c1">AddVertexMutation</span>(<span class="pl-c1">VertexSpace</span>(<span class="pl-c1">DenseSpace</span>(<span class="pl-c1">5</span>, identity)), <span class="pl-c1">IdentityWeightInit</span>())

<span class="pl-c"><span class="pl-c">#</span> Chaining mutations is also useful:</span>
noutmut <span class="pl-k">=</span> <span class="pl-c1">NeuronSelectMutation</span>(<span class="pl-c1">NoutMutation</span>(<span class="pl-k">-</span><span class="pl-c1">0.8</span>, <span class="pl-c1">0.8</span>))
mutation <span class="pl-k">=</span> <span class="pl-c1">VertexMutation</span>(<span class="pl-c1">MutationChain</span>(addmut, noutmut))
<span class="pl-c"><span class="pl-c">#</span> For deeply composed blobs like this, it can be cumbersome to "dig up" the NeuronSelectMutation.</span>
<span class="pl-c"><span class="pl-c">#</span> neuronselect helps finding NeuronSelectMutations in the compositional hierarchy</span>

<span class="pl-c"><span class="pl-c">#</span> PostMutation lets us add actions to perform after a mutation is done (such as neuronselect)</span>
<span class="pl-en">logselect</span>(m, g) <span class="pl-k">=</span> <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Selecting parameters...<span class="pl-pds">"</span></span>
mutation <span class="pl-k">=</span> <span class="pl-c1">PostMutation</span>(mutation, logselect, neuronselect)

<span class="pl-c1">@test_logs</span> (<span class="pl-c1">:info</span>, <span class="pl-s"><span class="pl-pds">"</span>Selecting parameters...<span class="pl-pds">"</span></span>) <span class="pl-c1">mutation</span>(graph)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> <span class="pl-c1">nout_org</span>.(<span class="pl-c1">vertices</span>(graph)) <span class="pl-k">==</span> [<span class="pl-c1">3</span>,<span class="pl-c1">8</span>,<span class="pl-c1">11</span>,<span class="pl-c1">10</span>]</pre></div>
<h3><a id="user-content-crossover" class="anchor" aria-hidden="true" href="#crossover"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Crossover</h3>
<p>Crossover is the way two candidates are combined to create new candidates. In NaiveGAflux crossover always maps two candidates into two new candidates. Just as for mutation, NaiveGAflux does this while preserving (to whatever extent possible) the parameters and alignment between layers of the combined models.</p>
<p>Crossover operations might not seem to make much sense when using parameter inheritance (i.e the concept that children retain the parameters of their parents). Randomly combining layers from two very different models will most likely not result in a well performing model. There are however a few potentially redeeming effects:</p>
<ul>
<li>Early in the evolution process parameters are not yet well fitted and inheriting parameters is not worse than random initialization</li>
<li>A mature population on the other hand will consist mostly of models which are close relatives and therefore have somewhat similar weights.</li>
</ul>
<p>Whether these effects actually make crossover a genuinely useful operation when evolving neural networks is not yet proven though. For now it is perhaps best to view the crossover operations as being provided mostly for the sake of completeness.</p>
<p>The following basic crossover operations are currently supported:</p>
<ol>
<li>Swap segments between two models using <code>CrossoverSwap</code>.</li>
<li>Swap optimizers between two candidates using <code>OptimizerCrossover</code>.</li>
<li>Swap learning rate between two candidates using <code>LearningRateCrossover</code>.</li>
</ol>
<p>Most of the mutation utilities also work with crossover operations. Here are a few examples:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveGAflux, Random
import NaiveGAflux: regraph
Random.seed!(NaiveGAflux.rng_default, 0)

invertex = inputvertex(&quot;A.in&quot;, 3, FluxDense())
layer1 = mutable(&quot;A.layer1&quot;, Dense(nout(invertex), 4), invertex; layerfun=ActivationContribution)
layer2 = mutable(&quot;A.layer2&quot;, Dense(nout(layer1), 5), layer1; layerfun=ActivationContribution)
layer3 = mutable(&quot;A.layer3&quot;, Dense(nout(layer2), 3), layer2; layerfun=ActivationContribution)
layer4 = mutable(&quot;A.layer4&quot;, Dense(nout(layer3), 2), layer3; layerfun=ActivationContribution)
modelA = CompGraph(invertex, layer4)

# Create an exact copy to show how parameter alignment is preserved
# Prefix names with B so we can show that something actually happened
changeprefix(str::String; cf) = replace(str, r&quot;^A.\.*&quot; =&gt; &quot;B.&quot;)
changeprefix(x...;cf=clone) = clone(x...; cf=cf)
modelB = copy(modelA, changeprefix)

indata = reshape(collect(Float32, 1:3*2), 3,2)
@test modelA(indata) == modelB(indata)

@test name.(vertices(modelA)) == [&quot;A.in&quot;, &quot;A.layer1&quot;, &quot;A.layer2&quot;, &quot;A.layer3&quot;, &quot;A.layer4&quot;]
@test name.(vertices(modelB)) == [&quot;B.in&quot;, &quot;B.layer1&quot;, &quot;B.layer2&quot;, &quot;B.layer3&quot;, &quot;B.layer4&quot;]

# CrossoverSwap takes ones vertex from each graph as input and swaps a random segment from each graph
# By default it tries to make segments as similar as possible
swapsame = CrossoverSwap()

swapA = vertices(modelA)[4]
swapB = vertices(modelB)[4]
newA, newB = swapsame((swapA, swapB))

# It returns vertices of a new graph to be compatible with mutation utilities
# Parent models are not modified
@test newA ∉ vertices(modelA)
@test newB ∉ vertices(modelB)

# This is an internal utility which should not be needed in normal use cases.
modelAnew = regraph(newA)
modelBnew = regraph(newB)

@test name.(vertices(modelAnew)) == [&quot;A.in&quot;, &quot;A.layer1&quot;, &quot;B.layer2&quot;, &quot;B.layer3&quot;, &quot;A.layer4&quot;] 
@test name.(vertices(modelBnew)) == [&quot;B.in&quot;, &quot;B.layer1&quot;, &quot;A.layer2&quot;, &quot;A.layer3&quot;, &quot;B.layer4&quot;]

@test modelA(indata) == modelB(indata) == modelAnew(indata) == modelBnew(indata)

# Deviation parameter will randomly make segments unequal
swapdeviation = CrossoverSwap(0.5)
modelAnew2, modelBnew2 = regraph.(swapdeviation((swapA, swapB)))

@test name.(vertices(modelAnew2)) == [&quot;A.in&quot;, &quot;A.layer1&quot;, &quot;A.layer2&quot;, &quot;B.layer1&quot;, &quot;B.layer2&quot;, &quot;B.layer3&quot;, &quot;A.layer4&quot;] 
@test name.(vertices(modelBnew2)) == [&quot;B.in&quot;, &quot;A.layer3&quot;, &quot;B.layer4&quot;]

# VertexCrossover applies the wrapped crossover operation to all vertices in a CompGraph
# It in addtion, it selects compatible pairs for us (i.e swapA and swapB).
# It also takes an optional deviation parameter which is used when pairing
crossoverall = VertexCrossover(swapdeviation, 0.5)

modelAnew3, modelBnew3 = crossoverall((modelA, modelB))

# I guess things got swapped back and forth so many times not much changed in the end
@test name.(vertices(modelAnew3)) == [&quot;A.in&quot;, &quot;A.layer2&quot;, &quot;A.layer4&quot;]
@test name.(vertices(modelBnew3)) ==  [&quot;B.in&quot;, &quot;B.layer3&quot;, &quot;B.layer1&quot;, &quot;B.layer2&quot;, &quot;A.layer1&quot;, &quot;A.layer3&quot;, &quot;B.layer4&quot;] 

# As advertised above, crossovers interop with most mutation utilities, just remember that input is a tuple
# Perform the swapping operation with a 30% probability for each valid vertex pair.
crossoversome = VertexCrossover(MutationProbability(LogMutation(((v1,v2)::Tuple) -&gt; &quot;Swap $(name(v1)) and $(name(v2))&quot;, swapdeviation), 0.3))

@test_logs (:info, &quot;Swap A.layer1 and B.layer1&quot;) (:info, &quot;Swap A.layer2 and B.layer2&quot;) crossoversome((modelA, modelB))
"><pre><span class="pl-k">using</span> NaiveGAflux, Random
<span class="pl-k">import</span> NaiveGAflux<span class="pl-k">:</span> regraph
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(NaiveGAflux<span class="pl-k">.</span>rng_default, <span class="pl-c1">0</span>)

invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>A.in<span class="pl-pds">"</span></span>, <span class="pl-c1">3</span>, <span class="pl-c1">FluxDense</span>())
layer1 <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>A.layer1<span class="pl-pds">"</span></span>, <span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(invertex), <span class="pl-c1">4</span>), invertex; layerfun<span class="pl-k">=</span>ActivationContribution)
layer2 <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>A.layer2<span class="pl-pds">"</span></span>, <span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(layer1), <span class="pl-c1">5</span>), layer1; layerfun<span class="pl-k">=</span>ActivationContribution)
layer3 <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>A.layer3<span class="pl-pds">"</span></span>, <span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(layer2), <span class="pl-c1">3</span>), layer2; layerfun<span class="pl-k">=</span>ActivationContribution)
layer4 <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>A.layer4<span class="pl-pds">"</span></span>, <span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(layer3), <span class="pl-c1">2</span>), layer3; layerfun<span class="pl-k">=</span>ActivationContribution)
modelA <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, layer4)

<span class="pl-c"><span class="pl-c">#</span> Create an exact copy to show how parameter alignment is preserved</span>
<span class="pl-c"><span class="pl-c">#</span> Prefix names with B so we can show that something actually happened</span>
<span class="pl-en">changeprefix</span>(str<span class="pl-k">::</span><span class="pl-c1">String</span>; cf) <span class="pl-k">=</span> <span class="pl-c1">replace</span>(str, <span class="pl-sr"><span class="pl-pds">r"</span>^A.<span class="pl-cce">\.</span>*<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>B.<span class="pl-pds">"</span></span>)
<span class="pl-en">changeprefix</span>(x<span class="pl-k">...</span>;cf<span class="pl-k">=</span>clone) <span class="pl-k">=</span> <span class="pl-c1">clone</span>(x<span class="pl-k">...</span>; cf<span class="pl-k">=</span>cf)
modelB <span class="pl-k">=</span> <span class="pl-c1">copy</span>(modelA, changeprefix)

indata <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">collect</span>(Float32, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span><span class="pl-k">*</span><span class="pl-c1">2</span>), <span class="pl-c1">3</span>,<span class="pl-c1">2</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">modelA</span>(indata) <span class="pl-k">==</span> <span class="pl-c1">modelB</span>(indata)

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelA)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>A.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer4<span class="pl-pds">"</span></span>]
<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelB)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>B.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer4<span class="pl-pds">"</span></span>]

<span class="pl-c"><span class="pl-c">#</span> CrossoverSwap takes ones vertex from each graph as input and swaps a random segment from each graph</span>
<span class="pl-c"><span class="pl-c">#</span> By default it tries to make segments as similar as possible</span>
swapsame <span class="pl-k">=</span> <span class="pl-c1">CrossoverSwap</span>()

swapA <span class="pl-k">=</span> <span class="pl-c1">vertices</span>(modelA)[<span class="pl-c1">4</span>]
swapB <span class="pl-k">=</span> <span class="pl-c1">vertices</span>(modelB)[<span class="pl-c1">4</span>]
newA, newB <span class="pl-k">=</span> <span class="pl-c1">swapsame</span>((swapA, swapB))

<span class="pl-c"><span class="pl-c">#</span> It returns vertices of a new graph to be compatible with mutation utilities</span>
<span class="pl-c"><span class="pl-c">#</span> Parent models are not modified</span>
<span class="pl-c1">@test</span> newA <span class="pl-k">∉</span> <span class="pl-c1">vertices</span>(modelA)
<span class="pl-c1">@test</span> newB <span class="pl-k">∉</span> <span class="pl-c1">vertices</span>(modelB)

<span class="pl-c"><span class="pl-c">#</span> This is an internal utility which should not be needed in normal use cases.</span>
modelAnew <span class="pl-k">=</span> <span class="pl-c1">regraph</span>(newA)
modelBnew <span class="pl-k">=</span> <span class="pl-c1">regraph</span>(newB)

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelAnew)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>A.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer4<span class="pl-pds">"</span></span>] 
<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelBnew)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>B.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer4<span class="pl-pds">"</span></span>]

<span class="pl-c1">@test</span> <span class="pl-c1">modelA</span>(indata) <span class="pl-k">==</span> <span class="pl-c1">modelB</span>(indata) <span class="pl-k">==</span> <span class="pl-c1">modelAnew</span>(indata) <span class="pl-k">==</span> <span class="pl-c1">modelBnew</span>(indata)

<span class="pl-c"><span class="pl-c">#</span> Deviation parameter will randomly make segments unequal</span>
swapdeviation <span class="pl-k">=</span> <span class="pl-c1">CrossoverSwap</span>(<span class="pl-c1">0.5</span>)
modelAnew2, modelBnew2 <span class="pl-k">=</span> <span class="pl-c1">regraph</span>.(<span class="pl-c1">swapdeviation</span>((swapA, swapB)))

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelAnew2)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>A.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer4<span class="pl-pds">"</span></span>] 
<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelBnew2)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>B.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer4<span class="pl-pds">"</span></span>]

<span class="pl-c"><span class="pl-c">#</span> VertexCrossover applies the wrapped crossover operation to all vertices in a CompGraph</span>
<span class="pl-c"><span class="pl-c">#</span> It in addtion, it selects compatible pairs for us (i.e swapA and swapB).</span>
<span class="pl-c"><span class="pl-c">#</span> It also takes an optional deviation parameter which is used when pairing</span>
crossoverall <span class="pl-k">=</span> <span class="pl-c1">VertexCrossover</span>(swapdeviation, <span class="pl-c1">0.5</span>)

modelAnew3, modelBnew3 <span class="pl-k">=</span> <span class="pl-c1">crossoverall</span>((modelA, modelB))

<span class="pl-c"><span class="pl-c">#</span> I guess things got swapped back and forth so many times not much changed in the end</span>
<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelAnew3)) <span class="pl-k">==</span> [<span class="pl-s"><span class="pl-pds">"</span>A.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer4<span class="pl-pds">"</span></span>]
<span class="pl-c1">@test</span> <span class="pl-c1">name</span>.(<span class="pl-c1">vertices</span>(modelBnew3)) <span class="pl-k">==</span>  [<span class="pl-s"><span class="pl-pds">"</span>B.in<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer1<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>A.layer3<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>B.layer4<span class="pl-pds">"</span></span>] 

<span class="pl-c"><span class="pl-c">#</span> As advertised above, crossovers interop with most mutation utilities, just remember that input is a tuple</span>
<span class="pl-c"><span class="pl-c">#</span> Perform the swapping operation with a 30% probability for each valid vertex pair.</span>
crossoversome <span class="pl-k">=</span> <span class="pl-c1">VertexCrossover</span>(<span class="pl-c1">MutationProbability</span>(<span class="pl-c1">LogMutation</span>(((v1,v2)<span class="pl-k">::</span><span class="pl-c1">Tuple</span>) <span class="pl-k">-&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>Swap <span class="pl-v">$(<span class="pl-c1">name</span>(v1))</span> and <span class="pl-v">$(<span class="pl-c1">name</span>(v2))</span><span class="pl-pds">"</span></span>, swapdeviation), <span class="pl-c1">0.3</span>))

<span class="pl-c1">@test_logs</span> (<span class="pl-c1">:info</span>, <span class="pl-s"><span class="pl-pds">"</span>Swap A.layer1 and B.layer1<span class="pl-pds">"</span></span>) (<span class="pl-c1">:info</span>, <span class="pl-s"><span class="pl-pds">"</span>Swap A.layer2 and B.layer2<span class="pl-pds">"</span></span>) <span class="pl-c1">crossoversome</span>((modelA, modelB))</pre></div>
<h3><a id="user-content-fitness-functions" class="anchor" aria-hidden="true" href="#fitness-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fitness functions</h3>
<p>A handful of ways to compute the fitness of a model are supplied. Apart from the obvious accuracy on some (typically held out) data set, it is also possible to measure fitness as how many (few) parameters a model has and how long it takes to compute the fitness. Fitness metrics can of course be combined to create objectives which balance several factors.</p>
<p>As seen in the very first basic example above, training of a model is just another fitness strategy. This might seem unintuitive at first, but reading it out like "the chosen fitness strategy is to first train the model for N batches, then compute the accuracy on the validation set" makes sense. The practical advantages are that it becomes straight forward to implement fitness strategies which don't involve model training (e.g. using the neural tangent kernel) as well as fitness strategies which measure some aspect of the model training (e.g. time to train for X iterations or training memory consumption). Another useful property is that models which produce <code>NaN</code>s or <code>Inf</code>s or take very long to train can be assigned a low fitness score.</p>
<p>Examples:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Function to compute fitness for does not have to be a CompGraph, or even a neural network
# They must be wrapped in an AbstractCandidate since fitness functions generally need to query the candidate for 
# things which affect the fitness, such as the model but also things like optimizers and loss functions.
candidate1 = CandidateModel(x -&gt; 3:-1:1)
candidate2 = CandidateModel(Dense(ones(Float32, 3,3), collect(Float32, 1:3)))

# Fitness is accuracy on the provided data set
accfitness = AccuracyFitness([(ones(Float32, 3, 1), 1:3)])

@test fitness(accfitness, candidate1) == 0
@test fitness(accfitness, candidate2) == 1

# Measure how long time it takes to evaluate the fitness and add that in addition to the accuracy
let timedfitness = TimeFitness(accfitness)
    c1time, c1acc = fitness(timedfitness, candidate1)
    c2time, c2acc = fitness(timedfitness, candidate2) 
    @test c1acc == 0
    @test c2acc == 1
    @test 0 &lt; c1time 
    @test 0 &lt; c2time 
end

# Use the number of parameters to compute fitness
bigmodelfitness = SizeFitness()
@test fitness(bigmodelfitness, candidate1) == 0
@test fitness(bigmodelfitness, candidate2) == 12

# One typically wants to map high number of params to lower fitness:
smallmodelfitness = MapFitness(bigmodelfitness) do nparameters
    return min(1, 1 / nparameters)
end
@test fitness(smallmodelfitness, candidate1) == 1
@test fitness(smallmodelfitness, candidate2) == 1/12

# Combining fitness is straight forward
combined = AggFitness(+, accfitness, smallmodelfitness, bigmodelfitness)

@test fitness(combined, candidate1) == 1
@test fitness(combined, candidate2) == 13 + 1/12

# GpuFitness moves the candidates to GPU (as selected by Flux.gpu) before computing the wrapped fitness
# Note that any data in the wrapped fitness must also be moved to the same GPU before being fed to the model
gpuaccfitness = GpuFitness(AccuracyFitness(GpuIterator(accfitness.dataset)))

@test fitness(gpuaccfitness, candidate1) == 0
@test fitness(gpuaccfitness, candidate2) == 1

"><pre><span class="pl-c"><span class="pl-c">#</span> Function to compute fitness for does not have to be a CompGraph, or even a neural network</span>
<span class="pl-c"><span class="pl-c">#</span> They must be wrapped in an AbstractCandidate since fitness functions generally need to query the candidate for </span>
<span class="pl-c"><span class="pl-c">#</span> things which affect the fitness, such as the model but also things like optimizers and loss functions.</span>
candidate1 <span class="pl-k">=</span> <span class="pl-c1">CandidateModel</span>(x <span class="pl-k">-&gt;</span> <span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-k">-</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>)
candidate2 <span class="pl-k">=</span> <span class="pl-c1">CandidateModel</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">ones</span>(Float32, <span class="pl-c1">3</span>,<span class="pl-c1">3</span>), <span class="pl-c1">collect</span>(Float32, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>)))

<span class="pl-c"><span class="pl-c">#</span> Fitness is accuracy on the provided data set</span>
accfitness <span class="pl-k">=</span> <span class="pl-c1">AccuracyFitness</span>([(<span class="pl-c1">ones</span>(Float32, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>), <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>)])

<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(accfitness, candidate1) <span class="pl-k">==</span> <span class="pl-c1">0</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(accfitness, candidate2) <span class="pl-k">==</span> <span class="pl-c1">1</span>

<span class="pl-c"><span class="pl-c">#</span> Measure how long time it takes to evaluate the fitness and add that in addition to the accuracy</span>
<span class="pl-k">let</span> timedfitness <span class="pl-k">=</span> <span class="pl-c1">TimeFitness</span>(accfitness)
    c1time, c1acc <span class="pl-k">=</span> <span class="pl-c1">fitness</span>(timedfitness, candidate1)
    c2time, c2acc <span class="pl-k">=</span> <span class="pl-c1">fitness</span>(timedfitness, candidate2) 
    <span class="pl-c1">@test</span> c1acc <span class="pl-k">==</span> <span class="pl-c1">0</span>
    <span class="pl-c1">@test</span> c2acc <span class="pl-k">==</span> <span class="pl-c1">1</span>
    <span class="pl-c1">@test</span> <span class="pl-c1">0</span> <span class="pl-k">&lt;</span> c1time 
    <span class="pl-c1">@test</span> <span class="pl-c1">0</span> <span class="pl-k">&lt;</span> c2time 
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Use the number of parameters to compute fitness</span>
bigmodelfitness <span class="pl-k">=</span> <span class="pl-c1">SizeFitness</span>()
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(bigmodelfitness, candidate1) <span class="pl-k">==</span> <span class="pl-c1">0</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(bigmodelfitness, candidate2) <span class="pl-k">==</span> <span class="pl-c1">12</span>

<span class="pl-c"><span class="pl-c">#</span> One typically wants to map high number of params to lower fitness:</span>
smallmodelfitness <span class="pl-k">=</span> <span class="pl-c1">MapFitness</span>(bigmodelfitness) <span class="pl-k">do</span> nparameters
    <span class="pl-k">return</span> <span class="pl-c1">min</span>(<span class="pl-c1">1</span>, <span class="pl-c1">1</span> <span class="pl-k">/</span> nparameters)
<span class="pl-k">end</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(smallmodelfitness, candidate1) <span class="pl-k">==</span> <span class="pl-c1">1</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(smallmodelfitness, candidate2) <span class="pl-k">==</span> <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">12</span>

<span class="pl-c"><span class="pl-c">#</span> Combining fitness is straight forward</span>
combined <span class="pl-k">=</span> <span class="pl-c1">AggFitness</span>(<span class="pl-k">+</span>, accfitness, smallmodelfitness, bigmodelfitness)

<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(combined, candidate1) <span class="pl-k">==</span> <span class="pl-c1">1</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(combined, candidate2) <span class="pl-k">==</span> <span class="pl-c1">13</span> <span class="pl-k">+</span> <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">12</span>

<span class="pl-c"><span class="pl-c">#</span> GpuFitness moves the candidates to GPU (as selected by Flux.gpu) before computing the wrapped fitness</span>
<span class="pl-c"><span class="pl-c">#</span> Note that any data in the wrapped fitness must also be moved to the same GPU before being fed to the model</span>
gpuaccfitness <span class="pl-k">=</span> <span class="pl-c1">GpuFitness</span>(<span class="pl-c1">AccuracyFitness</span>(<span class="pl-c1">GpuIterator</span>(accfitness<span class="pl-k">.</span>dataset)))

<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(gpuaccfitness, candidate1) <span class="pl-k">==</span> <span class="pl-c1">0</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(gpuaccfitness, candidate2) <span class="pl-k">==</span> <span class="pl-c1">1</span>
</pre></div>
<h3><a id="user-content-candidate-utilities" class="anchor" aria-hidden="true" href="#candidate-utilities"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Candidate utilities</h3>
<p>As seen above, fitness strategies require an <code>AbstractCandidate</code> to compute fitness. To be used by NaiveGAflux, an <code>AbstractCandidate</code> needs to</p>
<ol>
<li>Provide the data needed by the fitness strategy, most commonly the model but also things like lossfunctions and optimizers</li>
<li>Be able to create a new version of itself given a function which maps its fields to new fields.</li>
</ol>
<p>Capability 1. is generally performed through functions of the format <code>someproperty(candidate; default)</code> where in general <code>someproperty(::AbstractCandidate; default=nothing) = default</code>. The following such functions are currently implemented by NaiveGAflux:</p>
<ul>
<li><code>graph(c; default)</code>  : Return a model</li>
<li><code>opt(c; default)</code>    : Return an optimizer</li>
<li><code>lossfun(c; default)</code> : Return a lossfunction</li>
</ul>
<p>All such functions are obviously not used by all fitness strategies and some are used more often than others. Whether an <code>AbstractCandidate</code> returns something other than <code>default</code> generally depends on whether it is a hyperparameter which is being searched for or not. For example, the very simple <code>CandidateModel</code> has only a <code>model</code> while <code>CandidateOptModel</code> has both a model and an own optimizer which may be mutated/crossedover when evolving.</p>
<p>Capability 2. is what is used then evolving a candidate into a new version of itself. The function to implement for new <code>AbstractCandidate</code> types is <code>newcand(c::MyCandidate, mapfields)</code> which in most cases has the implementation <code>newcand(c::MyCandidate, mapfield) = MyCandidate(map(mapfield, getproperty.(c, fieldnames(MyCandidate)))...)</code>.</p>
<p>Example with a new candidate type and a new fitness strategy for said type:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="struct ExampleCandidate &lt;: AbstractCandidate
    a::Int
    b::Int
end
aval(c::ExampleCandidate; default=nothing) = c.a
bval(c::ExampleCandidate; default=nothing) = c.b

struct ExampleFitness &lt;: AbstractFitness end
NaiveGAflux._fitness(::ExampleFitness, c::AbstractCandidate) = aval(c; default=10) - bval(c; default=5)

# Ok, this is alot of work for quite little in this dummy example
@test fitness(ExampleFitness(), ExampleCandidate(4, 3)) === 1

ctime, examplemetric = fitness(TimeFitness(ExampleFitness()), ExampleCandidate(3,1))
@test examplemetric === 2
@test ctime &gt; 0
"><pre><span class="pl-k">struct</span> ExampleCandidate <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractCandidate</span>
    a<span class="pl-k">::</span><span class="pl-c1">Int</span>
    b<span class="pl-k">::</span><span class="pl-c1">Int</span>
<span class="pl-k">end</span>
<span class="pl-en">aval</span>(c<span class="pl-k">::</span><span class="pl-c1">ExampleCandidate</span>; default<span class="pl-k">=</span><span class="pl-c1">nothing</span>) <span class="pl-k">=</span> c<span class="pl-k">.</span>a
<span class="pl-en">bval</span>(c<span class="pl-k">::</span><span class="pl-c1">ExampleCandidate</span>; default<span class="pl-k">=</span><span class="pl-c1">nothing</span>) <span class="pl-k">=</span> c<span class="pl-k">.</span>b

<span class="pl-k">struct</span> ExampleFitness <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractFitness</span> <span class="pl-k">end</span>
NaiveGAflux<span class="pl-k">.</span><span class="pl-en">_fitness</span>(<span class="pl-k">::</span><span class="pl-c1">ExampleFitness</span>, c<span class="pl-k">::</span><span class="pl-c1">AbstractCandidate</span>) <span class="pl-k">=</span> <span class="pl-c1">aval</span>(c; default<span class="pl-k">=</span><span class="pl-c1">10</span>) <span class="pl-k">-</span> <span class="pl-c1">bval</span>(c; default<span class="pl-k">=</span><span class="pl-c1">5</span>)

<span class="pl-c"><span class="pl-c">#</span> Ok, this is alot of work for quite little in this dummy example</span>
<span class="pl-c1">@test</span> <span class="pl-c1">fitness</span>(<span class="pl-c1">ExampleFitness</span>(), <span class="pl-c1">ExampleCandidate</span>(<span class="pl-c1">4</span>, <span class="pl-c1">3</span>)) <span class="pl-k">===</span> <span class="pl-c1">1</span>

ctime, examplemetric <span class="pl-k">=</span> <span class="pl-c1">fitness</span>(<span class="pl-c1">TimeFitness</span>(<span class="pl-c1">ExampleFitness</span>()), <span class="pl-c1">ExampleCandidate</span>(<span class="pl-c1">3</span>,<span class="pl-c1">1</span>))
<span class="pl-c1">@test</span> examplemetric <span class="pl-k">===</span> <span class="pl-c1">2</span>
<span class="pl-c1">@test</span> ctime <span class="pl-k">&gt;</span> <span class="pl-c1">0</span></pre></div>
<h3><a id="user-content-evolution-strategies" class="anchor" aria-hidden="true" href="#evolution-strategies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evolution Strategies</h3>
<p>Evolution strategies are the functions used to evolve the population in the genetic algorithm from one generation to the next. The following is performed by evolution strategies:</p>
<ul>
<li>Select which candidates to use for the next generation</li>
<li>Produce new candidates, e.g by mutating the selected candidates</li>
</ul>
<p>Important to note about evolution strategies is that they generally expect candidates which can provide a precomputed fitness value, e.g. <code>FittedCandidate</code>s. This is because the fitness value is used by things like sorting where it is not only impractical to recompute it, but is also might lead to undefined behaviour if it is not always the same. Use <code>Population</code> to get some help with computing fitness for all candidates before passing them on to evolution.</p>
<p>Note that there is no general requirement on an evolution strategy to return the same population size as it was given. It is also free to create completely new candidates without basing anything on any given candidate.</p>
<p>Examples:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# For controlled randomness in the examples
struct FakeRng end
Base.rand(::FakeRng) = 0.7

# Dummy candidate for brevity
struct Cand &lt;: AbstractCandidate
    fitness
end
NaiveGAflux.fitness(d::Cand) = d.fitness

# EliteSelection selects the n best candidates
elitesel = EliteSelection(2)
@test evolve(elitesel, Cand.(1:10)) == Cand.([10, 9])

# EvolveCandidates maps candidates to new candidates (e.g. through mutation)
evocands = EvolveCandidates(c -&gt; Cand(fitness(c) + 0.1))
@test evolve(evocands, Cand.(1:10)) == Cand.(1.1:10.1)

# SusSelection selects n random candidates using stochastic uniform sampling
# Selected candidates will be forwarded to the wrapped evolution strategy before returned
sussel = SusSelection(5, evocands, FakeRng())
@test evolve(sussel, Cand.(1:10)) == Cand.([4.1, 6.1, 8.1, 9.1, 10.1])

# CombinedEvolution combines the populations from several evolution strategies
comb = CombinedEvolution(elitesel, sussel)
@test evolve(comb, Cand.(1:10)) == Cand.(Any[10, 9, 4.1, 6.1, 8.1, 9.1, 10.1])
"><pre><span class="pl-c"><span class="pl-c">#</span> For controlled randomness in the examples</span>
<span class="pl-k">struct</span> FakeRng <span class="pl-k">end</span>
Base<span class="pl-k">.</span><span class="pl-en">rand</span>(<span class="pl-k">::</span><span class="pl-c1">FakeRng</span>) <span class="pl-k">=</span> <span class="pl-c1">0.7</span>

<span class="pl-c"><span class="pl-c">#</span> Dummy candidate for brevity</span>
<span class="pl-k">struct</span> Cand <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractCandidate</span>
    fitness
<span class="pl-k">end</span>
NaiveGAflux<span class="pl-k">.</span><span class="pl-en">fitness</span>(d<span class="pl-k">::</span><span class="pl-c1">Cand</span>) <span class="pl-k">=</span> d<span class="pl-k">.</span>fitness

<span class="pl-c"><span class="pl-c">#</span> EliteSelection selects the n best candidates</span>
elitesel <span class="pl-k">=</span> <span class="pl-c1">EliteSelection</span>(<span class="pl-c1">2</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">evolve</span>(elitesel, <span class="pl-c1">Cand</span>.(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>)) <span class="pl-k">==</span> <span class="pl-c1">Cand</span>.([<span class="pl-c1">10</span>, <span class="pl-c1">9</span>])

<span class="pl-c"><span class="pl-c">#</span> EvolveCandidates maps candidates to new candidates (e.g. through mutation)</span>
evocands <span class="pl-k">=</span> <span class="pl-c1">EvolveCandidates</span>(c <span class="pl-k">-&gt;</span> <span class="pl-c1">Cand</span>(<span class="pl-c1">fitness</span>(c) <span class="pl-k">+</span> <span class="pl-c1">0.1</span>))
<span class="pl-c1">@test</span> <span class="pl-c1">evolve</span>(evocands, <span class="pl-c1">Cand</span>.(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>)) <span class="pl-k">==</span> <span class="pl-c1">Cand</span>.(<span class="pl-c1">1.1</span><span class="pl-k">:</span><span class="pl-c1">10.1</span>)

<span class="pl-c"><span class="pl-c">#</span> SusSelection selects n random candidates using stochastic uniform sampling</span>
<span class="pl-c"><span class="pl-c">#</span> Selected candidates will be forwarded to the wrapped evolution strategy before returned</span>
sussel <span class="pl-k">=</span> <span class="pl-c1">SusSelection</span>(<span class="pl-c1">5</span>, evocands, <span class="pl-c1">FakeRng</span>())
<span class="pl-c1">@test</span> <span class="pl-c1">evolve</span>(sussel, <span class="pl-c1">Cand</span>.(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>)) <span class="pl-k">==</span> <span class="pl-c1">Cand</span>.([<span class="pl-c1">4.1</span>, <span class="pl-c1">6.1</span>, <span class="pl-c1">8.1</span>, <span class="pl-c1">9.1</span>, <span class="pl-c1">10.1</span>])

<span class="pl-c"><span class="pl-c">#</span> CombinedEvolution combines the populations from several evolution strategies</span>
comb <span class="pl-k">=</span> <span class="pl-c1">CombinedEvolution</span>(elitesel, sussel)
<span class="pl-c1">@test</span> <span class="pl-c1">evolve</span>(comb, <span class="pl-c1">Cand</span>.(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>)) <span class="pl-k">==</span> <span class="pl-c1">Cand</span>.(Any[<span class="pl-c1">10</span>, <span class="pl-c1">9</span>, <span class="pl-c1">4.1</span>, <span class="pl-c1">6.1</span>, <span class="pl-c1">8.1</span>, <span class="pl-c1">9.1</span>, <span class="pl-c1">10.1</span>])</pre></div>
<h3><a id="user-content-iterators" class="anchor" aria-hidden="true" href="#iterators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Iterators</h3>
<p>While not part of the scope of this package, some simple utilities for iterating over data sets is provided.</p>
<p>The only iterator which is in some sense special for this package is <code>RepeatPartitionIterator</code> which produces iterators over a subset of its wrapped iterator. This is useful when one wants to ensure that all models see the same (possibly randomly augmented) data in the same order. Note that this is not certain to be the best strategy for finding good models for a given data set and this package does (intentionally) blur the lines a bit between model training protocol and architecture search.</p>
<p>Examples:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="data = reshape(collect(1:4*5), 4,5)

# mini-batching
biter = BatchIterator(data, 2)
@test size(first(biter)) == (4, 2)

# shuffle data before mini-batching
# Warning: Must use different rng instances with the same seed for features and labels!
siter = ShuffleIterator(data, 2, MersenneTwister(123))
@test size(first(siter)) == size(first(biter))
@test first(siter) != first(biter)

# Apply a function to each batch
miter = MapIterator(x -&gt; 2 .* x, biter)
@test first(miter) == 2 .* first(biter)

# Move data to gpu
giter = GpuIterator(miter)
@test first(giter) == first(miter) |&gt; gpu

labels = collect(0:5)

# Possible to use Flux.onehotbatch for many iterators
biter_labels = Flux.onehotbatch(BatchIterator(labels, 2), 0:5)
@test first(biter_labels) == Flux.onehotbatch(0:1, 0:5)

# This is the only iterator which is &quot;special&quot; for this package:
rpiter = RepeatPartitionIterator(zip(biter, biter_labels), 2)
# It produces iterators over a subset of the wrapped iterator (2 batches in this case)
piter = first(rpiter)
@test length(piter) == 2
# This allows for easily training several models on the same subset of the data
expiter = zip(biter, biter_labels)
for modeli in 1:3
    for ((feature, label), (expf, expl)) in zip(piter, expiter)
        @test feature == expf
        @test label == expl
    end
end

# StatefulGenerationIter is typically used in conjunction with TrainThenFitness to map a generation
# number to an iterator from a RepeatStatefulIterator 
sgiter = StatefulGenerationIter(rpiter)
for (generationnr, topiter) in enumerate(rpiter)
    gendata = collect(NaiveGAflux.itergeneration(sgiter, generationnr))
    expdata = collect(topiter)
    @test gendata == expdata
end

# Timed iterator is useful for preventing that models which take very long time 
# to train slow down the process. We can just stop training them in that case
timediter = TimedIterator(;timelimit=0.1, patience=4, timeoutaction = () -&gt; TimedIteratorStop, accumulate_timeouts=false, base=1:100)

last = 0
for i in timediter
    last = i
    if i &gt; 2
        sleep(0.11)
    end
end
@test last === 6 # Sleep after 2, then 4 patience
"><pre>data <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">collect</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">5</span>), <span class="pl-c1">4</span>,<span class="pl-c1">5</span>)

<span class="pl-c"><span class="pl-c">#</span> mini-batching</span>
biter <span class="pl-k">=</span> <span class="pl-c1">BatchIterator</span>(data, <span class="pl-c1">2</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">first</span>(biter)) <span class="pl-k">==</span> (<span class="pl-c1">4</span>, <span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> shuffle data before mini-batching</span>
<span class="pl-c"><span class="pl-c">#</span> Warning: Must use different rng instances with the same seed for features and labels!</span>
siter <span class="pl-k">=</span> <span class="pl-c1">ShuffleIterator</span>(data, <span class="pl-c1">2</span>, <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123</span>))
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">first</span>(siter)) <span class="pl-k">==</span> <span class="pl-c1">size</span>(<span class="pl-c1">first</span>(biter))
<span class="pl-c1">@test</span> <span class="pl-c1">first</span>(siter) <span class="pl-k">!=</span> <span class="pl-c1">first</span>(biter)

<span class="pl-c"><span class="pl-c">#</span> Apply a function to each batch</span>
miter <span class="pl-k">=</span> <span class="pl-c1">MapIterator</span>(x <span class="pl-k">-&gt;</span> <span class="pl-c1">2</span> <span class="pl-k">.*</span> x, biter)
<span class="pl-c1">@test</span> <span class="pl-c1">first</span>(miter) <span class="pl-k">==</span> <span class="pl-c1">2</span> <span class="pl-k">.*</span> <span class="pl-c1">first</span>(biter)

<span class="pl-c"><span class="pl-c">#</span> Move data to gpu</span>
giter <span class="pl-k">=</span> <span class="pl-c1">GpuIterator</span>(miter)
<span class="pl-c1">@test</span> <span class="pl-c1">first</span>(giter) <span class="pl-k">==</span> <span class="pl-c1">first</span>(miter) <span class="pl-k">|&gt;</span> gpu

labels <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">5</span>)

<span class="pl-c"><span class="pl-c">#</span> Possible to use Flux.onehotbatch for many iterators</span>
biter_labels <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">onehotbatch</span>(<span class="pl-c1">BatchIterator</span>(labels, <span class="pl-c1">2</span>), <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">5</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">first</span>(biter_labels) <span class="pl-k">==</span> Flux<span class="pl-k">.</span><span class="pl-c1">onehotbatch</span>(<span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">1</span>, <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">5</span>)

<span class="pl-c"><span class="pl-c">#</span> This is the only iterator which is "special" for this package:</span>
rpiter <span class="pl-k">=</span> <span class="pl-c1">RepeatPartitionIterator</span>(<span class="pl-c1">zip</span>(biter, biter_labels), <span class="pl-c1">2</span>)
<span class="pl-c"><span class="pl-c">#</span> It produces iterators over a subset of the wrapped iterator (2 batches in this case)</span>
piter <span class="pl-k">=</span> <span class="pl-c1">first</span>(rpiter)
<span class="pl-c1">@test</span> <span class="pl-c1">length</span>(piter) <span class="pl-k">==</span> <span class="pl-c1">2</span>
<span class="pl-c"><span class="pl-c">#</span> This allows for easily training several models on the same subset of the data</span>
expiter <span class="pl-k">=</span> <span class="pl-c1">zip</span>(biter, biter_labels)
<span class="pl-k">for</span> modeli <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>
    <span class="pl-k">for</span> ((feature, label), (expf, expl)) <span class="pl-k">in</span> <span class="pl-c1">zip</span>(piter, expiter)
        <span class="pl-c1">@test</span> feature <span class="pl-k">==</span> expf
        <span class="pl-c1">@test</span> label <span class="pl-k">==</span> expl
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> StatefulGenerationIter is typically used in conjunction with TrainThenFitness to map a generation</span>
<span class="pl-c"><span class="pl-c">#</span> number to an iterator from a RepeatStatefulIterator </span>
sgiter <span class="pl-k">=</span> <span class="pl-c1">StatefulGenerationIter</span>(rpiter)
<span class="pl-k">for</span> (generationnr, topiter) <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(rpiter)
    gendata <span class="pl-k">=</span> <span class="pl-c1">collect</span>(NaiveGAflux<span class="pl-k">.</span><span class="pl-c1">itergeneration</span>(sgiter, generationnr))
    expdata <span class="pl-k">=</span> <span class="pl-c1">collect</span>(topiter)
    <span class="pl-c1">@test</span> gendata <span class="pl-k">==</span> expdata
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Timed iterator is useful for preventing that models which take very long time </span>
<span class="pl-c"><span class="pl-c">#</span> to train slow down the process. We can just stop training them in that case</span>
timediter <span class="pl-k">=</span> <span class="pl-c1">TimedIterator</span>(;timelimit<span class="pl-k">=</span><span class="pl-c1">0.1</span>, patience<span class="pl-k">=</span><span class="pl-c1">4</span>, timeoutaction <span class="pl-k">=</span> () <span class="pl-k">-&gt;</span> TimedIteratorStop, accumulate_timeouts<span class="pl-k">=</span><span class="pl-c1">false</span>, base<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)

last <span class="pl-k">=</span> <span class="pl-c1">0</span>
<span class="pl-k">for</span> i <span class="pl-k">in</span> timediter
    last <span class="pl-k">=</span> i
    <span class="pl-k">if</span> i <span class="pl-k">&gt;</span> <span class="pl-c1">2</span>
        <span class="pl-c1">sleep</span>(<span class="pl-c1">0.11</span>)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c1">@test</span> last <span class="pl-k">===</span> <span class="pl-c1">6</span> <span class="pl-c"><span class="pl-c">#</span> Sleep after 2, then 4 patience</span></pre></div>
<h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<p>All contributions are welcome. Please file an issue before creating a PR.</p>
</article></div>