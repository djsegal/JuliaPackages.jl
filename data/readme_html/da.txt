<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-discriminant-analysis" class="anchor" aria-hidden="true" href="#discriminant-analysis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Discriminant Analysis</h1>
<p><a href="http://discriminantanalysis.readthedocs.org/en/latest/?badge=latest" rel="nofollow"><img src="https://camo.githubusercontent.com/dc3101ad878d7757f93d4f101fb3645962a58ed4/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6469736372696d696e616e74616e616c797369732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/discriminantanalysis/badge/?version=latest" style="max-width:100%;"></a>
<a href="https://travis-ci.org/trthatcher/DiscriminantAnalysis.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/b61e2f230e5cc496342ca728aef9d1d48d712667/68747470733a2f2f7472617669732d63692e6f72672f747274686174636865722f4469736372696d696e616e74416e616c797369732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/trthatcher/DiscriminantAnalysis.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/trthatcher/DiscriminantAnalysis.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/e49ca3f849f06056108355982d2dbe2785826262/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f747274686174636865722f4469736372696d696e616e74416e616c797369732e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/trthatcher/DiscriminantAnalysis.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a></p>
<h4><a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Summary</h4>
<p><strong>DiscriminantAnalysis.jl</strong> is a Julia package for multiple linear and quadratic
regularized discriminant analysis (LDA &amp; QDA respectively). LDA and QDA are
distribution-based classifiers with the underlying assumption that data follows
a multivariate normal distribution. LDA differs from QDA in the assumption about
the class variability; LDA assumes that all classes share the same within-class
covariance matrix whereas QDA relaxes that constraint and allows for distinct
within-class covariance matrices. This results in LDA being a linear classifier
and QDA being a quadratic classifier.</p>
<h4><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h4>
<p>Full <a href="http://discriminantanalysis.readthedocs.org/en/latest/" rel="nofollow">documentation</a> is
available on Read the Docs.</p>
<h4><a id="user-content-visualization" class="anchor" aria-hidden="true" href="#visualization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Visualization</h4>
<p>When the data is modelled via linear discriminant analysis, the resulting
classification boundaries are hyperplanes (lines in two dimensions):</p>
<p align="center"><a target="_blank" rel="noopener noreferrer" href="doc/visualization/lda.png"><img alt="Linear Discriminant Analysis" src="doc/visualization/lda.png" style="max-width:100%;"></a></p>
<p>Using quadratic discriminant analysis, the resulting classification boundaries
are quadratics:</p>
<p align="center"><a target="_blank" rel="noopener noreferrer" href="doc/visualization/qda.png"><img alt="Quadratic Discriminant Analysis" src="doc/visualization/qda.png" style="max-width:100%;"></a></p>
<h2><a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Getting Started</h2>
<p>If the package has not yet been installed, run the following line of code:</p>
<div class="highlight highlight-source-julia"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>DiscriminantAnalysis<span class="pl-pds">"</span></span>)</pre></div>
<p>Sample code is available in <a href="example/example.jl">example.jl</a>. To get started,
run the following block of code:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DataFrames, DiscriminantAnalysis;

iris_df <span class="pl-k">=</span> <span class="pl-c1">readtable</span>(<span class="pl-s"><span class="pl-pds">"</span>iris.csv<span class="pl-pds">"</span></span>);
<span class="pl-c1">pool!</span>(iris_df, [<span class="pl-c1">:Species</span>]);  <span class="pl-c"><span class="pl-c">#</span> Ensure species is made a pooled data vector</span></pre></div>
<p>Although <a href="https://github.com/JuliaStats/DataFrames.jl">Dataframes.jl</a> is not
required, it will help to stage the data. We require a data matrix (row-major
or column-major ordering of the observations). We then define the data matrix
<code>X</code> and the class reference vector <code>y</code> (vector of 1-based class indicators):</p>
<div class="highlight highlight-source-julia"><pre>X <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Array{Float64}, iris_df[[<span class="pl-c1">:PetalLength</span>, <span class="pl-c1">:PetalWidth</span>, <span class="pl-c1">:SepalLength</span>, <span class="pl-c1">:SepalWidth</span>]]);
y <span class="pl-k">=</span> iris_df[<span class="pl-c1">:Species</span>]<span class="pl-k">.</span>refs;  <span class="pl-c"><span class="pl-c">#</span> Class indices</span></pre></div>
<p>A Linear Discriminant model can be created using the <code>lda</code> function:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> model1 <span class="pl-k">=</span> <span class="pl-c1">lda</span>(X, y)
Linear Discriminant Model

Regularization Parameters<span class="pl-k">:</span>
    γ <span class="pl-k">=</span> N<span class="pl-k">/</span>A

Class Priors<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>
    Class <span class="pl-c1">2</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>
    Class <span class="pl-c1">3</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>

Class Means<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">1.464</span>, <span class="pl-c1">0.244</span>, <span class="pl-c1">5.006</span>, <span class="pl-c1">3.418</span>]
    Class <span class="pl-c1">2</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">4.26</span>, <span class="pl-c1">1.326</span>, <span class="pl-c1">5.936</span>, <span class="pl-c1">2.77</span>]
    Class <span class="pl-c1">3</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">5.552</span>, <span class="pl-c1">2.026</span>, <span class="pl-c1">6.588</span>, <span class="pl-c1">2.974</span>]</pre></div>
<p>Once the model is defined, we can use it to classify new data with the
<code>classify</code> function:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> y_pred1 <span class="pl-k">=</span> <span class="pl-c1">classify</span>(model1, X);

julia<span class="pl-k">&gt;</span> accuracy1 <span class="pl-k">=</span> <span class="pl-c1">sum</span>(y_pred1 <span class="pl-k">.==</span> y)<span class="pl-k">/</span><span class="pl-c1">length</span>(y)
<span class="pl-c1">0.98</span></pre></div>
<p>By default, the LDA model assumes the data is in row-major ordering of
observations, the class weights are equal and that the regularization parameter
gamma is unspecified.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> model2 <span class="pl-k">=</span> <span class="pl-c1">lda</span>(X<span class="pl-k">'</span>, y, order <span class="pl-k">=</span> Val{<span class="pl-c1">:col</span>}, gamma <span class="pl-k">=</span> <span class="pl-c1">0.1</span>, priors <span class="pl-k">=</span> [<span class="pl-c1">2</span><span class="pl-k">/</span><span class="pl-c1">5</span>; <span class="pl-c1">2</span><span class="pl-k">/</span><span class="pl-c1">5</span>; <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">5</span>])
Linear Discriminant Model

Regularization Parameters<span class="pl-k">:</span>
    γ <span class="pl-k">=</span> <span class="pl-c1">0.1</span>

Class Priors<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Probability<span class="pl-k">:</span> <span class="pl-c1">40.0</span><span class="pl-k">%</span>
    Class <span class="pl-c1">2</span> Probability<span class="pl-k">:</span> <span class="pl-c1">40.0</span><span class="pl-k">%</span>
    Class <span class="pl-c1">3</span> Probability<span class="pl-k">:</span> <span class="pl-c1">20.0</span><span class="pl-k">%</span>

Class Means<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">1.464</span>, <span class="pl-c1">0.244</span>, <span class="pl-c1">5.006</span>, <span class="pl-c1">3.418</span>]
    Class <span class="pl-c1">2</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">4.26</span>, <span class="pl-c1">1.326</span>, <span class="pl-c1">5.936</span>, <span class="pl-c1">2.77</span>]
    Class <span class="pl-c1">3</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">5.552</span>, <span class="pl-c1">2.026</span>, <span class="pl-c1">6.588</span>, <span class="pl-c1">2.974</span>]



julia<span class="pl-k">&gt;</span> y_pred2 <span class="pl-k">=</span> <span class="pl-c1">classify</span>(model2, X<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> accuracy2 <span class="pl-k">=</span> <span class="pl-c1">sum</span>(y_pred2 <span class="pl-k">.==</span> y)<span class="pl-k">/</span><span class="pl-c1">length</span>(y)
<span class="pl-c1">0.98</span></pre></div>
<p>We may also fit a Quadratic Discriminant model using the <code>qda</code> function:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> model3 <span class="pl-k">=</span> <span class="pl-c1">qda</span>(X, y, lambda <span class="pl-k">=</span> <span class="pl-c1">0.1</span>, gamma <span class="pl-k">=</span> <span class="pl-c1">0.1</span>, priors <span class="pl-k">=</span> [<span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>; <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>; <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>])
Quadratic Discriminant Model

Regularization Parameters<span class="pl-k">:</span>
    γ <span class="pl-k">=</span> <span class="pl-c1">0.1</span>
    λ <span class="pl-k">=</span> <span class="pl-c1">0.1</span>

Class Priors<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>
    Class <span class="pl-c1">2</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>
    Class <span class="pl-c1">3</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>

Class Means<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">1.464</span>, <span class="pl-c1">0.244</span>, <span class="pl-c1">5.006</span>, <span class="pl-c1">3.418</span>]
    Class <span class="pl-c1">2</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">4.26</span>, <span class="pl-c1">1.326</span>, <span class="pl-c1">5.936</span>, <span class="pl-c1">2.77</span>]
    Class <span class="pl-c1">3</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">5.552</span>, <span class="pl-c1">2.026</span>, <span class="pl-c1">6.588</span>, <span class="pl-c1">2.974</span>]



julia<span class="pl-k">&gt;</span> y_pred3 <span class="pl-k">=</span> <span class="pl-c1">classify</span>(model3, X);

julia<span class="pl-k">&gt;</span> accuracy3 <span class="pl-k">=</span> <span class="pl-c1">sum</span>(y_pred3 <span class="pl-k">.==</span> y)<span class="pl-k">/</span><span class="pl-c1">length</span>(y)
<span class="pl-c1">0.9866666666666667</span></pre></div>
<p>Finally, if the number of classes is less than or equal to the number of
parameters, then a Canonical Discriminant model may be used in place of a Linear
Discriminant model to simultaneously reduce the dimensionality of the
classifier:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> model4 <span class="pl-k">=</span> <span class="pl-c1">cda</span>(X, y, gamma <span class="pl-k">=</span> <span class="pl-c1">Nullable</span><span class="pl-c1">{Float64}</span>(), priors <span class="pl-k">=</span> [<span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>; <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>; <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>])
Canonical Discriminant Model

Regularization Parameters<span class="pl-k">:</span>
    γ <span class="pl-k">=</span> N<span class="pl-k">/</span>A

Class Priors<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>
    Class <span class="pl-c1">2</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>
    Class <span class="pl-c1">3</span> Probability<span class="pl-k">:</span> <span class="pl-c1">33.3333</span><span class="pl-k">%</span>

Class Means<span class="pl-k">:</span>
    Class <span class="pl-c1">1</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">1.464</span>, <span class="pl-c1">0.244</span>, <span class="pl-c1">5.006</span>, <span class="pl-c1">3.418</span>]
    Class <span class="pl-c1">2</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">4.26</span>, <span class="pl-c1">1.326</span>, <span class="pl-c1">5.936</span>, <span class="pl-c1">2.77</span>]
    Class <span class="pl-c1">3</span> Mean<span class="pl-k">:</span> [<span class="pl-c1">5.552</span>, <span class="pl-c1">2.026</span>, <span class="pl-c1">6.588</span>, <span class="pl-c1">2.974</span>]



julia<span class="pl-k">&gt;</span> y_pred4 <span class="pl-k">=</span> <span class="pl-c1">classify</span>(model4, X);

julia<span class="pl-k">&gt;</span> accuracy4 <span class="pl-k">=</span> <span class="pl-c1">sum</span>(y_pred4 <span class="pl-k">.==</span> y)<span class="pl-k">/</span><span class="pl-c1">length</span>(y)
<span class="pl-c1">0.98</span></pre></div>
</article></div>