<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-partialleastsquaresregressorjl" class="anchor" aria-hidden="true" href="#partialleastsquaresregressorjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>PartialLeastSquaresRegressor.jl</h1>
<p dir="auto"><a href="https://travis-ci.com/lalvim/PartialLeastSquaresRegressor.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/793276c148c5ee51dc6ae038c79bd53be625185799a4070bad0cafc6780d6c3d/68747470733a2f2f7472617669732d63692e636f6d2f6c616c76696d2f5061727469616c4c6561737453717561726573526567726573736f722e6a6c2e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://travis-ci.com/lalvim/PartialLeastSquaresRegressor.jl.svg?branch=master" style="max-width: 100%;"></a> <a href="https://codecov.io/gh/lalvim/PartialLeastSquaresRegressor.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c7d25b298313b214946331549986893d8cb0fd2f2ae93bf06b50acac35dcb706/68747470733a2f2f636f6465636f762e696f2f67682f6c616c76696d2f5061727469616c4c6561737453717561726573526567726573736f722e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d31335472507367616b4f" alt="" data-canonical-src="https://codecov.io/gh/lalvim/PartialLeastSquaresRegressor.jl/branch/master/graph/badge.svg?token=13TrPsgakO" style="max-width: 100%;"></a> <a href="https://coveralls.io/github/lalvim/PartialLeastSquaresRegressor.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/58a92c89077e035dd1b1614faae62d92f22b895a01a39e5928ede19ba7624c96/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6c616c76696d2f5061727469616c4c6561737453717561726573526567726573736f722e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://coveralls.io/repos/github/lalvim/PartialLeastSquaresRegressor.jl/badge.svg?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">The PartialLeastSquaresRegressor.jl package is a package with Partial Least Squares Regressor methods. Contains PLS1, PLS2 and Kernel PLS2 NIPALS algorithms.
Can be used mainly for regression. However, for classification task, binarizing targets and then obtaining multiple targets, you can apply KPLS.</p>
<h2 dir="auto"><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install</h2>
<p dir="auto">The package can be installed with the Julia package manager.
From the Julia REPL, type <code>]</code> to enter the Pkg REPL mode and run:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="pkg&gt; add PartialLeastSquaresRegressor"><pre class="notranslate"><code>pkg&gt; add PartialLeastSquaresRegressor
</code></pre></div>
<p dir="auto">Or, equivalently, via the <code>Pkg</code> API:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; import Pkg; Pkg.add(&quot;PartialLeastSquaresRegressor&quot;)"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">import</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>PartialLeastSquaresRegressor<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-using" class="anchor" aria-hidden="true" href="#using"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Using</h2>
<p dir="auto">PartialLeastSquaresRegressor is used with <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine learning framework. Here are a few examples to show the Package functionalities:</p>
<h3 dir="auto"><a id="user-content-example-1" class="anchor" aria-hidden="true" href="#example-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example 1</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJBase, RDatasets, MLJModels
PLSRegressor = @load PLSRegressor pkg=PartialLeastSquaresRegressor

# loading data and selecting some features
data = dataset(&quot;datasets&quot;, &quot;longley&quot;)[:, 2:5]

# unpacking the target
y, X = unpack(data, ==(:GNP))

# loading the model
regressor = PLSRegressor(n_factors=2)

# building a pipeline with scaling on data
pipe = Standardizer |&gt; regressor
model = TransformedTargetModel(pipe, transformer=Standardizer())

# a simple hould out
(Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.7, rng=123, multi=true)

mach = machine(model, Xtest, ytest)

fit!(mach)
yhat = predict(mach, Xtest)

mae(yhat, ytest) |&gt; mean"><pre><span class="pl-k">using</span> MLJBase, RDatasets, MLJModels
PLSRegressor <span class="pl-k">=</span> <span class="pl-c1">@load</span> PLSRegressor pkg<span class="pl-k">=</span>PartialLeastSquaresRegressor

<span class="pl-c"><span class="pl-c">#</span> loading data and selecting some features</span>
data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>longley<span class="pl-pds">"</span></span>)[:, <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">5</span>]

<span class="pl-c"><span class="pl-c">#</span> unpacking the target</span>
y, X <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(data, <span class="pl-k">==</span>(<span class="pl-c1">:GNP</span>))

<span class="pl-c"><span class="pl-c">#</span> loading the model</span>
regressor <span class="pl-k">=</span> <span class="pl-c1">PLSRegressor</span>(n_factors<span class="pl-k">=</span><span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> building a pipeline with scaling on data</span>
pipe <span class="pl-k">=</span> Standardizer <span class="pl-k">|&gt;</span> regressor
model <span class="pl-k">=</span> <span class="pl-c1">TransformedTargetModel</span>(pipe, transformer<span class="pl-k">=</span><span class="pl-c1">Standardizer</span>())

<span class="pl-c"><span class="pl-c">#</span> a simple hould out</span>
(Xtrain, Xtest), (ytrain, ytest) <span class="pl-k">=</span> <span class="pl-c1">partition</span>((X, y), <span class="pl-c1">0.7</span>, rng<span class="pl-k">=</span><span class="pl-c1">123</span>, multi<span class="pl-k">=</span><span class="pl-c1">true</span>)

mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(model, Xtest, ytest)

<span class="pl-c1">fit!</span>(mach)
yhat <span class="pl-k">=</span> <span class="pl-c1">predict</span>(mach, Xtest)

<span class="pl-c1">mae</span>(yhat, ytest) <span class="pl-k">|&gt;</span> mean</pre></div>
<h3 dir="auto"><a id="user-content-example-2" class="anchor" aria-hidden="true" href="#example-2"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example 2</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJBase, RDatasets, MLJTuning, MLJModels
@load KPLSRegressor pkg=PartialLeastSquaresRegressor

# loading data and selecting some features
data = dataset(&quot;datasets&quot;, &quot;longley&quot;)[:, 2:5]

# unpacking the target
y, X = unpack(data, ==(:GNP), colname -&gt; true)

# loading the model
pls_model = KPLSRegressor()

# defining hyperparams for tunning
r1 = range(pls_model, :width, lower=0.001, upper=100.0, scale=:log)

# attaching tune
self_tuning_pls_model = TunedModel(model =          pls_model,
                                   resampling = CV(nfolds = 10),
                                   tuning = Grid(resolution = 100),
                                   range = [r1],
                                   measure = mae)

# putting into the machine
self_tuning_pls = machine(self_tuning_pls_model, X, y)

# fitting with tunning
fit!(self_tuning_pls, verbosity=0)

# getting the report
report(self_tuning_pls)"><pre><span class="pl-k">using</span> MLJBase, RDatasets, MLJTuning, MLJModels
<span class="pl-c1">@load</span> KPLSRegressor pkg<span class="pl-k">=</span>PartialLeastSquaresRegressor

<span class="pl-c"><span class="pl-c">#</span> loading data and selecting some features</span>
data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>longley<span class="pl-pds">"</span></span>)[:, <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">5</span>]

<span class="pl-c"><span class="pl-c">#</span> unpacking the target</span>
y, X <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(data, <span class="pl-k">==</span>(<span class="pl-c1">:GNP</span>), colname <span class="pl-k">-&gt;</span> <span class="pl-c1">true</span>)

<span class="pl-c"><span class="pl-c">#</span> loading the model</span>
pls_model <span class="pl-k">=</span> <span class="pl-c1">KPLSRegressor</span>()

<span class="pl-c"><span class="pl-c">#</span> defining hyperparams for tunning</span>
r1 <span class="pl-k">=</span> <span class="pl-c1">range</span>(pls_model, <span class="pl-c1">:width</span>, lower<span class="pl-k">=</span><span class="pl-c1">0.001</span>, upper<span class="pl-k">=</span><span class="pl-c1">100.0</span>, scale<span class="pl-k">=</span><span class="pl-c1">:log</span>)

<span class="pl-c"><span class="pl-c">#</span> attaching tune</span>
self_tuning_pls_model <span class="pl-k">=</span> <span class="pl-c1">TunedModel</span>(model <span class="pl-k">=</span>          pls_model,
                                   resampling <span class="pl-k">=</span> <span class="pl-c1">CV</span>(nfolds <span class="pl-k">=</span> <span class="pl-c1">10</span>),
                                   tuning <span class="pl-k">=</span> <span class="pl-c1">Grid</span>(resolution <span class="pl-k">=</span> <span class="pl-c1">100</span>),
                                   range <span class="pl-k">=</span> [r1],
                                   measure <span class="pl-k">=</span> mae)

<span class="pl-c"><span class="pl-c">#</span> putting into the machine</span>
self_tuning_pls <span class="pl-k">=</span> <span class="pl-c1">machine</span>(self_tuning_pls_model, X, y)

<span class="pl-c"><span class="pl-c">#</span> fitting with tunning</span>
<span class="pl-c1">fit!</span>(self_tuning_pls, verbosity<span class="pl-k">=</span><span class="pl-c1">0</span>)

<span class="pl-c"><span class="pl-c">#</span> getting the report</span>
<span class="pl-c1">report</span>(self_tuning_pls)</pre></div>
<h2 dir="auto"><a id="user-content-what-is-implemented" class="anchor" aria-hidden="true" href="#what-is-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>What is Implemented</h2>
<ul dir="auto">
<li>A fast linear algorithm for single targets (PLS1 - NIPALS)</li>
<li>A linear algorithm for multiple targets (PLS2 - NIPALS)</li>
<li>A non linear algorithm for multiple targets (Kernel PLS2 - NIPALS)</li>
</ul>
<h2 dir="auto"><a id="user-content-model-description" class="anchor" aria-hidden="true" href="#model-description"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model Description</h2>
<ul dir="auto">
<li>
<p dir="auto">PLS - PLS MLJ model (PLS1 or PLS2)</p>
<ul dir="auto">
<li>n_factors::Int = 10 - The number of latent variables to explain the data.</li>
</ul>
</li>
<li>
<p dir="auto">KPLS - Kernel PLS MLJ model</p>
<ul dir="auto">
<li>nfactors::Int = 10 - The number of latent variables to explain the data.</li>
<li>kernel::AbstractString = "rbf" - use a non linear kernel.</li>
<li>width::AbstractFloat   = 1.0 - If you want to z-score columns. Recommended if not z-scored yet.</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<ul dir="auto">
<li>
<p dir="auto">PLS1 and PLS2 based on</p>
<ul dir="auto">
<li>Bob Collins Slides, LPAC Group. <a href="http://vision.cse.psu.edu/seminars/talks/PLSpresentation.pdf" rel="nofollow">http://vision.cse.psu.edu/seminars/talks/PLSpresentation.pdf</a></li>
</ul>
</li>
<li>
<p dir="auto">A Kernel PLS2 based on</p>
<ul dir="auto">
<li>Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space" by Roman Rosipal and Leonard J Trejo. Journal of Machine Learning Research 2 (2001) 97-123 <a href="http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf" rel="nofollow">http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf</a></li>
</ul>
</li>
<li>
<p dir="auto">NIPALS: Nonlinear Iterative Partial Least Squares</p>
<ul dir="auto">
<li>Wold, H. (1966). Estimation of principal components and related models
by iterative least squares. In P.R. Krishnaiaah (Ed.). Multivariate Analysis.
(pp.391-420) New York: Academic Press.</li>
</ul>
</li>
<li>
<p dir="auto">SIMPLS: more efficient, optimal result</p>
<ul dir="auto">
<li>Supports multivariate Y</li>
<li>De Jong, S., 1993. SIMPLS: an alternative approach to partial least squares
regression. Chemometrics and Intelligent Laboratory Systems, 18: 251â€“
263</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">The PartialLeastSquaresRegressor.jl is free software: you can redistribute it and/or modify it under the terms of the MIT "Expat"
License. A copy of this license is provided in <code>LICENSE</code></p>
</article></div>