<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AutoGrad</h1>

<p><a href="https://travis-ci.org/denizyuret/AutoGrad.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/2594095c1bdcef127b1377729e29df5ee37a530f/68747470733a2f2f7472617669732d63692e6f72672f64656e697a79757265742f4175746f477261642e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/denizyuret/AutoGrad.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/denizyuret/AutoGrad.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/c43fbe0cb5be3f1aae1714fcf3250243d19fd1e9/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f64656e697a79757265742f4175746f477261642e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="coveralls" data-canonical-src="https://coveralls.io/repos/github/denizyuret/AutoGrad.jl/badge.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/denizyuret/AutoGrad.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/5317f16c2854ff34a65b1464975ed26efec899bc/68747470733a2f2f636f6465636f762e696f2f67682f64656e697a79757265742f4175746f477261642e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/denizyuret/AutoGrad.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>AutoGrad.jl is an automatic differentiation package for Julia.  It started as a port of the
popular Python <a href="https://github.com/HIPS/autograd">autograd</a> package and forms the foundation
of the <a href="https://github.com/denizyuret/Knet.jl">Knet</a> Julia deep learning framework.
AutoGrad can differentiate regular Julia code that includes loops, conditionals, helper
functions, closures etc. by keeping track of the primitive operations and using this
execution trace to compute gradients.  It uses reverse mode differentiation
(a.k.a. backpropagation) so it can efficiently handle functions with large array inputs and
scalar outputs.  It can compute gradients of gradients to handle higher order derivatives.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>You can install AutoGrad in Julia using:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>AutoGrad<span class="pl-pds">"</span></span>)</pre></div>
<p>In order to use it in your code start with:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> AutoGrad</pre></div>
<h2><a id="user-content-interface" class="anchor" aria-hidden="true" href="#interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Interface</h2>
<div class="highlight highlight-source-julia"><pre>x <span class="pl-k">=</span> <span class="pl-c1">Param</span>([<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>])		<span class="pl-c"><span class="pl-c">#</span> user declares parameters</span>
x <span class="pl-k">=&gt;</span> <span class="pl-c1">P</span>([<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>])			<span class="pl-c"><span class="pl-c">#</span> they are wrapped in a struct</span>
<span class="pl-c1">value</span>(x) <span class="pl-k">=&gt;</span> [<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>]		<span class="pl-c"><span class="pl-c">#</span> we can get the original value</span>
<span class="pl-c1">sum</span>(abs2,x) <span class="pl-k">=&gt;</span> <span class="pl-c1">14</span>		<span class="pl-c"><span class="pl-c">#</span> they act like regular values outside of differentiation</span>
y <span class="pl-k">=</span> <span class="pl-c1">@diff</span> <span class="pl-c1">sum</span>(abs2,x)	        <span class="pl-c"><span class="pl-c">#</span> if you want the gradients</span>
y <span class="pl-k">=&gt;</span> <span class="pl-c1">T</span>(<span class="pl-c1">14</span>)			<span class="pl-c"><span class="pl-c">#</span> you get another struct</span>
<span class="pl-c1">value</span>(y) <span class="pl-k">=&gt;</span> <span class="pl-c1">14</span>			<span class="pl-c"><span class="pl-c">#</span> which represents the same value</span>
<span class="pl-c1">grad</span>(y,x) <span class="pl-k">=&gt;</span> [<span class="pl-c1">2</span>,<span class="pl-c1">4</span>,<span class="pl-c1">6</span>]	        <span class="pl-c"><span class="pl-c">#</span> but also contains gradients for all Params</span></pre></div>
<h2><a id="user-content-old-interface" class="anchor" aria-hidden="true" href="#old-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Old Interface</h2>
<p>Pre v1.1 AutoGrad only supported the following <code>grad</code> interface. This is still supported.</p>
<div class="highlight highlight-source-julia"><pre>x <span class="pl-k">=</span> [<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>]
<span class="pl-en">f</span>(x) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,x)
g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(f)
<span class="pl-c1">f</span>(x) <span class="pl-k">=&gt;</span> <span class="pl-c1">14</span>
<span class="pl-c1">g</span>(x) <span class="pl-k">=&gt;</span> [<span class="pl-c1">2</span>,<span class="pl-c1">4</span>,<span class="pl-c1">6</span>]</pre></div>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h2>
<p>Here is a linear regression example using <a href="https://docs.julialang.org/en/stable/manual/methods/#Function-like-objects-1" rel="nofollow">callable objects</a>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">struct</span> Linear; w; b; <span class="pl-k">end</span>		<span class="pl-c"><span class="pl-c">#</span> user defines a model</span>
(f<span class="pl-k">::</span><span class="pl-c1">Linear</span>)(x) <span class="pl-k">=</span> (f<span class="pl-k">.</span>w <span class="pl-k">*</span> x <span class="pl-k">.+</span> f<span class="pl-k">.</span>b)

<span class="pl-c"><span class="pl-c">#</span> Initialize a model as a callable object with parameters:</span>
f <span class="pl-k">=</span> <span class="pl-c1">Linear</span>(<span class="pl-c1">Param</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">10</span>,<span class="pl-c1">100</span>)), <span class="pl-c1">Param</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">10</span>)))

<span class="pl-c"><span class="pl-c">#</span> SGD training loop:</span>
<span class="pl-k">for</span> (x,y) <span class="pl-k">in</span> data
    loss <span class="pl-k">=</span> <span class="pl-c1">@diff</span> <span class="pl-c1">sum</span>(abs2,<span class="pl-c1">f</span>(x)<span class="pl-k">-</span>y)
    <span class="pl-k">for</span> w <span class="pl-k">in</span> <span class="pl-c1">params</span>(f)
        g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(loss,w)
	<span class="pl-c1">axpy!</span>(<span class="pl-k">-</span><span class="pl-c1">0.01</span>, g, w)
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p>See the <a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/examples">examples directory</a>
for more examples.</p>
<h2><a id="user-content-extending-autograd" class="anchor" aria-hidden="true" href="#extending-autograd"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Extending AutoGrad</h2>
<p>AutoGrad can only handle a function if the primitives it uses have known gradients.  You can
add your own primitives with gradients using the <code>@primitive</code> and <code>@zerograd</code> macros in
<a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/src/macros.jl">macros.jl</a> Here is an
example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@primitive</span> <span class="pl-c1">log</span>(x),dy,y  (dy <span class="pl-k">.*</span> (<span class="pl-c1">1</span> <span class="pl-k">./</span> x))</pre></div>
<p>The <code>@primitive</code> macro marks the <code>log(::Any)</code> method as a new primitive and the next
expression defines a gradient function wrt the first argument.  The gradient expressions can
refer to the parameter(s) <code>x</code>, the return variable <code>y</code> and its gradient <code>dy</code> (optionally
indicated after the argument list) in the method declaration. For functions with multiple
inputs multiple gradient expressions may be given. Non-existent or zero gradients can be
specified by omitting a gradient expression or using <code>nothing</code> in place of one. By default
the broadcasting version <code>log.(x)</code> is also defined as a primitive, use the <code>@primitive1</code>
macro if you don't want this.</p>
<p>Note that Julia supports multiple-dispatch, i.e. a function may have multiple methods each
supporting different argument types.  For example <code>log(::Float32)</code> and <code>log(::BigFloat)</code> are
two different log methods.  In AutoGrad.jl each method can be defined independently as a
primitive and can have its own specific gradient. Generally AutoGrad defines gradients
without using argument types to keep the rules generic.</p>
<h2><a id="user-content-debugging-and-profiling" class="anchor" aria-hidden="true" href="#debugging-and-profiling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Debugging and Profiling</h2>
<p>To view the contents of the computational graph after differentiating a function you can use
the following:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> AutoGrad<span class="pl-k">.</span><span class="pl-en">gcnode</span>(<span class="pl-k">::</span><span class="pl-c1">AutoGrad.Node</span>)<span class="pl-k">=</span><span class="pl-c1">nothing</span>  <span class="pl-c"><span class="pl-c">#</span> without this some values may be lost</span>
julia<span class="pl-k">&gt;</span> w <span class="pl-k">=</span> <span class="pl-c1">Param</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)); b <span class="pl-k">=</span> <span class="pl-c1">Param</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">1</span>)); x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">4</span>); y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>);
julia<span class="pl-k">&gt;</span> J <span class="pl-k">=</span> <span class="pl-c1">@diff</span> <span class="pl-c1">sum</span>(abs2, w<span class="pl-k">*</span>x <span class="pl-k">.+</span> b <span class="pl-k">-</span> y)
<span class="pl-c1">T</span>(<span class="pl-c1">14.695603907991153</span>)
julia<span class="pl-k">&gt;</span> [J]  <span class="pl-c"><span class="pl-c">#</span> displaying J in an Array causes pretty printing</span>
<span class="pl-c1">1.</span> <span class="pl-c1">P</span>(<span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)) ∇<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)
<span class="pl-c1">2.</span> <span class="pl-en">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>) <span class="pl-k">=</span> <span class="pl-k">*</span>(<span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>), <span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">3</span>,<span class="pl-c1">4</span>))) ∇<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>)
<span class="pl-c1">3.</span> <span class="pl-c1">P</span>(<span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">1</span>)) ∇<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">1</span>)
<span class="pl-c1">4.</span> <span class="pl-en">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>) <span class="pl-k">=</span> <span class="pl-c1">broadcast</span>(<span class="pl-k">+</span>, <span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>), <span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">1</span>))) ∇<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>)
<span class="pl-c1">5.</span> <span class="pl-en">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>) <span class="pl-k">=</span> <span class="pl-k">-</span>(<span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>), <span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>))) ∇<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>)
<span class="pl-c1">6.</span> <span class="pl-c1">14.695603907991153</span> <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2, <span class="pl-c1">Array</span><span class="pl-c1">{Float64,2}</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>))) ∇<span class="pl-k">=</span><span class="pl-c1">1.0</span>
julia<span class="pl-k">&gt;</span> z <span class="pl-k">=</span> <span class="pl-c1">collect</span>(J<span class="pl-k">.</span>list)  <span class="pl-c"><span class="pl-c">#</span> collect creates a Node array with reverse order</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">dump</span>(z[<span class="pl-c1">5</span>], maxdepth<span class="pl-k">=</span><span class="pl-c1">1</span>)  <span class="pl-c"><span class="pl-c">#</span> allowing you to look at individual Nodes and Values</span>
AutoGrad<span class="pl-k">.</span>Node
  Value<span class="pl-k">:</span> AutoGrad<span class="pl-k">.</span>Result{Array{Float64,<span class="pl-c1">2</span>}}
  parents<span class="pl-k">:</span> <span class="pl-c1">Array</span><span class="pl-c1">{AutoGrad.Node}</span>((<span class="pl-c1">2</span>,))
  children<span class="pl-k">:</span> <span class="pl-c1">Array</span><span class="pl-c1">{AutoGrad.Node}</span>((<span class="pl-c1">1</span>,))
  outgrad<span class="pl-k">:</span> <span class="pl-c1">Array</span><span class="pl-c1">{Float64}</span>((<span class="pl-c1">2</span>, <span class="pl-c1">4</span>)) [<span class="pl-c1">3.82753</span> <span class="pl-c1">2.19124</span> <span class="pl-c1">3.26769</span> <span class="pl-c1">3.0075</span>; <span class="pl-c1">2.81565</span> <span class="pl-c1">2.3903</span> <span class="pl-c1">1.84373</span> <span class="pl-c1">1.60228</span>]
  cdr<span class="pl-k">:</span> AutoGrad<span class="pl-k">.</span>Node
julia<span class="pl-k">&gt;</span> <span class="pl-c1">dump</span>(z[<span class="pl-c1">5</span>]<span class="pl-k">.</span>Value, maxdepth<span class="pl-k">=</span><span class="pl-c1">2</span>)
AutoGrad<span class="pl-k">.</span>Result{Array{Float64,<span class="pl-c1">2</span>}}
  value<span class="pl-k">:</span> <span class="pl-c1">Array</span><span class="pl-c1">{Float64}</span>((<span class="pl-c1">2</span>, <span class="pl-c1">4</span>)) [<span class="pl-c1">1.16724</span> <span class="pl-c1">1.07224</span> <span class="pl-c1">0.935047</span> <span class="pl-c1">0.895262</span>; <span class="pl-c1">0.687182</span> <span class="pl-c1">0.589704</span> <span class="pl-c1">0.517114</span> <span class="pl-c1">0.495718</span>]
  func<span class="pl-k">:</span> <span class="pl-k">*</span> (<span class="pl-k">function</span> of type <span class="pl-c1">typeof</span>(<span class="pl-k">*</span>))
  args<span class="pl-k">:</span> Tuple{Param{Array{Float64,<span class="pl-c1">2</span>}},Array{Float64,<span class="pl-c1">2</span>}}
    <span class="pl-c1">1</span><span class="pl-k">:</span> Param{Array{Float64,<span class="pl-c1">2</span>}}
    <span class="pl-c1">2</span><span class="pl-k">:</span> <span class="pl-c1">Array</span><span class="pl-c1">{Float64}</span>((<span class="pl-c1">3</span>, <span class="pl-c1">4</span>)) [<span class="pl-c1">0.515282</span> <span class="pl-c1">0.257471</span> <span class="pl-c1">0.140791</span> <span class="pl-c1">0.127632</span>; <span class="pl-c1">0.705288</span> <span class="pl-c1">0.783289</span> <span class="pl-c1">0.361965</span> <span class="pl-c1">0.311965</span>; <span class="pl-c1">0.780549</span> <span class="pl-c1">0.691645</span> <span class="pl-c1">0.853317</span> <span class="pl-c1">0.843374</span>]
  kwargs<span class="pl-k">:</span> Base<span class="pl-k">.</span>Iterators<span class="pl-k">.</span>Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}
    data<span class="pl-k">:</span> NamedTuple{(),Tuple{}} <span class="pl-c1">NamedTuple</span>()
    itr<span class="pl-k">:</span> Tuple{} ()</pre></div>
<p>To profile AutoGrad using TimerOutputs.jl, set the environment variable
<code>ENV["AUTOGRAD_TIMER"]="true"</code> and rebuild AutoGrad with <code>Pkg.build("AutoGrad")</code>, before
evaluating <code>using AutoGrad</code>. The environment variable <code>AUTOGRAD_TIMER</code> is only checked at
compile time, not at run time for performance reasons. This will collect detailed timing
information but slows the code down, when you are done don't forget to
<code>delete!(ENV,"AUTOGRAD_TIMER")</code> and rebuild AutoGrad. In the example below, the symbol <code>sum</code>
indicates the time spent on the forward pass of the <code>sum</code> function and <code>sum[2]</code> indicates
the time spent on the backward pass for the second argument. <code>record</code> and <code>sum_outgrads</code> are
functions internal to AutoGrad.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> ENV[<span class="pl-s"><span class="pl-pds">"</span>AUTOGRAD_TIMER<span class="pl-pds">"</span></span>]<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>true<span class="pl-pds">"</span></span>
julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">build</span>(<span class="pl-s"><span class="pl-pds">"</span>AutoGrad<span class="pl-pds">"</span></span>)
julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> AutoGrad, TimerOutputs
julia<span class="pl-k">&gt;</span> <span class="pl-c1">reset_timer!</span>(AutoGrad<span class="pl-k">.</span>to)
julia<span class="pl-k">&gt;</span> w <span class="pl-k">=</span> <span class="pl-c1">Param</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)); b <span class="pl-k">=</span> <span class="pl-c1">Param</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">1</span>)); x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">4</span>); y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">4</span>);
julia<span class="pl-k">&gt;</span> J <span class="pl-k">=</span> <span class="pl-c1">@diff</span> <span class="pl-c1">sum</span>(abs2, w<span class="pl-k">*</span>x <span class="pl-k">.+</span> b <span class="pl-k">-</span> y)
julia<span class="pl-k">&gt;</span> AutoGrad<span class="pl-k">.</span>to
───────────────────────────────────────────────────────────────────────
                                Time                   Allocations      
                        ──────────────────────   ───────────────────────
    Tot <span class="pl-k">/</span> <span class="pl-k">%</span> measured<span class="pl-k">:</span>        <span class="pl-c1">4.62</span>s <span class="pl-k">/</span> <span class="pl-c1">30.4</span><span class="pl-k">%</span>            <span class="pl-c1">546</span>MiB <span class="pl-k">/</span> <span class="pl-c1">25.0</span><span class="pl-k">%</span>    

 Section        ncalls     time   <span class="pl-k">%</span>tot     avg     alloc   <span class="pl-k">%</span>tot      avg
 ───────────────────────────────────────────────────────────────────────
 <span class="pl-k">+</span>.[<span class="pl-c1">2</span>]               <span class="pl-c1">1</span>    <span class="pl-c1">328</span>ms  <span class="pl-c1">23.3</span><span class="pl-k">%</span>   <span class="pl-c1">328</span>ms   <span class="pl-c1">46.4</span>MiB  <span class="pl-c1">34.1</span><span class="pl-k">%</span>  <span class="pl-c1">46.4</span>MiB
 sum[<span class="pl-c1">2</span>]              <span class="pl-c1">1</span>    <span class="pl-c1">288</span>ms  <span class="pl-c1">20.5</span><span class="pl-k">%</span>   <span class="pl-c1">288</span>ms   <span class="pl-c1">40.0</span>MiB  <span class="pl-c1">29.4</span><span class="pl-k">%</span>  <span class="pl-c1">40.0</span>MiB
   <span class="pl-k">*</span>                 <span class="pl-c1">1</span>   <span class="pl-c1">38.8</span>ms  <span class="pl-c1">2.76</span><span class="pl-k">%</span>  <span class="pl-c1">38.8</span>ms    <span class="pl-c1">595</span>KiB  <span class="pl-c1">0.43</span><span class="pl-k">%</span>   <span class="pl-c1">595</span>KiB
 <span class="pl-k">*</span>                   <span class="pl-c1">1</span>    <span class="pl-c1">269</span>ms  <span class="pl-c1">19.2</span><span class="pl-k">%</span>   <span class="pl-c1">269</span>ms    <span class="pl-c1">955</span>KiB  <span class="pl-c1">0.68</span><span class="pl-k">%</span>   <span class="pl-c1">955</span>KiB
 <span class="pl-k">+</span>.                  <span class="pl-c1">1</span>    <span class="pl-c1">139</span>ms  <span class="pl-c1">9.92</span><span class="pl-k">%</span>   <span class="pl-c1">139</span>ms   <span class="pl-c1">20.4</span>MiB  <span class="pl-c1">15.0</span><span class="pl-k">%</span>  <span class="pl-c1">20.4</span>MiB
 <span class="pl-k">*</span>[<span class="pl-c1">1</span>]                <span class="pl-c1">1</span>    <span class="pl-c1">117</span>ms  <span class="pl-c1">8.33</span><span class="pl-k">%</span>   <span class="pl-c1">117</span>ms   <span class="pl-c1">9.41</span>MiB  <span class="pl-c1">6.90</span><span class="pl-k">%</span>  <span class="pl-c1">9.41</span>MiB
 record              <span class="pl-c1">4</span>   <span class="pl-c1">88.7</span>ms  <span class="pl-c1">6.31</span><span class="pl-k">%</span>  <span class="pl-c1">22.2</span>ms   <span class="pl-c1">3.49</span>MiB  <span class="pl-c1">2.56</span><span class="pl-k">%</span>   <span class="pl-c1">894</span>KiB
 <span class="pl-k">-</span>[<span class="pl-c1">1</span>]                <span class="pl-c1">1</span>   <span class="pl-c1">65.9</span>ms  <span class="pl-c1">4.69</span><span class="pl-k">%</span>  <span class="pl-c1">65.9</span>ms   <span class="pl-c1">10.0</span>MiB  <span class="pl-c1">7.32</span><span class="pl-k">%</span>  <span class="pl-c1">10.0</span>MiB
 <span class="pl-k">-</span>                   <span class="pl-c1">1</span>   <span class="pl-c1">55.8</span>ms  <span class="pl-c1">3.97</span><span class="pl-k">%</span>  <span class="pl-c1">55.8</span>ms    <span class="pl-c1">929</span>KiB  <span class="pl-c1">0.67</span><span class="pl-k">%</span>   <span class="pl-c1">929</span>KiB
 sum                 <span class="pl-c1">1</span>   <span class="pl-c1">50.0</span>ms  <span class="pl-c1">3.56</span><span class="pl-k">%</span>  <span class="pl-c1">50.0</span>ms   <span class="pl-c1">4.68</span>MiB  <span class="pl-c1">3.44</span><span class="pl-k">%</span>  <span class="pl-c1">4.68</span>MiB
 <span class="pl-k">+</span>.[<span class="pl-c1">1</span>]               <span class="pl-c1">1</span>   <span class="pl-c1">1.78</span>ms  <span class="pl-c1">0.13</span><span class="pl-k">%</span>  <span class="pl-c1">1.78</span>ms   <span class="pl-c1">37.7</span>KiB  <span class="pl-c1">0.03</span><span class="pl-k">%</span>  <span class="pl-c1">37.7</span>KiB
 sum_outgrads        <span class="pl-c1">5</span>   <span class="pl-c1">1.41</span>ms  <span class="pl-c1">0.10</span><span class="pl-k">%</span>   <span class="pl-c1">282</span>μs   <span class="pl-c1">28.2</span>KiB  <span class="pl-c1">0.02</span><span class="pl-k">%</span>  <span class="pl-c1">5.64</span>KiB
 ───────────────────────────────────────────────────────────────────────</pre></div>
<h2><a id="user-content-code-structure" class="anchor" aria-hidden="true" href="#code-structure"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code structure</h2>
<p><a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/src/core.jl">core.jl</a> implements the
main functionality and acts as the main documentation source.
<a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/src/macros.jl">macros.jl</a> has some
support functions to define and test new primitives.
<a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/src/getindex.jl">getindex.jl</a>,
<a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/src/iterate.jl">iterate.jl</a> and
<a href="https://github.com/denizyuret/AutoGrad.jl/blob/master/src/cat.jl">cat.jl</a> set up support
for common data structures including Arrays, Tuples, and Dictionaries.  The numerical
gradients are defined in files such as <code>base.jl</code> and <code>math.jl</code>.</p>
<h2><a id="user-content-current-status-and-future-work" class="anchor" aria-hidden="true" href="#current-status-and-future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Current status and future work</h2>
<p>The gradient coverage and unit testing are spotty, I am still adding more gradients and
tests to cover the Julia base. Documentation needs to be improved. Overwriting functions
(e.g. <code>setindex!</code>) are not supported. Efficiency could be improved by reducing runtime
compilation, memoization, and support for static computation.</p>
<h2><a id="user-content-acknowledgments-and-references" class="anchor" aria-hidden="true" href="#acknowledgments-and-references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgments and references</h2>
<p>AutoGrad.jl was written by <a href="http://www.denizyuret.com" rel="nofollow">Deniz Yuret</a>. Parts of the code were
initially ported from the Python <a href="https://github.com/HIPS/autograd">autograd</a> package.  I'd
like to thank autograd author Dougal Maclaurin for his support.  See <a href="https://arxiv.org/abs/1502.05767" rel="nofollow">(Baydin et
al. 2015)</a> for a general review of automatic
differentiation, <a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md">autograd
tutorial</a> for some Python
examples, and Dougal's PhD thesis for design principles.
<a href="http://www.juliadiff.org/" rel="nofollow">JuliaDiff</a> and <a href="https://github.com/FluxML">FluxML</a> have
alternative differentiation tools for Julia.  I would like to thank the current
contributors:</p>
<ul>
<li>Carlo Lucibello</li>
<li>Ekin Akyürek</li>
<li>Emre Yolcu</li>
<li>Jarrett Revels</li>
<li>Mike Innes</li>
<li>Ozan Arkan Can</li>
<li>Rene Donner</li>
</ul>
<p>The suggested citation for AutoGrad is:</p>
<pre><code>@inproceedings{knet2016mlsys,
  author={Yuret, Deniz},
  title={Knet: beginning deep learning with 100 lines of Julia},
  year={2016},
  booktitle={Machine Learning Systems Workshop at NIPS 2016}
}
</code></pre>
</article></div>