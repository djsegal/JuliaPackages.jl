<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-factorloadingmatricesjl" class="anchor" aria-hidden="true" href="#factorloadingmatricesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FactorLoadingMatrices.jl</h1>
<p dir="auto"><em>Julia package to construct loading matrices for factor analysis</em></p>
<p dir="auto"><a href="https://github.com/eloceanografo/FactorLoadingMatrices.jl/actions"><img src="https://github.com/eloceanografo/FactorLoadingMatrices.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a></p>
<p dir="auto">This is a lightweight package to construct loading matrices for probabilistic <a href="https://en.wikipedia.org/wiki/Factor_analysis" rel="nofollow">factor analysis</a> and dimensionality reduction.  If you just need traditional factor analysis, that's available in <a href="https://github.com/JuliaStats/MultivariateStats.jl">MultivariateStats.jl</a>.  However, if you are doing factor analysis in a Bayesian or probabilistic context, or as part of a more complex statistical model, the utilities in this package may be useful.</p>
<h2 dir="auto"><a id="user-content-factor-analysis-and-loading-matrices" class="anchor" aria-hidden="true" href="#factor-analysis-and-loading-matrices"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Factor analysis and loading matrices</h2>
<p dir="auto">Factor analysis is a statistical method where  <em>n</em>-dimensional vector-valued variables <strong>x</strong> are represented as linear combination <em>m</em>-dimensional vectors of "factors" <strong>f</strong>. This linear combination is specified by an <em>n × m</em> loading matrix <em>L</em>,</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=\mathbf{x}_i = L \mathbf{f}_i"><img src="https://render.githubusercontent.com/render/math?math=\mathbf{x}_i = L \mathbf{f}_i" style="max-width: 100%;"></a>,</p>
<p dir="auto">where <em>i</em> indexes each observation. If we collect all observations of <strong>x</strong> and <strong>f</strong> in the columns of matrices <em>X</em> and <em>F</em>, we can write the system as</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math={X = L F}"><img src="https://render.githubusercontent.com/render/math?math={X = L F}" style="max-width: 100%;"></a>.</p>
<p dir="auto">Factor analysis is useful when the elements of <strong>x</strong> are correlated with each other, so most of their variability can be captured using a small number of factors.  That means we can set <em>m &lt; n</em>, and that <em>L</em> will be a rectangular matrix with more rows than columns.  Factor analysis is thus related to other dimensionality-reduction methods, like principle-component analysis/empirical orthogonal functions (PCA/EOF).</p>
<p dir="auto">There is no unique way to do this decomposition, but we want all the columns of <em>L</em> to be linearly independent.  A simple way to enforce this requirement is to set all entries above the diagonal to zero.  <code>FactorLoadingMatrices</code> exports two functions for constructing matrices with this property:</p>
<ul dir="auto">
<li><code>nnz_loading(nx, nfactor)</code> calculates the number of nonzero entries in a lower-triangular matrix with size <code>(nx, nfactor)</code>.</li>
<li><code>loading_matrix(values, nx, nfactor)</code> arranges the numers in the vector <code>values</code> in the lower triangle of the matrix with the specified size.</li>
</ul>
<p dir="auto">The following example shows how to use them.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using FactorLoadingMatrices

julia&gt; nx = 5;

julia&gt; nfactor = 3;

julia&gt; nnz = nnz_loading(5, 3)
12

julia&gt; L = loading_matrix(randn(nnz), nx, nfactor)
5×3 Array{Float64,2}:
 -1.3836     0.0         0.0
 -1.6667     0.436374    0.0
  0.285922  -0.0585805  -1.0485
  0.512425  -0.457097   -1.43772
 -2.74867   -0.128697    0.301587"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> FactorLoadingMatrices

julia<span class="pl-k">&gt;</span> nx <span class="pl-k">=</span> <span class="pl-c1">5</span>;

julia<span class="pl-k">&gt;</span> nfactor <span class="pl-k">=</span> <span class="pl-c1">3</span>;

julia<span class="pl-k">&gt;</span> nnz <span class="pl-k">=</span> <span class="pl-c1">nnz_loading</span>(<span class="pl-c1">5</span>, <span class="pl-c1">3</span>)
<span class="pl-c1">12</span>

julia<span class="pl-k">&gt;</span> L <span class="pl-k">=</span> <span class="pl-c1">loading_matrix</span>(<span class="pl-c1">randn</span>(nnz), nx, nfactor)
<span class="pl-c1">5</span><span class="pl-k">×</span><span class="pl-c1">3</span> Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-k">-</span><span class="pl-c1">1.3836</span>     <span class="pl-c1">0.0</span>         <span class="pl-c1">0.0</span>
 <span class="pl-k">-</span><span class="pl-c1">1.6667</span>     <span class="pl-c1">0.436374</span>    <span class="pl-c1">0.0</span>
  <span class="pl-c1">0.285922</span>  <span class="pl-k">-</span><span class="pl-c1">0.0585805</span>  <span class="pl-k">-</span><span class="pl-c1">1.0485</span>
  <span class="pl-c1">0.512425</span>  <span class="pl-k">-</span><span class="pl-c1">0.457097</span>   <span class="pl-k">-</span><span class="pl-c1">1.43772</span>
 <span class="pl-k">-</span><span class="pl-c1">2.74867</span>   <span class="pl-k">-</span><span class="pl-c1">0.128697</span>    <span class="pl-c1">0.301587</span></pre></div>
<p dir="auto">This package also exports a function <code>varimax</code> (modified from the implementation by Haotian Li in <a href="https://github.com/haotian127/NGWP.jl">NGWP.jl</a>) to perform the <a href="https://en.wikipedia.org/wiki/Varimax_rotation" rel="nofollow">varimax</a> rotation on loading matrices.</p>
<h2 dir="auto"><a id="user-content-bayesian-factor-analysis-using-turing" class="anchor" aria-hidden="true" href="#bayesian-factor-analysis-using-turing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Bayesian factor analysis using Turing</h2>
<p dir="auto">The next example shows how the utility functions in <code>FactorLoadingMatrices.jl</code> make it easy to perform Bayesian probabilistic factor analysis in Turing.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using FactorLoadingMatrices
using Random
using Plots
using Turing
# using Memoization and reverse-mode autodiff speeds up MCMC sampling
# *a lot* when fitting high-dimensional models
using Memoization
using ReverseDiff
Turing.setadbackend(:reversediff)

Random.seed!(1)

nx = 50
nfactor = 3
nobs = 300
σ = 0.5

vals = randn(nnz_loading(nx, nfactor))
L = loading_matrix(vals, nx, nfactor)

F = randn(nfactor, nobs)
X = L * F .+ σ*randn()

@model function FactorModel(X, nfactor)
    nx, nobs = size(X)
    # set priors for factors, loading matrix, and observation noise
    F ~ filldist(Normal(0, 1), nfactor, nobs)
    vals ~ filldist(Normal(0, 2), nnz_loading(nx, nfactor))
    σ ~ Exponential(1.0)
    # make the loading matrix
    L = loading_matrix(vals, nx, nfactor)
    # observation likelihood
    X ~ arraydist(Normal.(L * F, σ))
end

mod = FactorModel(X, nfactor)
chn = sample(mod, NUTS(), 100)"><pre><span class="pl-k">using</span> FactorLoadingMatrices
<span class="pl-k">using</span> Random
<span class="pl-k">using</span> Plots
<span class="pl-k">using</span> Turing
<span class="pl-c"><span class="pl-c">#</span> using Memoization and reverse-mode autodiff speeds up MCMC sampling</span>
<span class="pl-c"><span class="pl-c">#</span> *a lot* when fitting high-dimensional models</span>
<span class="pl-k">using</span> Memoization
<span class="pl-k">using</span> ReverseDiff
Turing<span class="pl-k">.</span><span class="pl-c1">setadbackend</span>(<span class="pl-c1">:reversediff</span>)

Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">1</span>)

nx <span class="pl-k">=</span> <span class="pl-c1">50</span>
nfactor <span class="pl-k">=</span> <span class="pl-c1">3</span>
nobs <span class="pl-k">=</span> <span class="pl-c1">300</span>
σ <span class="pl-k">=</span> <span class="pl-c1">0.5</span>

vals <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">nnz_loading</span>(nx, nfactor))
L <span class="pl-k">=</span> <span class="pl-c1">loading_matrix</span>(vals, nx, nfactor)

F <span class="pl-k">=</span> <span class="pl-c1">randn</span>(nfactor, nobs)
X <span class="pl-k">=</span> L <span class="pl-k">*</span> F <span class="pl-k">.+</span> σ<span class="pl-k">*</span><span class="pl-c1">randn</span>()

<span class="pl-c1">@model</span> <span class="pl-k">function</span> <span class="pl-en">FactorModel</span>(X, nfactor)
    nx, nobs <span class="pl-k">=</span> <span class="pl-c1">size</span>(X)
    <span class="pl-c"><span class="pl-c">#</span> set priors for factors, loading matrix, and observation noise</span>
    F <span class="pl-k">~</span> <span class="pl-c1">filldist</span>(<span class="pl-c1">Normal</span>(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>), nfactor, nobs)
    vals <span class="pl-k">~</span> <span class="pl-c1">filldist</span>(<span class="pl-c1">Normal</span>(<span class="pl-c1">0</span>, <span class="pl-c1">2</span>), <span class="pl-c1">nnz_loading</span>(nx, nfactor))
    σ <span class="pl-k">~</span> <span class="pl-c1">Exponential</span>(<span class="pl-c1">1.0</span>)
    <span class="pl-c"><span class="pl-c">#</span> make the loading matrix</span>
    L <span class="pl-k">=</span> <span class="pl-c1">loading_matrix</span>(vals, nx, nfactor)
    <span class="pl-c"><span class="pl-c">#</span> observation likelihood</span>
    X <span class="pl-k">~</span> <span class="pl-c1">arraydist</span>(<span class="pl-c1">Normal</span>.(L <span class="pl-k">*</span> F, σ))
<span class="pl-k">end</span>

mod <span class="pl-k">=</span> <span class="pl-c1">FactorModel</span>(X, nfactor)
chn <span class="pl-k">=</span> <span class="pl-c1">sample</span>(mod, <span class="pl-c1">NUTS</span>(), <span class="pl-c1">100</span>)</pre></div>
<p dir="auto">Once the sampler finishes running, we can extract the posterior for <em>L</em>.  Note that the matrix is sampled inside the model as a flat vector of its nonzero entries, so we have to reconstruct the loading matrices from those.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="vals_post = Array(group(chn, :vals))
L_post = [loading_matrix(v, nx, nfactor) for v in eachrow(vals_post)]"><pre>vals_post <span class="pl-k">=</span> <span class="pl-c1">Array</span>(<span class="pl-c1">group</span>(chn, <span class="pl-c1">:vals</span>))
L_post <span class="pl-k">=</span> [<span class="pl-c1">loading_matrix</span>(v, nx, nfactor) <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">eachrow</span>(vals_post)]</pre></div>
<p dir="auto">Before comparing the matrices in <code>L_post</code> to the true <code>L</code>, we'll apply <code>varimax</code> to each one.  There's no guarantee that the model will converge on the same <code>L</code> and <code>F</code> we started with, but they should be the same (approximately) after applying a variance-maximizing rotation.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="L_post_vm = varimax.(L_post)

plot(mean(L_post_vm), 1:nx, xerror=2std(L_post_vm), markerstrokecolor=1,
    layout=(1, 3), label=&quot;Model&quot;, title=[&quot;L[:, 1]&quot; &quot;L[:, 2]&quot; &quot;L[:, 3]&quot;])
plot!(varimax(L), 1:nx, layout=(1, 3), label=&quot;Truth&quot;)"><pre>L_post_vm <span class="pl-k">=</span> <span class="pl-c1">varimax</span>.(L_post)

<span class="pl-c1">plot</span>(<span class="pl-c1">mean</span>(L_post_vm), <span class="pl-c1">1</span><span class="pl-k">:</span>nx, xerror<span class="pl-k">=</span><span class="pl-c1">2</span><span class="pl-c1">std</span>(L_post_vm), markerstrokecolor<span class="pl-k">=</span><span class="pl-c1">1</span>,
    layout<span class="pl-k">=</span>(<span class="pl-c1">1</span>, <span class="pl-c1">3</span>), label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Model<span class="pl-pds">"</span></span>, title<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>L[:, 1]<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>L[:, 2]<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>L[:, 3]<span class="pl-pds">"</span></span>])
<span class="pl-c1">plot!</span>(<span class="pl-c1">varimax</span>(L), <span class="pl-c1">1</span><span class="pl-k">:</span>nx, layout<span class="pl-k">=</span>(<span class="pl-c1">1</span>, <span class="pl-c1">3</span>), label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Truth<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="mcmc_factors.png"><img src="mcmc_factors.png" alt="MCMC posteriors for loading matrix" style="max-width: 100%;"></a></p>
<p dir="auto">Columns 1 and 3 of the rotated matrix were recovered quite well by the model, though it struggled a bit with column 2.</p>
<p dir="auto">This is a relatively simple use case, but it would be straightforward to add other layers and processes to the model (e.g. covariates, non-Gaussian observations, etc.).</p>
</article></div>