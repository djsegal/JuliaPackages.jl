<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-runtestsjl" class="anchor" aria-hidden="true" href="#runtestsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>RunTests.jl</h1>
<p dir="auto"><a href="https://travis-ci.org/burrowsa/RunTests.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e46fd88348f4e3b0c1365b791e8f12af961bb675c35d7851d2699ff7e32ca6e9/68747470733a2f2f7472617669732d63692e6f72672f627572726f7773612f52756e54657374732e6a6c2e706e673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/burrowsa/RunTests.jl.png?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">RunTests.jl is a test running framework for Julia. In its simplest form RunTests.jl saves you from writing <code>test/runtests.jl</code> scripts that look like this:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="my_tests = [&quot;sometests.jl&quot;,
            &quot;somemoretests.jl&quot;,
            &quot;evenmoretests.jl&quot;]

println(&quot;Running tests:&quot;)
for my_test in my_tests
  include(my_test)
end"><pre class="notranslate"><code>my_tests = ["sometests.jl",
            "somemoretests.jl",
            "evenmoretests.jl"]

println("Running tests:")
for my_test in my_tests
  include(my_test)
end
</code></pre></div>
<p dir="auto">and allows you to write them, more simply, like this:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using RunTests
exit(run_tests())"><pre class="notranslate"><code>using RunTests
exit(run_tests())
</code></pre></div>
<p dir="auto">Or if you wish to be explicit what test files are run and in what order (not recomended, ideally the order tests are run should not matter) you can do this:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using RunTests
exit(run_tests([&quot;sometests.jl&quot;,
                &quot;somemoretests.jl&quot;,
                &quot;evenmoretests.jl&quot;]))"><pre class="notranslate"><code>using RunTests
exit(run_tests(["sometests.jl",
                "somemoretests.jl",
                "evenmoretests.jl"]))
</code></pre></div>
<p dir="auto">But it has more to offer than that! RunTests.jl builds on top of Julia's <code>Base.Test</code> library to make it easy to add structure to your tests. Structuring your tests with RunTests.jl gives the following advantages:</p>
<ul dir="auto">
<li>All the tests are run - the tests script doesn't bomb out after the first failure so you can see all your test results at once.</li>
<li>A summary of how many tests passed/failed is produced so you can judge at a glance how the test run went.</li>
<li>The <code>STDOUT</code> and <code>STDERR</code> output from each test is captured and reported along with the details of the test failure.</li>
<li>You get a progress bar showing how far through the tests you are, it is green while all the tests are passing and goes red if any fail</li>
<li>Using modules and functions to structure test files gives you a natural isolation between tests.</li>
<li>You can selectively skip tests with <code>@skipif</code> and mark failing tests with <code>@xfail</code>.</li>
<li>Using <code>@parameterize</code> you can run the same test again again with different parameters and see which pass and which fail.</li>
</ul>
<p dir="auto">Here is an example test file written using RunTests.jl that demonstrates a number of features of the package:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using RunTests
using Base.Test

@testmodule ExampleTests begin

  function test_one()
      @test true
  end

  function test_two()
    println(&quot;seen&quot;)
    @test true
    println(&quot;also seen&quot;)
    @test false
    println(&quot;never seen&quot;)
  end

  @skipif false function test_not_skipped()
    @test true
  end

  @skipif true function test_skipped()
    @test true
  end

  @xfail function test_xfails()
    @test false
  end

  @xfail function test_xpasses()
    @test true
  end

  @parameterize 1:4 function test_parameterized(x)
    @test x&lt;3
  end

end"><pre class="notranslate"><code>using RunTests
using Base.Test

@testmodule ExampleTests begin

  function test_one()
      @test true
  end

  function test_two()
    println("seen")
    @test true
    println("also seen")
    @test false
    println("never seen")
  end

  @skipif false function test_not_skipped()
    @test true
  end

  @skipif true function test_skipped()
    @test true
  end

  @xfail function test_xfails()
    @test false
  end

  @xfail function test_xpasses()
    @test true
  end

  @parameterize 1:4 function test_parameterized(x)
    @test x&lt;3
  end

end
</code></pre></div>
<p dir="auto">Running the file will run the tests and you will get this output:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Running 10 tests 100%|##############################| Time: 0:00:01

Tests:
======

ExampleTests.test_not_skipped PASSED
ExampleTests.test_one PASSED
ExampleTests.test_parameterized[1] PASSED
ExampleTests.test_parameterized[2] PASSED
ExampleTests.test_parameterized[3] FAILED
ExampleTests.test_parameterized[4] FAILED
ExampleTests.test_skipped SKIPPED
ExampleTests.test_two FAILED
ExampleTests.test_xfails XFAILED
ExampleTests.test_xpasses XPASSED

=================================== Failures ===================================

---------------------- ExampleTests.test_parameterized[3] ---------------------

test failed: :((x&lt;3))
 in error at error.jl:21
 in default_handler at test.jl:19
 in do_test at test.jl:39

 --------------------------------------------------------------------------------

---------------------- ExampleTests.test_parameterized[4] ---------------------

test failed: :((x&lt;3))
 in error at error.jl:21
 in default_handler at test.jl:19
 in do_test at test.jl:39

--------------------------------------------------------------------------------

----------------------------- ExampleTests.test_two ----------------------------

test failed: false
 in error at error.jl:21
 in default_handler at test.jl:19

Captured Output:
================

seen
also seen

--------------------------------------------------------------------------------    

================ 3 failed 4 passed 1 skipped 1 xfailed 1 xpassed ==============="><pre class="notranslate"><code>Running 10 tests 100%|##############################| Time: 0:00:01

Tests:
======

ExampleTests.test_not_skipped PASSED
ExampleTests.test_one PASSED
ExampleTests.test_parameterized[1] PASSED
ExampleTests.test_parameterized[2] PASSED
ExampleTests.test_parameterized[3] FAILED
ExampleTests.test_parameterized[4] FAILED
ExampleTests.test_skipped SKIPPED
ExampleTests.test_two FAILED
ExampleTests.test_xfails XFAILED
ExampleTests.test_xpasses XPASSED

=================================== Failures ===================================

---------------------- ExampleTests.test_parameterized[3] ---------------------

test failed: :((x&lt;3))
 in error at error.jl:21
 in default_handler at test.jl:19
 in do_test at test.jl:39

 --------------------------------------------------------------------------------

---------------------- ExampleTests.test_parameterized[4] ---------------------

test failed: :((x&lt;3))
 in error at error.jl:21
 in default_handler at test.jl:19
 in do_test at test.jl:39

--------------------------------------------------------------------------------

----------------------------- ExampleTests.test_two ----------------------------

test failed: false
 in error at error.jl:21
 in default_handler at test.jl:19

Captured Output:
================

seen
also seen

--------------------------------------------------------------------------------    

================ 3 failed 4 passed 1 skipped 1 xfailed 1 xpassed ===============
</code></pre></div>
<p dir="auto">But you can also run the file along with many others by putting them under the same directory (sub directories work too) and running them all together with:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using RunTests
exit(run_tests(&lt;path_to_directory_containing_tests&gt;))"><pre class="notranslate"><code>using RunTests
exit(run_tests(&lt;path_to_directory_containing_tests&gt;))
</code></pre></div>
<p dir="auto">When you run many test files together, like this, all their tests are pooled and you get one report for them all. If you don't specify a directory <code>run_tests</code> will default to running tests from the "test" folder.</p>
<p dir="auto">RunTests.jl is extensible, in fact <code>@xfail</code>, <code>@skipif</code> and <code>@parameterize</code> are implemented as extensions. You can extend RunTests.jl to add further types of tests or categories of test result.</p>
</article></div>