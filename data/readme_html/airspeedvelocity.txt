<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-airspeedvelocityjl" class="anchor" aria-hidden="true" href="#airspeedvelocityjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>AirspeedVelocity.jl</h1>
<p dir="auto"><a href="https://MilesCranmer.github.io/AirspeedVelocity.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://MilesCranmer.github.io/AirspeedVelocity.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/MilesCranmer/AirspeedVelocity.jl/actions/workflows/CI.yml?query=branch%3Amaster"><img src="https://github.com/MilesCranmer/AirspeedVelocity.jl/actions/workflows/CI.yml/badge.svg?branch=master" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://coveralls.io/github/MilesCranmer/AirspeedVelocity.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/b1bc45648a1e8dcfc8e9b9bdca229b89e535750f7f4d7db2a57d6bb22ce55cf9/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4d696c65734372616e6d65722f416972737065656456656c6f636974792e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage" data-canonical-src="https://coveralls.io/repos/github/MilesCranmer/AirspeedVelocity.jl/badge.svg?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">AirspeedVelocity.jl strives to make it easy to benchmark Julia packages over their lifetime.
It is inspired by <a href="https://asv.readthedocs.io/en/stable/" rel="nofollow">asv</a>.</p>
<p dir="auto">This package allows you to:</p>
<ul dir="auto">
<li>Generate benchmarks directly from the terminal with an easy-to-use CLI.</li>
<li>Compare many commits/tags/branches at once.</li>
<li>Plot those benchmarks, automatically flattening your benchmark suite into a list of plots with generated titles.</li>
<li>Run as a GitHub action to create benchmark comparisons for every submitted PR (in a bot comment).</li>
</ul>
<p dir="auto">This package also freezes the benchmark script at a particular revision,
so there is no worry about the old history overwriting the benchmark.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">You can install the CLI with:</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia -e 'using Pkg; Pkg.add(&quot;AirspeedVelocity&quot;); Pkg.build(&quot;AirspeedVelocity&quot;)'"><pre>julia -e <span class="pl-s"><span class="pl-pds">'</span>using Pkg; Pkg.add("AirspeedVelocity"); Pkg.build("AirspeedVelocity")<span class="pl-pds">'</span></span></pre></div>
<p dir="auto">This will install two executables at <code>~/.julia/bin</code> - make sure to have it on your <code>PATH</code>.</p>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">You may then use the CLI to generate benchmarks for any package with, e.g.,</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="benchpkg Transducers \
    --rev=v0.4.20,v0.4.70,master \
    --bench-on=v0.4.20"><pre>benchpkg Transducers \
    --rev=v0.4.20,v0.4.70,master \
    --bench-on=v0.4.20</pre></div>
<p dir="auto">which will benchmark <code>Transducers.jl</code>,
at the revisions <code>v0.4.20</code>, <code>v0.4.70</code>, and <code>master</code>,
using the benchmark script <code>benchmark/benchmarks.jl</code> as it was defined at <code>v0.4.20</code>,
and then save the JSON results in the current directory.</p>
<p dir="auto">We can then generate plots of the revisions with:</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="benchpkgplot Transducers \
    --rev=v0.4.20,v0.4.70,master \
    --format=pdf \
    --npart=5"><pre>benchpkgplot Transducers \
    --rev=v0.4.20,v0.4.70,master \
    --format=pdf \
    --npart=5</pre></div>
<p dir="auto">which will generate a pdf file for each set of 5 plots,
showing the change with each revision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/7593028/229543368-14b1da88-8315-437b-b38f-fff143f26e3a.png"><img width="877" alt="Screenshot 2023-04-03 at 10 36 16 AM" src="https://user-images.githubusercontent.com/7593028/229543368-14b1da88-8315-437b-b38f-fff143f26e3a.png" style="max-width: 100%;"></a></p>
<p dir="auto">You can also provide a custom benchmark.
For example, let's say you have a file <code>script.jl</code>, defining
a benchmark for <code>SymbolicRegression.jl</code> (we always need to define
the <code>SUITE</code> variable as a <code>BenchmarkGroup</code>):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using BenchmarkTools, SymbolicRegression
const SUITE = BenchmarkGroup()

# Create hierarchy of benchmarks:
SUITE[&quot;eval_tree_array&quot;] = BenchmarkGroup()

options = Options(; binary_operators=[+, -, *], unary_operators=[cos])
tree = Node(; feature=1) + cos(3.2f0 * Node(; feature=2))


for n in [10, 20]
    SUITE[&quot;eval_tree_array&quot;][n] = @benchmarkable(
        eval_tree_array($tree, X, $options),
        evals=10,
        samples=1000,
        setup=(X=randn(Float32, 2, $n))
    )
end
"><pre><span class="pl-k">using</span> BenchmarkTools, SymbolicRegression
<span class="pl-k">const</span> SUITE <span class="pl-k">=</span> <span class="pl-c1">BenchmarkGroup</span>()

<span class="pl-c"><span class="pl-c">#</span> Create hierarchy of benchmarks:</span>
SUITE[<span class="pl-s"><span class="pl-pds">"</span>eval_tree_array<span class="pl-pds">"</span></span>] <span class="pl-k">=</span> <span class="pl-c1">BenchmarkGroup</span>()

options <span class="pl-k">=</span> <span class="pl-c1">Options</span>(; binary_operators<span class="pl-k">=</span>[<span class="pl-k">+</span>, <span class="pl-k">-</span>, <span class="pl-k">*</span>], unary_operators<span class="pl-k">=</span>[cos])
tree <span class="pl-k">=</span> <span class="pl-c1">Node</span>(; feature<span class="pl-k">=</span><span class="pl-c1">1</span>) <span class="pl-k">+</span> <span class="pl-c1">cos</span>(<span class="pl-c1">3.2f0</span> <span class="pl-k">*</span> <span class="pl-c1">Node</span>(; feature<span class="pl-k">=</span><span class="pl-c1">2</span>))


<span class="pl-k">for</span> n <span class="pl-k">in</span> [<span class="pl-c1">10</span>, <span class="pl-c1">20</span>]
    SUITE[<span class="pl-s"><span class="pl-pds">"</span>eval_tree_array<span class="pl-pds">"</span></span>][n] <span class="pl-k">=</span> <span class="pl-c1">@benchmarkable</span>(
        <span class="pl-c1">eval_tree_array</span>(<span class="pl-k">$</span>tree, X, <span class="pl-k">$</span>options),
        evals<span class="pl-k">=</span><span class="pl-c1">10</span>,
        samples<span class="pl-k">=</span><span class="pl-c1">1000</span>,
        setup<span class="pl-k">=</span>(X<span class="pl-k">=</span><span class="pl-c1">randn</span>(Float32, <span class="pl-c1">2</span>, <span class="pl-k">$</span>n))
    )
<span class="pl-k">end</span>
</pre></div>
<p dir="auto">Inside this script, we will also have access to the <code>PACKAGE_VERSION</code> constant,
to allow for different behavior depending on tag.
We can run this benchmark over the history of <code>SymbolicRegression.jl</code> with:</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="benchpkg SymbolicRegression \
    -r v0.15.3,v0.16.2 \
    -s script.jl \
    -o results/ \
    --exeflags=&quot;--threads=4 -O3&quot;"><pre>benchpkg SymbolicRegression \
    -r v0.15.3,v0.16.2 \
    -s script.jl \
    -o results/ \
    --exeflags=<span class="pl-s"><span class="pl-pds">"</span>--threads=4 -O3<span class="pl-pds">"</span></span></pre></div>
<p dir="auto">where we have also specified the output directory and extra flags to pass to the
<code>julia</code> executable. We can also now visualize this:</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="benchpkgplot SymbolicRegression \
    -r v0.15.3,v0.16.2 \
    -i results/ \
    -o plots/"><pre>benchpkgplot SymbolicRegression \
    -r v0.15.3,v0.16.2 \
    -i results/ \
    -o plots/</pre></div>
<h2 dir="auto"><a id="user-content-using-in-ci" class="anchor" aria-hidden="true" href="#using-in-ci"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Using in CI</h2>
<p dir="auto">You can use this package in GitHub actions to benchmark every PR submitted to your package,
by copying the example: <a href="https://github.com/MilesCranmer/AirspeedVelocity.jl/blob/master/.github/workflows/benchmark_pr.yml"><code>.github/workflows/benchmark_pr.yml</code></a>.</p>
<p dir="auto">Every time a PR is submitted to your package, this workflow will run
and generate plots of the performance of the PR against the default branch,
as well as a markdown table, showing whether the PR improves or worsens performance:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/7593028/230768154-beb001e0-d115-4aaa-bd22-89376c1f7556.jpg"><img src="https://user-images.githubusercontent.com/7593028/230768154-beb001e0-d115-4aaa-bd22-89376c1f7556.jpg" alt="regression_example" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">For running benchmarks, you can use the <code>benchpkg</code> command, which is
built into the <code>~/.julia/bin</code> folder:</p>
<div class="highlight highlight-source-gfm notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="    benchpkg package_name [-r --rev &lt;arg&gt;] [-o, --output-dir &lt;arg&gt;]
                          [-s, --script &lt;arg&gt;] [-e, --exeflags &lt;arg&gt;]
                          [-a, --add &lt;arg&gt;] [--tune]
                          [--url &lt;arg&gt;] [--path &lt;arg&gt;]
                          [--bench-on &lt;arg&gt;] [--nsamples-load-time &lt;arg&gt;]

Benchmark a package over a set of revisions.

# Arguments

- `package_name`: Name of the package.

# Options

- `-r, --rev &lt;arg&gt;`: Revisions to test (delimit by comma).
- `-o, --output-dir &lt;arg&gt;`: Where to save the JSON results.
- `-s, --script &lt;arg&gt;`: The benchmark script. Default: `benchmark/benchmarks.jl` downloaded from `stable`.
- `-e, --exeflags &lt;arg&gt;`: CLI flags for Julia (default: none).
- `-a, --add &lt;arg&gt;`: Extra packages needed (delimit by comma).
- `--url &lt;arg&gt;`: URL of the package.
- `--path &lt;arg&gt;`: Path of the package.
- `--bench-on &lt;arg&gt;`: If the script is not set, this specifies the revision at which
  to download `benchmark/benchmarks.jl` from the package.
- `--nsamples-load-time &lt;arg&gt;`: Number of samples to take when measuring load time of
    the package (default: 5). (This means starting a Julia process for each sample.)

# Flags

- `--tune`: Whether to run benchmarks with tuning (default: false)."><pre>    benchpkg package_name [-r --rev &lt;arg&gt;] [-o, --output-dir &lt;arg&gt;]
                          [-s, --script &lt;arg&gt;] [-e, --exeflags &lt;arg&gt;]
                          [-a, --add &lt;arg&gt;] [--tune]
                          [--url &lt;arg&gt;] [--path &lt;arg&gt;]
                          [--bench-on &lt;arg&gt;] [--nsamples-load-time &lt;arg&gt;]

Benchmark a package over a set of revisions.

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Arguments</span>

<span class="pl-v">-</span> <span class="pl-c1">`package_name`</span>: Name of the package.

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Options</span>

<span class="pl-v">-</span> <span class="pl-c1">`-r, --rev &lt;arg&gt;`</span>: Revisions to test (delimit by comma).
<span class="pl-v">-</span> <span class="pl-c1">`-o, --output-dir &lt;arg&gt;`</span>: Where to save the JSON results.
<span class="pl-v">-</span> <span class="pl-c1">`-s, --script &lt;arg&gt;`</span>: The benchmark script. Default: <span class="pl-c1">`benchmark/benchmarks.jl`</span> downloaded from <span class="pl-c1">`stable`</span>.
<span class="pl-v">-</span> <span class="pl-c1">`-e, --exeflags &lt;arg&gt;`</span>: CLI flags for Julia (default: none).
<span class="pl-v">-</span> <span class="pl-c1">`-a, --add &lt;arg&gt;`</span>: Extra packages needed (delimit by comma).
<span class="pl-v">-</span> <span class="pl-c1">`--url &lt;arg&gt;`</span>: URL of the package.
<span class="pl-v">-</span> <span class="pl-c1">`--path &lt;arg&gt;`</span>: Path of the package.
<span class="pl-v">-</span> <span class="pl-c1">`--bench-on &lt;arg&gt;`</span>: If the script is not set, this specifies the revision at which
  to download <span class="pl-c1">`benchmark/benchmarks.jl`</span> from the package.
<span class="pl-v">-</span> <span class="pl-c1">`--nsamples-load-time &lt;arg&gt;`</span>: Number of samples to take when measuring load time of
    the package (default: 5). (This means starting a Julia process for each sample.)

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Flags</span>

<span class="pl-v">-</span> <span class="pl-c1">`--tune`</span>: Whether to run benchmarks with tuning (default: false).</pre></div>
<p dir="auto">For plotting, you can use the <code>benchpkgplot</code> function:</p>
<div class="highlight highlight-source-gfm notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="    benchpkgplot package_name [-r --rev &lt;arg&gt;] [-i --input-dir &lt;arg&gt;]
                              [-o --output-dir &lt;arg&gt;] [-n --npart &lt;arg&gt;]
                              [--format &lt;arg&gt;]

Plot the benchmarks of a package as created with `benchpkg`.

# Arguments

- `package_name`: Name of the package.

# Options

- `-r, --rev &lt;arg&gt;`: Revisions to test (delimit by comma).
- `-i, --input-dir &lt;arg&gt;`: Where the JSON results were saved (default: &quot;.&quot;).
- `-o, --output-dir &lt;arg&gt;`: Where to save the plots results (default: &quot;.&quot;).
- `-n, --npart &lt;arg&gt;`: Max number of plots per page (default: 10).
- `--format &lt;arg&gt;`: File type to save the plots as (default: &quot;png&quot;)."><pre>    benchpkgplot package_name [-r --rev &lt;arg&gt;] [-i --input-dir &lt;arg&gt;]
                              [-o --output-dir &lt;arg&gt;] [-n --npart &lt;arg&gt;]
                              [--format &lt;arg&gt;]

Plot the benchmarks of a package as created with <span class="pl-c1">`benchpkg`</span>.

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Arguments</span>

<span class="pl-v">-</span> <span class="pl-c1">`package_name`</span>: Name of the package.

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Options</span>

<span class="pl-v">-</span> <span class="pl-c1">`-r, --rev &lt;arg&gt;`</span>: Revisions to test (delimit by comma).
<span class="pl-v">-</span> <span class="pl-c1">`-i, --input-dir &lt;arg&gt;`</span>: Where the JSON results were saved (default: ".").
<span class="pl-v">-</span> <span class="pl-c1">`-o, --output-dir &lt;arg&gt;`</span>: Where to save the plots results (default: ".").
<span class="pl-v">-</span> <span class="pl-c1">`-n, --npart &lt;arg&gt;`</span>: Max number of plots per page (default: 10).
<span class="pl-v">-</span> <span class="pl-c1">`--format &lt;arg&gt;`</span>: File type to save the plots as (default: "png").</pre></div>
<p dir="auto">You can also just generate a table:</p>
<div class="highlight highlight-source-gfm notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="    benchpkgtable package_name [-r --rev &lt;arg&gt;] [-i --input-dir &lt;arg&gt;]
                               [--ratio]

Print a table of the benchmarks of a package as created with `benchpkg`.

# Arguments

- `package_name`: Name of the package.

# Options

- `-r, --rev &lt;arg&gt;`: Revisions to test (delimit by comma).
- `-i, --input-dir &lt;arg&gt;`: Where the JSON results were saved (default: &quot;.&quot;).

# Flags

- `--ratio`: Whether to include the ratio (default: false). Only applies when
    comparing two revisions."><pre>    benchpkgtable package_name [-r --rev &lt;arg&gt;] [-i --input-dir &lt;arg&gt;]
                               [--ratio]

Print a table of the benchmarks of a package as created with <span class="pl-c1">`benchpkg`</span>.

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Arguments</span>

<span class="pl-v">-</span> <span class="pl-c1">`package_name`</span>: Name of the package.

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Options</span>

<span class="pl-v">-</span> <span class="pl-c1">`-r, --rev &lt;arg&gt;`</span>: Revisions to test (delimit by comma).
<span class="pl-v">-</span> <span class="pl-c1">`-i, --input-dir &lt;arg&gt;`</span>: Where the JSON results were saved (default: ".").

<span class="pl-mh"><span class="pl-mh">#</span><span class="pl-mh"> </span>Flags</span>

<span class="pl-v">-</span> <span class="pl-c1">`--ratio`</span>: Whether to include the ratio (default: false). Only applies when
    comparing two revisions.</pre></div>
<p dir="auto">If you prefer to use the Julia API, you can use the <code>benchmark</code> function for generating data.
The API is given <a href="https://astroautomata.com/AirspeedVelocity.jl/dev/api/" rel="nofollow">here</a>.</p>
<h2 dir="auto"><a id="user-content-related-packages" class="anchor" aria-hidden="true" href="#related-packages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Related packages</h2>
<p dir="auto">Also be sure to check out <a href="https://github.com/JuliaCI/PkgBenchmark.jl">PkgBenchmark.jl</a>.
PkgBenchmark.jl is a simple wrapper of BenchmarkTools.jl to interface it with Git, and
is a good choice for building custom analysis workflows.</p>
<p dir="auto">However, for me this wrapper is a bit too thin, which is why I created this package.
AirspeedVelocity.jl tries to have more features and workflows readily-available.
It also emphasizes a CLI (though there is a Julia API), as my subjective view
is that this is more suitable for interacting side-by-side with <code>git</code>.</p>
</article></div>