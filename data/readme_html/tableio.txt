<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-tableio" class="anchor" aria-hidden="true" href="#tableio"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TableIO</h1>
<p><a href="https://travis-ci.com/lungben/TableIO.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/373b2bb2c46c4100e6e9ffaaa9efa323e9023da2963e8d1efec6b2d0ee5c493b/68747470733a2f2f7472617669732d63692e636f6d2f6c756e6762656e2f5461626c65494f2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/lungben/TableIO.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/lungben/TableIO.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6e4585feafb37c9da9370891c748e17df57f5f8533a77ec3fc84c78c0865ce3e/68747470733a2f2f636f6465636f762e696f2f67682f6c756e6762656e2f5461626c65494f2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/lungben/TableIO.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>A small "glue" package for reading and writing tabular data. It aims to provide a uniform api for reading and writing tabular data from and to multiple sources.
This package is "intelligent" in this sense that it automatically selects the right reading / writing methods depending on the file extension.</p>
<h2><a id="user-content-supported-formats" class="anchor" aria-hidden="true" href="#supported-formats"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supported Formats</h2>
<ul>
<li>CSV via <a href="https://github.com/JuliaData/CSV.jl">https://github.com/JuliaData/CSV.jl</a></li>
<li>JSON via <a href="https://github.com/JuliaData/JSONTables.jl">https://github.com/JuliaData/JSONTables.jl</a></li>
<li>Zipped CSV or JSON via <a href="https://github.com/fhs/ZipFile.jl">https://github.com/fhs/ZipFile.jl</a></li>
<li>JDF via <a href="https://github.com/xiaodaigh/JDF.jl">https://github.com/xiaodaigh/JDF.jl</a></li>
<li>Parquet via <a href="https://github.com/JuliaIO/Parquet.jl">https://github.com/JuliaIO/Parquet.jl</a></li>
<li>Apache Arrow via <a href="https://github.com/JuliaData/Arrow.jl">https://github.com/JuliaData/Arrow.jl</a></li>
<li>Excel (xlsx) via <a href="https://github.com/felipenoris/XLSX.jl">https://github.com/felipenoris/XLSX.jl</a></li>
<li>SQLite via <a href="https://github.com/JuliaDatabases/SQLite.jl">https://github.com/JuliaDatabases/SQLite.jl</a></li>
<li>PostgreSQL via <a href="https://github.com/invenia/LibPQ.jl">https://github.com/invenia/LibPQ.jl</a> - note that CSV.jl is required for PostgreSQL, too.</li>
<li>Read-only: Stata (dta), SPSS (dat) and SAS (sas7bdat) via <a href="https://github.com/queryverse/StatFiles.jl">https://github.com/queryverse/StatFiles.jl</a></li>
<li>HDF5 (Pandas format) via <a href="https://github.com/JuliaPy/Pandas.jl">Pandas.jl</a> - requires Python with Pandas and PyTables installed on Python side.</li>
</ul>
<p>The underlying packages are not direct dependencies of TableIO and are therefore not installed automatically with it.
This is for reduction of installation size and package load time.</p>
<p>For installation of the Python requirements for Pandas HDF5 use the following Julia commands:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="ENV[&quot;PYTHON&quot;] = &quot;&quot; # to use a separate Conda environment for Julia
using Pkg
Pkg.add([&quot;PyCall&quot;, &quot;Conda&quot;, &quot;Pandas&quot;])
Pkg.build(&quot;PyCall&quot;)
using Conda
Conda.add(&quot;pandas&quot;)
Conda.add(&quot;pytables&quot;)
"><pre><code>ENV["PYTHON"] = "" # to use a separate Conda environment for Julia
using Pkg
Pkg.add(["PyCall", "Conda", "Pandas"])
Pkg.build("PyCall")
using Conda
Conda.add("pandas")
Conda.add("pytables")
</code></pre></div>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using Pkg
pkg&quot;add https://github.com/lungben/TableIO.jl&quot;
using TableIO
"><pre><code>using Pkg
pkg"add https://github.com/lungben/TableIO.jl"
using TableIO
</code></pre></div>
<p>Before using a specific format, the corresponding package needs to be installed:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Pkg.add(&quot;JDF&quot;)
"><pre><code>Pkg.add("JDF")
</code></pre></div>
<p>When using <code>read_table</code> or <code>write_table!</code> with a specific file extension, TableIO automatically imports the corresponding library (if it is installed) and loads the corresponding wrapper code (using Requires.jl).</p>
<p>If the file format specific library is not installed, an error message is raised.</p>
<h2><a id="user-content-reading-data" class="anchor" aria-hidden="true" href="#reading-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reading Data</h2>
<p>The function</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="read_table
"><pre><code>read_table
</code></pre></div>
<p>reads a data source (file or database) and returns a Table.jl interface, e.g. for creating a DataFrame.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using TableIO, DataFrames
"><pre><code>using TableIO, DataFrames
</code></pre></div>
<p>CSV Format:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="df = DataFrame(read_table(&quot;my_data.csv&quot;); copycols=false) # Keyword arguments can be passed to the CSV reader (CSV.jl)

df = DataFrame(read_table(&quot;my_data.zip&quot;); copycols=false) # zipped CSV format (assuming there is only 1 file in the archive)
"><pre><code>df = DataFrame(read_table("my_data.csv"); copycols=false) # Keyword arguments can be passed to the CSV reader (CSV.jl)

df = DataFrame(read_table("my_data.zip"); copycols=false) # zipped CSV format (assuming there is only 1 file in the archive)
</code></pre></div>
<p>JSON Format:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using Dates
df = read_table(&quot;my_data.json&quot;) |&gt; DataFrame # note that |&gt; DataFrame(; copycols=false) gives wrong column types!
df.my_date_col = Dates.(df.my_date_col) # Dates are imported as strings by default, need to be manually converted

df = read_table(&quot;my_data.zip&quot;, &quot;my_data.json&quot;) |&gt; DataFrame
"><pre><code>using Dates
df = read_table("my_data.json") |&gt; DataFrame # note that |&gt; DataFrame(; copycols=false) gives wrong column types!
df.my_date_col = Dates.(df.my_date_col) # Dates are imported as strings by default, need to be manually converted

df = read_table("my_data.zip", "my_data.json") |&gt; DataFrame
</code></pre></div>
<p>Binary Formats:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="df = DataFrame(read_table(&quot;my_data.jdf&quot;); copycols=false) # JDF (compressed binary format)
df = DataFrame(read_table(&quot;my_data.parquet&quot;); copycols=false) # Parquet
df = DataFrame(read_table(&quot;my_data.arrow&quot;); copycols=false) # Apache Arrow

df = DataFrame(read_table(&quot;my_data.hdf&quot;, &quot;key&quot;); copycols=false) # HDF5 (via Pandas)
"><pre><code>df = DataFrame(read_table("my_data.jdf"); copycols=false) # JDF (compressed binary format)
df = DataFrame(read_table("my_data.parquet"); copycols=false) # Parquet
df = DataFrame(read_table("my_data.arrow"); copycols=false) # Apache Arrow

df = DataFrame(read_table("my_data.hdf", "key"); copycols=false) # HDF5 (via Pandas)
</code></pre></div>
<p>Excel:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="df = DataFrame(read_table(&quot;my_data.xlsx&quot;); copycols=false) # imports 1st sheet
df = DataFrame(read_table(&quot;my_data.xlsx&quot;, &quot;MyAwesomeSheet&quot;); copycols=false) # imports named sheet
"><pre><code>df = DataFrame(read_table("my_data.xlsx"); copycols=false) # imports 1st sheet
df = DataFrame(read_table("my_data.xlsx", "MyAwesomeSheet"); copycols=false) # imports named sheet
</code></pre></div>
<p>SQLite:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="df = DataFrame(read_table(&quot;my_data.db&quot;, &quot;my_table&quot;); copycols=false) # SQLite from file, table name must be given
"><pre><code>df = DataFrame(read_table("my_data.db", "my_table"); copycols=false) # SQLite from file, table name must be given
</code></pre></div>
<p>Alternatively, SQLite database objects could be used:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using SQLite
sqlite_db = SQLite.DB(&quot;my_data.db&quot;)
df = DataFrame(read_table(sqlite_db, &quot;my_table&quot;); copycols=false) # SQLite from database connection, table name must be given
"><pre><code>using SQLite
sqlite_db = SQLite.DB("my_data.db")
df = DataFrame(read_table(sqlite_db, "my_table"); copycols=false) # SQLite from database connection, table name must be given
</code></pre></div>
<p>PostgreSQL:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using LibPQ, CSV # CSV is required here because `write_table!` for PostgreSQL depends on CSV
postgres_conn = LibPQ.Connection(&quot;dbname=postgres user=postgres&quot;)
df = DataFrame(read_table(postgres_conn, &quot;my_table&quot;); copycols=false) # reading from Postgres connection
"><pre><code>using LibPQ, CSV # CSV is required here because `write_table!` for PostgreSQL depends on CSV
postgres_conn = LibPQ.Connection("dbname=postgres user=postgres")
df = DataFrame(read_table(postgres_conn, "my_table"); copycols=false) # reading from Postgres connection
</code></pre></div>
<p>StatFiles.jl integration:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="df = DataFrame(read_table(&quot;my_data.dta&quot;); copycols=false) # Stata
df = DataFrame(read_table(&quot;my_data.sav&quot;); copycols=false) # SPSS
df = DataFrame(read_table(&quot;my_data.sas7bdat&quot;); copycols=false) # SAS
"><pre><code>df = DataFrame(read_table("my_data.dta"); copycols=false) # Stata
df = DataFrame(read_table("my_data.sav"); copycols=false) # SPSS
df = DataFrame(read_table("my_data.sas7bdat"); copycols=false) # SAS
</code></pre></div>
<p>For data formats supporting multiple tables inside a file, the function <code>list_tables</code> returns an alphabetically sorted list of table names.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="table_names = list_tables(filename)
"><pre><code>table_names = list_tables(filename)
</code></pre></div>
<h2><a id="user-content-writing-data" class="anchor" aria-hidden="true" href="#writing-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Writing Data</h2>
<p>The function</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!
"><pre><code>write_table!
</code></pre></div>
<p>writes a Table.jl compatible data source into a file or databse.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using TableIO, DataFrames
"><pre><code>using TableIO, DataFrames
</code></pre></div>
<p>CSV Format:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!(&quot;my_data.csv&quot;, df)

write_table!(&quot;my_data.zip&quot;, df) # zipped CSV. If no &quot;inner&quot; file name is given, the table is always stored in csv format with the same file name as the zip archive
"><pre><code>write_table!("my_data.csv", df)

write_table!("my_data.zip", df) # zipped CSV. If no "inner" file name is given, the table is always stored in csv format with the same file name as the zip archive
</code></pre></div>
<p>JSON Format:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!(&quot;my_data.json&quot;, df) # dictionary of arrays
write_table!(&quot;my_data.json&quot;, df, orientation=:objecttable) # dictionary of arrays
write_table!(&quot;my_data.json&quot;, df, orientation=:arraytable) # array of dictionaries

write_table!(&quot;my_data.zip&quot;, &quot;my_data.json&quot;, df) # need to explicitly give a file name inside zip archive, otherwise csv format is used
"><pre><code>write_table!("my_data.json", df) # dictionary of arrays
write_table!("my_data.json", df, orientation=:objecttable) # dictionary of arrays
write_table!("my_data.json", df, orientation=:arraytable) # array of dictionaries

write_table!("my_data.zip", "my_data.json", df) # need to explicitly give a file name inside zip archive, otherwise csv format is used
</code></pre></div>
<p>Binary Formats:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!(&quot;my_data.jdf&quot;, df) # JDF (compressed binary format)
write_table!(&quot;my_data.parquet&quot;, df) # Parquet - note that Date element type is not supported yet
write_table!(&quot;my_data.arrow&quot;, df) # Apache Arrow

write_table!(&quot;my_data.hdf&quot;, &quot;key&quot;, df) # HDF5 (via Pandas)
"><pre><code>write_table!("my_data.jdf", df) # JDF (compressed binary format)
write_table!("my_data.parquet", df) # Parquet - note that Date element type is not supported yet
write_table!("my_data.arrow", df) # Apache Arrow

write_table!("my_data.hdf", "key", df) # HDF5 (via Pandas)
</code></pre></div>
<p>Excel:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!(&quot;my_data.xlsx&quot;, &quot;test_sheet_42&quot;, df) # creates sheet with defined name
"><pre><code>write_table!("my_data.xlsx", "test_sheet_42", df) # creates sheet with defined name
</code></pre></div>
<p>SQLite:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!(&quot;my_data.db&quot;, &quot;my_table&quot;, df) # SQLite from file, table must not exist

using SQLite
sqlite_db = SQLite.DB(&quot;my_data.db&quot;)
write_table!(sqlite_db, &quot;my_table&quot;, df) # SQLite from database connection
"><pre><code>write_table!("my_data.db", "my_table", df) # SQLite from file, table must not exist

using SQLite
sqlite_db = SQLite.DB("my_data.db")
write_table!(sqlite_db, "my_table", df) # SQLite from database connection
</code></pre></div>
<p>PostgreSQL:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using LibPQ, CSV
postgres_conn = LibPQ.Connection(&quot;dbname=postgres user=postgres&quot;)
write_table!(postgres_conn, &quot;my_table&quot;, df) # table must exist and be compatible with the input data
"><pre><code>using LibPQ, CSV
postgres_conn = LibPQ.Connection("dbname=postgres user=postgres")
write_table!(postgres_conn, "my_table", df) # table must exist and be compatible with the input data
</code></pre></div>
<p>StatFiles.jl integration: <code>write_table!</code> is not supported.</p>
<p>Additionally, it is possible to export tabular data into Julia code (<code>.jl</code> files):</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="write_table!(&quot;my_data.jl&quot;, &quot;my_table&quot;, df)
"><pre><code>write_table!("my_data.jl", "my_table", df)
</code></pre></div>
<p>To read this data, the corresponding Julia source code file can be included:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="include(&quot;my_data.jl&quot;)
@assert DataFrame(my_table) == df
"><pre><code>include("my_data.jl")
@assert DataFrame(my_table) == df
</code></pre></div>
<h2><a id="user-content-conversions" class="anchor" aria-hidden="true" href="#conversions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conversions</h2>
<p>It is possible to pass the output of <code>read_table</code> directly as input to <code>write_table!</code> for converting tabular data between different formats:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="name1 = joinpath(testpath, &quot;test.zip&quot;) # zipped CSV
name2 = joinpath(testpath, &quot;testx.jdf&quot;) # binary
name3 = joinpath(testpath, &quot;testx.xlsx&quot;) # Excel
name4 = joinpath(testpath, &quot;testx.db&quot;) # SQLite

write_table!(name2, read_table(name1))
write_table!(name3, read_table(name2))
write_table!(name4, &quot;my_table&quot;, read_table(name3))

df_recovered = DataFrame(read_table(name4, &quot;my_table&quot;); copycols=false)
"><pre><code>name1 = joinpath(testpath, "test.zip") # zipped CSV
name2 = joinpath(testpath, "testx.jdf") # binary
name3 = joinpath(testpath, "testx.xlsx") # Excel
name4 = joinpath(testpath, "testx.db") # SQLite

write_table!(name2, read_table(name1))
write_table!(name3, read_table(name2))
write_table!(name4, "my_table", read_table(name3))

df_recovered = DataFrame(read_table(name4, "my_table"); copycols=false)
</code></pre></div>
<h2><a id="user-content-plutoui-integration" class="anchor" aria-hidden="true" href="#plutoui-integration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PlutoUI Integration</h2>
<p>In a <a href="https://github.com/fonsp/Pluto.jl">Pluto.jl</a> notebook, TableIO can be used directly on a <a href="https://github.com/fonsp/PlutoUI.jl">PlutoUI.jl</a> FilePicker output.</p>
<p>Example (run in a Pluto.jl notebook):</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using PlutoUI, TableIO, DataFrames
@bind f PlutoUI.FilePicker() # pick any supported file type
df = DataFrame(read_table(f); copycols=false)
"><pre><code>using PlutoUI, TableIO, DataFrames
@bind f PlutoUI.FilePicker() # pick any supported file type
df = DataFrame(read_table(f); copycols=false)
</code></pre></div>
<p>This functionality works for all supported file formats if the corresponding import packages are installed.
It is not required to import them, this is done automatically.</p>
<h2><a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Testing</h2>
<p>The PostgreSQL component requires a running PostgreSQL database for unit tests. This database can be started using the following command:</p>
<p><code>docker run --rm --detach --name test-libpqjl -e POSTGRES_HOST_AUTH_METHOD=trust -p 5432:5432 postgres</code></p>
<h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Disclaimer</h2>
<p>If you encounter warnings like</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="┌ Warning: Package TableIO does not have CSV in its dependencies:
│ - If you have TableIO checked out for development and have
│   added CSV as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with TableIO
└ Loading CSV into TableIO from project dependency, future warnings for TableIO are suppressed.
"><pre><code>┌ Warning: Package TableIO does not have CSV in its dependencies:
│ - If you have TableIO checked out for development and have
│   added CSV as a dependency but haven't updated your primary
│   environment's manifest file, try `Pkg.resolve()`.
│ - Otherwise you may need to report an issue with TableIO
└ Loading CSV into TableIO from project dependency, future warnings for TableIO are suppressed.
</code></pre></div>
<p>please ignore them.</p>
<p>TableIO purposely has not included the libraries for the specific file formats as its own dependencies.</p>
</article></div>