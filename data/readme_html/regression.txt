<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Regression</h1>
<p>A Julia package for regression analysis.</p>
<p><a href="https://travis-ci.org/lindahua/Regression.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/757fae85e1550219660008a53435fca1738fc94215589a471f4636d33447bc97/68747470733a2f2f7472617669732d63692e6f72672f6c696e64616875612f52656772657373696f6e2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/lindahua/Regression.jl.svg?branch=master" style="max-width:100%;"></a></p>
<hr>
<h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview</h2>
<p>This package is based on <a href="https://github.com/lindahua/EmpiricalRisks.jl">EmpiricalRisks</a>, and provides a set of algorithms to perform regression analysis.</p>
<p>This package supports all regression problems that can be formulated as <em>regularized empirical risk minimization</em>, as</p>
<p><a target="_blank" rel="noopener noreferrer" href="imgs/regerm.png"><img src="imgs/regerm.png" alt="regerm" style="max-width:100%;"></a></p>
<p>In particular, it supports:</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Linear regression</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Ridge regression</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> LASSO</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Logistic regression</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Multinomial Logistic regression</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Problems with customized loss and regularizers</li>
</ul>
<p>The package also provides a variety of solvers</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Analytical solution (for linear &amp; ridge regression)</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Gradient descent</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> BFGS</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> L-BFGS</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Proximal gradient descent (recommended for LASSO &amp; sparse regression)</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Accelerated gradient descent (<em>experimental</em>)</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Accelerated proximal gradient descent (<em>experimental</em>)</li>
</ul>
<hr>
<h2><a id="user-content-high-level-interface" class="anchor" aria-hidden="true" href="#high-level-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>High Level Interface</h2>
<p>The package provides a high-level interface to simplify typical use.</p>
<p><strong>Example:</strong></p>
<p>The following script shows how one can use this package to perform <em>logistic regression</em>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="d = 3      # sample dimension
n = 1000   # number of samples

# prepare data
w = randn(d+1)    # generate the weight vector
X = randn(d, n)   # generate input features
y = sign(X'w[1:d] + w[d+1] + 0.2 * randn(n))  # generate (noisy) response

# perform estimation
ret = Regression.solve(
    logisticreg(X, y; bias=1.0),   # construct a logistic regression problem
    reg=SqrL2Reg(1.0e-2),          # apply squared L2 regularization
    options=Options(verbosity=:iter, grtol=1.0e-6 * n))  # set options

# extract results
w_e = ret.sol
"><pre>d <span class="pl-k">=</span> <span class="pl-c1">3</span>      <span class="pl-c"><span class="pl-c">#</span> sample dimension</span>
n <span class="pl-k">=</span> <span class="pl-c1">1000</span>   <span class="pl-c"><span class="pl-c">#</span> number of samples</span>

<span class="pl-c"><span class="pl-c">#</span> prepare data</span>
w <span class="pl-k">=</span> <span class="pl-c1">randn</span>(d<span class="pl-k">+</span><span class="pl-c1">1</span>)    <span class="pl-c"><span class="pl-c">#</span> generate the weight vector</span>
X <span class="pl-k">=</span> <span class="pl-c1">randn</span>(d, n)   <span class="pl-c"><span class="pl-c">#</span> generate input features</span>
y <span class="pl-k">=</span> <span class="pl-c1">sign</span>(X<span class="pl-k">'</span>w[<span class="pl-c1">1</span><span class="pl-k">:</span>d] <span class="pl-k">+</span> w[d<span class="pl-k">+</span><span class="pl-c1">1</span>] <span class="pl-k">+</span> <span class="pl-c1">0.2</span> <span class="pl-k">*</span> <span class="pl-c1">randn</span>(n))  <span class="pl-c"><span class="pl-c">#</span> generate (noisy) response</span>

<span class="pl-c"><span class="pl-c">#</span> perform estimation</span>
ret <span class="pl-k">=</span> Regression<span class="pl-k">.</span><span class="pl-c1">solve</span>(
    <span class="pl-c1">logisticreg</span>(X, y; bias<span class="pl-k">=</span><span class="pl-c1">1.0</span>),   <span class="pl-c"><span class="pl-c">#</span> construct a logistic regression problem</span>
    reg<span class="pl-k">=</span><span class="pl-c1">SqrL2Reg</span>(<span class="pl-c1">1.0e-2</span>),          <span class="pl-c"><span class="pl-c">#</span> apply squared L2 regularization</span>
    options<span class="pl-k">=</span><span class="pl-c1">Options</span>(verbosity<span class="pl-k">=</span><span class="pl-c1">:iter</span>, grtol<span class="pl-k">=</span><span class="pl-c1">1.0e-6</span> <span class="pl-k">*</span> n))  <span class="pl-c"><span class="pl-c">#</span> set options</span>

<span class="pl-c"><span class="pl-c">#</span> extract results</span>
w_e <span class="pl-k">=</span> ret<span class="pl-k">.</span>sol</pre></div>
<p>The high-level interface involves two parts: <em>problem construction</em> and <em>problem solving</em>.</p>
<h3><a id="user-content-constructing-problems" class="anchor" aria-hidden="true" href="#constructing-problems"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Constructing Problems</h3>
<p>The package provide several functions to construct regression problems:</p>
<ul>
<li>
<p><strong>UnivariateRegression</strong>(loss, X, Y, bias)</p>
<p>Construct a <em>univariate regression</em> problem, where the both arguments to the loss function are scalars.</p>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>loss</code></td>
<td>the loss function, which should be an instance of <a href="http://empiricalrisksjl.readthedocs.org/en/latest/loss.html#loss-functions" rel="nofollow">UnivariateLoss</a>.</td>
</tr>
<tr>
<td><code>X</code></td>
<td>a matrix of inputs (as columns)</td>
</tr>
<tr>
<td><code>y</code></td>
<td>a vector of corresponding outputs</td>
</tr>
<tr>
<td><code>bias</code></td>
<td>The bias term</td>
</tr>
</tbody>
</table>
<p>Let <code>d</code> be the length of each input.
When <code>bias</code> is zero, the parameter <code>w</code> is a vector of length <code>d</code>, and the prediction is given by <code>w'x</code>.
When <code>bias</code> is non-zero, the parameter <code>w</code> is a vector of length <code>d+1</code>, and the prediction is given by
<code>w[1:d]'x + w[d+1]</code>.</p>
</li>
<li>
<p><strong>MultivariateRegression</strong>(loss, X, Y, k, bias)</p>
<p>Construct a <em>multivariate regression</em> problem, where the prediction is a vector.</p>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>loss</code></td>
<td>the loss function, which should be an instance of <a href="http://empiricalrisksjl.readthedocs.org/en/latest/loss.html#loss-functions" rel="nofollow">MultivariateLoss</a>.</td>
</tr>
<tr>
<td><code>X</code></td>
<td>a matrix of inputs (as columns)</td>
</tr>
<tr>
<td><code>y</code></td>
<td>a matrix of corresponding outputs (as columns)</td>
</tr>
<tr>
<td><code>k</code></td>
<td>The length of each prediction output</td>
</tr>
<tr>
<td><code>bias</code></td>
<td>The bias term</td>
</tr>
</tbody>
</table>
<p>Let <code>d</code> be the length of each input.
When <code>bias</code> is zero, the parameter <code>W</code> is a matrix of size <code>(k, d)</code>, and the prediction is given by <code>W * x</code>.
When <code>bias</code> is non-zero, the parameter <code>W</code> is a matrix of size <code>(k, d+1)</code>, and the prediction is given by
<code>W[:, 1:d] * x + W[:,d+1]</code>.</p>
</li>
</ul>
<p>The package also provides convenience functions to construct common problems:</p>
<ul>
<li>
<p><strong>linearreg</strong>(X, Y[; bias=0])</p>
<p>Construct a linear regression problem.</p>
<p>When <code>Y</code> is a vector, it is a univariate regression problem, when <code>Y</code> is a matrix, it is a multivariate regression problem.</p>
<p>Note that each column of <code>X</code> corresponds to a sample. The same applies to <code>Y</code> when <code>Y</code> is a matrix.</p>
</li>
<li>
<p><strong>logisticreg</strong>(X, y[; bias=0])</p>
<p>Construct a logistic regression problem.</p>
</li>
<li>
<p><strong>mlogisticreg</strong>(X, y, k[; bias=0])</p>
<p>Construct a multinomial logistic regression problem.</p>
<p>Here, <code>X</code> is a sample matrix, <code>y</code> is a vector of class labels (values in <code>1:k</code>), and <code>k</code> be the number of classes.</p>
</li>
</ul>
<h3><a id="user-content-solving-problems" class="anchor" aria-hidden="true" href="#solving-problems"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Solving Problems</h3>
<p>With a constructed problem, you can solve the problem with the <code>solve</code> function.</p>
<p><strong>Note:</strong> The <code>solve</code> function is not exported (in order to avoid confliction with other optimization packages). You should write <code>Regression.solve</code> when calling this function.</p>
<ul>
<li>
<p><strong>Regression.solve</strong>(pb[; ...])</p>
<p>Solve the regression problem <code>pb</code>, which can be constructed using the construction functions above.</p>
<p>This function allows the users to supply the following keyword arguments:</p>
<table>
<thead>
<tr>
<th>params</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>reg</td>
<td>The regularizer. (See <a href="http://empiricalrisksjl.readthedocs.org/en/latest/regularizers.html#regularizers" rel="nofollow">documentation on regularizers</a> for details.)</td>
</tr>
<tr>
<td>init</td>
<td>The initial guess of the parameters. (If omitted, we use all-zeros as initial guess by default)</td>
</tr>
<tr>
<td>solver</td>
<td>The chosen solver (see below for details). The default is <code>BFGS()</code></td>
</tr>
<tr>
<td>options</td>
<td>The options to control the solving procedure (see below for details)</td>
</tr>
<tr>
<td>callback</td>
<td>The callback function, which will be invoked at each iteration. in the following way: <code>callback(t, theta, v, g)</code>, where <code>t</code> is the iteration number, <code>theta</code> is the solution at current step, <code>v</code> is the current objective value, and <code>g</code> is the current gradient. Default is <code>no_op</code>, which does nothing.</td>
</tr>
</tbody>
</table>
</li>
<li>
<p><strong>Regression.Options</strong>(...)</p>
<p>Construct an option struct to control the solving procedure.</p>
<p>It accepts the following keyword arguments:</p>
<table>
<thead>
<tr>
<th>params</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>maxiter</td>
<td>The maximum number of iterations (default = <code>200</code>)</td>
</tr>
<tr>
<td>ftol</td>
<td>Tolerance of function value changes (default = <code>1.0e-6</code>)</td>
</tr>
<tr>
<td>xtol</td>
<td>Tolerance of solution change (default = <code>1.0e-8</code>)</td>
</tr>
<tr>
<td>grtol</td>
<td>Tolerance of the gradient norm (default = <code>1.0e-8</code>)</td>
</tr>
<tr>
<td>armijo</td>
<td>The <em>Armijo</em> coefficient in line search</td>
</tr>
<tr>
<td>beta</td>
<td>The back tracking ratio in line search</td>
</tr>
<tr>
<td>verbosity</td>
<td>The level of display, which is a symbol, whose value can be <code>:none</code>, <code>:final</code>, or <code>:iter</code>. (default = <code>:none</code>)</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h3><a id="user-content-solvers" class="anchor" aria-hidden="true" href="#solvers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Solvers</h3>
<p>As mentioned, the package implements a variety of solvers, one can construct a solver using the following functions:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="GD()       # Gradient descent
BFGS()     # BFGS Quasi-Newton method
LBFGS(m)   # L-BFGS method (with history size m)
ProxGD()   # Proximal gradient descent (suitable for sparse learning, etc)

# the following solver remains in experimental status
AGD()      # Accelerated gradient descent
ProxAGD()  # Accelerated proximal gradient descent
"><pre><span class="pl-c1">GD</span>()       <span class="pl-c"><span class="pl-c">#</span> Gradient descent</span>
<span class="pl-c1">BFGS</span>()     <span class="pl-c"><span class="pl-c">#</span> BFGS Quasi-Newton method</span>
<span class="pl-c1">LBFGS</span>(m)   <span class="pl-c"><span class="pl-c">#</span> L-BFGS method (with history size m)</span>
<span class="pl-c1">ProxGD</span>()   <span class="pl-c"><span class="pl-c">#</span> Proximal gradient descent (suitable for sparse learning, etc)</span>

<span class="pl-c"><span class="pl-c">#</span> the following solver remains in experimental status</span>
<span class="pl-c1">AGD</span>()      <span class="pl-c"><span class="pl-c">#</span> Accelerated gradient descent</span>
<span class="pl-c1">ProxAGD</span>()  <span class="pl-c"><span class="pl-c">#</span> Accelerated proximal gradient descent</span></pre></div>
<hr>
<h2><a id="user-content-lower-level-interface" class="anchor" aria-hidden="true" href="#lower-level-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Lower Level Interface</h2>
<p>Those who care more on performance can directly call the <code>Regression.solve!</code> function, as follows:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Note: solve! will update the solution θ inplace
function solve!{T&lt;:FloatingPoint}(
    solver::DescentSolver,  # the chosen solver
    f::Functional{T},       # the objective functional
    θ::Array{T},            # the solution (which would be updated inplace)
    options::Options,       # options to control the procedure
    callback::Function)     # callback function

# Here, the functional f can be constructed using the following functions:

# empirical risk minimization
f = RiskFun(rmodel, X, Y)   # rmodel is the risk model

# regularized empirical risk minimization
f = RegRiskFun(rmodel, reg, X, Y)   # rmodel is the risk model, reg is the regularizer
"><pre><span class="pl-c"><span class="pl-c">#</span> Note: solve! will update the solution θ inplace</span>
<span class="pl-k">function</span> <span class="pl-en">solve!</span><span class="pl-c1">{T&lt;:FloatingPoint}</span>(
    solver<span class="pl-k">::</span><span class="pl-c1">DescentSolver</span>,  <span class="pl-c"><span class="pl-c">#</span> the chosen solver</span>
    f<span class="pl-k">::</span><span class="pl-c1">Functional{T}</span>,       <span class="pl-c"><span class="pl-c">#</span> the objective functional</span>
    θ<span class="pl-k">::</span><span class="pl-c1">Array{T}</span>,            <span class="pl-c"><span class="pl-c">#</span> the solution (which would be updated inplace)</span>
    options<span class="pl-k">::</span><span class="pl-c1">Options</span>,       <span class="pl-c"><span class="pl-c">#</span> options to control the procedure</span>
    callback<span class="pl-k">::</span><span class="pl-c1">Function</span>)     <span class="pl-c"><span class="pl-c">#</span> callback function</span>

<span class="pl-c"><span class="pl-c">#</span> Here, the functional f can be constructed using the following functions:</span>

<span class="pl-c"><span class="pl-c">#</span> empirical risk minimization</span>
f <span class="pl-k">=</span> <span class="pl-c1">RiskFun</span>(rmodel, X, Y)   <span class="pl-c"><span class="pl-c">#</span> rmodel is the risk model</span>

<span class="pl-c"><span class="pl-c">#</span> regularized empirical risk minimization</span>
f <span class="pl-k">=</span> <span class="pl-c1">RegRiskFun</span>(rmodel, reg, X, Y)   <span class="pl-c"><span class="pl-c">#</span> rmodel is the risk model, reg is the regularizer</span></pre></div>
<hr>
<h2><a id="user-content-algebraic-solution-to-linear--ridge-regression" class="anchor" aria-hidden="true" href="#algebraic-solution-to-linear--ridge-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Algebraic Solution to Linear &amp; Ridge Regression</h2>
<p>Note that for linear regression and ridge regression, there exists analytic solution. The package also provides functions that directly compute the analytic solution to these problems, using linear algebraic methods.</p>
<ul>
<li>
<p><strong>llsq</strong>(X, Y; ...)</p>
<p>Solve a linear least square problem.</p>
<p>This function allows keyword arguments as follows:</p>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td>trans</td>
<td>If <code>trans == true</code>, it minimizes ``</td>
</tr>
<tr>
<td>bias</td>
<td>The bias term, namely the value to be augmented to the inputs. Default = <code>0</code>, which indicates no augmentation</td>
</tr>
<tr>
<td>method</td>
<td>A symbol to indicate the matrix factorization method to be used, whose value can be <code>qrlq</code>, <code>orth</code>, or <code>svd</code>. Default = <code>qrlq</code>.</td>
</tr>
</tbody>
</table>
</li>
<li>
<p><strong>ridgereg</strong>(X, Y, r; ...)</p>
<p>Solve a ridge regression problem analytically.</p>
<p>This function allows keyword arguments as follows:</p>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td>trans</td>
<td>If <code>trans == true</code>, it minimizes ``</td>
</tr>
<tr>
<td>bias</td>
<td>The bias term, namely the value to be augmented to the inputs. Default = <code>0</code>, which indicates no augmentation</td>
</tr>
</tbody>
</table>
</li>
</ul>
</article></div>