<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-mljlinearmodelsjl" class="anchor" aria-hidden="true" href="#mljlinearmodelsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MLJLinearModels.jl</h1>
<table>
<thead>
<tr>
<th align="left">[Linux]</th>
<th align="left">Coverage</th>
<th align="left">Documentation</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://github.com/alan-turing-institute/MLJLinearModels.jl/actions"><img src="https://github.com/alan-turing-institute/MLJLinearModels.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a></td>
<td align="left"><a href="http://codecov.io/github/alan-turing-institute/MLJLinearModels.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/66738740b7f8df5a6a64b47233803ba5fc04376a3bb7c72ac2f011f6c06dc76a/687474703a2f2f636f6465636f762e696f2f6769746875622f616c616e2d747572696e672d696e737469747574652f4d4c4a4c696e6561724d6f64656c732e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/alan-turing-institute/MLJLinearModels.jl/coverage.svg?branch=master" style="max-width:100%;"></a></td>
<td align="left"><a href="https://alan-turing-institute.github.io/MLJLinearModels.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="stable-doc" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a> <a href="https://alan-turing-institute.github.io/MLJLinearModels.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="dev-doc" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>This is a package gathering functionalities to solve a number of generalised linear regression/classification problems which, inherently, correspond to an optimisation problem of the form</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="L(y, Xθ) + P(θ)
"><pre><code>L(y, Xθ) + P(θ)
</code></pre></div>
<p>where <code>L</code> is a loss function and <code>P</code>  is a penalty function (both of those can be scaled or composed).
Additional regression/classification methods which do not directly correspond to this formulation may be added in the future.</p>
<p>The core aims of this package are:</p>
<ul>
<li>make these regressions models "easy to call" and callable in a unified way,</li>
<li>interface with <a href="https://github.com/alan-turing-institute/MLJ.jl"><code>MLJ.jl</code></a>,</li>
<li>focus on performance including in "big data" settings exploiting packages such as <a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim.jl</code></a>, <a href="https://github.com/JuliaMath/IterativeSolvers.jl"><code>IterativeSolvers.jl</code></a>,</li>
<li>use a "machine learning" perspective, i.e.: focus essentially on prediction, hyper-parameters should be obtained via a data-driven procedure such as cross-validation.</li>
</ul>
<p>Head to the <a href="https://alan-turing-institute.github.io/MLJLinearModels.jl/dev/quickstart/" rel="nofollow">quickstart section of the docs</a> to see how to use this package.</p>
<h1><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NOTES</h1>
<p>This section is only useful if you're interested in implementation details or would like to help extend the library. For usage instruction please head to the docs.</p>
<h2><a id="user-content-implemented" class="anchor" aria-hidden="true" href="#implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Implemented</h2>
<table>
<thead>
<tr>
<th align="left">Regressors</th>
<th align="left">Formulation¹</th>
<th align="left">Available solvers</th>
<th align="left">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">OLS &amp; Ridge</td>
<td align="left">L2Loss + 0/L2</td>
<td align="left">Analytical² or CG³</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Lasso &amp; Elastic-Net</td>
<td align="left">L2Loss + 0/L2 + L1</td>
<td align="left">(F)ISTA⁴</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Robust 0/L2</td>
<td align="left">RobustLoss⁵ + 0/L2</td>
<td align="left">Newton, NewtonCG, LBFGS, IWLS-CG⁶</td>
<td align="left">no scale⁷</td>
</tr>
<tr>
<td align="left">Robust L1/EN</td>
<td align="left">RobustLoss + 0/L2 + L1</td>
<td align="left">(F)ISTA</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Quantile⁸ + 0/L2</td>
<td align="left">RobustLoss + 0/L2</td>
<td align="left">LBFGS, IWLS-CG</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Quantile L1/EN</td>
<td align="left">RobustLoss + 0/L2 + L1</td>
<td align="left">(F)ISTA</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<ol>
<li>"0" stands for no penalty</li>
<li>Analytical means the solution is computed in "one shot" using the <code>\</code> solver,</li>
<li>CG = conjugate gradient</li>
<li>(Accelerated) Proximal Gradient Descent</li>
<li><em>Huber</em>, <em>Andrews</em>, <em>Bisquare</em>, <em>Logistic</em>, <em>Fair</em> and <em>Talwar</em> weighing functions available.</li>
<li>Iteratively re-Weighted Least Squares where each system is solved iteratively via CG</li>
<li>In other packages such as Scikit-Learn, a scale factor is estimated along with the parameters, this is a bit ad-hoc and corresponds more to a statistical perspective, further it does not work well with penalties; we recommend using cross-validation to set the parameter of the Huber Loss.</li>
<li>Includes as special case the <em>least absolute deviation</em> (LAD) regression when <code>δ=0.5</code>.</li>
</ol>
<table>
<thead>
<tr>
<th align="left">Classifiers</th>
<th align="left">Formulation</th>
<th align="left">Available solvers</th>
<th align="left">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Logistic 0/L2</td>
<td align="left">LogisticLoss + 0/L2</td>
<td align="left">Newton, Newton-CG, LBFGS</td>
<td align="left"><code>yᵢ∈{±1}</code></td>
</tr>
<tr>
<td align="left">Logistic L1/EN</td>
<td align="left">LogisticLoss + 0/L2 + L1</td>
<td align="left">(F)ISTA</td>
<td align="left"><code>yᵢ∈{±1}</code></td>
</tr>
<tr>
<td align="left">Multinomial 0/L2</td>
<td align="left">MultinomialLoss + 0/L2</td>
<td align="left">Newton-CG, LBFGS</td>
<td align="left"><code>yᵢ∈{1,...,c}</code></td>
</tr>
<tr>
<td align="left">Multinomial L1/EN</td>
<td align="left">MultinomialLoss + 0/L2 + L1</td>
<td align="left">ISTA, FISTA</td>
<td align="left"><code>yᵢ∈{1,...,c}</code></td>
</tr>
</tbody>
</table>
<p>Unless otherwise specified:</p>
<ul>
<li>Newton-like solvers use Hager-Zhang line search (default in <code>Optim.jl</code>)</li>
<li>ISTA, FISTA solvers use backtracking line search and a shrinkage factor of <code>β=0.8</code></li>
</ul>
<p><strong>Note</strong>: these models were all tested for correctness whenever a direct comparison with another package was possible, usually by comparing the objective function at the coefficients returned (cf. the tests):</p>
<ul>
<li>(<em>against <a href="https://scikit-learn.org/" rel="nofollow">scikit-learn</a></em>): Lasso, Elastic-Net, Logistic (L1/L2/EN), Multinomial (L1/L2/EN)</li>
<li>(<em>against <a href="https://cran.r-project.org/web/packages/quantreg/index.html" rel="nofollow">quantreg</a></em>): Quantile (0/L1)</li>
</ul>
<p>Systematic timing benchmarks have not been run yet but it's planned (see <a href="https://github.com/alan-turing-institute/MLJLinearModels.jl/issues/14">this issue</a>).</p>
<h3><a id="user-content-current-limitations" class="anchor" aria-hidden="true" href="#current-limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Current limitations</h3>
<ul>
<li>The models are built and tested assuming <code>n &gt; p</code>; if this doesn't hold, tricks should be employed to speed up computations; these have not been implemented yet.</li>
<li>CV-aware code not implemented yet (code that re-uses computations when fitting over a number of hyper-parameters);  "Meta" functionalities such as One-vs-All or Cross-Validation are left to other packages such as MLJ.</li>
<li>No support yet for sparse matrices.</li>
<li>Stochastic solvers have not yet been implemented.</li>
<li>All computations are assumed to be done in Float64.</li>
</ul>
<h3><a id="user-content-possible-future-models" class="anchor" aria-hidden="true" href="#possible-future-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Possible future models</h3>
<h4><a id="user-content-future" class="anchor" aria-hidden="true" href="#future"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Future</h4>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Formulation</th>
<th align="left">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Group Lasso</td>
<td align="left">L2Loss + ∑L1 over groups</td>
<td align="left">⭒</td>
</tr>
<tr>
<td align="left">Adaptive Lasso</td>
<td align="left">L2Loss + weighted L1</td>
<td align="left">⭒ <a href="http://myweb.uiowa.edu/pbreheny/7600/s16/notes/2-29.pdf" rel="nofollow">A</a></td>
</tr>
<tr>
<td align="left">SCAD</td>
<td align="left">L2Loss + SCAD</td>
<td align="left">A, <a href="https://arxiv.org/abs/0903.5474" rel="nofollow">B</a>, <a href="https://orfe.princeton.edu/~jqfan/papers/01/penlike.pdf" rel="nofollow">C</a></td>
</tr>
<tr>
<td align="left">MCP</td>
<td align="left">L2Loss + MCP</td>
<td align="left">A</td>
</tr>
<tr>
<td align="left">OMP</td>
<td align="left">L2Loss + L0Loss</td>
<td align="left"><a href="https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf" rel="nofollow">D</a></td>
</tr>
<tr>
<td align="left">SGD Classifiers</td>
<td align="left">*Loss + No/L2/L1  and OVA</td>
<td align="left"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" rel="nofollow">SkL</a></td>
</tr>
</tbody>
</table>
<ul>
<li>(⭒) should be added soon</li>
</ul>
<h4><a id="user-content-other-regression-models" class="anchor" aria-hidden="true" href="#other-regression-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Other regression models</h4>
<p>There are a number of other regression models that may be included in this package in the longer term but may not directly correspond to the paradigm <code>Loss+Penalty</code> introduced earlier.</p>
<p>In some cases it will make more sense to just use <a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a>.</p>
<p>Sklearn's list: <a href="https://scikit-learn.org/stable/supervised_learning.html#supervised-learning" rel="nofollow">https://scikit-learn.org/stable/supervised_learning.html#supervised-learning</a></p>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Note</th>
<th align="left">Link(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">LARS</td>
<td align="left">--</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Quantile Regression</td>
<td align="left">--</td>
<td align="left"><a href="https://www.stat.berkeley.edu/~mmahoney/pubs/quantile-icml13.pdf" rel="nofollow">Yang et al, 2013</a>, <a href="https://github.com/pkofod/QuantileRegression.jl">QuantileRegression.jl</a></td>
</tr>
<tr>
<td align="left">L∞ approx (Logsumexp)</td>
<td align="left">--</td>
<td align="left"><a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F15/L15.pdf" rel="nofollow">slides</a></td>
</tr>
<tr>
<td align="left">Passive Agressive</td>
<td align="left">--</td>
<td align="left"><a href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf" rel="nofollow">Crammer et al, 2006</a> <a href="https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms" rel="nofollow">SkL</a></td>
</tr>
<tr>
<td align="left">Orthogonal Matching Pursuit</td>
<td align="left">--</td>
<td align="left"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit" rel="nofollow">SkL</a></td>
</tr>
<tr>
<td align="left">Least Median of Squares</td>
<td align="left">--</td>
<td align="left"><a href="http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf" rel="nofollow">Rousseeuw, 1984</a></td>
</tr>
<tr>
<td align="left">RANSAC, Theil-Sen</td>
<td align="left">Robust reg</td>
<td align="left"><a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf" rel="nofollow">Overview RANSAC</a>, <a href="https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors" rel="nofollow">SkL</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" rel="nofollow">SkL</a>, <a href="http://www.cs.tau.ac.il/~turkel/imagepapers/RANSAC4Dummies.pdf" rel="nofollow">More Ransac</a></td>
</tr>
<tr>
<td align="left">Ordinal regression</td>
<td align="left"><em>need to figure out how they work</em></td>
<td align="left"><a href="https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf" rel="nofollow">E</a></td>
</tr>
<tr>
<td align="left">Count regression</td>
<td align="left"><em>need to figure out how they work</em></td>
<td align="left"><a href="https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf" rel="nofollow">R</a></td>
</tr>
<tr>
<td align="left">Robust M estimators</td>
<td align="left">--</td>
<td align="left"><a href="https://arxiv.org/pdf/1508.01967.pdf" rel="nofollow">F</a></td>
</tr>
<tr>
<td align="left">Perceptron, MIRA classifier</td>
<td align="left">Sklearn just does OVA with binary in SGDClassif</td>
<td align="left"><a href="https://cl.lingfil.uu.se/~nivre/master/ml7-18.pdf" rel="nofollow">H</a></td>
</tr>
<tr>
<td align="left">Robust PTS and LTS</td>
<td align="left">--</td>
<td align="left"><a href="https://arxiv.org/pdf/0901.0876.pdf" rel="nofollow">PTS</a> <a href="https://arxiv.org/pdf/1304.4773.pdf" rel="nofollow">LTS</a></td>
</tr>
</tbody>
</table>
<h2><a id="user-content-what-about-other-packages" class="anchor" aria-hidden="true" href="#what-about-other-packages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What about other packages</h2>
<p>While the functionalities in this package overlap with a number of existing packages, the hope is that this package will offer a general entry point for all of them in a way that won't require too much thinking from an end user (similar to how someone would use the tools from <code>sklearn.linear_model</code>).
If you're looking for specific functionalities/algorithms, it's probably a good idea to look at one of the packages below:</p>
<ul>
<li><a href="https://github.com/joshday/SparseRegression.jl">SparseRegression.jl</a></li>
<li><a href="https://github.com/JuliaStats/Lasso.jl">Lasso.jl</a></li>
<li><a href="https://github.com/pkofod/QuantileRegression.jl">QuantileRegression.jl</a></li>
<li>(unmaintained) <a href="https://github.com/lindahua/Regression.jl">Regression.jl</a></li>
<li>(unmaintained) <a href="https://github.com/simonster/LARS.jl">LARS.jl</a></li>
<li>(unmaintained) <a href="https://github.com/klkeys/FISTA.jl">FISTA.jl</a></li>
<li>(unmaintained) <a href="https://github.com/FugroRoames/RobustLeastSquares.jl">RobustLeastSquares.jl</a></li>
</ul>
<p>There's also <a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a> which is more geared towards statistical analysis for reasonably-sized datasets and does (as far as I'm aware) lack a few key functionalities for ML such as penalised regressions or multinomial regression.</p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li><strong>Minka</strong>, <a href="https://tminka.github.io/papers/logreg/minka-logreg.pdf" rel="nofollow">Algorithms for Maximum Likelihood Regression</a>, 2003. For a review of numerical methods for the binary Logistic Regression.</li>
<li><strong>Beck</strong> and <strong>Teboulle</strong>, <a href="https://tinyurl.com/beck-teboulle-fista" rel="nofollow">A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems</a>, 2009. For the ISTA and FISTA algorithms.</li>
<li><strong>Raman</strong> et al, <a href="https://arxiv.org/pdf/1604.04706.pdf" rel="nofollow">DS-MLR: Exploiting Double Separability for Scaling up DistributedMultinomial Logistic Regression</a>, 2018. For a discussion of multinomial regression.</li>
<li><em>Robust regression</em>
<ul>
<li><strong>Mastronardi</strong>, <a href="https://pdfs.semanticscholar.org/5d54/df9fc59b26027ede8599af850cd46cdf2255.pdf" rel="nofollow">Fast Robust Regression Algorithms for Problems with Toeplitz Structure</a>, 2007. For a discussion on algorithms for robust regression.</li>
<li><strong>Fox</strong> and <strong>Weisberg</strong>, <a href="http://users.stat.umn.edu/~sandy/courses/8053/handouts/robust.pdf" rel="nofollow">Robust Regression</a>, 2013. For a discussion on robust regression and the IWLS algorithm.</li>
<li><em>Statsmodels</em>, <a href="https://www.statsmodels.org/dev/examples/notebooks/generated/robust_models_1.html" rel="nofollow">M Estimators for Robust Linear Modeling</a>. For a list of weight functions beyond Huber's.</li>
<li><strong>O'Leary</strong>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.8839&amp;rep=rep1&amp;type=pdf" rel="nofollow">Robust Regression Computation using Iteratively Reweighted Least Squares</a>, 1990. Discussion of a few common robust regressions and implementation with IWLS.</li>
</ul>
</li>
</ul>
<h2><a id="user-content-dev-notes" class="anchor" aria-hidden="true" href="#dev-notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dev notes</h2>
<ul>
<li>Probit Loss --&gt; via StatsFuns // Φ(x) (normcdf); ϕ(x) (normpdf); -xϕ(x)</li>
<li>Newton, LBFGS take linesearches, seems NewtonCG doesn't</li>
<li>several ways of doing backtracking (e.g. <a href="https://archive.siam.org/books/mo25/mo25_ch10.pdf" rel="nofollow">https://archive.siam.org/books/mo25/mo25_ch10.pdf</a>); for FISTA many though see <a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/fista.pdf" rel="nofollow">http://www.seas.ucla.edu/~vandenbe/236C/lectures/fista.pdf</a>; probably best to have "decent safe defaults"; also this for FISTA <a href="http://150.162.46.34:8080/icassp2017/pdfs/0004521.pdf" rel="nofollow">http://150.162.46.34:8080/icassp2017/pdfs/0004521.pdf</a> ; <a href="https://github.com/tiepvupsu/FISTA#in-case-lf-is-hard-to-find">https://github.com/tiepvupsu/FISTA#in-case-lf-is-hard-to-find</a> ; <a href="https://hal.archives-ouvertes.fr/hal-01596103/document" rel="nofollow">https://hal.archives-ouvertes.fr/hal-01596103/document</a>; not so great <a href="https://github.com/klkeys/FISTA.jl/blob/master/src/lasso.jl">https://github.com/klkeys/FISTA.jl/blob/master/src/lasso.jl</a> ;</li>
<li><a href="https://www.ljll.math.upmc.fr/~plc/prox.pdf" rel="nofollow">https://www.ljll.math.upmc.fr/~plc/prox.pdf</a></li>
<li>proximal QN <a href="http://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/24-prox-newton.pdf" rel="nofollow">http://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/24-prox-newton.pdf</a>; <a href="https://www.cs.utexas.edu/~inderjit/public_papers/Prox-QN_nips2014.pdf" rel="nofollow">https://www.cs.utexas.edu/~inderjit/public_papers/Prox-QN_nips2014.pdf</a>; <a href="https://github.com/yuekai/PNOPT">https://github.com/yuekai/PNOPT</a>; <a href="https://arxiv.org/pdf/1206.1623.pdf" rel="nofollow">https://arxiv.org/pdf/1206.1623.pdf</a></li>
<li>group lasso <a href="http://myweb.uiowa.edu/pbreheny/7600/s16/notes/4-27.pdf" rel="nofollow">http://myweb.uiowa.edu/pbreheny/7600/s16/notes/4-27.pdf</a></li>
</ul>
</article></div>