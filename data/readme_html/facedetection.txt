<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 align="center" dir="auto"><a id="user-content----facedetection-using-viola-jones-robust-algorithm-for-object-detection" class="anchor" aria-hidden="true" href="#---facedetection-using-viola-jones-robust-algorithm-for-object-detection"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>
   FaceDetection using Viola-Jones' Robust Algorithm for Object Detection
</h1>

<p dir="auto"><a href="https://jakewilliami.github.io/FaceDetection.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/jakewilliami/FaceDetection.jl/actions?query=workflow%3ACI"><img src="https://github.com/invenia/PkgTemplates.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://github.com/invenia/BlueStyle"><img src="https://camo.githubusercontent.com/c18fbaa52d94d16b90b19701fc90d289b8a5bb920c74c79bab200b14e75420a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c75652d3434393564312e737667" alt="Code Style: Blue" data-canonical-src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7fbf197c8358c4828cfaf8739134a64c1e152bdb985c69dfb7e01461d517de73/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374617475732d6d61747572696e672d677265656e"><img src="https://camo.githubusercontent.com/7fbf197c8358c4828cfaf8739134a64c1e152bdb985c69dfb7e01461d517de73/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374617475732d6d61747572696e672d677265656e" alt="Project Status" data-canonical-src="https://img.shields.io/badge/status-maturing-green" style="max-width: 100%;"></a></p>
<hr>
<h3 dir="auto"><a id="user-content-caveats-help-wanted" class="anchor" aria-hidden="true" href="#caveats-help-wanted"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Caveats;— help wanted</h3>
<p dir="auto">Currently, there are two main areas I would love some help with:</p>
<ol dir="auto">
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/issues/20" data-hovercard-type="issue" data-hovercard-url="/jakewilliami/FaceDetection.jl/issues/20/hovercard">Reading</a> from <a href="https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml">pre-trained</a> data files would greatly help those who are not training the object detection framework themselves; and</li>
<li>Creating (<a href="https://github.com/jakewilliami/FaceDetection.jl/issues/21" data-hovercard-type="issue" data-hovercard-url="/jakewilliami/FaceDetection.jl/issues/21/hovercard"><em>scalable</em></a>) <a href="https://github.com/jakewilliami/FaceDetection.jl/issues/6" data-hovercard-type="issue" data-hovercard-url="/jakewilliami/FaceDetection.jl/issues/6/hovercard">bounding boxes around detected objects</a>.  Everyone loves a visual; you can show your grandmother a face with a box around it, but she'll switch off if you show her any code.  People want pictures!</li>
</ol>
<p dir="auto">Since this project helped me with the <code>get_faceness</code> function (this idea of scoring the "faceness" of an image), now that I have data from this I have put this project <a href="https://dictionary.cambridge.org/dictionary/english/on-the-back-burner" rel="nofollow">on the back burner</a>.  However, in the interest of others, if anyone were to help with these two items (and any other <a href="https://github.com/jakewilliami/FaceDetection.jl/issues/">issues</a>), they would forever have my gratitude.</p>
<hr>
<h2 dir="auto"><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto">This is a Julia implementation of <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.6807" rel="nofollow">Viola-Jones' Object Detection algorithm</a>.  Although there is an <a href="https://github.com/JuliaOpenCV/OpenCV.jl">OpenCV port in Julia</a>, it seems to be ill-maintained.  As this algorithm was created for commercial use, there seem to be few widely-used or well-documented implementations of it on GitHub.  The implementation this repository is based off is <a href="https://github.com/Simon-Hohberg/Viola-Jones">Simon Hohberg's Pythonic repository</a>, as it seems to be well written (and the most starred Python implementation on GitHub, though this is not necessarily a good measure). Julia and Python alike are easy to read and write in — my thinking was that this would be easy enough to replicate in Julia, except for Pythonic classes, where I would have to use <code>struct</code>s (or at least easier to replicate from than, for example, <a href="https://github.com/alexdemartos/ViolaAndJones">C++</a> or <a href="https://github.com/foo123/HAAR.js">JS</a> — two other highly-starred repositories.).</p>
<p dir="auto">I <em>implore</em> collaboration.  I am an undergraduate student with no formal education in computer science (or computer vision of any form for that matter); I am certain this code can be refined/optimised by better programmers than myself.  Please, help me out if you like!</p>
<h2 dir="auto"><a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it works</h2>
<p dir="auto">In an over-simplified manner, the Viola-Jones algorithm has some four stages:</p>
<ol dir="auto">
<li>Takes an image, converts it into an array of intensity values (i.e., in grey-scale), and constructs an <a href="https://en.wikipedia.org/wiki/Summed-area_table" rel="nofollow">Integral Image</a>, such that for every element in the array, the Integral Image element is the sum of all elements above and to the left of it.  This makes calculations easier for step 2.</li>
<li>Finds <a href="https://en.wikipedia.org/wiki/Haar-like_feature" rel="nofollow">Haar-like Features</a> from Integral Image.</li>
<li>There is now a training phase using sets of faces and non-faces.  This phase uses something called Adaboost (short for Adaptive Boosting).  Boosting is one method of Ensemble Learning. There are other Ensemble Learning methods like Bagging, Stacking, &amp;c.. The differences between Bagging, Boosting, Stacking are:
<ul dir="auto">
<li>Bagging uses equal weight voting. Trains each model with a random drawn subset of training set.</li>
<li>Boosting trains each new model instance to emphasize the training instances that previous models mis-classified. Has better accuracy comparing to bagging, but also tends to overfit.</li>
<li>Stacking trains a learning algorithm to combine the predictions of several other learning algorithms.
Despite this method being developed at the start of the century, it is blazingly fast compared to some machine learning algorithms, and still widely used.</li>
</ul>
</li>
<li>Finally, this algorithm uses <a href="https://en.wikipedia.org/wiki/Cascading_classifiers" rel="nofollow">Cascading Classifiers</a> to identify faces.  (See page 12 of the original paper for the specific cascade).</li>
</ol>
<p dir="auto">For a better explanation, read <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.6807" rel="nofollow">the paper from 2001</a>, or see <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework" rel="nofollow">the Wikipedia page</a> on this algorithm.</p>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using FaceDetection

# Constants
pos_training_path, neg_training_path = &quot;...&quot;, &quot;...&quot;
num_faces, num_non_faces = length(filtered_ls(pos_testing_path)), length(filtered_ls(neg_testing_path))  # You can also just put in a simple number here, if you know how many training images you have
num_classifiers = 10
min_feature_height, max_feature_height = 0, 19
min_feature_width, max_feature_width = 0, 19
scale, scale_to = true, (19, 19)  # we want all training images to be standardised to size 19x19

# Train a model
classifiers = FaceDetection.learn(pos_training_path, neg_training_path, num_classifiers, min_feature_height, max_feature_height, min_feature_width, max_feature_width; scale = scale, scale_to = scale_to)

# Results
correct_faces = sum(ensemble_vote_all(pos_testing_path, classifiers, scale=scale, scale_to=scale_to))
correct_non_faces = num_non_faces - sum(ensemble_vote_all(neg_testing_path, classifiers, scale=scale, scale_to=scale_to))
println((correct_faces / num_faces) * 100, &quot;% of faces were recognised as faces&quot;)
println((correct_non_faces / num_non_faces) * 100, &quot;% of non-faces were identified as non-faces&quot;)"><pre><span class="pl-k">using</span> FaceDetection

<span class="pl-c"><span class="pl-c">#</span> Constants</span>
pos_training_path, neg_training_path <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>...<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>...<span class="pl-pds">"</span></span>
num_faces, num_non_faces <span class="pl-k">=</span> <span class="pl-c1">length</span>(<span class="pl-c1">filtered_ls</span>(pos_testing_path)), <span class="pl-c1">length</span>(<span class="pl-c1">filtered_ls</span>(neg_testing_path))  <span class="pl-c"><span class="pl-c">#</span> You can also just put in a simple number here, if you know how many training images you have</span>
num_classifiers <span class="pl-k">=</span> <span class="pl-c1">10</span>
min_feature_height, max_feature_height <span class="pl-k">=</span> <span class="pl-c1">0</span>, <span class="pl-c1">19</span>
min_feature_width, max_feature_width <span class="pl-k">=</span> <span class="pl-c1">0</span>, <span class="pl-c1">19</span>
scale, scale_to <span class="pl-k">=</span> <span class="pl-c1">true</span>, (<span class="pl-c1">19</span>, <span class="pl-c1">19</span>)  <span class="pl-c"><span class="pl-c">#</span> we want all training images to be standardised to size 19x19</span>

<span class="pl-c"><span class="pl-c">#</span> Train a model</span>
classifiers <span class="pl-k">=</span> FaceDetection<span class="pl-k">.</span><span class="pl-c1">learn</span>(pos_training_path, neg_training_path, num_classifiers, min_feature_height, max_feature_height, min_feature_width, max_feature_width; scale <span class="pl-k">=</span> scale, scale_to <span class="pl-k">=</span> scale_to)

<span class="pl-c"><span class="pl-c">#</span> Results</span>
correct_faces <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">ensemble_vote_all</span>(pos_testing_path, classifiers, scale<span class="pl-k">=</span>scale, scale_to<span class="pl-k">=</span>scale_to))
correct_non_faces <span class="pl-k">=</span> num_non_faces <span class="pl-k">-</span> <span class="pl-c1">sum</span>(<span class="pl-c1">ensemble_vote_all</span>(neg_testing_path, classifiers, scale<span class="pl-k">=</span>scale, scale_to<span class="pl-k">=</span>scale_to))
<span class="pl-c1">println</span>((correct_faces <span class="pl-k">/</span> num_faces) <span class="pl-k">*</span> <span class="pl-c1">100</span>, <span class="pl-s"><span class="pl-pds">"</span>% of faces were recognised as faces<span class="pl-pds">"</span></span>)
<span class="pl-c1">println</span>((correct_non_faces <span class="pl-k">/</span> num_non_faces) <span class="pl-k">*</span> <span class="pl-c1">100</span>, <span class="pl-s"><span class="pl-pds">"</span>% of non-faces were identified as non-faces<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">For more examples like this, see <a href="examples/"><code>examples/</code></a>.</p>
<h2 dir="auto"><a id="user-content-miscellaneous-notes" class="anchor" aria-hidden="true" href="#miscellaneous-notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Miscellaneous Notes</h2>
<h3 dir="auto"><a id="user-content-timeline-of-progression" class="anchor" aria-hidden="true" href="#timeline-of-progression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Timeline of Progression</h3>
<ul dir="auto">
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/a79ab6f9">a79ab6f9</a> — Began working on the algorithm; mainly figuring out best way to go about this implementation.</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/fd5e645c">fd5e645c</a> — First "Julia" adaptation of the algorithm; still a <em>lot</em> of bugs to figure out.</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/2fcae630">2fcae630</a> — Started bug fixing using <code>src/FDA.jl</code> (the main example file).</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/f1f5b5ea">f1f5b5ea</a> — Getting along very well with bug fixing (created a <code>struct</code> for Haar-like feature; updated weighting calculations; fixed <code>hstack</code> translation with nested arrays).  Added detailed comments on each function.</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/a9e10eb4">a9e10eb4</a> — First working draft of the algorithm (without image reconstruction)!</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/6b35f6d5">6b35f6d5</a> — Finally, the algorithm works as it should.  Just enhancements from here on out.</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/854bba32">854bba32</a> and <a href="https://github.com/jakewilliami/FaceDetection.jl/commit/655e0e14">655e0e14</a> — Implemented facelike scoring and wrote score data to CSV (see <a href="https://github.com/jakewilliami/FaceDetection.jl/issues/7" data-hovercard-type="issue" data-hovercard-url="/jakewilliami/FaceDetection.jl/issues/7/hovercard">#7</a>).</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/e7295f8d">e7295f8d</a> — Implemented writing training data to file and reading from that data to save computation time.</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/e9116987">e9116987</a> — Changed to sequential processing.</li>
<li><a href="https://github.com/jakewilliami/FaceDetection.jl/commit/750aa22d">750aa22d</a>–<a href="https://github.com/jakewilliami/FaceDetection.jl/commit/b3aec6b8">b3aec6b8</a> — Optimised performance.</li>
</ul>
 
<h3 dir="auto"><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h3>
<p dir="auto">Thank you to:</p>
<ul dir="auto">
<li><a href="https://github.com/Simon-Hohberg"><strong>Simon Honberg</strong></a> for the original open-source Python code upon which this repository is largely based.  This has provided me with an easy-to-read and clear foundation for the Julia implementation of this algorithm;</li>
<li><a href="https://www.merl.com/people/mjones" rel="nofollow"><strong>Michael Jones</strong></a> for (along with <a href="https://people.wgtn.ac.nz/tirta.susilo" rel="nofollow">Tirta Susilo</a>) suggesting the method for a <em>facelike-ness</em> measure;</li>
<li><a href="https://environment.leeds.ac.uk/staff/9408/dr-mahdi-rezaei" rel="nofollow"><strong>Mahdi Rezaei</strong></a> for helping me understand the full process of Viola-Jones' object detection;</li>
<li><a href="https://ecs.wgtn.ac.nz/Main/GradYingBi" rel="nofollow"><strong>Ying Bi</strong></a> for always being happy to answer questions (which mainly turned out to be a lack of programming knowledge rather than conceptual; also with help from <a href="https://homepages.ecs.vuw.ac.nz/~xuebing/index.html" rel="nofollow"><strong>Bing Xue</strong></a>);</li>
<li><strong>Mr. H. Lockwood</strong> and <strong>Mr. D. Peck</strong> are Comp. Sci. students who have answered a few questions of mine;</li>
<li>Finally, the people in the Julia slack channel, for dealing with many (probably stupid) questions.  Just a few who come to mind: Micket, David Sanders, Eric Forgy, Jakob Nissen, and Roel.</li>
</ul>
<h3 dir="auto"><a id="user-content-a-note-on-running-on-bsd" class="anchor" aria-hidden="true" href="#a-note-on-running-on-bsd"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>A Note on running on BSD:</h3>
<p dir="auto">The default JuliaPlots backend <code>GR</code> does not provide binaries for FreeBSD.  <a href="https://github.com/jheinen/GR.jl/issues/268#issuecomment-584389111" data-hovercard-type="issue" data-hovercard-url="/jheinen/GR.jl/issues/268/hovercard">Here's how you can build it from source.</a>.  That said, <code>StatsPlots</code> is only a dependency for an example, and not for the main package.</p>
</article></div>