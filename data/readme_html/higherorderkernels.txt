<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-higherorderkernels" class="anchor" aria-hidden="true" href="#higherorderkernels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>HigherOrderKernels</h1>
<p>This package provides basic kernel density estimation using <em>higher order kernels</em> also known as <em>bias reducing kernels</em>.
At the moment only kernels from the polynomial family described in [1] and the Gaussian family described in [2]. This includes some widely used kernels:</p>
<ul>
<li>Uniform</li>
<li>Epanechnikov</li>
<li>Biweight</li>
<li>Triweight</li>
<li>Gaussian</li>
</ul>
<p>and higher order versions. Kernel code is generated automatically, so any order is possible.</p>
<p>For bandwidht selection, Silverman's rule-of-thumb is implemented for all kernels.</p>
<p><a target="_blank" rel="noopener noreferrer" href="kernels.svg"><img src="kernels.svg" alt="" style="max-width:100%;"></a></p>
<h1><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h1>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using HigherOrderKernels
using Plots
using Distributions

data = randn(10000)
xgrid = linspace(-2, 2, 100)
plt = plot()
for ν = 2:2:8
  k = EpanechnikovKernel{ν}
  h = bandwidth(k, data)
  p = kpdf.(k, xgrid, [data], h)
  plot!(plt, xgrid, p, label=&quot;Order $ν&quot;)
end
plot!(x -&gt; pdf(Normal(), x), label=&quot;Exact&quot;)
"><pre><span class="pl-k">using</span> HigherOrderKernels
<span class="pl-k">using</span> Plots
<span class="pl-k">using</span> Distributions

data <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">10000</span>)
xgrid <span class="pl-k">=</span> <span class="pl-c1">linspace</span>(<span class="pl-k">-</span><span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">100</span>)
plt <span class="pl-k">=</span> <span class="pl-c1">plot</span>()
<span class="pl-k">for</span> ν <span class="pl-k">=</span> <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">8</span>
  k <span class="pl-k">=</span> EpanechnikovKernel{ν}
  h <span class="pl-k">=</span> <span class="pl-c1">bandwidth</span>(k, data)
  p <span class="pl-k">=</span> <span class="pl-c1">kpdf</span>.(k, xgrid, [data], h)
  <span class="pl-c1">plot!</span>(plt, xgrid, p, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Order <span class="pl-v">$ν</span><span class="pl-pds">"</span></span>)
<span class="pl-k">end</span>
<span class="pl-c1">plot!</span>(x <span class="pl-k">-&gt;</span> <span class="pl-c1">pdf</span>(<span class="pl-c1">Normal</span>(), x), label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Exact<span class="pl-pds">"</span></span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="example.svg"><img src="example.svg" alt="" style="max-width:100%;"></a></p>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h1>
<ol>
<li>Hansen, B. E. (2005). EXACT MEAN INTEGRATED SQUARED ERROR OF HIGHER ORDER KERNEL ESTIMATORS. Econometric Theory, 21(6), 1031–1057. <a href="http://doi.org/10.1017/S0266466605050528" rel="nofollow">http://doi.org/10.1017/S0266466605050528</a></li>
<li>Wand, M. P., &amp; Schucany, W. R. (1990). Gaussian-based kernels. The Canadian Journal of Statistics. La Revue Canadienne De Statistique, 18(3), 197–204. <a href="http://doi.org/10.2307/3315450?refreqid=search-gateway:c3a7da0239447971afa03cb0995530fd" rel="nofollow">http://doi.org/10.2307/3315450?refreqid=search-gateway:c3a7da0239447971afa03cb0995530fd</a></li>
</ol>
</article></div>