<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-ptools" class="anchor" aria-hidden="true" href="#ptools"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>PTools</h1>
<p>A collection of utilities for parallel computation in Julia</p>
<p>Currently the following are available.</p>
<ul>
<li>
<p><code>WorkerSet</code> - The ability to logically pool a set of workers for specific tasks.</p>
</li>
<li>
<p><code>pfork</code> - Parallelism using the UNIX <code>fork</code> system call.</p>
</li>
<li>
<p>ServerTasks - These are long running tasks that simply processes incoming requests in a loop. Useful in situations where
state needs to be maintained across function calls. State can be maintained and retrieved using the task_local_storage methods.</p>
</li>
<li>
<p>SharedMemory - Useful in the event of parallel procesing on a single large multi-core machine. Avoids the overhead associated
with sending/recieving large data sets.</p>
</li>
<li>
<p><a href="QUEUEDTASKS.md">QueuedTasks</a> - Schedule tasks to be executed by remote worker processes. Set/change priorities on the task to control order of execution.</p>
</li>
</ul>
<h1><a id="user-content-platforms" class="anchor" aria-hidden="true" href="#platforms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Platforms</h1>
<ul>
<li>Tested on Ubuntu 13.04</li>
<li>Should work on OSX</li>
<li>SharedMemory will not work on Windows. ServerTasks should.</li>
<li>pfork is a UNIX only implementation</li>
</ul>
<h1><a id="user-content-workerset" class="anchor" aria-hidden="true" href="#workerset"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>WorkerSet</h1>
<ul>
<li>
<p>A WorkerSet is just an array of process ids</p>
</li>
<li>
<p>A WorkerSet is created using <code>WorkerSet(w::Array{Integer}, mode)</code> where <code>mode</code> is either of
WS_MODE_RR or WS_MODE_FF where</p>
</li>
<li>
<p>WS_MODE_RR enables the workers to be used in a round-robin fashion</p>
</li>
<li>
<p>WS_MODE_FF tracks which of the workers are busy and sends the request only to a free one.
It queues the requests if all the workers in the set are busy.</p>
</li>
<li>
<p>The <code>remotecall*</code> functions have been extended to support WorkerSet</p>
</li>
</ul>
<pre><code>remotecall(ws::WorkerSet, f, args...) 
remotecall_fetch(ws::WorkerSet, f, args...) 
remotecall_wait(ws::WorkerSet, f, args...) 
</code></pre>
<ul>
<li>The behaviour is the same, except that the functions are executed only on the processes belonging to the
WorkerSet and follow the model as specified by <code>mode</code>.</li>
</ul>
<h1><a id="user-content-pfork" class="anchor" aria-hidden="true" href="#pfork"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>pfork</h1>
<ul>
<li>
<p>It uses the UNIX <code>fork()</code> system call to execute a function in parallel.</p>
</li>
<li>
<p><code>pfork(nforks::Integer, f::Function, args...)</code> forks <code>nforks</code> times and executes <code>f</code> in each child.</p>
</li>
<li>
<p>The first parameter to <code>f</code> is the forked child index.</p>
</li>
<li>
<p><code>pfork</code> returns an array of size <code>nforks</code>, where the nth element is the value returned by the nth forked child.</p>
</li>
<li>
<p>Passing huge amounts of data to the function in the child process is a non-issue since it is a fork.</p>
</li>
<li>
<p>In the event the parent process has a shared memory segment, the children have full visibility
into the same and can modify the segment in place. They don't have to bother with returning huge
amounts of data either.</p>
</li>
<li>
<p>Currently can only be executed when nprocs() == 1, i.e., this model is incompatible with the worker model.</p>
</li>
<li>
<p>Unpredictable behavior if the function to be executed in the forked children calls yield() AND other I/O tasks
are concurrently active. Can be used only where f is fully compute bound.</p>
</li>
</ul>
<h1><a id="user-content-usage-of-server-tasks" class="anchor" aria-hidden="true" href="#usage-of-server-tasks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage of Server Tasks</h1>
<p>Typical usage pattern will be</p>
<ul>
<li>
<p><code>start_stasks</code> - Start Server Tasks, optionally with shared memory mappings.</p>
</li>
<li>
<p>Execute a series of functions in parallel on these tasks using multiple invocations of <code>pmap_stasks</code></p>
</li>
<li>
<p>SomeFunction</p>
</li>
<li>
<p>SomeOtherFunction</p>
</li>
<li>
<p>SomeOtherFunction
.
.
.</p>
</li>
<li>
<p><code>stop_stasks</code> - Stop all Server Tasks and free shared memory if required.</p>
</li>
</ul>
<p>The user specified functions in pmap_stasks can store and retrieve state information using the task_local_storage functions.</p>
<h1><a id="user-content-example-for-shared-memory-and-server-tasks" class="anchor" aria-hidden="true" href="#example-for-shared-memory-and-server-tasks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example for shared memory and server tasks</h1>
<p>The best way to understand what is available is by example:</p>
<ul>
<li>specify shared memory configuration.</li>
</ul>
<pre><code>using PTools

shmcfg = [ShmCfg(:svar1, Int32, 64*1024), ShmCfg(:svar2, Uint8, (100,100))]
</code></pre>
<ul>
<li>
<p>the above line requests for a 64K Int32 array bound to <code>svar1</code>, and a 100x100 Uint8 array bound to <code>svar2</code></p>
</li>
<li>
<p>Start tasks.</p>
</li>
</ul>
<pre><code>h = start_stasks(shmcfg)
ntasks = count_stasks(h)
</code></pre>
<ul>
<li>
<p>The tasks are started and symbols pointing to shared memory segments are added as task local storage. A handle is returned.</p>
</li>
<li>
<p>The shared memory segments are also mapped in the current tasks local storage.</p>
</li>
<li>
<p>NOTE: If nprocs() &gt; 1, then only the Worker Julia processes are used to start the Server Tasks, i.e., if nprocs() == 5, then ntasks above would be 4.</p>
</li>
<li>
<p>Prepare arguments for our pmap call</p>
</li>
</ul>
<pre><code>offset_list = [i for i in 1:ntasks]
ntasks_list = [ntasks for i in 1:ntasks]
</code></pre>
<ul>
<li>Execute our function in parallel.</li>
</ul>
<pre><code>resp = pmap_stasks(h, (offset, ntasks) -&gt; begin
                        # get local refernces to shared memory mapped arrays
                        svar1 = task_local_storage(:svar1)
                        svar2 = task_local_storage(:svar2)
                        
                        mypid = myid()
                        for x in offset:ntasks:64*1024
                            svar1[x] = mypid
                        end
                        
                        true
                        
                    end,
                    
                    offset_list, ntasks_list)
</code></pre>
<ul>
<li>Access shared memory segments and view changes</li>
</ul>
<pre><code>svar1 = task_local_storage(:svar1)
println(svar1)
</code></pre>
<p>svar1 will the values as updated by the Server Tasks.</p>
<ul>
<li>Finally stop the tasks</li>
</ul>
<pre><code>stop_stasks(h, shmcfg)
</code></pre>
<p>This causes all tasks to be stopped and the shared memory unmapped.</p>
<h1><a id="user-content-exported-functions-for-servertasks" class="anchor" aria-hidden="true" href="#exported-functions-for-servertasks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Exported functions for ServerTasks</h1>
<p><code>start_stasks(shmcfg=false, shmpfx=false)</code> where shmcfg is an optional parameter. It is either a ShmCfg(name::Symbol, t::Type, d::dims) or Array{ShmCfg, 1}.
Returns a handle to the set of ServerTasks.</p>
<p><code>pmap_stasks(h::STasks, f::Function, lsts...)</code> is similar to pmap, except that the first parameter is the handle returned by start_tasks.
NOTE: that the length of <code>lsts</code> and number of ServerTasks must be identical.</p>
<p><code>stop_stasks(h::STasks, shmcfg=false, shmpfx=false)</code> stops all tasks and also frees the shared memory</p>
<p><code>count_stasks(h::STasks)</code> returns the number of ServerTasks - can be used to partition the compute job.</p>
<p>NOTE: shmpfx should be set to a distinct string in case you are sharing the server with other users or have multiple self concurrent jobs active.</p>
<h1><a id="user-content-exported-functions-for-shared-memory-support" class="anchor" aria-hidden="true" href="#exported-functions-for-shared-memory-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Exported functions for Shared Memory support</h1>
<p><code>pmap_shm_create(shmcfg, shmpfx="")</code> - creates and maps the shared memory segments to global symbols in each Julia process. shmcfg
can be ShmCfg(name::Symbol, t::Type, d::dims) or Array{ShmCfg, 1}</p>
<p><code>unlink_shm(shmcfg, shmpfx="")</code> - frees the shared memory</p>
<p>NOTE: For a single run, it is important that shmpfx is passed with same value to all the methods.</p>
<p>Under Linux, you can view the shared memory mappings under /dev/shm
In the event of abnormal program termination, where unlink_shm has not been called it is important
to manually delete all segments allocated by the program. The name of the segments will be
of the form <code>/dev/julia_&lt;shmpfx&gt;_&lt;symbol_name&gt;</code></p>
</article></div>