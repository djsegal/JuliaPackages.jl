<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-merajl" class="anchor" aria-hidden="true" href="#merajl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MERA.jl</h1>
<p><a href="https://travis-ci.org/mhauru/MERA.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ec18f6add5074c503da19d95386b01a72ae75bb7/68747470733a2f2f7472617669732d63692e6f72672f6d68617572752f4d4552412e6a6c2e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://travis-ci.org/mhauru/MERA.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://codecov.io/gh/mhauru/MERA.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/52a4d71bd91f7d4af49e0a78bf49ef6d2990b328/68747470733a2f2f636f6465636f762e696f2f67682f6d68617572752f4d4552412e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/mhauru/MERA.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>MERA.jl provides Julia implementations of some basic <a href="https://arxiv.org/abs/quant-ph/0610099" rel="nofollow">Multiscale Entaglement Renormalization Ansatz</a> algorithms. It only implements infinite, translation invariant MERAs. At the moment it has implementations of ternary, binary, and modified binary MERAs, with functions for doing energy minimization, evaluating local expectation values, and computing scaling dimensions. An implementation is provided of the classic <a href="https://arxiv.org/abs/0707.1454" rel="nofollow">alternating energy minimization algorithm</a>, that in the code is called the Evenbly-Vidal, or EV, algorithm. Work is also ongoing on gradient based optimization methods. MERA.jl makes extensive use of <a href="https://github.com/Jutho/TensorKit.jl">TensorKit</a>, and uses it to support global internal symmetries, both Abelian and non-Abelian.</p>
<p>MERA.jl remains in active development as of April 2020.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p>The folder <code>demo</code> has a script <code>demo.jl</code>, that runs energy minimization on either the Ising or the XXZ model, and computes scaling dimensions and entanglement entropies from the resulting MERA. The best way to get going is to clone this repo, navigate to its folder, open a Julia prompt and do</p>
<pre><code>]activate .
include("demo/demo.jl")
</code></pre>
<p><code>demo.jl</code> writes to disk the MERAs it creates, by default in a folder called <code>JLMdata</code>. Another script, <code>demo/demo_refine.jl</code>, can be used to load these files, optimize the MERA further for better convergence, and write them back to disk. You can for instance first create a decent decent starting point for a MERA using <code>demo.jl</code>, since it builds the MERA up by slowly increasing bond dimension, and then use <code>demo_refine.jl</code> to push for proper convergence. Both of these scripts use <code>demo/demo_tools.jl</code>, which deals with creating Hamiltonians, writing to and reading from disk, and gradually increasing bond dimension during an optimization. Both <code>demo.jl</code> and <code>demo_refine.jl</code> take plenty of command line arguments, allowing things</p>
<pre><code>julia --project=. demo/demo.jl --model=XXZ --meratype=binary --chi=5 --layers=4 --symmetry=none
</code></pre>
<p>See the source code for more details.</p>
<p>The actual library is obviously in <code>src</code>. The type system is based on an abstract type <code>GenericMERA{T} where T &lt;: Layer</code>, and its concrete subtypes such as <code>TernaryMERA = GenericMERA{TernaryLayer}</code> and <code>BinaryMERA = GenericMERA{BinaryLayer}</code>. The file <code>src/genericmera.jl</code> implements functions that are independent of the exact type of MERA. <code>src/simplelayer.jl</code> implements methods for the abstract type <code>SimpleLayer</code> that all the concrete <code>Layer</code> types are subtypes of, that just assumes that each layer consists of a finite number of <code>TensorMap</code>s. <code>src/ternarylayer.jl</code>, <code>src/binarylayer.jl</code>, and <code>src/modifiedbinarylayer.jl</code> provide the details of things like ascending/descending superoperators, that depend on the specific MERA. <code>src/tensortools.jl</code> supplies some functions for TensorKit objects such as <code>TensorMap</code>s and vector spaces that the rest of the package needs.</p>
</article></div>