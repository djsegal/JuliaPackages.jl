<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-distlearnjl" class="anchor" aria-hidden="true" href="#distlearnjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DistLearn.jl</h1>
<p dir="auto">Example of distributed learning in Julia. Note this is <strong>not</strong> a full featured
distributed machine learning library, therefore we are <strong>not</strong> going to register
this in the Julia package system.</p>
<p dir="auto">Instead, this is a demonstrative project showing how to use Julia's parallel and
distributed computing interfaces to easily implement your own distributed
optimization / learning algorithms.</p>
<h2 dir="auto"><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dependencies</h2>
<p dir="auto">This example requires the following packages (with minimum version number specified):</p>
<ul dir="auto">
<li>Julia 0.6.0</li>
<li>ArgParse.jl 0.5.0</li>
</ul>
<h2 dir="auto"><a id="user-content-usages" class="anchor" aria-hidden="true" href="#usages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usages</h2>
<p dir="auto">The examples should be run with multiple Julia workers --- could be local processes or processes on remote nodes. To run Julia with <code>N</code> local worker process, type</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia -p N"><pre class="notranslate"><code>julia -p N
</code></pre></div>
<p dir="auto">or use <code>--machinefile &lt;file&gt;</code> to run workers on remote nodes specified by the hostnames listed in <code>&lt;file&gt;</code>. Please refer to <a href="https://docs.julialang.org/en/stable/manual/parallel-computing/" rel="nofollow">the Julia doc on parallel computing</a> for more details and other ways to run distributed Julia.</p>
<p dir="auto">The entry point for the examples are in <code>run.jl</code>. The general format is</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia -p &lt;N&gt; run.jl &lt;algor-name&gt; --&lt;algor-opts&gt; ..."><pre class="notranslate"><code>julia -p &lt;N&gt; run.jl &lt;algor-name&gt; --&lt;algor-opts&gt; ...
</code></pre></div>
<p dir="auto">Currently two algorithms are included: <code>asgd</code> and <code>cocoa</code>. Both algorithms are only implemented for binary classification with +1/-1 labels. ASGD is implemented with the hinge loss, and COCOA with the smoothed hinge loss. The reference for COCOA is</p>
<blockquote>
<p dir="auto">Jaggi, M., Smith, V., Tak√°c, M., Terhorst, J., Krishnan, S., Hofmann, T. and Jordan, M.I., 2014.
<em>Communication-efficient distributed dual coordinate ascent</em>.
In Advances in Neural Information Processing Systems (pp. 3068-3076).</p>
</blockquote>
<p dir="auto">For example, to run ASGD on the demo toy data with batch size 100 with 4 local worker processes:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia -p 4 run.jl asgd --batch-size=100"><pre class="notranslate"><code>julia -p 4 run.jl asgd --batch-size=100
</code></pre></div>
<p dir="auto">Or you can get a list of available options via</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="$ julia -p 2 run.jl asgd --help
usage: run.jl [--data DATA] [--n-epoch N-EPOCH]
              [--regu-coef REGU-COEF] [--batch-size BATCH-SIZE]
              [--learning-rate LEARNING-RATE] [-h]

optional arguments:
  --data DATA            (default: &quot;demo&quot;)
  --n-epoch N-EPOCH     (type: Int64, default: 100)
  --regu-coef REGU-COEF
                        (type: Float64, default: 0.01)
  --batch-size BATCH-SIZE
                        (type: Int64, default: 100)
  --learning-rate LEARNING-RATE
                        (type: Float64, default: 0.01)
  -h, --help            show this help message and exit"><pre>$ julia -p 2 run.jl asgd --help
usage: run.jl [--data DATA] [--n-epoch N-EPOCH]
              [--regu-coef REGU-COEF] [--batch-size BATCH-SIZE]
              [--learning-rate LEARNING-RATE] [-h]

optional arguments:
  --data DATA            (default: <span class="pl-s"><span class="pl-pds">"</span>demo<span class="pl-pds">"</span></span>)
  --n-epoch N-EPOCH     (type: Int64, default: 100)
  --regu-coef REGU-COEF
                        (type: Float64, default: 0.01)
  --batch-size BATCH-SIZE
                        (type: Int64, default: 100)
  --learning-rate LEARNING-RATE
                        (type: Float64, default: 0.01)
  -h, --help            show this <span class="pl-c1">help</span> message and <span class="pl-c1">exit</span></pre></div>
</article></div>