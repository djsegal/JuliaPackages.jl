<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-jlboostjl" class="anchor" aria-hidden="true" href="#jlboostjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>JLBoost.jl</h1>
<p>This is a 100%-Julia implementation of Gradient Boosting Regresssion Trees (GBRT) based heavily on the algorithms published in the XGBoost, LightGBM and Catboost papers. GBRT is also referred to as Gradient Boosting Decision Tree (GBDT).</p>
<h2><a id="user-content-limitations-for-now" class="anchor" aria-hidden="true" href="#limitations-for-now"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limitations for now</h2>
<ul>
<li>Currently, <code>Union{T, Missing}</code> feature type is not supported, but is <em>planned</em>.</li>
<li>Currently, only the single-valued models are supported. Multivariate-target models support is <em>planned</em>.</li>
<li>Currently, only the numeric and boolean features are supported. Categorical support is <em>planned</em>.</li>
<li>Currently, weights cannot be provided for each of the records. Support is <em>planned</em>.</li>
</ul>
<h2><a id="user-content-objectives" class="anchor" aria-hidden="true" href="#objectives"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Objectives</h2>
<ul>
<li>
<p>A full-featured &amp; batteries included Gradient Boosting Regression Tree library</p>
</li>
<li>
<p>Play nice with the Julia ecosystem e.g. Tables.jl, DataFrames.jl and CategoricalArrays.jl</p>
</li>
<li>
<p>100%-Julia</p>
</li>
<li>
<p>Fit models on large data</p>
</li>
<li>
<p>Easy to manipulate the tree after fitting; play with tree pruning and adjustments</p>
</li>
<li>
<p>"Easy" to deploy</p>
</li>
</ul>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quick-start</h2>
<h3><a id="user-content-fit-model-on-dataframe" class="anchor" aria-hidden="true" href="#fit-model-on-dataframe"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fit model on <code>DataFrame</code></h3>
<h4><a id="user-content-binary-classification" class="anchor" aria-hidden="true" href="#binary-classification"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Binary Classification</h4>
<p>We fit the model by predicting one of the iris Species. To fit a model on a <code>DataFrame</code> you need to specify the column and the features default to all columns other than the target.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> JLBoost, RDatasets
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)

iris[<span class="pl-k">!</span>, <span class="pl-c1">:is_setosa</span>] <span class="pl-k">=</span> iris[<span class="pl-k">!</span>, <span class="pl-c1">:Species</span>] <span class="pl-k">.==</span> <span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span>
target <span class="pl-k">=</span> <span class="pl-c1">:is_setosa</span>

features <span class="pl-k">=</span> <span class="pl-c1">setdiff</span>(<span class="pl-c1">names</span>(iris), [<span class="pl-c1">:Species</span>, <span class="pl-c1">:is_setosa</span>])

<span class="pl-c"><span class="pl-c">#</span> fit one tree</span>
<span class="pl-c"><span class="pl-c">#</span> ?jlboost for more details</span>
xgtreemodel <span class="pl-k">=</span> <span class="pl-c1">jlboost</span>(iris, target)</pre></div>
<pre><code>JLBoost.JLBoostTrees.JLBoostTreeModel(JLBoost.JLBoostTrees.AbstractJLBoostT
ree[eta = 1.0 (tree weight)

   -- PetalLength &lt;= 1.9
     ---- weight = 2.0

   -- PetalLength &gt; 1.9
     ---- weight = -2.0
], JLBoost.LogitLogLoss(), :is_setosa)
</code></pre>
<p>The returned model contains a vector of trees and the loss function and target</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">typeof</span>(<span class="pl-c1">trees</span>(xgtreemodel))</pre></div>
<pre><code>Array{JLBoost.JLBoostTrees.AbstractJLBoostTree,1}
</code></pre>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">typeof</span>(xgtreemodel<span class="pl-k">.</span>loss)</pre></div>
<pre><code>JLBoost.LogitLogLoss
</code></pre>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">typeof</span>(xgtreemodel<span class="pl-k">.</span>target)</pre></div>
<pre><code>Symbol
</code></pre>
<p>You can control parameters like  <code>max_depth</code> and <code>nrounds</code></p>
<div class="highlight highlight-source-julia"><pre>xgtreemodel2 <span class="pl-k">=</span> <span class="pl-c1">jlboost</span>(iris, target; nrounds <span class="pl-k">=</span> <span class="pl-c1">2</span>, max_depth <span class="pl-k">=</span> <span class="pl-c1">2</span>)</pre></div>
<pre><code>JLBoost.JLBoostTrees.JLBoostTreeModel(JLBoost.JLBoostTrees.AbstractJLBoostT
ree[eta = 1.0 (tree weight)

   -- PetalLength &lt;= 1.9
     ---- weight = 2.0

   -- PetalLength &gt; 1.9
     ---- weight = -2.0
, eta = 1.0 (tree weight)

   -- PetalLength &lt;= 1.9
     -- SepalLength &lt;= 4.8
       ---- weight = 1.1353352832366135

     -- SepalLength &gt; 4.8
       ---- weight = 1.1353352832366155

   -- PetalLength &gt; 1.9
     -- SepalLength &lt;= 7.9
       ---- weight = -1.1353352832366106

     -- SepalLength &gt; 7.9
       ---- weight = -1.1353352832366106
], JLBoost.LogitLogLoss(), :is_setosa)
</code></pre>
<p>Convenience <code>predict</code> function is provided. It can be used to score a tree or a vector of trees</p>
<div class="highlight highlight-source-julia"><pre>iris<span class="pl-k">.</span>pred1 <span class="pl-k">=</span> <span class="pl-c1">predict</span>(xgtreemodel, iris)
iris<span class="pl-k">.</span>pred2 <span class="pl-k">=</span> <span class="pl-c1">predict</span>(xgtreemodel2, iris)
iris<span class="pl-k">.</span>pred1_plus_2 <span class="pl-k">=</span> <span class="pl-c1">predict</span>(<span class="pl-c1">vcat</span>(xgtreemodel, xgtreemodel2), iris)</pre></div>
<pre><code>150-element Array{Float64,1}:
  5.135335283236616
  5.135335283236616
  5.135335283236613
  5.135335283236613
  5.135335283236616
  5.135335283236616
  5.135335283236613
  5.135335283236616
  5.135335283236613
  5.135335283236616
  ⋮
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
 -5.135335283236611
</code></pre>
<p>There are also convenience functions for computing the AUC and gini</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">AUC</span>(<span class="pl-k">-</span>iris<span class="pl-k">.</span>pred1, iris<span class="pl-k">.</span>is_setosa)</pre></div>
<pre><code>0.6666666666666667
</code></pre>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">gini</span>(<span class="pl-k">-</span>iris<span class="pl-k">.</span>pred1, iris<span class="pl-k">.</span>is_setosa)</pre></div>
<pre><code>0.3333333333333335
</code></pre>
<p>As a convenience feature, you can adjust the <code>eta</code> weight of each tree by multiplying it by a factor e.g.</p>
<div class="highlight highlight-source-julia"><pre>new_tree <span class="pl-k">=</span> <span class="pl-c1">0.3</span> <span class="pl-k">*</span> <span class="pl-c1">trees</span>(xgtreemodel)[<span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span> weight the first tree by 30%</span>
<span class="pl-c1">unique</span>(<span class="pl-c1">predict</span>(new_tree, iris) <span class="pl-k">./</span> <span class="pl-c1">predict</span>(<span class="pl-c1">trees</span>(xgtreemodel)[<span class="pl-c1">1</span>], iris)) <span class="pl-c"><span class="pl-c">#</span> 0.3</span></pre></div>
<h4><a id="user-content-feature-importances" class="anchor" aria-hidden="true" href="#feature-importances"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Feature Importances</h4>
<p>One can obtain the feature importance using the <code>feature_importance</code> function</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">feature_importance</span>(xgtreemodel, iris)</pre></div>
<pre><code>1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │
</code></pre>
<h4><a id="user-content-tablesjl-integration" class="anchor" aria-hidden="true" href="#tablesjl-integration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tables.jl integration</h4>
<p>Any Tables.jl compatible tabular data structure. So you can use any column accessible table with JLBoost. However, you are advised to define the following methods for <code>df</code> as the generic implementation in this package may not be efficient</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">nrow</span>(df) <span class="pl-c"><span class="pl-c">#</span> returns the number of rows</span>
<span class="pl-c1">ncol</span>(df)
<span class="pl-c1">view</span>(df, rows, cols)</pre></div>
<h4><a id="user-content-regression-model" class="anchor" aria-hidden="true" href="#regression-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Regression Model</h4>
<p>By default <code>JLBoost.jl</code> defines it's own <code>LogitLogLoss</code> type for  binary classification problems. You may replace the <code>loss</code> function-type from the <code>LossFunctions.jl</code> <code>SupervisedLoss</code> type. E.g for regression models you can choose the leaast squares loss called <code>L2DistLoss()</code></p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DataFrames
<span class="pl-k">using</span> JLBoost
df <span class="pl-k">=</span> <span class="pl-c1">DataFrame</span>(x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>) <span class="pl-k">*</span> <span class="pl-c1">100</span>)

df[<span class="pl-k">!</span>, <span class="pl-c1">:y</span>] <span class="pl-k">=</span> <span class="pl-c1">2</span><span class="pl-k">*</span>df<span class="pl-k">.</span>x <span class="pl-k">.+</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)

target <span class="pl-k">=</span> <span class="pl-c1">:y</span>
features <span class="pl-k">=</span> [<span class="pl-c1">:x</span>]
warm_start <span class="pl-k">=</span> <span class="pl-c1">fill</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">nrow</span>(df))


<span class="pl-k">using</span> LossFunctions<span class="pl-k">:</span> L2DistLoss
loss <span class="pl-k">=</span> <span class="pl-c1">L2DistLoss</span>()
<span class="pl-c1">jlboost</span>(df, target, features, warm_start, loss; max_depth<span class="pl-k">=</span><span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> default max_depth = 6</span></pre></div>
<pre><code>JLBoost.JLBoostTrees.JLBoostTreeModel(JLBoost.JLBoostTrees.AbstractJLBoostT
ree[eta = 1.0 (tree weight)

   -- x &lt;= 44.4379991667178
     -- x &lt;= 23.37486228930492
       ---- weight = 26.202932309423243

     -- x &gt; 23.37486228930492
       ---- weight = 70.14200049354278

   -- x &gt; 44.4379991667178
     -- x &lt;= 69.1144028107975
       ---- weight = 112.20011915070252

     -- x &gt; 69.1144028107975
       ---- weight = 174.87732294614628
], LossFunctions.LPDistLoss{2}(), :y)
</code></pre>
<h3><a id="user-content-save--load-models" class="anchor" aria-hidden="true" href="#save--load-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Save &amp; Load models</h3>
<p>You save the models using the <code>JLBoost.save</code> and load it with the <code>load</code> function</p>
<div class="highlight highlight-source-julia"><pre>JLBoost<span class="pl-k">.</span><span class="pl-c1">save</span>(xgtreemodel, <span class="pl-s"><span class="pl-pds">"</span>model.jlb<span class="pl-pds">"</span></span>)</pre></div>
<pre><code>testing save
</code></pre>
<div class="highlight highlight-source-julia"><pre>JLBoost<span class="pl-k">.</span><span class="pl-c1">save</span>(<span class="pl-c1">trees</span>(xgtreemodel), <span class="pl-s"><span class="pl-pds">"</span>model_tree.jlb<span class="pl-pds">"</span></span>)</pre></div>
<pre><code>testing save
</code></pre>
<div class="highlight highlight-source-julia"><pre>JLBoost<span class="pl-k">.</span><span class="pl-c1">load</span>(<span class="pl-s"><span class="pl-pds">"</span>model.jlb<span class="pl-pds">"</span></span>)
JLBoost<span class="pl-k">.</span><span class="pl-c1">load</span>(<span class="pl-s"><span class="pl-pds">"</span>model_tree.jlb<span class="pl-pds">"</span></span>)</pre></div>
<pre><code>Tree 1
eta = 1.0 (tree weight)

   -- PetalLength &lt;= 1.9
     ---- weight = 2.0

   -- PetalLength &gt; 1.9
     ---- weight = -2.0
</code></pre>
<h3><a id="user-content-fit-model-on-jdfjdffile---enabling-larger-than-ram-model-fit" class="anchor" aria-hidden="true" href="#fit-model-on-jdfjdffile---enabling-larger-than-ram-model-fit"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fit model on <code>JDF.JDFFile</code> - enabling larger-than-RAM model fit</h3>
<p>Sometimes, you may want to fit a model on a dataset that is too large to fit into RAM. You can convert the dataset to <a href="https://github.com/xiaodaigh/JDF.jl">JDF format</a> and then use <code>JDF.JDFFile</code> functionalities to fit the models. The interface <code>jlbosst</code> for <code>DataFrame</code> and <code>JDFFiLe</code> are the same.</p>
<p>The key advantage of fitting a model using <code>JDF.JDFFile</code> is that not all the data need to be loaded into memory. This is because <code>JDF</code> can load the columns one at a time. Hence this will enable larger models to be trained on a single computer.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> JLBoost, RDatasets, JDF
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)

iris[<span class="pl-k">!</span>, <span class="pl-c1">:is_setosa</span>] <span class="pl-k">=</span> iris[<span class="pl-k">!</span>, <span class="pl-c1">:Species</span>] <span class="pl-k">.==</span> <span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span>
target <span class="pl-k">=</span> <span class="pl-c1">:is_setosa</span>

features <span class="pl-k">=</span> <span class="pl-c1">setdiff</span>(<span class="pl-c1">names</span>(iris), [<span class="pl-c1">:Species</span>, <span class="pl-c1">:is_setosa</span>])

<span class="pl-c1">savejdf</span>(<span class="pl-s"><span class="pl-pds">"</span>iris.jdf<span class="pl-pds">"</span></span>, iris)
irisdisk <span class="pl-k">=</span> <span class="pl-c1">JDFFile</span>(<span class="pl-s"><span class="pl-pds">"</span>iris.jdf<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> fit using on disk JDF format</span>
xgtree1 <span class="pl-k">=</span> <span class="pl-c1">jlboost</span>(irisdisk, target, features)
xgtree2 <span class="pl-k">=</span> <span class="pl-c1">jlboost</span>(iris, target, features; nrounds <span class="pl-k">=</span> <span class="pl-c1">2</span>, max_depth <span class="pl-k">=</span> <span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> predict using on disk JDF format</span>
iris<span class="pl-k">.</span>pred1 <span class="pl-k">=</span> <span class="pl-c1">predict</span>(xgtree1, irisdisk)
iris<span class="pl-k">.</span>pred2 <span class="pl-k">=</span> <span class="pl-c1">predict</span>(xgtree2, irisdisk)

<span class="pl-c"><span class="pl-c">#</span> AUC</span>
<span class="pl-c1">AUC</span>(<span class="pl-k">-</span><span class="pl-c1">predict</span>(xgtree1, irisdisk), irisdisk[<span class="pl-k">!</span>, <span class="pl-c1">:is_setosa</span>])

<span class="pl-c"><span class="pl-c">#</span> gini</span>
<span class="pl-c1">gini</span>(<span class="pl-k">-</span><span class="pl-c1">predict</span>(xgtree1, irisdisk), irisdisk[<span class="pl-k">!</span>, <span class="pl-c1">:is_setosa</span>])

<span class="pl-c"><span class="pl-c">#</span> clean up</span>
<span class="pl-c1">rm</span>(<span class="pl-s"><span class="pl-pds">"</span>iris.jdf<span class="pl-pds">"</span></span>, force<span class="pl-k">=</span><span class="pl-c1">true</span>, recursive<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<h4><a id="user-content-mljjl" class="anchor" aria-hidden="true" href="#mljjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MLJ.jl</h4>
<p>Integration with MLJ.jl is available via the <a href="https://github.com/xiaodaigh/JLBoostMLJ.jl">JLBoostMLJ.jl</a> package</p>
<h2><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Notes</h2>
<p>Currently has a CPU implementation of the <code>xgboost</code> binary boosting algorithm as described in the original paper. I am trying to implement the algorithms in the original <code>xgboost</code> paper. I want to implement the algorithms mentioned in LigthGBM and Catboost and to port them to GPUs.</p>
<p>There is a similar project called <a href="https://github.com/Statfactory/JuML.jl">JuML.jl</a>.</p>
</article></div>