<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-knnjl" class="anchor" aria-hidden="true" href="#knnjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>kNN.jl</h1>
<p>Basic k-nearest neighbors classification and regression.</p>
<p>For a list of the distance metrics that can be used in k-NN classification, see <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a></p>
<p>For a list of the smoothing kernels that can be used in kernel regression, see <a href="https://github.com/johnmyleswhite/SmoothingKernels.jl">SmoothingKernel.jl</a></p>
<h1><a id="user-content-example-1-classification-using-euclidean-distance" class="anchor" aria-hidden="true" href="#example-1-classification-using-euclidean-distance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example 1: Classification using Euclidean distance</h1>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using kNN
using DataArrays
using DataFrames
using RDatasets
using Distances

iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
X = array(iris[:, 1:4])'
y = array(iris[:, 5])
model = knn(X, y, metric = Euclidean())

predict_k1 = predict(model, X, 1)
predict_k2 = predict(model, X, 2)
predict_k3 = predict(model, X, 3)
predict_k4 = predict(model, X, 4)
predict_k5 = predict(model, X, 5)

mean(predict_k1 .== y)
mean(predict_k2 .== y)
mean(predict_k3 .== y)
mean(predict_k4 .== y)
mean(predict_k5 .== y)
"><pre><code>using kNN
using DataArrays
using DataFrames
using RDatasets
using Distances

iris = dataset("datasets", "iris")
X = array(iris[:, 1:4])'
y = array(iris[:, 5])
model = knn(X, y, metric = Euclidean())

predict_k1 = predict(model, X, 1)
predict_k2 = predict(model, X, 2)
predict_k3 = predict(model, X, 3)
predict_k4 = predict(model, X, 4)
predict_k5 = predict(model, X, 5)

mean(predict_k1 .== y)
mean(predict_k2 .== y)
mean(predict_k3 .== y)
mean(predict_k4 .== y)
mean(predict_k5 .== y)
</code></pre></div>
<h1><a id="user-content-example-2-classification-using-manhattan-distance" class="anchor" aria-hidden="true" href="#example-2-classification-using-manhattan-distance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example 2: Classification using Manhattan distance</h1>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using kNN
using DataArrays
using DataFrames
using RDatasets
using Distances

iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
X = array(iris[:, 1:4])'
y = array(iris[:, 5])
model = knn(X, y, metric = Cityblock())

predict_k1 = predict(model, X, 1)
predict_k2 = predict(model, X, 2)
predict_k3 = predict(model, X, 3)
predict_k4 = predict(model, X, 4)
predict_k5 = predict(model, X, 5)

mean(predict_k1 .== y)
mean(predict_k2 .== y)
mean(predict_k3 .== y)
mean(predict_k4 .== y)
mean(predict_k5 .== y)
"><pre><code>using kNN
using DataArrays
using DataFrames
using RDatasets
using Distances

iris = dataset("datasets", "iris")
X = array(iris[:, 1:4])'
y = array(iris[:, 5])
model = knn(X, y, metric = Cityblock())

predict_k1 = predict(model, X, 1)
predict_k2 = predict(model, X, 2)
predict_k3 = predict(model, X, 3)
predict_k4 = predict(model, X, 4)
predict_k5 = predict(model, X, 5)

mean(predict_k1 .== y)
mean(predict_k2 .== y)
mean(predict_k3 .== y)
mean(predict_k4 .== y)
mean(predict_k5 .== y)
</code></pre></div>
<h1><a id="user-content-example-3-regression-using-gaussian-kernel" class="anchor" aria-hidden="true" href="#example-3-regression-using-gaussian-kernel"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example 3: Regression using Gaussian kernel</h1>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using Base.Test
using kNN
using StatsBase

srand(1)
n = 1_000
x = 10 * randn(n)
y = sin(x) + 0.5 * randn(n)

fit = kernelregression(x, y, kernel = :gaussian)
grid = minimum(x):0.1:maximum(x)
predictions = predict(fit, grid)
"><pre><code>using Base.Test
using kNN
using StatsBase

srand(1)
n = 1_000
x = 10 * randn(n)
y = sin(x) + 0.5 * randn(n)

fit = kernelregression(x, y, kernel = :gaussian)
grid = minimum(x):0.1:maximum(x)
predictions = predict(fit, grid)
</code></pre></div>
<h1><a id="user-content-to-do" class="anchor" aria-hidden="true" href="#to-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>To Do</h1>
<ul>
<li>Allow user to request that <code>knn</code> generate a ball tree, KD-tree or cover tree as a method for conducting nearest neighbor searches.</li>
<li>Allow user to request that approximate nearest neighbors be returned instead of exact nearest neighbors.</li>
<li>Clean up API</li>
</ul>
</article></div>