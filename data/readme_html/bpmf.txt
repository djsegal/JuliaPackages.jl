<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-bpmfjl" class="anchor" aria-hidden="true" href="#bpmfjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BPMF.jl</h1>
<p>A Julia package for Bayesian Probabilistic Matrix Factorization (BPMF).</p>
<hr>
<h2><a id="user-content-how-to-install" class="anchor" aria-hidden="true" href="#how-to-install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to install</h2>
<p>You can install BPMF.jl running the following commands.</p>
<ol>
<li>Into The Pkg REPL-mode</li>
</ol>
<ul>
<li>Enter the key<code>]</code> on the Julia REPL.</li>
</ul>
<ol start="2">
<li>Install the package</li>
</ol>
<ul>
<li>Enter the following commond.</li>
</ul>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="(v1.0) pkg&gt; add https://github.com/RottenFruits/BPMF.jl
"><pre>(v1.<span class="pl-c1">0</span>) pkg<span class="pl-k">&gt;</span> add https<span class="pl-k">:</span><span class="pl-k">//</span>github<span class="pl-k">.</span>com<span class="pl-k">/</span>RottenFruits<span class="pl-k">/</span>BPMF<span class="pl-k">.</span>jl</pre></div>
<h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview</h2>
<p>This package is implementation bayesian probabilistic matrix factorization (BPMF).</p>
<p>Supported features:</p>
<ul>
<li>Gibbs sampling algorithm</li>
<li>Variational Inference Algorithm</li>
</ul>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h2>
<p>Here are examples.</p>
<p>At first read package and generate data.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using BPMF

#data
R = [
    0 0 7;
    0 1 6;
    0 2 7;
    0 3 4;
    0 4 5;
    0 5 4;
    1 0 6;
    1 1 7;
    1 3 4;
    1 4 3;
    1 5 4;
    2 1 3;
    2 2 3;
    2 3 1;
    2 4 1;
    3 0 1;
    3 1 2;
    3 2 2;
    3 3 3;
    3 4 3;
    3 5 4;
    4 0 1;
    4 2 1;
    4 3 2;
    4 4 3;
    4 5 3;
]
"><pre><span class="pl-k">using</span> BPMF

<span class="pl-c"><span class="pl-c">#</span>data</span>
R <span class="pl-k">=</span> [
    <span class="pl-c1">0</span> <span class="pl-c1">0</span> <span class="pl-c1">7</span>;
    <span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">6</span>;
    <span class="pl-c1">0</span> <span class="pl-c1">2</span> <span class="pl-c1">7</span>;
    <span class="pl-c1">0</span> <span class="pl-c1">3</span> <span class="pl-c1">4</span>;
    <span class="pl-c1">0</span> <span class="pl-c1">4</span> <span class="pl-c1">5</span>;
    <span class="pl-c1">0</span> <span class="pl-c1">5</span> <span class="pl-c1">4</span>;
    <span class="pl-c1">1</span> <span class="pl-c1">0</span> <span class="pl-c1">6</span>;
    <span class="pl-c1">1</span> <span class="pl-c1">1</span> <span class="pl-c1">7</span>;
    <span class="pl-c1">1</span> <span class="pl-c1">3</span> <span class="pl-c1">4</span>;
    <span class="pl-c1">1</span> <span class="pl-c1">4</span> <span class="pl-c1">3</span>;
    <span class="pl-c1">1</span> <span class="pl-c1">5</span> <span class="pl-c1">4</span>;
    <span class="pl-c1">2</span> <span class="pl-c1">1</span> <span class="pl-c1">3</span>;
    <span class="pl-c1">2</span> <span class="pl-c1">2</span> <span class="pl-c1">3</span>;
    <span class="pl-c1">2</span> <span class="pl-c1">3</span> <span class="pl-c1">1</span>;
    <span class="pl-c1">2</span> <span class="pl-c1">4</span> <span class="pl-c1">1</span>;
    <span class="pl-c1">3</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span>;
    <span class="pl-c1">3</span> <span class="pl-c1">1</span> <span class="pl-c1">2</span>;
    <span class="pl-c1">3</span> <span class="pl-c1">2</span> <span class="pl-c1">2</span>;
    <span class="pl-c1">3</span> <span class="pl-c1">3</span> <span class="pl-c1">3</span>;
    <span class="pl-c1">3</span> <span class="pl-c1">4</span> <span class="pl-c1">3</span>;
    <span class="pl-c1">3</span> <span class="pl-c1">5</span> <span class="pl-c1">4</span>;
    <span class="pl-c1">4</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span>;
    <span class="pl-c1">4</span> <span class="pl-c1">2</span> <span class="pl-c1">1</span>;
    <span class="pl-c1">4</span> <span class="pl-c1">3</span> <span class="pl-c1">2</span>;
    <span class="pl-c1">4</span> <span class="pl-c1">4</span> <span class="pl-c1">3</span>;
    <span class="pl-c1">4</span> <span class="pl-c1">5</span> <span class="pl-c1">3</span>;
]</pre></div>
<h3><a id="user-content-using-gibbs-sampling-algorithm" class="anchor" aria-hidden="true" href="#using-gibbs-sampling-algorithm"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using Gibbs Sampling Algorithm</h3>
<p>Following example is gibbs sampling algorithm.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="N = length(unique(R[:, 1])) #number of unique users
M = length(unique(R[:, 2])) #number of unique items
D = 3 #number of latent dimensions
T = 100 #number of iterations
U = [] #user's latent factor
V = [] #item's latent factor
α = 2 #hyper parameter
β₀ = 2 #hyper parameter
μ₀ = 0 #hyper parameter
ν₀ = D #hyper parameter
W₀ = one(zeros(D, D)) #hyper parameter

#learning
gibbs_model = BPMF.GBPMFModel(R, N, M, D, T, U, V, α, β₀, μ₀, ν₀, W₀)
BPMF.fit(gibbs_model)

#predict new data
bi = 10 #burn-in
BPMF.predict(gibbs_model, R, bi)
"><pre>N <span class="pl-k">=</span> <span class="pl-c1">length</span>(<span class="pl-c1">unique</span>(R[:, <span class="pl-c1">1</span>])) <span class="pl-c"><span class="pl-c">#</span>number of unique users</span>
M <span class="pl-k">=</span> <span class="pl-c1">length</span>(<span class="pl-c1">unique</span>(R[:, <span class="pl-c1">2</span>])) <span class="pl-c"><span class="pl-c">#</span>number of unique items</span>
D <span class="pl-k">=</span> <span class="pl-c1">3</span> <span class="pl-c"><span class="pl-c">#</span>number of latent dimensions</span>
T <span class="pl-k">=</span> <span class="pl-c1">100</span> <span class="pl-c"><span class="pl-c">#</span>number of iterations</span>
U <span class="pl-k">=</span> [] <span class="pl-c"><span class="pl-c">#</span>user's latent factor</span>
V <span class="pl-k">=</span> [] <span class="pl-c"><span class="pl-c">#</span>item's latent factor</span>
α <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>
β₀ <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>
μ₀ <span class="pl-k">=</span> <span class="pl-c1">0</span> <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>
ν₀ <span class="pl-k">=</span> D <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>
W₀ <span class="pl-k">=</span> <span class="pl-c1">one</span>(<span class="pl-c1">zeros</span>(D, D)) <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>

<span class="pl-c"><span class="pl-c">#</span>learning</span>
gibbs_model <span class="pl-k">=</span> BPMF<span class="pl-k">.</span><span class="pl-c1">GBPMFModel</span>(R, N, M, D, T, U, V, α, β₀, μ₀, ν₀, W₀)
BPMF<span class="pl-k">.</span><span class="pl-c1">fit</span>(gibbs_model)

<span class="pl-c"><span class="pl-c">#</span>predict new data</span>
bi <span class="pl-k">=</span> <span class="pl-c1">10</span> <span class="pl-c"><span class="pl-c">#</span>burn-in</span>
BPMF<span class="pl-k">.</span><span class="pl-c1">predict</span>(gibbs_model, R, bi)</pre></div>
<h3><a id="user-content-using-variational-inference-algorithm" class="anchor" aria-hidden="true" href="#using-variational-inference-algorithm"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using Variational Inference Algorithm</h3>
<p>Following example is variational inference algorithm.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="N = length(unique(R[:, 1])) #number of unique users
M = length(unique(R[:, 2])) #number of unique items
D = 3 #number of latent dimensions
L = 10 #number of iterations
U = [] #user's latent factor
V = [] #item's latent factor
τ² = 1 #hyper parameter
σ² = [] #hyper parameter
ρ² = [] #hyper parameter

#learning
variational_model = BPMF.VBPMFModel(R, N, M, D, L, U, V, τ², σ², ρ²)
BPMF.fit(variational_model)

#predict new data
BPMF.predict(variational_model, R)
"><pre>N <span class="pl-k">=</span> <span class="pl-c1">length</span>(<span class="pl-c1">unique</span>(R[:, <span class="pl-c1">1</span>])) <span class="pl-c"><span class="pl-c">#</span>number of unique users</span>
M <span class="pl-k">=</span> <span class="pl-c1">length</span>(<span class="pl-c1">unique</span>(R[:, <span class="pl-c1">2</span>])) <span class="pl-c"><span class="pl-c">#</span>number of unique items</span>
D <span class="pl-k">=</span> <span class="pl-c1">3</span> <span class="pl-c"><span class="pl-c">#</span>number of latent dimensions</span>
L <span class="pl-k">=</span> <span class="pl-c1">10</span> <span class="pl-c"><span class="pl-c">#</span>number of iterations</span>
U <span class="pl-k">=</span> [] <span class="pl-c"><span class="pl-c">#</span>user's latent factor</span>
V <span class="pl-k">=</span> [] <span class="pl-c"><span class="pl-c">#</span>item's latent factor</span>
τ² <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>
σ² <span class="pl-k">=</span> [] <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>
ρ² <span class="pl-k">=</span> [] <span class="pl-c"><span class="pl-c">#</span>hyper parameter</span>

<span class="pl-c"><span class="pl-c">#</span>learning</span>
variational_model <span class="pl-k">=</span> BPMF<span class="pl-k">.</span><span class="pl-c1">VBPMFModel</span>(R, N, M, D, L, U, V, τ², σ², ρ²)
BPMF<span class="pl-k">.</span><span class="pl-c1">fit</span>(variational_model)

<span class="pl-c"><span class="pl-c">#</span>predict new data</span>
BPMF<span class="pl-k">.</span><span class="pl-c1">predict</span>(variational_model, R)</pre></div>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li>
<p>Salakhutdinov, Ruslan, and Andriy Mnih. "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo." Proceedings of the 25th international conference on Machine learning. ACM, 2008.</p>
</li>
<li>
<p>Lim, Yew Jin, and Yee Whye Teh. "Variational Bayesian approach to movie rating prediction." Proceedings of KDD cup and workshop. Vol. 7. 2007.</p>
</li>
</ul>
</article></div>