<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-lilith" class="anchor" aria-hidden="true" href="#lilith"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Lilith</h1>
<blockquote>
<p dir="auto"><g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> <strong>This package has been renamed to <a href="https://github.com/dfdx/Avalon.jl">Avalon.jl</a>, all new features will be added there</strong></p>
</blockquote>
<p dir="auto"><a href="https://travis-ci.org/dfdx/Lilith.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/db9fc82ca6a2bbcbc4a353ec87770b94d40ecc9eff6cf8f935fbf3959426bc6d/68747470733a2f2f7472617669732d63692e6f72672f646664782f4c696c6974682e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/dfdx/Lilith.jl.svg?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto"><strong>Lilith</strong> is a deep learning library in Julia with focus on <strong>high performance</strong> and <strong>interoperability with existing DL frameworks</strong>. Its main features include:</p>
<ul dir="auto">
<li>tracing autograd engine - models are just structs, transformations are just functions</li>
<li>optimizing code generator based on hackable computational graph</li>
<li>GPU support</li>
<li>layer API similar to PyTorch's to ease translation of existing Python code to Julia</li>
<li>high backward compatibility to allow accumulation of models</li>
</ul>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">To get you a feeling of what Lilith is like, here's a definition of a small convolutional neural network:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Lilith


mutable struct Net
    conv1::Conv2d
    conv2::Conv2d
    fc1::Linear
    fc2::Linear
end


Net() = Net(
    Conv2d(1, 20, 5),
    Conv2d(20, 50, 5),
    Linear(4 * 4 * 50, 500),
    Linear(500, 10)
)

function (m::Net)(x::AbstractArray)
    x = maxpool2d(relu.(m.conv1(x)), (2, 2))
    x = maxpool2d(relu.(m.conv2(x)), (2, 2))
    x = reshape(x, 4*4*50, :)
    x = relu.(m.fc1(x))
    x = logsoftmax(m.fc2(x))
    return x
end"><pre><span class="pl-k">using</span> Lilith


<span class="pl-k">mutable struct</span> Net
    conv1<span class="pl-k">::</span><span class="pl-c1">Conv2d</span>
    conv2<span class="pl-k">::</span><span class="pl-c1">Conv2d</span>
    fc1<span class="pl-k">::</span><span class="pl-c1">Linear</span>
    fc2<span class="pl-k">::</span><span class="pl-c1">Linear</span>
<span class="pl-k">end</span>


<span class="pl-en">Net</span>() <span class="pl-k">=</span> <span class="pl-c1">Net</span>(
    <span class="pl-c1">Conv2d</span>(<span class="pl-c1">1</span>, <span class="pl-c1">20</span>, <span class="pl-c1">5</span>),
    <span class="pl-c1">Conv2d</span>(<span class="pl-c1">20</span>, <span class="pl-c1">50</span>, <span class="pl-c1">5</span>),
    <span class="pl-c1">Linear</span>(<span class="pl-c1">4</span> <span class="pl-k">*</span> <span class="pl-c1">4</span> <span class="pl-k">*</span> <span class="pl-c1">50</span>, <span class="pl-c1">500</span>),
    <span class="pl-c1">Linear</span>(<span class="pl-c1">500</span>, <span class="pl-c1">10</span>)
)

<span class="pl-k">function</span> (m<span class="pl-k">::</span><span class="pl-c1">Net</span>)(x<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>)
    x <span class="pl-k">=</span> <span class="pl-c1">maxpool2d</span>(<span class="pl-c1">relu</span>.(m<span class="pl-k">.</span><span class="pl-c1">conv1</span>(x)), (<span class="pl-c1">2</span>, <span class="pl-c1">2</span>))
    x <span class="pl-k">=</span> <span class="pl-c1">maxpool2d</span>(<span class="pl-c1">relu</span>.(m<span class="pl-k">.</span><span class="pl-c1">conv2</span>(x)), (<span class="pl-c1">2</span>, <span class="pl-c1">2</span>))
    x <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(x, <span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">50</span>, :)
    x <span class="pl-k">=</span> <span class="pl-c1">relu</span>.(m<span class="pl-k">.</span><span class="pl-c1">fc1</span>(x))
    x <span class="pl-k">=</span> <span class="pl-c1">logsoftmax</span>(m<span class="pl-k">.</span><span class="pl-c1">fc2</span>(x))
    <span class="pl-k">return</span> x
<span class="pl-k">end</span></pre></div>
<p dir="auto">For detailed explanation of this and other models see <a href="https://github.com/dfdx/Lilith.jl/tree/master/tutorial">the tutorial</a>. Some predefined models are also available in <a href="https://github.com/dfdx/Lilith.jl/tree/master/zoo">the zoo</a>.</p>
<h2 dir="auto"><a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance</h2>
<p dir="auto">Performance comparison between different libraries is hard and benchmarks are rarely fair, but here's our best shot in this direction:</p>
<h3 dir="auto"><a id="user-content-convolutional-neural-network" class="anchor" aria-hidden="true" href="#convolutional-neural-network"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Convolutional neural network</h3>
<p dir="auto">Code available <a href="https://github.com/dfdx/Lilith.jl/tree/master/benchmarks/cnn">here</a></p>
<table>
<thead>
<tr>
<th></th>
<th>training 1 epoch</th>
<th>training total time*</th>
<th>prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lilith (CPU)</td>
<td>170 s</td>
<td>1742 s</td>
<td>39 ms</td>
</tr>
<tr>
<td>Flux (CPU)</td>
<td>250 s</td>
<td>2515 s</td>
<td>42 ms</td>
</tr>
<tr>
<td>-------------</td>
<td>----------------</td>
<td>--------------------</td>
<td>----------</td>
</tr>
<tr>
<td>Lilith (GPU)</td>
<td>10 s</td>
<td>164 s</td>
<td>5 ms</td>
</tr>
<tr>
<td>Flux (GPU)</td>
<td>12 s</td>
<td>150 s</td>
<td>5 ms</td>
</tr>
<tr>
<td>PyTorch (GPU)</td>
<td>12 s</td>
<td>120 s</td>
<td>2 ms</td>
</tr>
</tbody>
</table>
<p dir="auto"><code>*</code> - total time includes 10 epochs + compilation time</p>
<p dir="auto">Note that in the test on GPU Lilith has longest compilation time and thus
longest total training time <em>after 10 epochs</em>. However, time per epoch
is the lowest, so Lilith is typically the fastest one in longer run.</p>
<h3 dir="auto"><a id="user-content-variational-autoencoder" class="anchor" aria-hidden="true" href="#variational-autoencoder"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Variational Autoencoder</h3>
<p dir="auto">Code available <a href="https://github.com/dfdx/Lilith.jl/tree/master/benchmarks/vae">here</a></p>
<table>
<thead>
<tr>
<th></th>
<th>training 1 epoch</th>
<th>training total time</th>
<th>prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lilith (CPU)</td>
<td>50 s</td>
<td>535 s</td>
<td>395 μs</td>
</tr>
<tr>
<td>Flux (CPU)</td>
<td>948 s</td>
<td>158 min</td>
<td>81 ms</td>
</tr>
<tr>
<td>-------------</td>
<td>----------------</td>
<td>--------------------</td>
<td>----------</td>
</tr>
<tr>
<td>Lilith (GPU)</td>
<td>3 s</td>
<td>93 s</td>
<td>194 μs</td>
</tr>
<tr>
<td>Flux (GPU)**</td>
<td>---</td>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>PyTorch (GPU)</td>
<td>7 s</td>
<td>66 s</td>
<td>501 µs</td>
</tr>
</tbody>
</table>
<p dir="auto"><code>**</code> - VAE example from the Flux zoo doesn't work on GPU</p>
<h2 dir="auto"><a id="user-content-api-stability" class="anchor" aria-hidden="true" href="#api-stability"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>API Stability</h2>
<p dir="auto">One of the central ideas behind Lilith is the ability to reuse existing code instead of writing everything from scratch.
To facilitate it, Lilith is committed to high, although not absolute backward compatibility. The following table
outlines stability level you should expect from various components of the library.</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>API Stable?</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic layers</td>
<td>Yes</td>
</tr>
<tr>
<td>CNN</td>
<td>Yes</td>
</tr>
<tr>
<td>RNN</td>
<td>No*</td>
</tr>
<tr>
<td>Losses</td>
<td>Mostly</td>
</tr>
<tr>
<td>Activations</td>
<td>Yes</td>
</tr>
<tr>
<td>Initializations</td>
<td>Mostly</td>
</tr>
<tr>
<td>Optimizers</td>
<td>Yes</td>
</tr>
<tr>
<td>Device API</td>
<td>Yes</td>
</tr>
<tr>
<td>Fitting API</td>
<td>No**</td>
</tr>
</tbody>
</table>
<p dir="auto"><code>*</code> - currently Lilith provides only basic implementations of vanilla RNN, LSTM and GRU; this implementation will be improved in future version and made more compatible with PyTorch version, but currently it cannot be considered stable</p>
<p dir="auto"><code>**</code> - function <code>fit!()</code> provides a convenient shortcut for training supervised learning models, but in its current state it's too basic for most real use cases; for more durable code consider writing your own method for training using <code>fit!()</code> as a template</p>
<p dir="auto">Please note that until version 1.0 "stable API" means that we will try our best to keep it unchanged, but we reserve the right to the break the rule in some rare and exceptional cases.</p>
</article></div>