<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-parameterhandling" class="anchor" aria-hidden="true" href="#parameterhandling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ParameterHandling</h1>
<p dir="auto"><a href="https://invenia.github.io/ParameterHandling.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://invenia.github.io/ParameterHandling.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/Invenia/Intervals.jl/actions?query=workflow%3ACI"><img src="https://github.com/Invenia/ParameterHandling.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/invenia/ParameterHandling.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/86efe5e336e3a75507bd227a212533bf15ff0da418931a54dda4e121a3be2b8d/68747470733a2f2f636f6465636f762e696f2f67682f696e76656e69612f506172616d6574657248616e646c696e672e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/invenia/ParameterHandling.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://github.com/SciML/ColPrac"><img src="https://camo.githubusercontent.com/a6c1efcb19a957860ecb25966a730260b03d6e05380d0c27992ee7f9e3b1feb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6c507261632d436f6e7472696275746f72277325323047756964652d626c756576696f6c6574" alt="ColPrac: Contributor's Guide on Collaborative Practices for Community Packages" data-canonical-src="https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet" style="max-width: 100%;"></a></p>
<p dir="auto">ParameterHandling.jl is an experiment in handling constrained tunable parameters of models.</p>
<h1 dir="auto"><a id="user-content-the-parameter-handling-problem" class="anchor" aria-hidden="true" href="#the-parameter-handling-problem"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The Parameter Handling Problem</h1>
<p dir="auto">Consider the following common situation: you have a function <code>build_model</code> that maps a
collection of parameters <code>θ</code> to a <code>model</code> of some kind:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="model = build_model(θ)"><pre>model <span class="pl-k">=</span> <span class="pl-c1">build_model</span>(θ)</pre></div>
<p dir="auto">The <code>model</code> might, for example, be a function that maps some input <code>x</code> to some sort of
prediction <code>y</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="y = model(x)"><pre>y <span class="pl-k">=</span> <span class="pl-c1">model</span>(x)</pre></div>
<p dir="auto">where <code>x</code> and <code>y</code> could essentially be anything that you like.
You might also wish to somehow "learn" or "tune" or "infer" the parameters <code>θ</code> by plugging
<code>build_model</code> into some other function, lets call it <code>learn</code>, that tries out various
different parameter values in some clever way and determines which ones are good -- think
loss minimisation / objective maximisation, (approximate) Bayesian inference, etc.
We'll not worry about exactly what procedure <code>learn</code> employs to try out a number of
different parameter values, but suppose that <code>learn</code> has the interface:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="learned_θ = learn(build_model, initial_θ)"><pre>learned_θ <span class="pl-k">=</span> <span class="pl-c1">learn</span>(build_model, initial_θ)</pre></div>
<p dir="auto">So far so good, but now consider how one actually goes about writing <code>build_model</code>.
There are more or less two things that must be written:</p>
<ol dir="auto">
<li><code>θ</code> must be in a format that <code>learn</code> knows how to handle. A popular approach is to
require that <code>θ</code> be a <code>Vector</code> of <code>Real</code> numbers -- or, rather, some concrete subtype of
<code>Real</code>.</li>
<li>The code required to turn <code>θ</code> into <code>model</code> inside <code>build_model</code> mustn't be too onerous to
write, read, or modify.</li>
</ol>
<p dir="auto">While the first point is fairly straightforward, the second point is a bit subtle, so it's
worth dwelling on it a little.</p>
<p dir="auto">For the sake of concreteness, let's suppose that we adopt the convention that <code>θ</code> is a
<code>Vector{Float64}</code>. In the case of linear regression, we might assume that <code>θ</code> comprises
a length <code>D</code> "weight vector" <code>w</code>, and a scalar "bias" <code>b</code>. So the function to build the
model might be something like</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="function build_model(θ::Vector{Float64})
    return x -&gt; dot(θ[1:end-1], x) + θ[end]
end"><pre><span class="pl-k">function</span> <span class="pl-en">build_model</span>(θ<span class="pl-k">::</span><span class="pl-c1">Vector{Float64}</span>)
    <span class="pl-k">return</span> x <span class="pl-k">-&gt;</span> <span class="pl-c1">dot</span>(θ[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>], x) <span class="pl-k">+</span> θ[<span class="pl-c1">end</span>]
<span class="pl-k">end</span></pre></div>
<p dir="auto">The easiest way to see that this is a less than ideal solution is to consider what this
function would look like if <code>θ</code> was, say, a <code>NamedTuple</code> with fields <code>w</code> and <code>b</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="function build_model(θ::NamedTuple)
    return x -&gt; dot(θ.w, x) + θ.b
end"><pre><span class="pl-k">function</span> <span class="pl-en">build_model</span>(θ<span class="pl-k">::</span><span class="pl-c1">NamedTuple</span>)
    <span class="pl-k">return</span> x <span class="pl-k">-&gt;</span> <span class="pl-c1">dot</span>(θ<span class="pl-k">.</span>w, x) <span class="pl-k">+</span> θ<span class="pl-k">.</span>b
<span class="pl-k">end</span></pre></div>
<p dir="auto">This version of the function is much easier to read -- moreover if you want to inspect the
values of <code>w</code> and <code>b</code> at some other point in time, you don't need to know precisely how to
chop up the vector.</p>
<p dir="auto">Moreover it seems probable that the latter approach is less
bug-prone -- suppose for some reason one refactored the code so that the first element of
<code>θ</code> became <code>b</code> and the last <code>D</code> elements <code>w</code>; any code that depended upon the original
ordering will now be incorrect and likely fail silently. The <code>NamedTuple</code> approach simply
doesn't have this issue.</p>
<p dir="auto">Granted, in this simple case it's not too much of a problem, but it's easy to find
situations in which things become considerably more difficult. For example, suppose that we
instead had pretty much any kind of neural network, Gaussian process, ODE, or really just
any model with more than a couple of distinct parameters. From the perspective of
writing complicated models, implementing things in terms of a single vector of
parameters that is <em>manually</em> chopped up is an <em>extremely</em> bad design choice. It simply
doesn't scale.</p>
<p dir="auto">However, a single vector of e.g. <code>Float64</code>s <em>is</em> extremely convenient when writing general
purpose optimisers / approximate inference routines --
<a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> and
<a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a> being two obvious examples.</p>
<h1 dir="auto"><a id="user-content-the-parameterhandlingjl-approach" class="anchor" aria-hidden="true" href="#the-parameterhandlingjl-approach"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The ParameterHandling.jl Approach</h1>
<p dir="auto"><code>ParameterHandling.jl</code> aims to give you the best of both worlds by providing the tools
required to automate the transformation between a "structured" representation (e.g. nested
<code>NamedTuple</code> / <code>Dict</code> etc) and a "flattened" (e.g. <code>Vector{Float64}</code>) of your model
parameters.</p>
<p dir="auto">The function <code>flatten</code> eats a structured representation of some parameters, returning the
flattened representation <em>and</em> a function that converts the flattened thing back into its
structured representation.</p>
<p dir="auto"><code>flatten</code> is implemented recursively, with a <em>very</em> small number of base-implementations
that don't themselves call <code>flatten</code>.</p>
<p dir="auto">You should expect to occassionally have to extend <code>flatten</code> to handle your own types and, if
you wind up doing this for a function in <code>Base</code> that this package doesn't yet cover, a PR
including that implementation will be very welcome.</p>
<p dir="auto">See <code>test/parameters.jl</code> for a couple of examples that utilise <code>flatten</code> to do something
similar to the task described above.</p>
<h1 dir="auto"><a id="user-content-dealing-with-constrained-parameters" class="anchor" aria-hidden="true" href="#dealing-with-constrained-parameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dealing with Constrained Parameters</h1>
<p dir="auto">It is very common to need to handle constraints on parameters e.g. it may be necessary for a
particular scalar to always be positive. While <code>flatten</code> is great for changing between
representations of your parameters, it doesn't really have anything to say about this
constraint problem.</p>
<p dir="auto">For this we introduce a collection of new <code>AbstractParameter</code> types (whether we really need
them to have some mutual supertype is unclear at present) that play nicely with <code>flatten</code>
and allow one to specify that e.g. a particular scalar must remain positive, or should be
fixed across iterations. See <code>src/parameters.jl</code> and <code>test/parameters.jl</code> for more examples.</p>
<p dir="auto">The approach to implementing these types typically revolves around some kind of <code>Deferred</code> /
delayed computation. For example, a <code>Positive</code> parameter is represented by an
"unconstrained" number, and a "transform" that maps from the entire real line to the
positive half. The <code>value</code> of a <code>Positive</code> is given by the application of this transform to
the unconstrained number. <code>flatten</code>ing a <code>Positive</code> yields a length-1 vector containing the
<em>unconstrained</em> number, rather than the value represented by the <code>Positive</code> object. For
example</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using ParameterHandling

julia&gt; x_constrained = 1.0 # Specify constrained value.
1.0

julia&gt; x = positive(x_constrained) # Construct a number that should remain positive.
ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8)

julia&gt; ParameterHandling.value(x) # Get the constrained value by applying the transform.
1.0

julia&gt; v, unflatten = flatten(x); # Supports the `flatten` interface.

julia&gt; v
1-element Vector{Float64}:
 -1.490116130486996e-8

julia&gt; new_v = randn(1) # Pick a random new value.
1-element Vector{Float64}:
 2.3482666974328716

julia&gt; ParameterHandling.value(unflatten(new_v)) # Obtain constrained value.
10.467410816707215"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ParameterHandling

julia<span class="pl-k">&gt;</span> x_constrained <span class="pl-k">=</span> <span class="pl-c1">1.0</span> <span class="pl-c"><span class="pl-c">#</span> Specify constrained value.</span>
<span class="pl-c1">1.0</span>

julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">positive</span>(x_constrained) <span class="pl-c"><span class="pl-c">#</span> Construct a number that should remain positive.</span>
ParameterHandling<span class="pl-k">.</span><span class="pl-c1">Positive</span><span class="pl-c1">{Float64, typeof(exp), Float64}</span>(<span class="pl-k">-</span><span class="pl-c1">1.490116130486996e-8</span>, exp, <span class="pl-c1">1.4901161193847656e-8</span>)

julia<span class="pl-k">&gt;</span> ParameterHandling<span class="pl-k">.</span><span class="pl-c1">value</span>(x) <span class="pl-c"><span class="pl-c">#</span> Get the constrained value by applying the transform.</span>
<span class="pl-c1">1.0</span>

julia<span class="pl-k">&gt;</span> v, unflatten <span class="pl-k">=</span> <span class="pl-c1">flatten</span>(x); <span class="pl-c"><span class="pl-c">#</span> Supports the `flatten` interface.</span>

julia<span class="pl-k">&gt;</span> v
<span class="pl-c1">1</span><span class="pl-k">-</span>element Vector{Float64}<span class="pl-k">:</span>
 <span class="pl-k">-</span><span class="pl-c1">1.490116130486996e-8</span>

julia<span class="pl-k">&gt;</span> new_v <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> Pick a random new value.</span>
<span class="pl-c1">1</span><span class="pl-k">-</span>element Vector{Float64}<span class="pl-k">:</span>
 <span class="pl-c1">2.3482666974328716</span>

julia<span class="pl-k">&gt;</span> ParameterHandling<span class="pl-k">.</span><span class="pl-c1">value</span>(<span class="pl-c1">unflatten</span>(new_v)) <span class="pl-c"><span class="pl-c">#</span> Obtain constrained value.</span>
<span class="pl-c1">10.467410816707215</span></pre></div>
<p dir="auto">We also provide the utility function <code>value_flatten</code> which returns an unflattening function
equivalent to <code>value(unflatten(v))</code>. The above could then be implemented as</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; v, unflatten = value_flatten(x);

julia&gt; unflatten(v)
1.0"><pre>julia<span class="pl-k">&gt;</span> v, unflatten <span class="pl-k">=</span> <span class="pl-c1">value_flatten</span>(x);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">unflatten</span>(v)
<span class="pl-c1">1.0</span></pre></div>
<p dir="auto">It is straightforward to implement your own parameters that interoperate with those already
written by implementing <code>value</code> and <code>flatten</code> for them. You might want to do this if this
package doesn't currently support the functionality that you need.</p>
<h1 dir="auto"><a id="user-content-a-worked-example" class="anchor" aria-hidden="true" href="#a-worked-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>A Worked Example</h1>
<p dir="auto">We use a model involving a Gaussian process (GP) -- you don't need to know anything about
Gaussian processes other than</p>
<ol dir="auto">
<li>they are a class of probabilistic model which can be used for regression (amongst other things).</li>
<li>they have some tunable parameters that are usually chosen by optimising a scalar objective function using an iterative
optimisation algorithm -- typically a variant of gradient descent.
Ths is representative of a large number of models in ML / statistics / optimisation.</li>
</ol>
<p dir="auto">This example can be copy+pasted into a REPL session.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Install some packages.
using Pkg
Pkg.add(&quot;ParameterHandling&quot;)
Pkg.add(&quot;Optim&quot;)
Pkg.add(&quot;Zygote&quot;)
Pkg.add(&quot;AbstractGPs&quot;)

using ParameterHandling # load up this package.
using Optim # generic optimisation
using Zygote # algorithmic differentiation
using AbstractGPs # package containing the models we'll be working with

# Declare a NamedTuple containing an initial guess at parameters.
raw_initial_params = (
    k1 = (
        var=positive(0.9),
        precision=positive(1.0),
    ),
    k2 = (
        var=positive(0.1),
        precision=positive(0.3),
    ),
    noise_var=positive(0.2),
)

# Using ParameterHandling.value_flatten, we can obtain both a Vector{Float64} representation of
# these parameters, and a mapping from that vector back to the original (unconstrained) parameter values.
flat_initial_params, unflatten = ParameterHandling.value_flatten(raw_initial_params)

# ParameterHandling.value strips out all of the Positive types in initial_params,
# returning a plain named tuple of named tuples and Float64s.
julia&gt; initial_params = ParameterHandling.value(raw_initial_params)
(k1 = (var = 0.9, precision = 1.0), k2 = (var = 0.10000000000000002, precision = 0.30000000000000004), noise_var = 0.19999999999999998)

# GP-specific functionality. Don't worry about the details, just
# note the use of the structured representation of the parameters.
function build_gp(params::NamedTuple)
    k1 = params.k1.var * Matern52Kernel() ∘ ScaleTransform(params.k1.precision)
    k2 = params.k2.var * SEKernel() ∘ ScaleTransform(params.k2.precision)
    return GP(k1 + k2)
end

# Generate some synthetic training data.
# Again, don't worry too much about the specifics here.
const x = range(-5.0, 5.0; length=100)
const y = rand(build_gp(initial_params)(x, initial_params.noise_var))

# Specify an objective function in terms of x and y.
function objective(params::NamedTuple)
    f = build_gp(params)
    return -logpdf(f(x, params.noise_var), y)
end

# Use Optim.jl to minimise the objective function w.r.t. the params.
# The important thing here is to note that we're passing in the flat vector of parameters to
# Optim, which is something that Optim knows how to work with, and using `unflatten` to convert
# from this representation to the structured one that our objective function knows about
# using `unflatten` -- we've used ParameterHandling to build a bridge between Optim and an
# entirely unrelated package.
training_results = Optim.optimize(
    objective ∘ unflatten,
    θ -&gt; only(Zygote.gradient(objective ∘ unflatten, θ)),
    flat_initial_params,
    BFGS(
        alphaguess = Optim.LineSearches.InitialStatic(scaled=true),
        linesearch = Optim.LineSearches.BackTracking(),
    ),
    Optim.Options(show_trace = true);
    inplace=false,
)

# Extracting the final values of the parameters.
final_params = unflatten(training_results.minimizer)
f_trained = build_gp(final_params)"><pre><span class="pl-c"><span class="pl-c">#</span> Install some packages.</span>
<span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>ParameterHandling<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Optim<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Zygote<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>AbstractGPs<span class="pl-pds">"</span></span>)

<span class="pl-k">using</span> ParameterHandling <span class="pl-c"><span class="pl-c">#</span> load up this package.</span>
<span class="pl-k">using</span> Optim <span class="pl-c"><span class="pl-c">#</span> generic optimisation</span>
<span class="pl-k">using</span> Zygote <span class="pl-c"><span class="pl-c">#</span> algorithmic differentiation</span>
<span class="pl-k">using</span> AbstractGPs <span class="pl-c"><span class="pl-c">#</span> package containing the models we'll be working with</span>

<span class="pl-c"><span class="pl-c">#</span> Declare a NamedTuple containing an initial guess at parameters.</span>
raw_initial_params <span class="pl-k">=</span> (
    k1 <span class="pl-k">=</span> (
        var<span class="pl-k">=</span><span class="pl-c1">positive</span>(<span class="pl-c1">0.9</span>),
        precision<span class="pl-k">=</span><span class="pl-c1">positive</span>(<span class="pl-c1">1.0</span>),
    ),
    k2 <span class="pl-k">=</span> (
        var<span class="pl-k">=</span><span class="pl-c1">positive</span>(<span class="pl-c1">0.1</span>),
        precision<span class="pl-k">=</span><span class="pl-c1">positive</span>(<span class="pl-c1">0.3</span>),
    ),
    noise_var<span class="pl-k">=</span><span class="pl-c1">positive</span>(<span class="pl-c1">0.2</span>),
)

<span class="pl-c"><span class="pl-c">#</span> Using ParameterHandling.value_flatten, we can obtain both a Vector{Float64} representation of</span>
<span class="pl-c"><span class="pl-c">#</span> these parameters, and a mapping from that vector back to the original (unconstrained) parameter values.</span>
flat_initial_params, unflatten <span class="pl-k">=</span> ParameterHandling<span class="pl-k">.</span><span class="pl-c1">value_flatten</span>(raw_initial_params)

<span class="pl-c"><span class="pl-c">#</span> ParameterHandling.value strips out all of the Positive types in initial_params,</span>
<span class="pl-c"><span class="pl-c">#</span> returning a plain named tuple of named tuples and Float64s.</span>
julia<span class="pl-k">&gt;</span> initial_params <span class="pl-k">=</span> ParameterHandling<span class="pl-k">.</span><span class="pl-c1">value</span>(raw_initial_params)
(k1 <span class="pl-k">=</span> (var <span class="pl-k">=</span> <span class="pl-c1">0.9</span>, precision <span class="pl-k">=</span> <span class="pl-c1">1.0</span>), k2 <span class="pl-k">=</span> (var <span class="pl-k">=</span> <span class="pl-c1">0.10000000000000002</span>, precision <span class="pl-k">=</span> <span class="pl-c1">0.30000000000000004</span>), noise_var <span class="pl-k">=</span> <span class="pl-c1">0.19999999999999998</span>)

<span class="pl-c"><span class="pl-c">#</span> GP-specific functionality. Don't worry about the details, just</span>
<span class="pl-c"><span class="pl-c">#</span> note the use of the structured representation of the parameters.</span>
<span class="pl-k">function</span> <span class="pl-en">build_gp</span>(params<span class="pl-k">::</span><span class="pl-c1">NamedTuple</span>)
    k1 <span class="pl-k">=</span> params<span class="pl-k">.</span>k1<span class="pl-k">.</span>var <span class="pl-k">*</span> <span class="pl-c1">Matern52Kernel</span>() <span class="pl-k">∘</span> <span class="pl-c1">ScaleTransform</span>(params<span class="pl-k">.</span>k1<span class="pl-k">.</span>precision)
    k2 <span class="pl-k">=</span> params<span class="pl-k">.</span>k2<span class="pl-k">.</span>var <span class="pl-k">*</span> <span class="pl-c1">SEKernel</span>() <span class="pl-k">∘</span> <span class="pl-c1">ScaleTransform</span>(params<span class="pl-k">.</span>k2<span class="pl-k">.</span>precision)
    <span class="pl-k">return</span> <span class="pl-c1">GP</span>(k1 <span class="pl-k">+</span> k2)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Generate some synthetic training data.</span>
<span class="pl-c"><span class="pl-c">#</span> Again, don't worry too much about the specifics here.</span>
<span class="pl-k">const</span> x <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>; length<span class="pl-k">=</span><span class="pl-c1">100</span>)
<span class="pl-k">const</span> y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">build_gp</span>(initial_params)(x, initial_params<span class="pl-k">.</span>noise_var))

<span class="pl-c"><span class="pl-c">#</span> Specify an objective function in terms of x and y.</span>
<span class="pl-k">function</span> <span class="pl-en">objective</span>(params<span class="pl-k">::</span><span class="pl-c1">NamedTuple</span>)
    f <span class="pl-k">=</span> <span class="pl-c1">build_gp</span>(params)
    <span class="pl-k">return</span> <span class="pl-k">-</span><span class="pl-c1">logpdf</span>(<span class="pl-c1">f</span>(x, params<span class="pl-k">.</span>noise_var), y)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Use Optim.jl to minimise the objective function w.r.t. the params.</span>
<span class="pl-c"><span class="pl-c">#</span> The important thing here is to note that we're passing in the flat vector of parameters to</span>
<span class="pl-c"><span class="pl-c">#</span> Optim, which is something that Optim knows how to work with, and using `unflatten` to convert</span>
<span class="pl-c"><span class="pl-c">#</span> from this representation to the structured one that our objective function knows about</span>
<span class="pl-c"><span class="pl-c">#</span> using `unflatten` -- we've used ParameterHandling to build a bridge between Optim and an</span>
<span class="pl-c"><span class="pl-c">#</span> entirely unrelated package.</span>
training_results <span class="pl-k">=</span> Optim<span class="pl-k">.</span><span class="pl-c1">optimize</span>(
    objective <span class="pl-k">∘</span> unflatten,
    θ <span class="pl-k">-&gt;</span> <span class="pl-c1">only</span>(Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(objective <span class="pl-k">∘</span> unflatten, θ)),
    flat_initial_params,
    <span class="pl-c1">BFGS</span>(
        alphaguess <span class="pl-k">=</span> Optim<span class="pl-k">.</span>LineSearches<span class="pl-k">.</span><span class="pl-c1">InitialStatic</span>(scaled<span class="pl-k">=</span><span class="pl-c1">true</span>),
        linesearch <span class="pl-k">=</span> Optim<span class="pl-k">.</span>LineSearches<span class="pl-k">.</span><span class="pl-c1">BackTracking</span>(),
    ),
    Optim<span class="pl-k">.</span><span class="pl-c1">Options</span>(show_trace <span class="pl-k">=</span> <span class="pl-c1">true</span>);
    inplace<span class="pl-k">=</span><span class="pl-c1">false</span>,
)

<span class="pl-c"><span class="pl-c">#</span> Extracting the final values of the parameters.</span>
final_params <span class="pl-k">=</span> <span class="pl-c1">unflatten</span>(training_results<span class="pl-k">.</span>minimizer)
f_trained <span class="pl-k">=</span> <span class="pl-c1">build_gp</span>(final_params)</pre></div>
<p dir="auto">Usually you would go on to make some predictions on test data using <code>f_trained</code>, or
something like that.
From the perspective of ParameterHandling.jl, we've seen the interesting stuff though.
In particular, we've seen an example of how ParameterHandling.jl can be used to bridge the
gap between the "flat" representation of parameters that <code>Optim</code> likes to work with, and the
"structured" representation that it's convenient to write optimisation algorithms with.</p>
<h1 dir="auto"><a id="user-content-gotchas-and-performance-tips" class="anchor" aria-hidden="true" href="#gotchas-and-performance-tips"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Gotchas and Performance Tips</h1>
<ol dir="auto">
<li><code>Integer</code>s typically don't take part in the kind of optimisation procedures that this package is designed to handle. Consequently, <code>flatten(::Integer)</code> produces an empty vector.</li>
<li><code>deferred</code> has some type-stability issues when used in conjunction with abstract types. For example, <code>flatten(deferred(Normal, 5.0, 4.0))</code> won't infer properly. A simple work around is to write a function <code>normal(args...) = Normal(args...)</code> and work with <code>deferred(normal, 5.0, 4.0)</code> instead.</li>
<li>Let <code>x</code> be an <code>Array{&lt;:Real}</code>. If you wish to constrain each of its values to be positive, prefer <code>positive(x)</code> over <code>map(positive, x)</code> or <code>positive.(x)</code>. <code>positive(x)</code> has been implemented the associated <code>unflatten</code> function has good performance, particularly when interacting with <code>Zygote</code> (when <code>map(positive, x)</code> is extremely slow). The same thing applies to <code>bounded</code> values. Prefer <code>bounded(x, lb, ub)</code> to e.g. <code>bounded.(x, lb, ub)</code>.</li>
</ol>
</article></div>