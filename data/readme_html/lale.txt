<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h3><a id="user-content-lalejl-julia-wrapper-of-pythons-lale-package" class="anchor" aria-hidden="true" href="#lalejl-julia-wrapper-of-pythons-lale-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Lale.jl: Julia wrapper of python's lale package</h3>
<table>
<thead>
<tr>
<th align="center"><strong>Documentation</strong></th>
<th align="center"><strong>Build Status</strong></th>
<th align="center"><strong>Help</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://ibm.github.io/Lale.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a> <a href="https://ibm.github.io/Lale.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/IBM/Lale.jl/actions/workflows/ci.yml"><img src="https://github.com/IBM/Lale.jl/actions/workflows/ci.yml/badge.svg" alt="" style="max-width:100%;"></a> <a href="https://codecov.io/gh/IBM/Lale.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a41ff60c9fad6d1d7f332db2c77c1d44ddd2d9ded1f12330a5a03ae925e3d53a/68747470733a2f2f636f6465636f762e696f2f67682f49424d2f4c616c652e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d594b363257394b513254" alt="" data-canonical-src="https://codecov.io/gh/IBM/Lale.jl/branch/main/graph/badge.svg?token=YK62W9KQ2T" style="max-width:100%;"></a></td>
<td align="center"><a href="https://julialang.slack.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/c192b6b30d22427a9ad86f7832a70c27f8dcbb028dae7dc2ca07181ef7dd9e13/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230736c61636b2d79656c6c6f772e737667" alt="" data-canonical-src="https://img.shields.io/badge/chat-on%20slack-yellow.svg" style="max-width:100%;"></a> <a href="https://gitter.im/Lale/community" rel="nofollow"><img src="https://camo.githubusercontent.com/3396a374a04ea09ec5934825a09583520fa11b49a44f155af40a2c75c2fa5ce7/68747470733a2f2f6261646765732e6769747465722e696d2f7070616c6d65732f4c616c652e6a6c2e737667" alt="" data-canonical-src="https://badges.gitter.im/ppalmes/Lale.jl.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<hr>
<p>Lale.jl is a Julia wrapper of Python's <a href="https://github.com/ibm/lale">Lale</a> library for semi-automated data science. Lale makes it easy to automatically select algorithms and tune hyperparameters of pipelines that are compatible with scikit-learn, in a type-safe fashion.</p>
<p>Instructions for Lale developers can be found <a href="./docs/DevInstruction.md">here</a>.</p>
<p>For a quick notebook demo: <a href="./demo/demo-lale-package-notebook.ipynb">Lale Notebook Demo</a> or you can view it with
<a href="https://nbviewer.jupyter.org/github/IBM/Lale.jl/blob/main/demo/demo-lale-package-notebook.ipynb" rel="nofollow">NBViewer</a>.</p>
<h3><a id="user-content-package-features" class="anchor" aria-hidden="true" href="#package-features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Package Features</h3>
<ul>
<li><strong>automation</strong>: provides a consistent high-level interface to existing pipeline search tools including Hyperopt, GridSearchCV, and SMAC</li>
<li><strong>correctness checks</strong>: uses JSON Schema to catch mistakes when there is a mismatch between hyperparameters and their type, or between data and operators</li>
<li><strong>interoperability</strong>: supports growing library of transformers and estimators</li>
</ul>
<p>Here is an example of a typical <code>Lale</code> pipeline using the following processing elements: Principal
Component Analysis (PCA), NoOp (no operation), Random Forest Regression (RFR),
and Decision Tree Regression (DTree):</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="lalepipe  = (PCA + NoOp) &gt;&gt; (RFR | DTree)
laleopt   = LalePipeOptimizer(lalepipe,max_evals = 10,cv = 3)
laletr    = fit!(laleopt, Xtrain,Ytrain)
pred      = transform!(laletr,Xtest)
"><pre>lalepipe  <span class="pl-k">=</span> (PCA <span class="pl-k">+</span> NoOp) <span class="pl-k">&gt;&gt;</span> (RFR <span class="pl-k">|</span> DTree)
laleopt   <span class="pl-k">=</span> <span class="pl-c1">LalePipeOptimizer</span>(lalepipe,max_evals <span class="pl-k">=</span> <span class="pl-c1">10</span>,cv <span class="pl-k">=</span> <span class="pl-c1">3</span>)
laletr    <span class="pl-k">=</span> <span class="pl-c1">fit!</span>(laleopt, Xtrain,Ytrain)
pred      <span class="pl-k">=</span> <span class="pl-c1">transform!</span>(laletr,Xtest)</pre></div>
<p>The block of code above will jointly search the optimal hyperparameters
of both Random Forest and Decision Tree learners and select the best
learner while at the same time searching the optimal hyperparameters
of the PCA.</p>
<p>The <em>pipe combinator</em>, <code>p1 &gt;&gt; p2</code>, first runs sub-pipeline
<code>p1</code> and then pipes its output into sub-pipeline <code>p2</code>.
The <em>union combinator</em>, <code>p1 + p2</code>, runs sub-pipelines <code>p1</code> and <code>p2</code> separately
over the same data, and then concatenates the output columns of both.
The <em>or combinator</em>, <code>p1 | p2</code>, creates an algorithmic choice for the optimizer
to search and select which between <code>p1</code> and <code>p2</code> yields better results.</p>
<h3><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h3>
<p>Lale is in the Julia General package registry. The latest
release can be installed from the julia prompt:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using Pkg
julia&gt; Pkg.update()
julia&gt; Pkg.add(&quot;Lale&quot;)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Pkg
julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">update</span>()
julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Lale<span class="pl-pds">"</span></span>)</pre></div>
<p>or use Julia's pkg shell which can be triggered by <code>]</code></p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; ]
pkg&gt; update
pkg&gt; add Lale
"><pre>julia<span class="pl-k">&gt;</span> ]
pkg<span class="pl-k">&gt;</span> update
pkg<span class="pl-k">&gt;</span> add Lale</pre></div>
<h4><a id="user-content-sample-lale-workflow" class="anchor" aria-hidden="true" href="#sample-lale-workflow"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Sample Lale Workflow</h4>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Lale
using DataFrames: DataFrame

# load data
iris = getiris()
Xreg = iris[:,1:3] |&gt; DataFrame
Yreg = iris[:,4]   |&gt; Vector
Xcl  = iris[:,1:4] |&gt; DataFrame
Ycl  = iris[:,5]   |&gt; Vector

# regression dataset
regsplit = train_test_split(Xreg,Yreg;testprop = 0.20)
trXreg,trYreg,tstXreg,tstYreg = regsplit

# classification dataset
clsplit = train_test_split(Xcl,Ycl;testprop = 0.20)
trXcl,trYcl,tstXcl,tstYcl = clsplit

# lale ops
pca     = laleoperator(&quot;PCA&quot;)
rb      = laleoperator(&quot;RobustScaler&quot;)
noop    = laleoperator(&quot;NoOp&quot;,&quot;lale&quot;)
rfr     = laleoperator(&quot;RandomForestRegressor&quot;)
rfc     = laleoperator(&quot;RandomForestClassifier&quot;)
treereg = laleoperator(&quot;DecisionTreeRegressor&quot;)

# Lale regression
lalepipe  = (pca + noop) &gt;&gt;  (rfr | treereg )
lale_hopt = LalePipeOptimizer(lalepipe,max_evals = 10,cv = 3)
laletrain = fit(lale_hopt,trXreg,trYreg)
lalepred  = transform(laletrain,tstXreg)
score(:rmse,lalepred,tstYreg) |&gt; println

# Lale classification
lalepipe  = (rb + pca) |&gt; rfc
lale_hopt = LalePipeOptimizer(lalepipe,max_evals = 10,cv = 3)
laletrain = fit(lale_hopt,trXcl,trYcl)
lalepred  = transform(laletrain,tstXcl)
score(:accuracy,lalepred,tstYcl) |&gt; println
"><pre><span class="pl-k">using</span> Lale
<span class="pl-k">using</span> DataFrames<span class="pl-k">:</span> DataFrame

<span class="pl-c"><span class="pl-c">#</span> load data</span>
iris <span class="pl-k">=</span> <span class="pl-c1">getiris</span>()
Xreg <span class="pl-k">=</span> iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>] <span class="pl-k">|&gt;</span> DataFrame
Yreg <span class="pl-k">=</span> iris[:,<span class="pl-c1">4</span>]   <span class="pl-k">|&gt;</span> Vector
Xcl  <span class="pl-k">=</span> iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>] <span class="pl-k">|&gt;</span> DataFrame
Ycl  <span class="pl-k">=</span> iris[:,<span class="pl-c1">5</span>]   <span class="pl-k">|&gt;</span> Vector

<span class="pl-c"><span class="pl-c">#</span> regression dataset</span>
regsplit <span class="pl-k">=</span> <span class="pl-c1">train_test_split</span>(Xreg,Yreg;testprop <span class="pl-k">=</span> <span class="pl-c1">0.20</span>)
trXreg,trYreg,tstXreg,tstYreg <span class="pl-k">=</span> regsplit

<span class="pl-c"><span class="pl-c">#</span> classification dataset</span>
clsplit <span class="pl-k">=</span> <span class="pl-c1">train_test_split</span>(Xcl,Ycl;testprop <span class="pl-k">=</span> <span class="pl-c1">0.20</span>)
trXcl,trYcl,tstXcl,tstYcl <span class="pl-k">=</span> clsplit

<span class="pl-c"><span class="pl-c">#</span> lale ops</span>
pca     <span class="pl-k">=</span> <span class="pl-c1">laleoperator</span>(<span class="pl-s"><span class="pl-pds">"</span>PCA<span class="pl-pds">"</span></span>)
rb      <span class="pl-k">=</span> <span class="pl-c1">laleoperator</span>(<span class="pl-s"><span class="pl-pds">"</span>RobustScaler<span class="pl-pds">"</span></span>)
noop    <span class="pl-k">=</span> <span class="pl-c1">laleoperator</span>(<span class="pl-s"><span class="pl-pds">"</span>NoOp<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>lale<span class="pl-pds">"</span></span>)
rfr     <span class="pl-k">=</span> <span class="pl-c1">laleoperator</span>(<span class="pl-s"><span class="pl-pds">"</span>RandomForestRegressor<span class="pl-pds">"</span></span>)
rfc     <span class="pl-k">=</span> <span class="pl-c1">laleoperator</span>(<span class="pl-s"><span class="pl-pds">"</span>RandomForestClassifier<span class="pl-pds">"</span></span>)
treereg <span class="pl-k">=</span> <span class="pl-c1">laleoperator</span>(<span class="pl-s"><span class="pl-pds">"</span>DecisionTreeRegressor<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> Lale regression</span>
lalepipe  <span class="pl-k">=</span> (pca <span class="pl-k">+</span> noop) <span class="pl-k">&gt;&gt;</span>  (rfr <span class="pl-k">|</span> treereg )
lale_hopt <span class="pl-k">=</span> <span class="pl-c1">LalePipeOptimizer</span>(lalepipe,max_evals <span class="pl-k">=</span> <span class="pl-c1">10</span>,cv <span class="pl-k">=</span> <span class="pl-c1">3</span>)
laletrain <span class="pl-k">=</span> <span class="pl-c1">fit</span>(lale_hopt,trXreg,trYreg)
lalepred  <span class="pl-k">=</span> <span class="pl-c1">transform</span>(laletrain,tstXreg)
<span class="pl-c1">score</span>(<span class="pl-c1">:rmse</span>,lalepred,tstYreg) <span class="pl-k">|&gt;</span> println

<span class="pl-c"><span class="pl-c">#</span> Lale classification</span>
lalepipe  <span class="pl-k">=</span> (rb <span class="pl-k">+</span> pca) <span class="pl-k">|&gt;</span> rfc
lale_hopt <span class="pl-k">=</span> <span class="pl-c1">LalePipeOptimizer</span>(lalepipe,max_evals <span class="pl-k">=</span> <span class="pl-c1">10</span>,cv <span class="pl-k">=</span> <span class="pl-c1">3</span>)
laletrain <span class="pl-k">=</span> <span class="pl-c1">fit</span>(lale_hopt,trXcl,trYcl)
lalepred  <span class="pl-k">=</span> <span class="pl-c1">transform</span>(laletrain,tstXcl)
<span class="pl-c1">score</span>(<span class="pl-c1">:accuracy</span>,lalepred,tstYcl) <span class="pl-k">|&gt;</span> println</pre></div>
<p>Moreover, Lale is also compatible with <a href="https://github.com/IBM/AutoMLPipeline.jl">AutoMLPipeline</a> <code>@pipeline</code> syntax:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# regression pipeline
regpipe      = @pipeline (pca + rb) |&gt;  rfr
regmodel     = fit(regpipe,trXreg, trYreg)
regpred      = transform(regmodel,tstXreg)
regperf(x,y) = score(:rmse,x,y)
regperf(regpred, tstYreg) |&gt; println
crossvalidate(regpipe,Xreg,Yreg,regperf)

# classification pipeline
clpipe         = @pipeline (pca + noop) |&gt;  rfc
clmodel        = fit(clpipe,trXcl, trYcl)
clpred         = transform(clmodel,tstXcl)
classperf(x,y) = score(:accuracy,x,y)
classperf(clpred, tstYcl) |&gt; println
crossvalidate(clpipe,Xcl,Ycl,classperf)
"><pre><span class="pl-c"><span class="pl-c">#</span> regression pipeline</span>
regpipe      <span class="pl-k">=</span> <span class="pl-c1">@pipeline</span> (pca <span class="pl-k">+</span> rb) <span class="pl-k">|&gt;</span>  rfr
regmodel     <span class="pl-k">=</span> <span class="pl-c1">fit</span>(regpipe,trXreg, trYreg)
regpred      <span class="pl-k">=</span> <span class="pl-c1">transform</span>(regmodel,tstXreg)
<span class="pl-en">regperf</span>(x,y) <span class="pl-k">=</span> <span class="pl-c1">score</span>(<span class="pl-c1">:rmse</span>,x,y)
<span class="pl-c1">regperf</span>(regpred, tstYreg) <span class="pl-k">|&gt;</span> println
<span class="pl-c1">crossvalidate</span>(regpipe,Xreg,Yreg,regperf)

<span class="pl-c"><span class="pl-c">#</span> classification pipeline</span>
clpipe         <span class="pl-k">=</span> <span class="pl-c1">@pipeline</span> (pca <span class="pl-k">+</span> noop) <span class="pl-k">|&gt;</span>  rfc
clmodel        <span class="pl-k">=</span> <span class="pl-c1">fit</span>(clpipe,trXcl, trYcl)
clpred         <span class="pl-k">=</span> <span class="pl-c1">transform</span>(clmodel,tstXcl)
<span class="pl-en">classperf</span>(x,y) <span class="pl-k">=</span> <span class="pl-c1">score</span>(<span class="pl-c1">:accuracy</span>,x,y)
<span class="pl-c1">classperf</span>(clpred, tstYcl) <span class="pl-k">|&gt;</span> println
<span class="pl-c1">crossvalidate</span>(clpipe,Xcl,Ycl,classperf)</pre></div>
</article></div>