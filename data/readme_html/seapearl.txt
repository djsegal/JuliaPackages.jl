<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-seapearljl" class="anchor" aria-hidden="true" href="#seapearljl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SeaPearl.jl</h1>
<p>SeaPearl is a Constraint Programming solver that can use Reinforcement Learning agents as value-selection heuristics, using graphs as inputs for the agent's approximator. It is to be seen as a tool for researchers that gives the possibility to go above and beyond what has already been done with it.</p>
<p>The paper accompanying this solver can be found on the <a href="https://arxiv.org/abs/2102.09193v1" rel="nofollow">arXiv</a>. If you use SeaPearl in your research, please cite our work.</p>
<p>The RL agents are defined using <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a>, their inputs are dealt with using <a href="https://github.com/FluxML/GeometricFlux.jl">GeometricFlux.jl</a> and <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>. The CP part, inspired from <a href="http://www.minicp.org/" rel="nofollow">MiniCP</a>, is focused on readability. The code is meant to be clear and modulable so that researchers could easily get access to CP data and use it as input for their ML model.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="]add SeaPearl
"><pre>]add SeaPearl</pre></div>
<h2><a id="user-content-use" class="anchor" aria-hidden="true" href="#use"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Use</h2>
<p>Working examples can be found in <a href="https://github.com/corail-research/SeaPearlZoo">SeaPearlZoo</a>.</p>
<p>SeaPearl can be use either as a classic CP solver that uses predefined variable and value selection heuristics or as Reinforcement Learning driven CP solver that is capable of learning trought solving automatically generated instances of a given problem ( knapsack, tsptw, graphcoloring, nurse rostering ...).</p>
<h3><a id="user-content-seapearl-as-a-classic-cp-solver-" class="anchor" aria-hidden="true" href="#seapearl-as-a-classic-cp-solver-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SeaPearl as a classic CP solver :</h3>
<p>To use SeaPearl as a classic CP solver, one needs to  :</p>
<ol>
<li>declare a variable selection heuristic :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="YourVariableSelectionHeuristic{TakeObjective} &lt;: SeaPearl.AbstractVariableSelection{TakeObjective}
"><pre>YourVariableSelectionHeuristic{TakeObjective} <span class="pl-k">&lt;:</span> <span class="pl-c1">SeaPearl.AbstractVariableSelection{TakeObjective}</span></pre></div>
<ol start="2">
<li>declare a value selection heuristic :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="BasicHeuristic &lt;: ValueSelection
"><pre>BasicHeuristic <span class="pl-k">&lt;:</span> <span class="pl-c1">ValueSelection</span></pre></div>
<ol start="3">
<li>create a Constraint Programming Model :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="trailer = SeaPearl.Trailer()
model = SeaPearl.CPModel(trailer)

#create variable : 
SeaPearl.addVariable!(...)

#add constraints : 
push!(model.constraints, SeaPearl.AbstractConstraint(...))

#add optionnal objective function : 
model.objective = ObjectiveVar

"><pre>trailer <span class="pl-k">=</span> SeaPearl<span class="pl-k">.</span><span class="pl-c1">Trailer</span>()
model <span class="pl-k">=</span> SeaPearl<span class="pl-k">.</span><span class="pl-c1">CPModel</span>(trailer)

<span class="pl-c"><span class="pl-c">#</span>create variable : </span>
SeaPearl<span class="pl-k">.</span><span class="pl-c1">addVariable!</span>(<span class="pl-k">...</span>)

<span class="pl-c"><span class="pl-c">#</span>add constraints : </span>
<span class="pl-c1">push!</span>(model<span class="pl-k">.</span>constraints, SeaPearl<span class="pl-k">.</span><span class="pl-c1">AbstractConstraint</span>(<span class="pl-k">...</span>))

<span class="pl-c"><span class="pl-c">#</span>add optionnal objective function : </span>
model<span class="pl-k">.</span>objective <span class="pl-k">=</span> ObjectiveVar
</pre></div>
<h3><a id="user-content-seapearl-as-a-rl-driven-cp-solver-" class="anchor" aria-hidden="true" href="#seapearl-as-a-rl-driven-cp-solver-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SeaPearl as a RL-driven CP solver :</h3>
<p>To use SeaPearl as a RL-driven CP solver, one needs to  :</p>
<ol>
<li>declare a variable selection heuristic :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="CustomVariableSelectionHeuristic{TakeObjective} &lt;: SeaPearl.AbstractVariableSelection{TakeObjective}
"><pre>CustomVariableSelectionHeuristic{TakeObjective} <span class="pl-k">&lt;:</span> <span class="pl-c1">SeaPearl.AbstractVariableSelection{TakeObjective}</span></pre></div>
<ol start="2">
<li>declare a value selection learnedheuristic :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="LearnedHeuristic{SR&lt;:AbstractStateRepresentation, R&lt;:AbstractReward, A&lt;:ActionOutput} &lt;: ValueSelection
"><pre>LearnedHeuristic{SR<span class="pl-k">&lt;:</span><span class="pl-c1">AbstractStateRepresentation</span>, R<span class="pl-k">&lt;:</span><span class="pl-c1">AbstractReward</span>, A<span class="pl-k">&lt;:</span><span class="pl-c1">ActionOutput</span>} <span class="pl-k">&lt;:</span> <span class="pl-c1">ValueSelection</span></pre></div>
<ol start="3">
<li><em>optionnaly</em>, declare some classic value selection heuristic for benchmarking purposes</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="basicHeuristic = SeaPearl.BasicHeuristic((x; cpmodel) -&gt; your_function(...))
"><pre>basicHeuristic <span class="pl-k">=</span> SeaPearl<span class="pl-k">.</span><span class="pl-c1">BasicHeuristic</span>((x; cpmodel) <span class="pl-k">-&gt;</span> <span class="pl-c1">your_function</span>(<span class="pl-k">...</span>))</pre></div>
<ol start="4">
<li>define an agent :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="agent = RL.Agent(
policy=(...),
trajectory=(...),
)
"><pre>agent <span class="pl-k">=</span> RL<span class="pl-k">.</span><span class="pl-c1">Agent</span>(
policy<span class="pl-k">=</span>(<span class="pl-k">...</span>),
trajectory<span class="pl-k">=</span>(<span class="pl-k">...</span>),
)</pre></div>
<ol start="5">
<li><em>optionnaly</em>, declare a custom reward :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="CustomReward &lt;: SeaPearl.AbstractReward 
"><pre>CustomReward <span class="pl-k">&lt;:</span> <span class="pl-c1">SeaPearl.AbstractReward</span> </pre></div>
<ol start="6">
<li><em>optionnaly</em>, declare a custom StateRepresentation ( instead of the Default tripartite-graph representation ) :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="CustomStateRepresentation &lt;: SeaPearl.AbstractStateRepresentation
"><pre>CustomStateRepresentation <span class="pl-k">&lt;:</span> <span class="pl-c1">SeaPearl.AbstractStateRepresentation</span></pre></div>
<ol start="7">
<li><em>optionnaly</em>, declare a custom featurization for the StateRepresentation :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="CustomFeaturization &lt;: SeaPearl.AbstractFeaturization
"><pre>CustomFeaturization <span class="pl-k">&lt;:</span> <span class="pl-c1">SeaPearl.AbstractFeaturization</span></pre></div>
<ol start="8">
<li>create a generator for your given problem, that will create different instances of the specific problem used during the learning process.</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="CustomProblemGenerator &lt;: AbstractModelGenerator
"><pre>CustomProblemGenerator <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractModelGenerator</span></pre></div>
<ol start="9">
<li>set a number of training epochs, declare an evaluator, a Strategy, a metric for benchmarking</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="nb_epochs = 3000
CustomStrategy &lt;: SearchStrategy #or use predefined one : SeaPearl.DFSearch
CustomEvaluator &lt;: AbstractEvaluator #or use predefined one : SeaPearl.SameInstancesEvaluator(...)
function CustomMetricsFun
"><pre>nb_epochs <span class="pl-k">=</span> <span class="pl-c1">3000</span>
CustomStrategy <span class="pl-k">&lt;:</span> <span class="pl-c1">SearchStrategy</span> <span class="pl-c"><span class="pl-c">#</span>or use predefined one : SeaPearl.DFSearch</span>
CustomEvaluator <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractEvaluator</span> <span class="pl-c"><span class="pl-c">#</span>or use predefined one : SeaPearl.SameInstancesEvaluator(...)</span>
<span class="pl-k">function</span> CustomMetricsFun</pre></div>
<ol start="9">
<li>launch the training :</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="bestsolutions, nodevisited,timeneeded, eval_nodevisited, eval_timeneeded = SeaPearl.train!(
valueSelectionArray=[learnedHeuristic, basicHeuristic], 
generator=CustomProblemGenerator,
nb_episodes=nb_episodes,
strategy=CustomStrategy,
variableHeuristic=CustomVariableSelectionHeuristic,
metricsFun=CustomMetricsFun,
evaluator=CustomEvaluator
"><pre>bestsolutions, nodevisited,timeneeded, eval_nodevisited, eval_timeneeded <span class="pl-k">=</span> SeaPearl<span class="pl-k">.</span><span class="pl-c1">train!</span>(
valueSelectionArray<span class="pl-k">=</span>[learnedHeuristic, basicHeuristic], 
generator<span class="pl-k">=</span>CustomProblemGenerator,
nb_episodes<span class="pl-k">=</span>nb_episodes,
strategy<span class="pl-k">=</span>CustomStrategy,
variableHeuristic<span class="pl-k">=</span>CustomVariableSelectionHeuristic,
metricsFun<span class="pl-k">=</span>CustomMetricsFun,
evaluator<span class="pl-k">=</span>CustomEvaluator</pre></div>
<p>)</p>
<h2><a id="user-content-contributing-to-seapearl" class="anchor" aria-hidden="true" href="#contributing-to-seapearl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing to SeaPearl</h2>
<p>All PRs and issues are welcome.
This repo contains README.md and images to facilitate the understanding of the code.
To contribute to Sealpearl, follow these steps:</p>
<ol>
<li>Fork this repository.</li>
<li>Create a branch: <code>git checkout -b &lt;branch_name&gt;</code>.</li>
<li>Make your changes and commit them: <code>git commit -m '&lt;commit_message&gt;'</code></li>
<li>Push to the original branch: <code>git push origin &lt;project_name&gt;/&lt;location&gt;</code></li>
<li>Create the pull request.</li>
</ol>
<p>Alternatively see the GitHub documentation on <a href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request">creating a pull request</a>.</p>
</article></div>