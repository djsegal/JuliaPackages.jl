<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-subsetselectioncio" class="anchor" aria-hidden="true" href="#subsetselectioncio"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SubsetSelectionCIO</h1>
<p>SubsetSelection is a Julia package that computes sparse L2-regularized estimators. Sparsity is enforced through explicit cardinality constraint. Supported loss functions for regression are least squares; for classification, logistic and L1 Hinge loss. The algorithm formulates the problem as a pure-integer convex optimization problem and solves it using a cutting plane algorithm.The package is built up on the <a href="https://github.com/jeanpauphilet/SubsetSelection.jl">SubsetSelection</a> package.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick start</h2>
<p>To install the package:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; Pkg.clone(&quot;git://github.com/jeanpauphilet/SubsetSelectionCIO.jl.git&quot;)
"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>git://github.com/jeanpauphilet/SubsetSelectionCIO.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<p>To fit a basic model:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using SubsetSelectionCIO, StatsBase

julia&gt; n = 100; p = 10000; k = 10;
julia&gt; indices = sort(sample(1:p, StatsBase.Weights(ones(p)/p), k, replace=false));
julia&gt; w = sample(-1:2:1, k);
julia&gt; X = randn(n,p); Y = X[:,indices]*w;
julia&gt; γ = 1/sqrt(size(X,1));
julia&gt; indices0, w0, Δt, status, Gap, cutCount = oa_formulation(SubsetSelection.OLS(), Y, X, k, γ)
([36, 184, 222, 240, 325, 347, 361, 605, 957, 973], [-0.950513, -0.94923, -0.950688, -0.956536, 0.951954, -0.953707, -0.954927, -0.9571, -0.959357, -0.95312], 0.26711583137512207, :Optimal, 0.0, 17)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> SubsetSelectionCIO, StatsBase

julia<span class="pl-k">&gt;</span> n <span class="pl-k">=</span> <span class="pl-c1">100</span>; p <span class="pl-k">=</span> <span class="pl-c1">10000</span>; k <span class="pl-k">=</span> <span class="pl-c1">10</span>;
julia<span class="pl-k">&gt;</span> indices <span class="pl-k">=</span> <span class="pl-c1">sort</span>(<span class="pl-c1">sample</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>p, StatsBase<span class="pl-k">.</span><span class="pl-c1">Weights</span>(<span class="pl-c1">ones</span>(p)<span class="pl-k">/</span>p), k, replace<span class="pl-k">=</span><span class="pl-c1">false</span>));
julia<span class="pl-k">&gt;</span> w <span class="pl-k">=</span> <span class="pl-c1">sample</span>(<span class="pl-k">-</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">1</span>, k);
julia<span class="pl-k">&gt;</span> X <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n,p); Y <span class="pl-k">=</span> X[:,indices]<span class="pl-k">*</span>w;
julia<span class="pl-k">&gt;</span> γ <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">sqrt</span>(<span class="pl-c1">size</span>(X,<span class="pl-c1">1</span>));
julia<span class="pl-k">&gt;</span> indices0, w0, Δt, status, Gap, cutCount <span class="pl-k">=</span> <span class="pl-c1">oa_formulation</span>(SubsetSelection<span class="pl-k">.</span><span class="pl-c1">OLS</span>(), Y, X, k, γ)
([<span class="pl-c1">36</span>, <span class="pl-c1">184</span>, <span class="pl-c1">222</span>, <span class="pl-c1">240</span>, <span class="pl-c1">325</span>, <span class="pl-c1">347</span>, <span class="pl-c1">361</span>, <span class="pl-c1">605</span>, <span class="pl-c1">957</span>, <span class="pl-c1">973</span>], [<span class="pl-k">-</span><span class="pl-c1">0.950513</span>, <span class="pl-k">-</span><span class="pl-c1">0.94923</span>, <span class="pl-k">-</span><span class="pl-c1">0.950688</span>, <span class="pl-k">-</span><span class="pl-c1">0.956536</span>, <span class="pl-c1">0.951954</span>, <span class="pl-k">-</span><span class="pl-c1">0.953707</span>, <span class="pl-k">-</span><span class="pl-c1">0.954927</span>, <span class="pl-k">-</span><span class="pl-c1">0.9571</span>, <span class="pl-k">-</span><span class="pl-c1">0.959357</span>, <span class="pl-k">-</span><span class="pl-c1">0.95312</span>], <span class="pl-c1">0.26711583137512207</span>, <span class="pl-c1">:Optimal</span>, <span class="pl-c1">0.0</span>, <span class="pl-c1">17</span>)</pre></div>
<p>The algorithm returns a set of indices <code>indices0</code>, the value of the estimator on the selected features only  <code>w0</code>, the time needed to compute the model <code>Δt</code>, the status of the MIP solver <code>status</code>, the sub-optimality gap <code>Gap</code> and the number of cuts required by the cutting-plane algorithm <code>cutCount</code>.</p>
<p>For classification, we use +1/-1 labels and the convention
<code>P ( Y = y | X = x ) = 1 / (1+e^{- y x^T w})</code>.</p>
<h2><a id="user-content-required-and-optional-parameters" class="anchor" aria-hidden="true" href="#required-and-optional-parameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Required and optional parameters</h2>
<p><code>oa_formulation</code> has five required parameters:</p>
<ul>
<li>the loss function to be minimized, to be chosen among least squares (<code>SubsetSelection.OLS()</code>), Logistic loss (<code>SubsetSelection.LogReg()</code>) and Hinge Loss (<code>SubsetSelection.L1SVM()</code>).</li>
<li>the vector of outputs <code>Y</code> of size <code>n</code>, the sample size. In classification settings, <code>Y</code> should be a vector of ±1s.</li>
<li>the matrix of covariates <code>X</code> of size <code>n</code>×<code>p</code>, where <code>n</code> and <code>p</code> are the number of samples and features respectively.</li>
<li>the level sparsity <code>k</code>; the algorithm will consider the hard constraint "||w||_0 &lt; k".</li>
<li>the value of the ℓ2-regularization parameter <code>γ</code>.</li>
</ul>
<p>In addition, <code>oa_formulation</code> accepts the following optional parameters:</p>
<ul>
<li>an initialization for the selected features, <code>indices0</code>.</li>
<li>a time limit <code>ΔT_max</code>, in seconds, set to 60 by default.</li>
<li><code>verbose</code>, a boolean. If true, the MIP solver information is displayed. By default, set to false.</li>
<li><code>Gap</code> a limit on the suboptimality gap to reach. By default, set to 0.</li>
</ul>
<h2><a id="user-content-best-practices" class="anchor" aria-hidden="true" href="#best-practices"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Best practices</h2>
<ul>
<li>Tuning the regularization parameter <code>γ</code>: Setting <code>γ</code> to 1/√n seems like an appropriate scaling in most regression instances. For an optimal performance, and especially in classification or noisy settings, we recommend performing a grid search and using cross-validation to assess out-of-sample performance. The grid search should start with a very low value for <code>γ</code>, such as</li>
</ul>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="    γ = 1.*p / k / n / maximum(sum(X[train,:].^2,2))
"><pre>    γ <span class="pl-k">=</span> <span class="pl-c1">1.</span><span class="pl-k">*</span>p <span class="pl-k">/</span> k <span class="pl-k">/</span> n <span class="pl-k">/</span> <span class="pl-c1">maximum</span>(<span class="pl-c1">sum</span>(X[train,:]<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>,<span class="pl-c1">2</span>))</pre></div>
<p>and iteratively increase it by a factor 2. Mean square error or Area Under the Curve (see <a href="https://github.com/davidavdav/ROCAnalysis.jl">ROCAnalysis</a> for implementation) are commonly used performance metrics for regression and classification tasks respectively.</p>
<ul>
<li>The mixed-integer solver greatly benefits from a good warm-start, even though the warm start is not feasible, i.e., <code>k</code>-sparse. Among other methods, one can use the output of <a href="https://github.com/jeanpauphilet/SubsetSelection.jl">SubsetSelection</a> or a Lasso estimator (see <a href="https://github.com/JuliaStats/GLMNet.jl">GLMNet</a> implementation for instance).</li>
</ul>
<h2><a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reference</h2>
<p>Dimitris Bertsimas, Jean Pauphilet, Bart Van Parys, <i> Sparse Classification : a scalable discrete optimization perspective </i>, available on <a href="http://arxiv.org/abs/1710.01352" rel="nofollow">Arxiv</a></p>
</article></div>