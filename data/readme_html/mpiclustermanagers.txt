<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-mpiclustermanagersjl" class="anchor" aria-hidden="true" href="#mpiclustermanagersjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MPIClusterManagers.jl</h1>
<p><a href="https://travis-ci.org/JuliaParallel/MPIClusterManagers.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/5616694e59ff0ee2180b9beb708abac6eaa993d405a8d25a57be8ea0c73b56f4/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961506172616c6c656c2f4d5049436c75737465724d616e61676572732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Travis Status" data-canonical-src="https://travis-ci.org/JuliaParallel/MPIClusterManagers.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://ci.appveyor.com/project/simonbyrne/mpiclustermanagers-jl/branch/master" rel="nofollow"><img src="https://camo.githubusercontent.com/d46874edd3c7b05b2d2b9ca0ce1e511e4758648170783ef6059bf5adad8a4cb2/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f78777063736d703066673538327466342f6272616e63682f6d61737465723f7376673d74727565" alt="Appveyor status" data-canonical-src="https://ci.appveyor.com/api/projects/status/xwpcsmp0fg582tf4/branch/master?svg=true" style="max-width:100%;"></a></p>
<h2><a id="user-content-mpi-and-julia-parallel-constructs-together" class="anchor" aria-hidden="true" href="#mpi-and-julia-parallel-constructs-together"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MPI and Julia parallel constructs together</h2>
<p>In order for MPI calls to be made from a Julia cluster, it requires the use of
<code>MPIManager</code>, a cluster manager that will start the julia workers using <code>mpirun</code></p>
<p>It has three modes of operation</p>
<ul>
<li>
<p>Only worker processes execute MPI code. The Julia master process executes outside of and
is not part of the MPI cluster. Free bi-directional TCP/IP connectivity is required
between all processes</p>
</li>
<li>
<p>All processes (including Julia master) are part of both the MPI as well as Julia cluster.
Free bi-directional TCP/IP connectivity is required between all processes.</p>
</li>
<li>
<p>All processes are part of both the MPI as well as Julia cluster. MPI is used as the transport
for julia messages. This is useful on environments which do not allow TCP/IP connectivity
between worker processes Note: This capability works with Julia 1.0, 1.1 and 1.2 and releases
after 1.4.2. It is broken for Julia 1.3, 1.4.0, and 1.4.1.</p>
</li>
</ul>
<h3><a id="user-content-mpimanager-only-workers-execute-mpi-code" class="anchor" aria-hidden="true" href="#mpimanager-only-workers-execute-mpi-code"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MPIManager: only workers execute MPI code</h3>
<p>An example is provided in <code>examples/juliacman.jl</code>.
The julia master process is NOT part of the MPI cluster. The main script should be
launched directly, <code>MPIManager</code> internally calls <code>mpirun</code> to launch julia/MPI workers.
All the workers started via <code>MPIManager</code> will be part of the MPI cluster.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="MPIManager(;np=Sys.CPU_THREADS, mpi_cmd=false, launch_timeout=60.0)
"><pre><code>MPIManager(;np=Sys.CPU_THREADS, mpi_cmd=false, launch_timeout=60.0)
</code></pre></div>
<p>If not specified, <code>mpi_cmd</code> defaults to <code>mpirun -np $np</code>
<code>stdout</code> from the launched workers is redirected back to the julia session calling <code>addprocs</code> via a TCP connection.
Thus the workers must be able to freely connect via TCP to the host session.
The following lines will be typically required on the julia master process to support both julia and MPI:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# to import MPIManager
using MPIClusterManagers

# need to also import Distributed to use addprocs()
using Distributed

# specify, number of mpi workers, launch cmd, etc.
manager=MPIManager(np=4)

# start mpi workers and add them as julia workers too.
addprocs(manager)
"><pre><span class="pl-c"><span class="pl-c">#</span> to import MPIManager</span>
<span class="pl-k">using</span> MPIClusterManagers

<span class="pl-c"><span class="pl-c">#</span> need to also import Distributed to use addprocs()</span>
<span class="pl-k">using</span> Distributed

<span class="pl-c"><span class="pl-c">#</span> specify, number of mpi workers, launch cmd, etc.</span>
manager<span class="pl-k">=</span><span class="pl-c1">MPIManager</span>(np<span class="pl-k">=</span><span class="pl-c1">4</span>)

<span class="pl-c"><span class="pl-c">#</span> start mpi workers and add them as julia workers too.</span>
<span class="pl-c1">addprocs</span>(manager)</pre></div>
<p>To execute code with MPI calls on all workers, use <code>@mpi_do</code>.</p>
<p><code>@mpi_do manager expr</code> executes <code>expr</code> on all processes that are part of <code>manager</code>.</p>
<p>For example:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@mpi_do manager begin
  using MPI
  comm=MPI.COMM_WORLD
  println(&quot;Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))&quot;)
end
"><pre><span class="pl-c1">@mpi_do</span> manager <span class="pl-k">begin</span>
  <span class="pl-k">using</span> MPI
  comm<span class="pl-k">=</span>MPI<span class="pl-k">.</span>COMM_WORLD
  <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Hello world, I am <span class="pl-v">$(MPI<span class="pl-k">.</span><span class="pl-c1">Comm_rank</span>(comm))</span> of <span class="pl-v">$(MPI<span class="pl-k">.</span><span class="pl-c1">Comm_size</span>(comm))</span><span class="pl-pds">"</span></span>)
<span class="pl-k">end</span></pre></div>
<p>executes on all MPI workers belonging to <code>manager</code> only</p>
<p><a href="https://github.com/JuliaParallel/MPIClusterManagers.jl/blob/master/examples/juliacman.jl"><code>examples/juliacman.jl</code></a> is a simple example of calling MPI functions on all workers interspersed with Julia parallel methods.</p>
<p>This should be run <em>without</em> <code>mpirun</code>:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia juliacman.jl
"><pre><code>julia juliacman.jl
</code></pre></div>
<p>A single instation of <code>MPIManager</code> can be used only once to launch MPI workers (via <code>addprocs</code>).
To create multiple sets of MPI clusters, use separate, distinct <code>MPIManager</code> objects.</p>
<p><code>procs(manager::MPIManager)</code> returns a list of julia pids belonging to <code>manager</code>
<code>mpiprocs(manager::MPIManager)</code> returns a list of MPI ranks belonging to <code>manager</code></p>
<p>Fields <code>j2mpi</code> and <code>mpi2j</code> of <code>MPIManager</code> are associative collections mapping julia pids to MPI ranks and vice-versa.</p>
<h3><a id="user-content-mpimanager-tcpip-transport---all-processes-execute-mpi-code" class="anchor" aria-hidden="true" href="#mpimanager-tcpip-transport---all-processes-execute-mpi-code"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MPIManager: TCP/IP transport - all processes execute MPI code</h3>
<p>Useful on environments which do not allow TCP connections outside of the cluster</p>
<p>An example is in <a href="https://github.com/JuliaParallel/MPIClusterManagers.jl/blob/master/examples/cman-transport.jl"><code>examples/cman-transport.jl</code></a>:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="mpirun -np 5 julia cman-transport.jl TCP
"><pre><code>mpirun -np 5 julia cman-transport.jl TCP
</code></pre></div>
<p>This launches a total of 5 processes, mpi rank 0 is the julia pid 1. mpi rank 1 is julia pid 2 and so on.</p>
<p>The program must call <code>MPIClusterManagers.start_main_loop(TCP_TRANSPORT_ALL)</code> with argument <code>TCP_TRANSPORT_ALL</code>.
On mpi rank 0, it returns a <code>manager</code> which can be used with <code>@mpi_do</code>
On other processes (i.e., the workers) the function does not return</p>
<h3><a id="user-content-mpimanager-mpi-transport---all-processes-execute-mpi-code" class="anchor" aria-hidden="true" href="#mpimanager-mpi-transport---all-processes-execute-mpi-code"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MPIManager: MPI transport - all processes execute MPI code</h3>
<p><code>MPIClusterManagers.start_main_loop</code> must be called with option <code>MPI_TRANSPORT_ALL</code> to use MPI as transport.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="mpirun -np 5 julia cman-transport.jl MPI
"><pre><code>mpirun -np 5 julia cman-transport.jl MPI
</code></pre></div>
<p>will run the example using MPI as transport.</p>
</article></div>