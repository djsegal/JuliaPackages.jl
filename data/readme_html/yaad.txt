<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-yaadjl" class="anchor" aria-hidden="true" href="#yaadjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>YAAD.jl</h1>
<p><a href="https://travis-ci.org/Roger-luo/YAAD.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dc20c23b6b709c79820757da4e48cb9e8bc87e84/68747470733a2f2f7472617669732d63692e6f72672f526f6765722d6c756f2f594141442e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Roger-luo/YAAD.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://rogerluo.me/YAAD.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://rogerluo.me/YAAD.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/57bae07ecd50a99519ad0516d91f4ec8f0f48e12/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width:100%;"></a></p>
<p>Yet Another Automatic Differentiation package in Julia.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>Press <code>]</code> and use <code>pkg</code> mode in Julia REPL, then type:</p>
<pre><code>pkg&gt; add YAAD
</code></pre>
<h2><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>You may want to check my blog post about it: <a href="http://blog.rogerluo.me/2018/10/23/write-an-ad-in-one-day/" rel="nofollow">Implement AD with Julia in ONE day</a></p>
<p>This project aims to provide a similar interface with PyTorch's autograd, while
keeping things simple. The core implementation only contains a straight-forward
200 line of Julia. It is highly inspired by <a href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad.jl</a>
and <a href="https://github.com/pytorch/pytorch">PyTorch</a></p>
<p>Every operation will directly return a <code>CachedNode</code>, which constructs a computation
graph dynamically without using a global tape.</p>
<p><strong>NOTE</strong>: This project is for self-use at the moment, it will be a place for me to do AD related
experimental coding, I don't guarantee the consistency and stability between versions (different version can be in-compatible). For practical usage, I would suggest you try <code>Flux.Tracker</code> or <code>Zygote</code>. They may have better performance and are aimed to be non-experimental projects.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<p>It is simple. Mark what you want to differentiate with <code>Variable</code>, which contains <code>value</code>
and <code>grad</code>. Each time you try to <code>backward</code> evaluate, the gradient will be accumulated to
<code>grad</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> LinearAlgebra
x1, x2 <span class="pl-k">=</span> <span class="pl-c1">Variable</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">30</span>, <span class="pl-c1">30</span>)), <span class="pl-c1">Variable</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">30</span>, <span class="pl-c1">30</span>))
y <span class="pl-k">=</span> <span class="pl-c1">tr</span>(x1 <span class="pl-k">*</span> x2) <span class="pl-c"><span class="pl-c">#</span> you get a tracked value here</span>
<span class="pl-c1">backward</span>(y) <span class="pl-c"><span class="pl-c">#</span> backward propagation</span>
<span class="pl-c1">print</span>(x1<span class="pl-k">.</span>grad) <span class="pl-c"><span class="pl-c">#</span> this is where gradient goes</span></pre></div>
<p>Or you can just register your own</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> first define how you want to create a node in computation graph</span>
<span class="pl-en">sin</span>(x<span class="pl-k">::</span><span class="pl-c1">AbstractNode</span>) <span class="pl-k">=</span> <span class="pl-c1">register</span>(sin, x)

<span class="pl-c"><span class="pl-c">#</span> then define the gradient</span>
<span class="pl-en">gradient</span>(<span class="pl-k">::</span><span class="pl-c1">typeof</span>(sin), grad, output, x) <span class="pl-k">=</span> grad <span class="pl-k">*</span> <span class="pl-c1">cos</span>(x)</pre></div>
<h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>License</h2>
<p>Apache License Version 2.0</p>
</article></div>