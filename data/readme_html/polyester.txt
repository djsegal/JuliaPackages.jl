<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-polyester" class="anchor" aria-hidden="true" href="#polyester"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Polyester</h1>
<p dir="auto"><a href="https://JuliaSIMD.github.io/Polyester.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://JuliaSIMD.github.io/Polyester.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/JuliaSIMD/Polyester.jl/actions/workflows/CI.yml"><img src="https://github.com/JuliaSIMD/Polyester.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://github.com/JuliaSIMD/Polyester.jl/actions/workflows/CI-julia-nightly.yml"><img src="https://github.com/JuliaSIMD/Polyester.jl/actions/workflows/CI-julia-nightly.yml/badge.svg" alt="CI-Nightly" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaSIMD/Polyester.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a3d3d9efea959a1d44e1730a09d4dc053cb328df92ff49e247a3b960ab0d41ff/68747470733a2f2f636f6465636f762e696f2f67682f4a756c696153494d442f506f6c7965737465722e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/JuliaSIMD/Polyester.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Note that <code>Polyester.@batch</code> moves arrays to threads by turning them into <a href="https://github.com/JuliaSIMD/StrideArraysCore.jl">StrideArraysCore.PtrArray</a>s.
This means that under an <code>@batch</code> slices will create <code>view</code>s by default, and that you may also need to start Julia with <code>--check-bounds=yes</code> while debugging.</p>
<p dir="auto">Polyester.jl provides low overhead threading.
The primary API is <code>@batch</code>, which can be used in place of <code>Threads.@threads</code>.
The number of available threads is still governed by <a href="https://docs.julialang.org/en/v1.6/manual/multi-threading/#Starting-Julia-with-multiple-threads" rel="nofollow"><code>--threads</code> or <code>JULIA_NUM_THREADS</code></a>, as reported by <code>Threads.nthreads()</code>.
Lets look at a simple benchmark.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Polyester, LinearAlgebra, BenchmarkHistograms
# Single threaded.
function axpy_serial!(y, a, x)
    @inbounds for i in eachindex(y,x)
        y[i] = muladd(a, x[i], y[i])
    end
end
# One thread per core, the default (the threads are not pinned)
function axpy_per_core!(y, a, x)
    @batch per=core for i in eachindex(y,x)
        y[i] = muladd(a, x[i], y[i])
    end
end
# One thread per thread
function axpy_per_thread!(y, a, x)
    @batch per=thread for i in eachindex(y,x)
        y[i] = muladd(a, x[i], y[i])
    end
end
# Set a minimum batch size of `200`
function axpy_minbatch!(y, a, x)
    @batch minbatch=2000 for i in eachindex(y,x)
        y[i] = muladd(a, x[i], y[i])
    end
end
# benchmark against `Threads.@threads`
function axpy_atthread!(y, a, x)
    Threads.@threads for i in eachindex(y,x)
        @inbounds y[i] = muladd(a, x[i], y[i])
    end
end

y = rand(10_000);
x = rand(10_000);
@benchmark axpy_serial!($y, eps(), $x)
@benchmark axpy!(eps(), $x, $y)
@benchmark axpy_atthread!($y, eps(), $x)
@benchmark axpy_per_core!($y, eps(), $x)
@benchmark axpy_per_thread!($y, eps(), $x)
@benchmark axpy_minbatch!($y, eps(), $x)
versioninfo()"><pre><span class="pl-k">using</span> Polyester, LinearAlgebra, BenchmarkHistograms
<span class="pl-c"><span class="pl-c">#</span> Single threaded.</span>
<span class="pl-k">function</span> <span class="pl-en">axpy_serial!</span>(y, a, x)
    <span class="pl-c1">@inbounds</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(y,x)
        y[i] <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(a, x[i], y[i])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> One thread per core, the default (the threads are not pinned)</span>
<span class="pl-k">function</span> <span class="pl-en">axpy_per_core!</span>(y, a, x)
    <span class="pl-c1">@batch</span> per<span class="pl-k">=</span>core <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(y,x)
        y[i] <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(a, x[i], y[i])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> One thread per thread</span>
<span class="pl-k">function</span> <span class="pl-en">axpy_per_thread!</span>(y, a, x)
    <span class="pl-c1">@batch</span> per<span class="pl-k">=</span>thread <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(y,x)
        y[i] <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(a, x[i], y[i])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> Set a minimum batch size of `200`</span>
<span class="pl-k">function</span> <span class="pl-en">axpy_minbatch!</span>(y, a, x)
    <span class="pl-c1">@batch</span> minbatch<span class="pl-k">=</span><span class="pl-c1">2000</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(y,x)
        y[i] <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(a, x[i], y[i])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> benchmark against `Threads.@threads`</span>
<span class="pl-k">function</span> <span class="pl-en">axpy_atthread!</span>(y, a, x)
    Threads<span class="pl-k">.</span><span class="pl-c1">@threads</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(y,x)
        <span class="pl-c1">@inbounds</span> y[i] <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(a, x[i], y[i])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">10_000</span>);
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">10_000</span>);
<span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_serial!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
<span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy!</span>(<span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x, <span class="pl-k">$</span>y)
<span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_atthread!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
<span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_per_core!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
<span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_per_thread!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
<span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_minbatch!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
<span class="pl-c1">versioninfo</span>()</pre></div>
<p dir="auto">With only <code>10_000</code> elements, this simply <code>muladd</code> loop can't afford the overhead of threads like <code>BLAS</code> or <code>Threads.@threads</code>,
they just slow the computations down. But these 10_000 elements can afford <code>Polyester</code>, giving up to a &gt;2x speedup on 4 cores.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; @benchmark axpy_serial!($y, eps(), $x)
samples: 10000; evals/sample: 10; memory estimate: 0 bytes; allocs estimate: 0
ns

 (1160.0 - 1240.0]  ██████████████████████████████ 9573
 (1240.0 - 1320.0]  █306
 (1320.0 - 1390.0]  ▎53
 (1390.0 - 1470.0]  ▏25
 (1470.0 - 1550.0]   0
 (1550.0 - 1620.0]   0
 (1620.0 - 1700.0]   0
 (1700.0 - 1780.0]   0
 (1780.0 - 1860.0]   0
 (1860.0 - 1930.0]   0
 (1930.0 - 2010.0]   0
 (2010.0 - 2090.0]   0
 (2090.0 - 2160.0]   0
 (2160.0 - 2240.0]  ▏32
 (2240.0 - 3230.0]  ▏11

                  Counts

min: 1.161 μs (0.00% GC); mean: 1.182 μs (0.00% GC); median: 1.169 μs (0.00% GC); max: 3.226 μs (0.00% GC).

julia&gt; @benchmark axpy!(eps(), $x, $y)
samples: 10000; evals/sample: 9; memory estimate: 0 bytes; allocs estimate: 0
ns

 (2030.0 - 2160.0]  ██████████████████████████████ 9415
 (2160.0 - 2300.0]  █▋497
 (2300.0 - 2430.0]  ▎49
 (2430.0 - 2570.0]  ▏5
 (2570.0 - 2700.0]   0
 (2700.0 - 2840.0]  ▏1
 (2840.0 - 2970.0]   0
 (2970.0 - 3110.0]   0
 (3110.0 - 3240.0]  ▏1
 (3240.0 - 3370.0]   0
 (3370.0 - 3510.0]   0
 (3510.0 - 3640.0]  ▏1
 (3640.0 - 3780.0]   0
 (3780.0 - 3910.0]  ▏21
 (3910.0 - 4880.0]  ▏10

                  Counts

min: 2.030 μs (0.00% GC); mean: 2.060 μs (0.00% GC); median: 2.039 μs (0.00% GC); max: 4.881 μs (0.00% GC).

julia&gt; @benchmark axpy_atthread!($y, eps(), $x)
samples: 10000; evals/sample: 7; memory estimate: 3.66 KiB; allocs estimate: 41
ns

 (3700.0  - 4600.0  ]  ██████████████████████████████▏7393
 (4600.0  - 5500.0  ]  ███▌852
 (5500.0  - 6400.0  ]  ██████▍1556
 (6400.0  - 7300.0  ]  ▊175
 (7300.0  - 8200.0  ]  ▏7
 (8200.0  - 9100.0  ]  ▏3
 (9100.0  - 10000.0 ]   0
 (10000.0 - 10900.0 ]   0
 (10900.0 - 11800.0 ]  ▏1
 (11800.0 - 12800.0 ]   0
 (12800.0 - 13700.0 ]  ▏1
 (13700.0 - 14600.0 ]   0
 (14600.0 - 15500.0 ]   0
 (15500.0 - 16400.0 ]  ▏1
 (16400.0 - 880700.0]  ▏11

                  Counts

min: 3.662 μs (0.00% GC); mean: 4.909 μs (6.36% GC); median: 4.226 μs (0.00% GC); max: 880.721 μs (93.63% GC).

julia&gt; @benchmark axpy_per_core!($y, eps(), $x)
samples: 10000; evals/sample: 194; memory estimate: 0 bytes; allocs estimate: 0
ns

 (496.0 - 504.0 ]  ██████████████████████████████ 5969
 (504.0 - 513.0 ]  ██████████████████3564
 (513.0 - 522.0 ]  ██▏420
 (522.0 - 531.0 ]  ▏9
 (531.0 - 539.0 ]  ▏4
 (539.0 - 548.0 ]  ▏1
 (548.0 - 557.0 ]  ▏7
 (557.0 - 565.0 ]  ▏3
 (565.0 - 574.0 ]  ▏2
 (574.0 - 583.0 ]   0
 (583.0 - 591.0 ]  ▏1
 (591.0 - 600.0 ]  ▏4
 (600.0 - 609.0 ]  ▏3
 (609.0 - 617.0 ]  ▏2
 (617.0 - 1181.0]  ▏11

                  Counts

min: 495.758 ns (0.00% GC); mean: 505.037 ns (0.00% GC); median: 503.884 ns (0.00% GC); max: 1.181 μs (0.00% GC).

julia&gt; @benchmark axpy_per_thread!($y, eps(), $x)
samples: 10000; evals/sample: 181; memory estimate: 0 bytes; allocs estimate: 0
ns

 (583.0 - 611.0 ]  ██████████████████████████████ 8489
 (611.0 - 640.0 ]  █████▎1453
 (640.0 - 669.0 ]  ▏21
 (669.0 - 697.0 ]  ▏12
 (697.0 - 726.0 ]  ▏5
 (726.0 - 755.0 ]  ▏2
 (755.0 - 783.0 ]  ▏2
 (783.0 - 812.0 ]  ▏1
 (812.0 - 841.0 ]   0
 (841.0 - 869.0 ]   0
 (869.0 - 898.0 ]  ▏1
 (898.0 - 927.0 ]   0
 (927.0 - 955.0 ]   0
 (955.0 - 984.0 ]  ▏3
 (984.0 - 9088.0]  ▏11

                  Counts

min: 582.608 ns (0.00% GC); mean: 609.063 ns (0.00% GC); median: 606.028 ns (0.00% GC); max: 9.088 μs (0.00% GC).

julia&gt; @benchmark axpy_minbatch!($y, eps(), $x)
samples: 10000; evals/sample: 195; memory estimate: 0 bytes; allocs estimate: 0
ns

 (484.0 - 514.0 ]  ██████████████████████████████9874
 (514.0 - 544.0 ]  ▎43
 (544.0 - 574.0 ]  ▏24
 (574.0 - 604.0 ]  ▏18
 (604.0 - 634.0 ]  ▏13
 (634.0 - 664.0 ]  ▏2
 (664.0 - 694.0 ]  ▏1
 (694.0 - 724.0 ]  ▏1
 (724.0 - 754.0 ]   0
 (754.0 - 784.0 ]  ▏8
 (784.0 - 814.0 ]   0
 (814.0 - 844.0 ]   0
 (844.0 - 874.0 ]  ▏2
 (874.0 - 904.0 ]  ▏3
 (904.0 - 3364.0]  ▏11

                  Counts

min: 484.082 ns (0.00% GC); mean: 502.104 ns (0.00% GC); median: 499.708 ns (0.00% GC); max: 3.364 μs (0.00% GC).

julia&gt; versioninfo()
Julia Version 1.7.0-DEV.1150
Commit a08a3ff1f9* (2021-05-22 21:10 UTC)
Platform Info:
  OS: Linux (x86_64-redhat-linux)
  CPU: 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-12.0.0 (ORCJIT, tigerlake)
Environment:
  JULIA_NUM_THREADS = 8"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_serial!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">10</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">1160.0</span> <span class="pl-k">-</span> <span class="pl-c1">1240.0</span>]  ██████████████████████████████ <span class="pl-c1">9573</span>
 (<span class="pl-c1">1240.0</span> <span class="pl-k">-</span> <span class="pl-c1">1320.0</span>]  █306
 (<span class="pl-c1">1320.0</span> <span class="pl-k">-</span> <span class="pl-c1">1390.0</span>]  ▎53
 (<span class="pl-c1">1390.0</span> <span class="pl-k">-</span> <span class="pl-c1">1470.0</span>]  ▏25
 (<span class="pl-c1">1470.0</span> <span class="pl-k">-</span> <span class="pl-c1">1550.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">1550.0</span> <span class="pl-k">-</span> <span class="pl-c1">1620.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">1620.0</span> <span class="pl-k">-</span> <span class="pl-c1">1700.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">1700.0</span> <span class="pl-k">-</span> <span class="pl-c1">1780.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">1780.0</span> <span class="pl-k">-</span> <span class="pl-c1">1860.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">1860.0</span> <span class="pl-k">-</span> <span class="pl-c1">1930.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">1930.0</span> <span class="pl-k">-</span> <span class="pl-c1">2010.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">2010.0</span> <span class="pl-k">-</span> <span class="pl-c1">2090.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">2090.0</span> <span class="pl-k">-</span> <span class="pl-c1">2160.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">2160.0</span> <span class="pl-k">-</span> <span class="pl-c1">2240.0</span>]  ▏32
 (<span class="pl-c1">2240.0</span> <span class="pl-k">-</span> <span class="pl-c1">3230.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">1.161</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">1.182</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">1.169</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">3.226</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy!</span>(<span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x, <span class="pl-k">$</span>y)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">9</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">2030.0</span> <span class="pl-k">-</span> <span class="pl-c1">2160.0</span>]  ██████████████████████████████ <span class="pl-c1">9415</span>
 (<span class="pl-c1">2160.0</span> <span class="pl-k">-</span> <span class="pl-c1">2300.0</span>]  █▋497
 (<span class="pl-c1">2300.0</span> <span class="pl-k">-</span> <span class="pl-c1">2430.0</span>]  ▎49
 (<span class="pl-c1">2430.0</span> <span class="pl-k">-</span> <span class="pl-c1">2570.0</span>]  ▏5
 (<span class="pl-c1">2570.0</span> <span class="pl-k">-</span> <span class="pl-c1">2700.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">2700.0</span> <span class="pl-k">-</span> <span class="pl-c1">2840.0</span>]  ▏1
 (<span class="pl-c1">2840.0</span> <span class="pl-k">-</span> <span class="pl-c1">2970.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">2970.0</span> <span class="pl-k">-</span> <span class="pl-c1">3110.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">3110.0</span> <span class="pl-k">-</span> <span class="pl-c1">3240.0</span>]  ▏1
 (<span class="pl-c1">3240.0</span> <span class="pl-k">-</span> <span class="pl-c1">3370.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">3370.0</span> <span class="pl-k">-</span> <span class="pl-c1">3510.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">3510.0</span> <span class="pl-k">-</span> <span class="pl-c1">3640.0</span>]  ▏1
 (<span class="pl-c1">3640.0</span> <span class="pl-k">-</span> <span class="pl-c1">3780.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">3780.0</span> <span class="pl-k">-</span> <span class="pl-c1">3910.0</span>]  ▏21
 (<span class="pl-c1">3910.0</span> <span class="pl-k">-</span> <span class="pl-c1">4880.0</span>]  ▏10

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">2.030</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">2.060</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">2.039</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">4.881</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_atthread!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">7</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">3.66</span> KiB; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">41</span>
ns

 (<span class="pl-c1">3700.0</span>  <span class="pl-k">-</span> <span class="pl-c1">4600.0</span>  ]  ██████████████████████████████▏7393
 (<span class="pl-c1">4600.0</span>  <span class="pl-k">-</span> <span class="pl-c1">5500.0</span>  ]  ███▌852
 (<span class="pl-c1">5500.0</span>  <span class="pl-k">-</span> <span class="pl-c1">6400.0</span>  ]  ██████▍1556
 (<span class="pl-c1">6400.0</span>  <span class="pl-k">-</span> <span class="pl-c1">7300.0</span>  ]  ▊175
 (<span class="pl-c1">7300.0</span>  <span class="pl-k">-</span> <span class="pl-c1">8200.0</span>  ]  ▏7
 (<span class="pl-c1">8200.0</span>  <span class="pl-k">-</span> <span class="pl-c1">9100.0</span>  ]  ▏3
 (<span class="pl-c1">9100.0</span>  <span class="pl-k">-</span> <span class="pl-c1">10000.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">10000.0</span> <span class="pl-k">-</span> <span class="pl-c1">10900.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">10900.0</span> <span class="pl-k">-</span> <span class="pl-c1">11800.0</span> ]  ▏1
 (<span class="pl-c1">11800.0</span> <span class="pl-k">-</span> <span class="pl-c1">12800.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">12800.0</span> <span class="pl-k">-</span> <span class="pl-c1">13700.0</span> ]  ▏1
 (<span class="pl-c1">13700.0</span> <span class="pl-k">-</span> <span class="pl-c1">14600.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">14600.0</span> <span class="pl-k">-</span> <span class="pl-c1">15500.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">15500.0</span> <span class="pl-k">-</span> <span class="pl-c1">16400.0</span> ]  ▏1
 (<span class="pl-c1">16400.0</span> <span class="pl-k">-</span> <span class="pl-c1">880700.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">3.662</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">4.909</span> μs (<span class="pl-c1">6.36</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">4.226</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">880.721</span> μs (<span class="pl-c1">93.63</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_per_core!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">194</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">496.0</span> <span class="pl-k">-</span> <span class="pl-c1">504.0</span> ]  ██████████████████████████████ <span class="pl-c1">5969</span>
 (<span class="pl-c1">504.0</span> <span class="pl-k">-</span> <span class="pl-c1">513.0</span> ]  ██████████████████3564
 (<span class="pl-c1">513.0</span> <span class="pl-k">-</span> <span class="pl-c1">522.0</span> ]  ██▏420
 (<span class="pl-c1">522.0</span> <span class="pl-k">-</span> <span class="pl-c1">531.0</span> ]  ▏9
 (<span class="pl-c1">531.0</span> <span class="pl-k">-</span> <span class="pl-c1">539.0</span> ]  ▏4
 (<span class="pl-c1">539.0</span> <span class="pl-k">-</span> <span class="pl-c1">548.0</span> ]  ▏1
 (<span class="pl-c1">548.0</span> <span class="pl-k">-</span> <span class="pl-c1">557.0</span> ]  ▏7
 (<span class="pl-c1">557.0</span> <span class="pl-k">-</span> <span class="pl-c1">565.0</span> ]  ▏3
 (<span class="pl-c1">565.0</span> <span class="pl-k">-</span> <span class="pl-c1">574.0</span> ]  ▏2
 (<span class="pl-c1">574.0</span> <span class="pl-k">-</span> <span class="pl-c1">583.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">583.0</span> <span class="pl-k">-</span> <span class="pl-c1">591.0</span> ]  ▏1
 (<span class="pl-c1">591.0</span> <span class="pl-k">-</span> <span class="pl-c1">600.0</span> ]  ▏4
 (<span class="pl-c1">600.0</span> <span class="pl-k">-</span> <span class="pl-c1">609.0</span> ]  ▏3
 (<span class="pl-c1">609.0</span> <span class="pl-k">-</span> <span class="pl-c1">617.0</span> ]  ▏2
 (<span class="pl-c1">617.0</span> <span class="pl-k">-</span> <span class="pl-c1">1181.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">495.758</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">505.037</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">503.884</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">1.181</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_per_thread!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">181</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">583.0</span> <span class="pl-k">-</span> <span class="pl-c1">611.0</span> ]  ██████████████████████████████ <span class="pl-c1">8489</span>
 (<span class="pl-c1">611.0</span> <span class="pl-k">-</span> <span class="pl-c1">640.0</span> ]  █████▎1453
 (<span class="pl-c1">640.0</span> <span class="pl-k">-</span> <span class="pl-c1">669.0</span> ]  ▏21
 (<span class="pl-c1">669.0</span> <span class="pl-k">-</span> <span class="pl-c1">697.0</span> ]  ▏12
 (<span class="pl-c1">697.0</span> <span class="pl-k">-</span> <span class="pl-c1">726.0</span> ]  ▏5
 (<span class="pl-c1">726.0</span> <span class="pl-k">-</span> <span class="pl-c1">755.0</span> ]  ▏2
 (<span class="pl-c1">755.0</span> <span class="pl-k">-</span> <span class="pl-c1">783.0</span> ]  ▏2
 (<span class="pl-c1">783.0</span> <span class="pl-k">-</span> <span class="pl-c1">812.0</span> ]  ▏1
 (<span class="pl-c1">812.0</span> <span class="pl-k">-</span> <span class="pl-c1">841.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">841.0</span> <span class="pl-k">-</span> <span class="pl-c1">869.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">869.0</span> <span class="pl-k">-</span> <span class="pl-c1">898.0</span> ]  ▏1
 (<span class="pl-c1">898.0</span> <span class="pl-k">-</span> <span class="pl-c1">927.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">927.0</span> <span class="pl-k">-</span> <span class="pl-c1">955.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">955.0</span> <span class="pl-k">-</span> <span class="pl-c1">984.0</span> ]  ▏3
 (<span class="pl-c1">984.0</span> <span class="pl-k">-</span> <span class="pl-c1">9088.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">582.608</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">609.063</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">606.028</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">9.088</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_minbatch!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">195</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">484.0</span> <span class="pl-k">-</span> <span class="pl-c1">514.0</span> ]  ██████████████████████████████9874
 (<span class="pl-c1">514.0</span> <span class="pl-k">-</span> <span class="pl-c1">544.0</span> ]  ▎43
 (<span class="pl-c1">544.0</span> <span class="pl-k">-</span> <span class="pl-c1">574.0</span> ]  ▏24
 (<span class="pl-c1">574.0</span> <span class="pl-k">-</span> <span class="pl-c1">604.0</span> ]  ▏18
 (<span class="pl-c1">604.0</span> <span class="pl-k">-</span> <span class="pl-c1">634.0</span> ]  ▏13
 (<span class="pl-c1">634.0</span> <span class="pl-k">-</span> <span class="pl-c1">664.0</span> ]  ▏2
 (<span class="pl-c1">664.0</span> <span class="pl-k">-</span> <span class="pl-c1">694.0</span> ]  ▏1
 (<span class="pl-c1">694.0</span> <span class="pl-k">-</span> <span class="pl-c1">724.0</span> ]  ▏1
 (<span class="pl-c1">724.0</span> <span class="pl-k">-</span> <span class="pl-c1">754.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">754.0</span> <span class="pl-k">-</span> <span class="pl-c1">784.0</span> ]  ▏8
 (<span class="pl-c1">784.0</span> <span class="pl-k">-</span> <span class="pl-c1">814.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">814.0</span> <span class="pl-k">-</span> <span class="pl-c1">844.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">844.0</span> <span class="pl-k">-</span> <span class="pl-c1">874.0</span> ]  ▏2
 (<span class="pl-c1">874.0</span> <span class="pl-k">-</span> <span class="pl-c1">904.0</span> ]  ▏3
 (<span class="pl-c1">904.0</span> <span class="pl-k">-</span> <span class="pl-c1">3364.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">484.082</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">502.104</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">499.708</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">3.364</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">versioninfo</span>()
Julia Version <span class="pl-c1">1.7</span>.<span class="pl-c1">0</span><span class="pl-k">-</span>DEV.<span class="pl-c1">1150</span>
Commit a08a3ff1f9<span class="pl-k">*</span> (<span class="pl-c1">2021</span><span class="pl-k">-</span><span class="pl-c1">05</span><span class="pl-k">-</span><span class="pl-c1">22</span> <span class="pl-c1">21</span><span class="pl-k">:</span><span class="pl-c1">10</span> UTC)
Platform Info<span class="pl-k">:</span>
  OS<span class="pl-k">:</span> Linux (x86_64<span class="pl-k">-</span>redhat<span class="pl-k">-</span>linux)
  CPU<span class="pl-k">:</span> <span class="pl-c1">11</span>th Gen <span class="pl-c1">Intel</span>(R) <span class="pl-c1">Core</span>(TM) i7<span class="pl-k">-</span><span class="pl-c1">1165</span>G7 @ <span class="pl-c1">2.80</span>GHz
  WORD_SIZE<span class="pl-k">:</span> <span class="pl-c1">64</span>
  LIBM<span class="pl-k">:</span> libopenlibm
  LLVM<span class="pl-k">:</span> libLLVM<span class="pl-k">-</span><span class="pl-c1">12.0</span>.<span class="pl-c1">0</span> (ORCJIT, tigerlake)
Environment<span class="pl-k">:</span>
  JULIA_NUM_THREADS <span class="pl-k">=</span> <span class="pl-c1">8</span></pre></div>
<p dir="auto">The <code>minbatch</code> argument lets us choose a minimum number of iterations per thread. That is, <code>minbatch=n</code> means it'll use at most
<code>number_loop_iterations ÷ n</code> threads. Setting <code>minbatch=2000</code> like we did above means that with only 4000 iterations, <code>@batch</code>
will use just 2 threads; with 3999 iterations, it'll only only 1.
This lets us control the pace with which it ramps up threads. By using only 2 threads with 4000 iterations, it is still much faster
than the serial version, while using 4 threads (<code>per=core</code>) it is only slightly faster, and the full 8 (<code>per=thread</code>) matches serial.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; x = rand(4_000); y = rand(4_000);

julia&gt; @benchmark axpy_serial!($y, eps(), $x)
samples: 10000; evals/sample: 196; memory estimate: 0 bytes; allocs estimate: 0
ns

 (477.0 - 484.0]  ██████1379
 (484.0 - 491.0]  ██████████████████████████████ 6931
 (491.0 - 499.0]  ████▍1004
 (499.0 - 506.0]  ▍71
 (506.0 - 513.0]  ▏28
 (513.0 - 520.0]  ▎47
 (520.0 - 528.0]  ▎45
 (528.0 - 535.0]  ▏20
 (535.0 - 542.0]  ██443
 (542.0 - 549.0]  ▏15
 (549.0 - 557.0]  ▏3
 (557.0 - 564.0]   0
 (564.0 - 571.0]   0
 (571.0 - 578.0]  ▏3
 (578.0 - 858.0]  ▏11

                  Counts

min: 476.867 ns (0.00% GC); mean: 490.402 ns (0.00% GC); median: 488.444 ns (0.00% GC); max: 858.056 ns (0.00% GC).

julia&gt; @benchmark axpy_minbatch!($y, eps(), $x)
samples: 10000; evals/sample: 276; memory estimate: 0 bytes; allocs estimate: 0
ns

 (287.0 - 297.0]  ██████████████████▌2510
 (297.0 - 306.0]  ██████████████████████████████ 4088
 (306.0 - 316.0]  ███████████████████████▋3205
 (316.0 - 325.0]  █▎158
 (325.0 - 335.0]  ▎24
 (335.0 - 344.0]   0
 (344.0 - 354.0]  ▏1
 (354.0 - 364.0]   0
 (364.0 - 373.0]   0
 (373.0 - 383.0]   0
 (383.0 - 392.0]   0
 (392.0 - 402.0]   0
 (402.0 - 411.0]  ▏1
 (411.0 - 421.0]  ▏2
 (421.0 - 689.0]  ▏11

                  Counts

min: 286.938 ns (0.00% GC); mean: 302.339 ns (0.00% GC); median: 299.721 ns (0.00% GC); max: 689.467 ns (0.00% GC).

julia&gt; @benchmark axpy_per_core!($y, eps(), $x)
samples: 10000; evals/sample: 213; memory estimate: 0 bytes; allocs estimate: 0
ns

 (344.0 - 351.0]  █▋325
 (351.0 - 359.0]  ██████████████████████████████ 6026
 (359.0 - 366.0]  ████████████████▏3229
 (366.0 - 373.0]  █▋321
 (373.0 - 381.0]  ▍55
 (381.0 - 388.0]  ▏12
 (388.0 - 396.0]  ▏7
 (396.0 - 403.0]  ▏6
 (403.0 - 410.0]  ▏1
 (410.0 - 418.0]   0
 (418.0 - 425.0]   0
 (425.0 - 433.0]  ▏1
 (433.0 - 440.0]  ▏5
 (440.0 - 447.0]  ▏1
 (447.0 - 795.0]  ▏11

                  Counts

min: 343.770 ns (0.00% GC); mean: 357.972 ns (0.00% GC); median: 357.270 ns (0.00% GC); max: 794.709 ns (0.00% GC).

julia&gt; @benchmark axpy_per_thread!($y, eps(), $x)
samples: 10000; evals/sample: 195; memory estimate: 0 bytes; allocs estimate: 0
ns

 (476.0 - 487.0 ]  ██████████████████████████████▏7273
 (487.0 - 499.0 ]  ██████████▉2625
 (499.0 - 510.0 ]  ▎48
 (510.0 - 522.0 ]  ▏15
 (522.0 - 533.0 ]  ▏6
 (533.0 - 545.0 ]  ▏2
 (545.0 - 557.0 ]  ▏5
 (557.0 - 568.0 ]  ▏5
 (568.0 - 580.0 ]  ▏3
 (580.0 - 591.0 ]  ▏2
 (591.0 - 603.0 ]   0
 (603.0 - 614.0 ]   0
 (614.0 - 626.0 ]  ▏3
 (626.0 - 638.0 ]  ▏2
 (638.0 - 2489.0]  ▏11

                  Counts

min: 475.564 ns (0.00% GC); mean: 486.650 ns (0.00% GC); median: 485.287 ns (0.00% GC); max: 2.489 μs (0.00% GC)."><pre>julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4_000</span>); y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4_000</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_serial!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">196</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">477.0</span> <span class="pl-k">-</span> <span class="pl-c1">484.0</span>]  ██████1379
 (<span class="pl-c1">484.0</span> <span class="pl-k">-</span> <span class="pl-c1">491.0</span>]  ██████████████████████████████ <span class="pl-c1">6931</span>
 (<span class="pl-c1">491.0</span> <span class="pl-k">-</span> <span class="pl-c1">499.0</span>]  ████▍1004
 (<span class="pl-c1">499.0</span> <span class="pl-k">-</span> <span class="pl-c1">506.0</span>]  ▍71
 (<span class="pl-c1">506.0</span> <span class="pl-k">-</span> <span class="pl-c1">513.0</span>]  ▏28
 (<span class="pl-c1">513.0</span> <span class="pl-k">-</span> <span class="pl-c1">520.0</span>]  ▎47
 (<span class="pl-c1">520.0</span> <span class="pl-k">-</span> <span class="pl-c1">528.0</span>]  ▎45
 (<span class="pl-c1">528.0</span> <span class="pl-k">-</span> <span class="pl-c1">535.0</span>]  ▏20
 (<span class="pl-c1">535.0</span> <span class="pl-k">-</span> <span class="pl-c1">542.0</span>]  ██443
 (<span class="pl-c1">542.0</span> <span class="pl-k">-</span> <span class="pl-c1">549.0</span>]  ▏15
 (<span class="pl-c1">549.0</span> <span class="pl-k">-</span> <span class="pl-c1">557.0</span>]  ▏3
 (<span class="pl-c1">557.0</span> <span class="pl-k">-</span> <span class="pl-c1">564.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">564.0</span> <span class="pl-k">-</span> <span class="pl-c1">571.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">571.0</span> <span class="pl-k">-</span> <span class="pl-c1">578.0</span>]  ▏3
 (<span class="pl-c1">578.0</span> <span class="pl-k">-</span> <span class="pl-c1">858.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">476.867</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">490.402</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">488.444</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">858.056</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_minbatch!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">276</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">287.0</span> <span class="pl-k">-</span> <span class="pl-c1">297.0</span>]  ██████████████████▌2510
 (<span class="pl-c1">297.0</span> <span class="pl-k">-</span> <span class="pl-c1">306.0</span>]  ██████████████████████████████ <span class="pl-c1">4088</span>
 (<span class="pl-c1">306.0</span> <span class="pl-k">-</span> <span class="pl-c1">316.0</span>]  ███████████████████████▋3205
 (<span class="pl-c1">316.0</span> <span class="pl-k">-</span> <span class="pl-c1">325.0</span>]  █▎158
 (<span class="pl-c1">325.0</span> <span class="pl-k">-</span> <span class="pl-c1">335.0</span>]  ▎24
 (<span class="pl-c1">335.0</span> <span class="pl-k">-</span> <span class="pl-c1">344.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">344.0</span> <span class="pl-k">-</span> <span class="pl-c1">354.0</span>]  ▏1
 (<span class="pl-c1">354.0</span> <span class="pl-k">-</span> <span class="pl-c1">364.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">364.0</span> <span class="pl-k">-</span> <span class="pl-c1">373.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">373.0</span> <span class="pl-k">-</span> <span class="pl-c1">383.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">383.0</span> <span class="pl-k">-</span> <span class="pl-c1">392.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">392.0</span> <span class="pl-k">-</span> <span class="pl-c1">402.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">402.0</span> <span class="pl-k">-</span> <span class="pl-c1">411.0</span>]  ▏1
 (<span class="pl-c1">411.0</span> <span class="pl-k">-</span> <span class="pl-c1">421.0</span>]  ▏2
 (<span class="pl-c1">421.0</span> <span class="pl-k">-</span> <span class="pl-c1">689.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">286.938</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">302.339</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">299.721</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">689.467</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_per_core!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">213</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">344.0</span> <span class="pl-k">-</span> <span class="pl-c1">351.0</span>]  █▋325
 (<span class="pl-c1">351.0</span> <span class="pl-k">-</span> <span class="pl-c1">359.0</span>]  ██████████████████████████████ <span class="pl-c1">6026</span>
 (<span class="pl-c1">359.0</span> <span class="pl-k">-</span> <span class="pl-c1">366.0</span>]  ████████████████▏3229
 (<span class="pl-c1">366.0</span> <span class="pl-k">-</span> <span class="pl-c1">373.0</span>]  █▋321
 (<span class="pl-c1">373.0</span> <span class="pl-k">-</span> <span class="pl-c1">381.0</span>]  ▍55
 (<span class="pl-c1">381.0</span> <span class="pl-k">-</span> <span class="pl-c1">388.0</span>]  ▏12
 (<span class="pl-c1">388.0</span> <span class="pl-k">-</span> <span class="pl-c1">396.0</span>]  ▏7
 (<span class="pl-c1">396.0</span> <span class="pl-k">-</span> <span class="pl-c1">403.0</span>]  ▏6
 (<span class="pl-c1">403.0</span> <span class="pl-k">-</span> <span class="pl-c1">410.0</span>]  ▏1
 (<span class="pl-c1">410.0</span> <span class="pl-k">-</span> <span class="pl-c1">418.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">418.0</span> <span class="pl-k">-</span> <span class="pl-c1">425.0</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">425.0</span> <span class="pl-k">-</span> <span class="pl-c1">433.0</span>]  ▏1
 (<span class="pl-c1">433.0</span> <span class="pl-k">-</span> <span class="pl-c1">440.0</span>]  ▏5
 (<span class="pl-c1">440.0</span> <span class="pl-k">-</span> <span class="pl-c1">447.0</span>]  ▏1
 (<span class="pl-c1">447.0</span> <span class="pl-k">-</span> <span class="pl-c1">795.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">343.770</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">357.972</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">357.270</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">794.709</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_per_thread!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">195</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">476.0</span> <span class="pl-k">-</span> <span class="pl-c1">487.0</span> ]  ██████████████████████████████▏7273
 (<span class="pl-c1">487.0</span> <span class="pl-k">-</span> <span class="pl-c1">499.0</span> ]  ██████████▉2625
 (<span class="pl-c1">499.0</span> <span class="pl-k">-</span> <span class="pl-c1">510.0</span> ]  ▎48
 (<span class="pl-c1">510.0</span> <span class="pl-k">-</span> <span class="pl-c1">522.0</span> ]  ▏15
 (<span class="pl-c1">522.0</span> <span class="pl-k">-</span> <span class="pl-c1">533.0</span> ]  ▏6
 (<span class="pl-c1">533.0</span> <span class="pl-k">-</span> <span class="pl-c1">545.0</span> ]  ▏2
 (<span class="pl-c1">545.0</span> <span class="pl-k">-</span> <span class="pl-c1">557.0</span> ]  ▏5
 (<span class="pl-c1">557.0</span> <span class="pl-k">-</span> <span class="pl-c1">568.0</span> ]  ▏5
 (<span class="pl-c1">568.0</span> <span class="pl-k">-</span> <span class="pl-c1">580.0</span> ]  ▏3
 (<span class="pl-c1">580.0</span> <span class="pl-k">-</span> <span class="pl-c1">591.0</span> ]  ▏2
 (<span class="pl-c1">591.0</span> <span class="pl-k">-</span> <span class="pl-c1">603.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">603.0</span> <span class="pl-k">-</span> <span class="pl-c1">614.0</span> ]   <span class="pl-c1">0</span>
 (<span class="pl-c1">614.0</span> <span class="pl-k">-</span> <span class="pl-c1">626.0</span> ]  ▏3
 (<span class="pl-c1">626.0</span> <span class="pl-k">-</span> <span class="pl-c1">638.0</span> ]  ▏2
 (<span class="pl-c1">638.0</span> <span class="pl-k">-</span> <span class="pl-c1">2489.0</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">475.564</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">486.650</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">485.287</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">2.489</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span></pre></div>
<p dir="auto">Seeing that we still see a substantial improvement with 2 threads for vectors of length 4000, we may thus expect to still see
improvement for vectors of length 3000, and could thus try <code>minbatch=1_500</code>. That'd also ensure we're still using just 2 threads
for vectos of length 4000.
However, performance with respect to size tends to have a lot discontinuities.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; function axpy_minbatch_1500!(y, a, x)
           @batch minbatch=1_500 for i in eachindex(y,x)
               y[i] = muladd(a, x[i], y[i])
           end
       end
axpy_minbatch_1500! (generic function with 1 method)

julia&gt; x = rand(3_000); y = rand(3_000);

julia&gt; @benchmark axpy_serial!($y, eps(), $x)
samples: 10000; evals/sample: 839; memory estimate: 0 bytes; allocs estimate: 0
ns

 (145.3 - 151.6]  ██████████████████████████████9289
 (151.6 - 157.9]  ▌133
 (157.9 - 164.3]  █▋484
 (164.3 - 170.6]  ▏14
 (170.6 - 176.9]   0
 (176.9 - 183.3]  ▏2
 (183.3 - 189.6]  ▏9
 (189.6 - 195.9]  ▏6
 (195.9 - 202.2]  ▏6
 (202.2 - 208.6]  ▏5
 (208.6 - 214.9]  ▏4
 (214.9 - 221.2]  ▏9
 (221.2 - 227.6]  ▏14
 (227.6 - 233.9]  ▏14
 (233.9 - 260.2]  ▏11

                  Counts

min: 145.273 ns (0.00% GC); mean: 148.314 ns (0.00% GC); median: 145.881 ns (0.00% GC); max: 260.150 ns (0.00% GC).

julia&gt; @benchmark axpy_minbatch!($y, eps(), $x)
samples: 10000; evals/sample: 807; memory estimate: 0 bytes; allocs estimate: 0
ns

 (148.6 - 153.6]  ██████████████████████████████ 8937
 (153.6 - 158.7]  ██▍674
 (158.7 - 163.8]  █292
 (163.8 - 168.9]  ▎71
 (168.9 - 174.0]  ▏4
 (174.0 - 179.0]  ▏3
 (179.0 - 184.1]   0
 (184.1 - 189.2]   0
 (189.2 - 194.3]   0
 (194.3 - 199.3]   0
 (199.3 - 204.4]   0
 (204.4 - 209.5]  ▏1
 (209.5 - 214.6]   0
 (214.6 - 219.6]  ▏7
 (219.6 - 742.4]  ▏11

                  Counts

min: 148.572 ns (0.00% GC); mean: 152.167 ns (0.00% GC); median: 152.376 ns (0.00% GC); max: 742.447 ns (0.00% GC).

julia&gt; @benchmark axpy_minbatch_1500!($y, eps(), $x)
samples: 10000; evals/sample: 233; memory estimate: 0 bytes; allocs estimate: 0
ns

 (317.7 - 323.9]  ▍43
 (323.9 - 330.2]  ████▉591
 (330.2 - 336.4]  ████████████████████▉2538
 (336.4 - 342.6]  ██████████████████████████████3669
 (342.6 - 348.8]  ██████████████████▉2299
 (348.8 - 355.0]  █████▌667
 (355.0 - 361.2]  █▏129
 (361.2 - 367.4]  ▎21
 (367.4 - 373.6]  ▏13
 (373.6 - 379.8]  ▏5
 (379.8 - 386.0]  ▏2
 (386.0 - 392.2]  ▏2
 (392.2 - 398.4]  ▏5
 (398.4 - 404.6]  ▏5
 (404.6 - 791.4]  ▏11

                  Counts

min: 317.738 ns (0.00% GC); mean: 339.868 ns (0.00% GC); median: 339.279 ns (0.00% GC); max: 791.361 ns (0.00% GC)."><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">axpy_minbatch_1500!</span>(y, a, x)
           <span class="pl-c1">@batch</span> minbatch<span class="pl-k">=</span><span class="pl-c1">1_500</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(y,x)
               y[i] <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(a, x[i], y[i])
           <span class="pl-k">end</span>
       <span class="pl-k">end</span>
axpy_minbatch_1500! (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3_000</span>); y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3_000</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_serial!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">839</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">145.3</span> <span class="pl-k">-</span> <span class="pl-c1">151.6</span>]  ██████████████████████████████9289
 (<span class="pl-c1">151.6</span> <span class="pl-k">-</span> <span class="pl-c1">157.9</span>]  ▌133
 (<span class="pl-c1">157.9</span> <span class="pl-k">-</span> <span class="pl-c1">164.3</span>]  █▋484
 (<span class="pl-c1">164.3</span> <span class="pl-k">-</span> <span class="pl-c1">170.6</span>]  ▏14
 (<span class="pl-c1">170.6</span> <span class="pl-k">-</span> <span class="pl-c1">176.9</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">176.9</span> <span class="pl-k">-</span> <span class="pl-c1">183.3</span>]  ▏2
 (<span class="pl-c1">183.3</span> <span class="pl-k">-</span> <span class="pl-c1">189.6</span>]  ▏9
 (<span class="pl-c1">189.6</span> <span class="pl-k">-</span> <span class="pl-c1">195.9</span>]  ▏6
 (<span class="pl-c1">195.9</span> <span class="pl-k">-</span> <span class="pl-c1">202.2</span>]  ▏6
 (<span class="pl-c1">202.2</span> <span class="pl-k">-</span> <span class="pl-c1">208.6</span>]  ▏5
 (<span class="pl-c1">208.6</span> <span class="pl-k">-</span> <span class="pl-c1">214.9</span>]  ▏4
 (<span class="pl-c1">214.9</span> <span class="pl-k">-</span> <span class="pl-c1">221.2</span>]  ▏9
 (<span class="pl-c1">221.2</span> <span class="pl-k">-</span> <span class="pl-c1">227.6</span>]  ▏14
 (<span class="pl-c1">227.6</span> <span class="pl-k">-</span> <span class="pl-c1">233.9</span>]  ▏14
 (<span class="pl-c1">233.9</span> <span class="pl-k">-</span> <span class="pl-c1">260.2</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">145.273</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">148.314</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">145.881</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">260.150</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_minbatch!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">807</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">148.6</span> <span class="pl-k">-</span> <span class="pl-c1">153.6</span>]  ██████████████████████████████ <span class="pl-c1">8937</span>
 (<span class="pl-c1">153.6</span> <span class="pl-k">-</span> <span class="pl-c1">158.7</span>]  ██▍674
 (<span class="pl-c1">158.7</span> <span class="pl-k">-</span> <span class="pl-c1">163.8</span>]  █292
 (<span class="pl-c1">163.8</span> <span class="pl-k">-</span> <span class="pl-c1">168.9</span>]  ▎71
 (<span class="pl-c1">168.9</span> <span class="pl-k">-</span> <span class="pl-c1">174.0</span>]  ▏4
 (<span class="pl-c1">174.0</span> <span class="pl-k">-</span> <span class="pl-c1">179.0</span>]  ▏3
 (<span class="pl-c1">179.0</span> <span class="pl-k">-</span> <span class="pl-c1">184.1</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">184.1</span> <span class="pl-k">-</span> <span class="pl-c1">189.2</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">189.2</span> <span class="pl-k">-</span> <span class="pl-c1">194.3</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">194.3</span> <span class="pl-k">-</span> <span class="pl-c1">199.3</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">199.3</span> <span class="pl-k">-</span> <span class="pl-c1">204.4</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">204.4</span> <span class="pl-k">-</span> <span class="pl-c1">209.5</span>]  ▏1
 (<span class="pl-c1">209.5</span> <span class="pl-k">-</span> <span class="pl-c1">214.6</span>]   <span class="pl-c1">0</span>
 (<span class="pl-c1">214.6</span> <span class="pl-k">-</span> <span class="pl-c1">219.6</span>]  ▏7
 (<span class="pl-c1">219.6</span> <span class="pl-k">-</span> <span class="pl-c1">742.4</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">148.572</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">152.167</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">152.376</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">742.447</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">axpy_minbatch_1500!</span>(<span class="pl-k">$</span>y, <span class="pl-c1">eps</span>(), <span class="pl-k">$</span>x)
samples<span class="pl-k">:</span> <span class="pl-c1">10000</span>; evals<span class="pl-k">/</span>sample<span class="pl-k">:</span> <span class="pl-c1">233</span>; memory estimate<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes; allocs estimate<span class="pl-k">:</span> <span class="pl-c1">0</span>
ns

 (<span class="pl-c1">317.7</span> <span class="pl-k">-</span> <span class="pl-c1">323.9</span>]  ▍43
 (<span class="pl-c1">323.9</span> <span class="pl-k">-</span> <span class="pl-c1">330.2</span>]  ████▉591
 (<span class="pl-c1">330.2</span> <span class="pl-k">-</span> <span class="pl-c1">336.4</span>]  ████████████████████▉2538
 (<span class="pl-c1">336.4</span> <span class="pl-k">-</span> <span class="pl-c1">342.6</span>]  ██████████████████████████████3669
 (<span class="pl-c1">342.6</span> <span class="pl-k">-</span> <span class="pl-c1">348.8</span>]  ██████████████████▉2299
 (<span class="pl-c1">348.8</span> <span class="pl-k">-</span> <span class="pl-c1">355.0</span>]  █████▌667
 (<span class="pl-c1">355.0</span> <span class="pl-k">-</span> <span class="pl-c1">361.2</span>]  █▏129
 (<span class="pl-c1">361.2</span> <span class="pl-k">-</span> <span class="pl-c1">367.4</span>]  ▎21
 (<span class="pl-c1">367.4</span> <span class="pl-k">-</span> <span class="pl-c1">373.6</span>]  ▏13
 (<span class="pl-c1">373.6</span> <span class="pl-k">-</span> <span class="pl-c1">379.8</span>]  ▏5
 (<span class="pl-c1">379.8</span> <span class="pl-k">-</span> <span class="pl-c1">386.0</span>]  ▏2
 (<span class="pl-c1">386.0</span> <span class="pl-k">-</span> <span class="pl-c1">392.2</span>]  ▏2
 (<span class="pl-c1">392.2</span> <span class="pl-k">-</span> <span class="pl-c1">398.4</span>]  ▏5
 (<span class="pl-c1">398.4</span> <span class="pl-k">-</span> <span class="pl-c1">404.6</span>]  ▏5
 (<span class="pl-c1">404.6</span> <span class="pl-k">-</span> <span class="pl-c1">791.4</span>]  ▏11

                  Counts

min<span class="pl-k">:</span> <span class="pl-c1">317.738</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); mean<span class="pl-k">:</span> <span class="pl-c1">339.868</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); median<span class="pl-k">:</span> <span class="pl-c1">339.279</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC); max<span class="pl-k">:</span> <span class="pl-c1">791.361</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)<span class="pl-k">.</span></pre></div>
<p dir="auto">By reducing the length of the vectors by just 1/3 (4000 -&gt; 3000), we saw over a 3.5x speedup in the serial version.
<code>minbatch=2000</code>, by also using only a single thread was able to match its performance. Thus, something around
<code>minbatch=2000</code> seems like the best choice for this particular function on this particular CPU.</p>
<p dir="auto">Note that <code>@batch</code> defaults to using up to one thread per physical core, instead of 1 thread per CPU thread. This
is because <a href="https://github.com/JuliaSIMD/LoopVectorization.jl">LoopVectorization.jl</a> currently only uses up to 1 thread per physical core, and switching the number of
threads incurs some overhead. See the docstring on <code>@batch</code> (i.e., <code>?@batch</code> in a Julia REPL) for some more discussion.</p>
<h2 dir="auto"><a id="user-content-local-per-thread-storage" class="anchor" aria-hidden="true" href="#local-per-thread-storage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Local per-thread storage</h2>
<p dir="auto">You also can define local storage for each thread, providing a vector containing each of the local storages at the end.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; let
           @batch threadlocal=rand(10:99) for i in 0:9
               println(&quot;index $i, thread $(Threads.threadid()), local storage $threadlocal&quot;)
               threadlocal += 1
           end
           println(threadlocal)
       end

index 8, thread 1, local storage 30
index 3, thread 3, local storage 49
index 9, thread 1, local storage 31
index 6, thread 4, local storage 33
index 0, thread 2, local storage 14
index 4, thread 3, local storage 50
index 7, thread 4, local storage 34
index 1, thread 2, local storage 15
index 5, thread 3, local storage 51
index 2, thread 2, local storage 16
Any[32, 17, 52, 35]"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">let</span>
           <span class="pl-c1">@batch</span> threadlocal<span class="pl-k">=</span><span class="pl-c1">rand</span>(<span class="pl-c1">10</span><span class="pl-k">:</span><span class="pl-c1">99</span>) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">9</span>
               <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>index <span class="pl-v">$i</span>, thread <span class="pl-v">$(Threads<span class="pl-k">.</span><span class="pl-c1">threadid</span>())</span>, local storage <span class="pl-v">$threadlocal</span><span class="pl-pds">"</span></span>)
               threadlocal <span class="pl-k">+=</span> <span class="pl-c1">1</span>
           <span class="pl-k">end</span>
           <span class="pl-c1">println</span>(threadlocal)
       <span class="pl-k">end</span>

index <span class="pl-c1">8</span>, thread <span class="pl-c1">1</span>, <span class="pl-k">local</span> storage <span class="pl-c1">30</span>
index <span class="pl-c1">3</span>, thread <span class="pl-c1">3</span>, <span class="pl-k">local</span> storage <span class="pl-c1">49</span>
index <span class="pl-c1">9</span>, thread <span class="pl-c1">1</span>, <span class="pl-k">local</span> storage <span class="pl-c1">31</span>
index <span class="pl-c1">6</span>, thread <span class="pl-c1">4</span>, <span class="pl-k">local</span> storage <span class="pl-c1">33</span>
index <span class="pl-c1">0</span>, thread <span class="pl-c1">2</span>, <span class="pl-k">local</span> storage <span class="pl-c1">14</span>
index <span class="pl-c1">4</span>, thread <span class="pl-c1">3</span>, <span class="pl-k">local</span> storage <span class="pl-c1">50</span>
index <span class="pl-c1">7</span>, thread <span class="pl-c1">4</span>, <span class="pl-k">local</span> storage <span class="pl-c1">34</span>
index <span class="pl-c1">1</span>, thread <span class="pl-c1">2</span>, <span class="pl-k">local</span> storage <span class="pl-c1">15</span>
index <span class="pl-c1">5</span>, thread <span class="pl-c1">3</span>, <span class="pl-k">local</span> storage <span class="pl-c1">51</span>
index <span class="pl-c1">2</span>, thread <span class="pl-c1">2</span>, <span class="pl-k">local</span> storage <span class="pl-c1">16</span>
Any[<span class="pl-c1">32</span>, <span class="pl-c1">17</span>, <span class="pl-c1">52</span>, <span class="pl-c1">35</span>]</pre></div>
<p dir="auto">Optionally, a type can be specified for the thread-local storage:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; let
           @batch threadlocal=rand(10:99)::Float16 for i in 0:9
           end
           println(threadlocal)
       end

Float16[83.0, 90.0, 27.0, 65.0]"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">let</span>
           <span class="pl-c1">@batch</span> threadlocal<span class="pl-k">=</span><span class="pl-c1">rand</span>(<span class="pl-c1">10</span><span class="pl-k">:</span><span class="pl-c1">99</span>)<span class="pl-k">::</span><span class="pl-c1">Float16</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">9</span>
           <span class="pl-k">end</span>
           <span class="pl-c1">println</span>(threadlocal)
       <span class="pl-k">end</span>

Float16[<span class="pl-c1">83.0</span>, <span class="pl-c1">90.0</span>, <span class="pl-c1">27.0</span>, <span class="pl-c1">65.0</span>]</pre></div>
<h2 dir="auto"><a id="user-content-disabling-polyester-threads" class="anchor" aria-hidden="true" href="#disabling-polyester-threads"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Disabling Polyester threads</h2>
<p dir="auto">When running many repetitions of a Polyester-multithreaded function (e.g. in an embarrassingly parallel problem that repeatedly executes a small already Polyester-multithreaded function), it can be beneficial to disable Polyester (the inner multithreaded loop) and multithread only at the outer level (e.g. with <code>Base.Threads</code>). This can be done with the <code>disable_polyester_threads</code> context manager. In the expandable section below you can see examples with benchmarks.</p>
<p dir="auto">It is best to call <code>disable_polyester_threads</code> only once, before any <code>@thread</code> uses happen, to avoid overhead. E.g. best to do it as:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="disable_polyester_threads() do
    @threads for i in 1:n
        f()
    end
end"><pre><span class="pl-c1">disable_polyester_threads</span>() <span class="pl-k">do</span>
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n
        <span class="pl-c1">f</span>()
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p dir="auto">instead of doing it in the following unnecessarily slow manner:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@threads for i in 1:n # DO NOT DO THIS
    disable_polyester_threads() do # IT HAS UNNECESSARY OVERHEAD
        f()
    end
end"><pre><span class="pl-c1">@threads</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n <span class="pl-c"><span class="pl-c">#</span> DO NOT DO THIS</span>
    <span class="pl-c1">disable_polyester_threads</span>() <span class="pl-k">do</span> <span class="pl-c"><span class="pl-c">#</span> IT HAS UNNECESSARY OVERHEAD</span>
        <span class="pl-c1">f</span>()
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<details>
<summary>Benchmarks of nested multi-threading with Polyester</summary>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Big inner problem, repeated only a few times

y = rand(10000000,4);
x = rand(size(y)...);

@btime inner($x,$y,1) # 73.319 ms (0 allocations: 0 bytes)
@btime inner_polyester($x,$y,1) # 8.936 ms (0 allocations: 0 bytes)
@btime inner_thread($x,$y,1) # 11.206 ms (49 allocations: 4.56 KiB)

@btime sequential_sequential($x,$y) # 274.926 ms (0 allocations: 0 bytes)
@btime sequential_polyester($x,$y) # 36.963 ms (0 allocations: 0 bytes)
@btime sequential_thread($x,$y) # 49.373 ms (196 allocations: 18.25 KiB)

@btime threads_of_polyester($x,$y) # 78.828 ms (58 allocations: 4.84 KiB)
# the following is a purposefully suboptimal way to disable threads
@btime threads_of_polyester_inner_disable($x,$y) # 70.182 ms (47 allocations: 4.50 KiB)
# the following is a good way to disable threads (the disable call happening once in the outer scope)
@btime Polyester.disable_polyester_threads() do; threads_of_polyester($x,$y) end; # 71.141 ms (47 allocations: 4.50 KiB)
@btime threads_of_sequential($x,$y) # 70.857 ms (46 allocations: 4.47 KiB)
@btime threads_of_thread($x,$y) # 45.116 ms (219 allocations: 22.00 KiB)

# Small inner problem, repeated many times

y = rand(1000,1000);
x = rand(size(y)...);

@btime inner($x,$y,1) # 7.028 μs (0 allocations: 0 bytes)
@btime inner_polyester($x,$y,1) # 1.917 μs (0 allocations: 0 bytes)
@btime inner_thread($x,$y,1) # 7.544 μs (45 allocations: 4.44 KiB)

@btime sequential_sequential($x,$y) # 6.790 ms (0 allocations: 0 bytes)
@btime sequential_polyester($x,$y) # 2.070 ms (0 allocations: 0 bytes)
@btime sequential_thread($x,$y) # 9.296 ms (49002 allocations: 4.46 MiB)

@btime threads_of_polyester($x,$y) # 2.090 ms (42 allocations: 4.34 KiB)
# the following is a purposefully suboptimal way to disable threads
@btime threads_of_polyester_inner_disable($x,$y) # 1.065 ms (42 allocations: 4.34 KiB)
# the following is a good way to disable threads (the disable call happening once in the outer scope)
@btime Polyester.disable_polyester_threads() do; threads_of_polyester($x,$y) end; # 997.918 μs (49 allocations: 4.56 KiB)
@btime threads_of_sequential($x,$y) # 1.057 ms (48 allocations: 4.53 KiB)
@btime threads_of_thread($x,$y) # 4.105 ms (42059 allocations: 4.25 MiB)

# The tested functions
# All of these would be better implemented by just using @tturbo,
# but these suboptimal implementations serve as good test case for
# Polyster-vs-Base thread scheduling.

function inner(x,y,j)
    for i ∈ axes(x,1)
        y[i,j] = sin(x[i,j])
    end
end

function inner_polyester(x,y,j)
    @batch for i ∈ axes(x,1)
        y[i,j] = sin(x[i,j])
    end
end

function inner_thread(x,y,j)
    @threads for i ∈ axes(x,1)
        y[i,j] = sin(x[i,j])
    end
end

function sequential_sequential(x,y)
    for j ∈ axes(x,2)
        inner(x,y,j)
    end
end

function sequential_polyester(x,y)
    for j ∈ axes(x,2)
        inner_polyester(x,y,j)
    end
end

function sequential_thread(x,y)
    for j ∈ axes(x,2)
        inner_thread(x,y,j)
    end
end

function threads_of_polyester(x,y)
    @threads for j ∈ axes(x,2)
        inner_polyester(x,y,j)
    end
end

function threads_of_polyester_inner_disable(x,y)
    # XXX This is a bad way to disable Polyester threads as
    # it causes unnecessary overhead for each @threads thread.
    # See the benchmarks above for a better way.
    @threads for j ∈ axes(x,2)
        Polyester.disable_polyester_threads() do
            inner_polyester(x,y,j)
        end
    end
end

function threads_of_thread(x,y)
    @threads for j ∈ axes(x,2)
        inner_thread(x,y,j)
    end
end

function threads_of_thread(x,y)
    @threads for j ∈ axes(x,2)
        inner_thread(x,y,j)
    end
end

function threads_of_sequential(x,y)
    @threads for j ∈ axes(x,2)
        inner(x,y,j)
    end
end"><pre><span class="pl-c"><span class="pl-c">#</span> Big inner problem, repeated only a few times</span>

y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">10000000</span>,<span class="pl-c1">4</span>);
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">size</span>(y)<span class="pl-k">...</span>);

<span class="pl-c1">@btime</span> <span class="pl-c1">inner</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 73.319 ms (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">inner_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 8.936 ms (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">inner_thread</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 11.206 ms (49 allocations: 4.56 KiB)</span>

<span class="pl-c1">@btime</span> <span class="pl-c1">sequential_sequential</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 274.926 ms (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">sequential_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 36.963 ms (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">sequential_thread</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 49.373 ms (196 allocations: 18.25 KiB)</span>

<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 78.828 ms (58 allocations: 4.84 KiB)</span>
<span class="pl-c"><span class="pl-c">#</span> the following is a purposefully suboptimal way to disable threads</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_polyester_inner_disable</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 70.182 ms (47 allocations: 4.50 KiB)</span>
<span class="pl-c"><span class="pl-c">#</span> the following is a good way to disable threads (the disable call happening once in the outer scope)</span>
<span class="pl-c1">@btime</span> Polyester<span class="pl-k">.</span><span class="pl-c1">disable_polyester_threads</span>() <span class="pl-k">do</span>; <span class="pl-c1">threads_of_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-k">end</span>; <span class="pl-c"><span class="pl-c">#</span> 71.141 ms (47 allocations: 4.50 KiB)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_sequential</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 70.857 ms (46 allocations: 4.47 KiB)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_thread</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 45.116 ms (219 allocations: 22.00 KiB)</span>

<span class="pl-c"><span class="pl-c">#</span> Small inner problem, repeated many times</span>

y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">1000</span>,<span class="pl-c1">1000</span>);
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">size</span>(y)<span class="pl-k">...</span>);

<span class="pl-c1">@btime</span> <span class="pl-c1">inner</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 7.028 μs (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">inner_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 1.917 μs (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">inner_thread</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 7.544 μs (45 allocations: 4.44 KiB)</span>

<span class="pl-c1">@btime</span> <span class="pl-c1">sequential_sequential</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 6.790 ms (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">sequential_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 2.070 ms (0 allocations: 0 bytes)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">sequential_thread</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 9.296 ms (49002 allocations: 4.46 MiB)</span>

<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 2.090 ms (42 allocations: 4.34 KiB)</span>
<span class="pl-c"><span class="pl-c">#</span> the following is a purposefully suboptimal way to disable threads</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_polyester_inner_disable</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 1.065 ms (42 allocations: 4.34 KiB)</span>
<span class="pl-c"><span class="pl-c">#</span> the following is a good way to disable threads (the disable call happening once in the outer scope)</span>
<span class="pl-c1">@btime</span> Polyester<span class="pl-k">.</span><span class="pl-c1">disable_polyester_threads</span>() <span class="pl-k">do</span>; <span class="pl-c1">threads_of_polyester</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-k">end</span>; <span class="pl-c"><span class="pl-c">#</span> 997.918 μs (49 allocations: 4.56 KiB)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_sequential</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 1.057 ms (48 allocations: 4.53 KiB)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">threads_of_thread</span>(<span class="pl-k">$</span>x,<span class="pl-k">$</span>y) <span class="pl-c"><span class="pl-c">#</span> 4.105 ms (42059 allocations: 4.25 MiB)</span>

<span class="pl-c"><span class="pl-c">#</span> The tested functions</span>
<span class="pl-c"><span class="pl-c">#</span> All of these would be better implemented by just using @tturbo,</span>
<span class="pl-c"><span class="pl-c">#</span> but these suboptimal implementations serve as good test case for</span>
<span class="pl-c"><span class="pl-c">#</span> Polyster-vs-Base thread scheduling.</span>

<span class="pl-k">function</span> <span class="pl-en">inner</span>(x,y,j)
    <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">1</span>)
        y[i,j] <span class="pl-k">=</span> <span class="pl-c1">sin</span>(x[i,j])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">inner_polyester</span>(x,y,j)
    <span class="pl-c1">@batch</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">1</span>)
        y[i,j] <span class="pl-k">=</span> <span class="pl-c1">sin</span>(x[i,j])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">inner_thread</span>(x,y,j)
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">1</span>)
        y[i,j] <span class="pl-k">=</span> <span class="pl-c1">sin</span>(x[i,j])
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">sequential_sequential</span>(x,y)
    <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">sequential_polyester</span>(x,y)
    <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner_polyester</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">sequential_thread</span>(x,y)
    <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner_thread</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">threads_of_polyester</span>(x,y)
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner_polyester</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">threads_of_polyester_inner_disable</span>(x,y)
    <span class="pl-c"><span class="pl-c">#</span> XXX This is a bad way to disable Polyester threads as</span>
    <span class="pl-c"><span class="pl-c">#</span> it causes unnecessary overhead for each @threads thread.</span>
    <span class="pl-c"><span class="pl-c">#</span> See the benchmarks above for a better way.</span>
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        Polyester<span class="pl-k">.</span><span class="pl-c1">disable_polyester_threads</span>() <span class="pl-k">do</span>
            <span class="pl-c1">inner_polyester</span>(x,y,j)
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">threads_of_thread</span>(x,y)
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner_thread</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">threads_of_thread</span>(x,y)
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner_thread</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">threads_of_sequential</span>(x,y)
    <span class="pl-c1">@threads</span> <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(x,<span class="pl-c1">2</span>)
        <span class="pl-c1">inner</span>(x,y,j)
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p dir="auto">Benchmarks executed on:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Julia Version 1.9.0-DEV.998
Commit e1739aa42a1 (2022-07-18 10:27 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: 16 × AMD Ryzen 7 1700 Eight-Core Processor
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-14.0.5 (ORCJIT, znver1)
  Threads: 8 on 16 virtual cores
Environment:
  JULIA_EDITOR = code
  JULIA_NUM_THREADS = 8"><pre class="notranslate"><code>Julia Version 1.9.0-DEV.998
Commit e1739aa42a1 (2022-07-18 10:27 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: 16 × AMD Ryzen 7 1700 Eight-Core Processor
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-14.0.5 (ORCJIT, znver1)
  Threads: 8 on 16 virtual cores
Environment:
  JULIA_EDITOR = code
  JULIA_NUM_THREADS = 8
</code></pre></div>
</details>
</article></div>