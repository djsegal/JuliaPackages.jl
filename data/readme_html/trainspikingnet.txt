<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto">TrainSpikingNet.jl uses recursive least squares to train fluctation-driven
spiking recurrent neural networks to recapitulate arbitrary temporal
activity patterns.  See <a href="https://www.biorxiv.org/content/10.1101/2022.09.26.509578v3.full" rel="nofollow">Arthur, Kim, Chen, Preibisch, and Darshan
(2022)</a>
for further details.</p>
<h1 dir="auto"><a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Requirements</h1>
<p dir="auto">The CPU version of TrainSpikingNet.jl can run on any machine.
To use a GPU you'll need Linux or Windows as the code (currently)
requires CUDA, and Nvidia does not support Macs.</p>
<h1 dir="auto"><a id="user-content-two-user-interfaces" class="anchor" aria-hidden="true" href="#two-user-interfaces"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Two user interfaces</h1>
<p dir="auto">Models can be trained by issuing commands on the Julia REPL.  Alternatively,
there is also a Linux / PowerShell interface that can be more convenient
when batching many jobs to a cluster, or for those not familiar with Julia.
Each are described in the next two sections.</p>
<h1 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h1>
<p dir="auto">Install Julia with <a href="https://github.com/JuliaLang/juliaup">juliaup</a>.</p>
<p dir="auto">Or, manually download Julia from <a href="https://julialang.org/" rel="nofollow">julialang.org</a>.  See
the <a href="https://julialang.org/downloads/platform" rel="nofollow">platform specific instructions</a>.</p>
<h2 dir="auto"><a id="user-content-julia-repl" class="anchor" aria-hidden="true" href="#julia-repl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Julia REPL</h2>
<p dir="auto">Add TrainSpikingNet to your environment and test that everything works:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; ] add TrainSpikingNet
julia&gt; ] test TrainSpikingNet"><pre class="notranslate"><code>julia&gt; ] add TrainSpikingNet
julia&gt; ] test TrainSpikingNet
</code></pre></div>
<p dir="auto">The tests, which are optional, take about an hour, so be patient or just
skip this step.</p>
<p dir="auto">(Get out of Pkg mode by pressing the Delete key.)</p>
<p dir="auto">That's it!</p>
<h2 dir="auto"><a id="user-content-linux--powershell-command-line" class="anchor" aria-hidden="true" href="#linux--powershell-command-line"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Linux / PowerShell command line</h2>
<p dir="auto">Download the TrainSpikingNet.jl repository with either the <a href="https://github.com/SpikingNetwork/TrainSpikingNet.jl/archive/refs/heads/master.zip">ZIP
link</a>
on github.com or by using git-clone:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ git clone --depth 1 https://github.com/SpikingNetwork/TrainSpikingNet.jl.git"><pre class="notranslate"><code>$ git clone --depth 1 https://github.com/SpikingNetwork/TrainSpikingNet.jl.git
</code></pre></div>
<p dir="auto">For convenience, add a new environment variable which contains the full
path to the just downloaded TrainSpikingNet directory.  Like this on Linux:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ echo &quot;export TSN_DIR=$PWD/TrainSpikingNet.jl&quot; &gt;&gt; ~/.bashrc"><pre class="notranslate"><code>$ echo "export TSN_DIR=$PWD/TrainSpikingNet.jl" &gt;&gt; ~/.bashrc
</code></pre></div>
<p dir="auto">Install all of the required packages:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ cd $TSN_DIR
$ julia --project=@.
        -e 'using Pkg;
            Pkg.activate(&quot;.&quot;);
            Pkg.instantiate();
            Pkg.activate(&quot;test&quot;);
            Pkg.instantiate()'"><pre class="notranslate"><code>$ cd $TSN_DIR
$ julia --project=@.
        -e 'using Pkg;
            Pkg.activate(".");
            Pkg.instantiate();
            Pkg.activate("test");
            Pkg.instantiate()'
</code></pre></div>
<p dir="auto">[Note that on Windows the double-quotes above need to be escaped by preceeding
them with a backslash.]</p>
<p dir="auto">Finally, (and optionally) test that everything works:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="cd $TSN_DIR/test
julia --project=@. runtests.jl"><pre class="notranslate"><code>cd $TSN_DIR/test
julia --project=@. runtests.jl
</code></pre></div>
<h1 dir="auto"><a id="user-content-tutorial" class="anchor" aria-hidden="true" href="#tutorial"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tutorial</h1>
<p dir="auto">Here we walk through how to train a default network with 4096 neurons to
learn dummy sinusoidal activity patterns with identical frequencies but
different phases.  As this network is small, and not everyone has a GPU,
we'll just be using the CPU code here.</p>
<h2 dir="auto"><a id="user-content-julia-repl-1" class="anchor" aria-hidden="true" href="#julia-repl-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Julia REPL</h2>
<p dir="auto">First, start Julia with the default number of threads.  You'll need to
do this on the OS command line as clicking on a desktop shortcut does not
provide a means to specify the number of threads, and the default is just one.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ julia --threads auto"><pre class="notranslate"><code>$ julia --threads auto
</code></pre></div>
<p dir="auto">Then, make a copy of the parameters file:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; using TrainSpikingNet
julia&gt; mkdir(&quot;my-data&quot;)
julia&gt; cp(joinpath(dirname(pathof(TrainSpikingNet)), &quot;param.jl&quot;), &quot;my-data/param.jl&quot;)"><pre class="notranslate"><code>julia&gt; using TrainSpikingNet
julia&gt; mkdir("my-data")
julia&gt; cp(joinpath(dirname(pathof(TrainSpikingNet)), "param.jl"), "my-data/param.jl")
</code></pre></div>
<p dir="auto">The parameters file is Julia code which sets various simulation variables
using constants and user-defined plugins.  To evaluate it, and save the
pertinent data to a JLD2 file, use the <code>param</code> command:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; p = param(&quot;my-data&quot;);

julia&gt; p.dt  # the simulation time step in millisecods
0.1

julia&gt; p.Ncells  # the number of neurons in the model
4096

julia&gt; p.cellModel_file  # the plugin which defines membrane potential and spiking
&quot;/home/arthurb/.julia/packages/TrainSpikingNet/XYpdq/src/cellModel-LIF.jl&quot;"><pre class="notranslate"><code>julia&gt; p = param("my-data");

julia&gt; p.dt  # the simulation time step in millisecods
0.1

julia&gt; p.Ncells  # the number of neurons in the model
4096

julia&gt; p.cellModel_file  # the plugin which defines membrane potential and spiking
"/home/arthurb/.julia/packages/TrainSpikingNet/XYpdq/src/cellModel-LIF.jl"
</code></pre></div>
<p dir="auto">In addition to "param.jld2", alluded to above, there is also now a file
called "rng-init.jld2" in your data folder.  It contains the initial state
of the random number generator used to initialize the model, which can be
used to exactly reproduce an experiment.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; readdir(&quot;my-data&quot;)
3-element Vector{String}:
 &quot;param.jl&quot;
 &quot;param.jld2&quot;
 &quot;rng-init.jld2&quot;         # &lt;--"><pre class="notranslate"><code>julia&gt; readdir("my-data")
3-element Vector{String}:
 "param.jl"
 "param.jld2"
 "rng-init.jld2"         # &lt;--
</code></pre></div>
<p dir="auto">Now use <code>config</code> to load simulation code that is customized to your particular
model architecture and machine hardware:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; config(&quot;my-data&quot;, :cpu)  # 2nd arg can also be :gpu"><pre class="notranslate"><code>julia&gt; config("my-data", :cpu)  # 2nd arg can also be :gpu
</code></pre></div>
<p dir="auto">While <code>param</code> only needs to be called again if you change "params.jl",
<code>config</code> needs to be called each time you restart the Julia REPL.</p>
<p dir="auto">Now use <code>init</code> to pick random synaptic weights and generate synaptic current
targets.  Without any keyword arguments, sinusoids are used for the latter.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; state = init();
mean excitatory firing rate: 3.427978515625 Hz
mean inhibitory firing rate: 6.153564453125 Hz

julia&gt; size(state.wpWeightIn) 
(4096,)  # ==N

julia&gt; size(state.wpWeightIn[1])
(58,)  # ==2L

julia&gt; size(state.P)
(4096,)

julia&gt; size(state.P[1])
(58,58)"><pre class="notranslate"><code>julia&gt; state = init();
mean excitatory firing rate: 3.427978515625 Hz
mean inhibitory firing rate: 6.153564453125 Hz

julia&gt; size(state.wpWeightIn) 
(4096,)  # ==N

julia&gt; size(state.wpWeightIn[1])
(58,)  # ==2L

julia&gt; size(state.P)
(4096,)

julia&gt; size(state.P[1])
(58,58)
</code></pre></div>
<p dir="auto">Printed to the terminal are the initial (i.e. the unlearned) firing rates.
And saved to disk are several files containing the matrices which define
the neural connectivity:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; readdir(&quot;my-data&quot;)
17-element Vector{String}:
 &quot;P.jld2&quot;
 &quot;X_stim.jld2&quot;
 &quot;nc0.jld2&quot;
 &quot;ncpIn.jld2&quot;
 &quot;ncpOut.jld2&quot;
 &quot;param.jl&quot;
 &quot;param.jld2&quot;
 &quot;rateX.jld2&quot;
 &quot;rng-init.jld2&quot;
 &quot;utarg.jld2&quot;
 &quot;w0Index.jld2&quot;
 &quot;w0Weights.jld2&quot;
 &quot;wpIndexConvert.jld2&quot;
 &quot;wpIndexIn.jld2&quot;
 &quot;wpIndexOut.jld2&quot;
 &quot;wpWeightIn.jld2&quot;
 &quot;wpWeightX.jld2&quot;"><pre class="notranslate"><code>julia&gt; readdir("my-data")
17-element Vector{String}:
 "P.jld2"
 "X_stim.jld2"
 "nc0.jld2"
 "ncpIn.jld2"
 "ncpOut.jld2"
 "param.jl"
 "param.jld2"
 "rateX.jld2"
 "rng-init.jld2"
 "utarg.jld2"
 "w0Index.jld2"
 "w0Weights.jld2"
 "wpIndexConvert.jld2"
 "wpIndexIn.jld2"
 "wpIndexOut.jld2"
 "wpWeightIn.jld2"
 "wpWeightX.jld2"
</code></pre></div>
<p dir="auto">To highlight just a few:  "wpWeightIn.jld2" stores the plastic synaptic
weights, "w0Weights.jld2" stores the static synaptic weights, and "utarg.jld2"
stores the target synaptic currents (sinusoidal in this case).  See the
comments in the code for more details.</p>
<p dir="auto">Now use <code>train</code> to iteratively update the plastic weights with sequential
presentations of the stimulus:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; weights = train(nloops=100);
Loop no. 1, task no. 1
correlation: -0.03810218983457164
elapsed time: 64.27813005447388 sec
firing rate: 4.238525390625 Hz
Loop no. 2, task no. 1
correlation: -0.009730533926830837
elapsed time: 11.158457040786743 sec
firing rate: 3.5748291015625 Hz
Loop no. 3, task no. 1
correlation: 0.019285263967765184
elapsed time: 10.458786010742188 sec
firing rate: 3.177490234375 Hz
Loop no. 4, task no. 1
correlation: 0.04037737332828045
elapsed time: 10.786484003067017 sec
firing rate: 2.952392578125 Hz
Loop no. 5, task no. 1
correlation: 0.06431122625571872
elapsed time: 10.612313032150269 sec
firing rate: 2.79833984375 Hz
&lt;SNIP&gt;
Loop no. 100, task no. 1
correlation: 0.7138109340783079
elapsed time: 14.510313034057617 sec
firing rate: 2.1953125 Hz

julia&gt; size(weights.wpWeightIn)
(4096,)

julia&gt; size(weights.wpWeightIn[1])
(58,)"><pre class="notranslate"><code>julia&gt; weights = train(nloops=100);
Loop no. 1, task no. 1
correlation: -0.03810218983457164
elapsed time: 64.27813005447388 sec
firing rate: 4.238525390625 Hz
Loop no. 2, task no. 1
correlation: -0.009730533926830837
elapsed time: 11.158457040786743 sec
firing rate: 3.5748291015625 Hz
Loop no. 3, task no. 1
correlation: 0.019285263967765184
elapsed time: 10.458786010742188 sec
firing rate: 3.177490234375 Hz
Loop no. 4, task no. 1
correlation: 0.04037737332828045
elapsed time: 10.786484003067017 sec
firing rate: 2.952392578125 Hz
Loop no. 5, task no. 1
correlation: 0.06431122625571872
elapsed time: 10.612313032150269 sec
firing rate: 2.79833984375 Hz
&lt;SNIP&gt;
Loop no. 100, task no. 1
correlation: 0.7138109340783079
elapsed time: 14.510313034057617 sec
firing rate: 2.1953125 Hz

julia&gt; size(weights.wpWeightIn)
(4096,)

julia&gt; size(weights.wpWeightIn[1])
(58,)
</code></pre></div>
<p dir="auto">The correlations to the targets are printed to the terminal, and the trained
weights and updated covariance matrix are stored in additional JLD2 files
suffixed with "-ckpt" for "checkpoint":</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; readdir(&quot;my-data&quot;)
21-element Vector{String}:
 &quot;P-ckpt100.jld2&quot;             # &lt;--
 &quot;P.jld2&quot;
 &quot;X_stim.jld2&quot;
 &quot;nc0.jld2&quot;
 &quot;ncpIn.jld2&quot;
 &quot;ncpOut.jld2&quot;
 &quot;param.jl&quot;
 &quot;param.jld2&quot;
 &quot;rateX.jld2&quot;
 &quot;rng-init.jld2&quot;
 &quot;rng-train.jld2&quot;
 &quot;utarg.jld2&quot;
 &quot;w0Index.jld2&quot;
 &quot;w0Weights.jld2&quot;
 &quot;wpIndexConvert.jld2&quot;
 &quot;wpIndexIn.jld2&quot;
 &quot;wpIndexOut.jld2&quot;
 &quot;wpWeightIn-ckpt100.jld2&quot;    # &lt;--
 &quot;wpWeightIn.jld2&quot;
 &quot;wpWeightX-ckpt100.jld2&quot;     # &lt;--
 &quot;wpWeightX.jld2&quot;"><pre class="notranslate"><code>julia&gt; readdir("my-data")
21-element Vector{String}:
 "P-ckpt100.jld2"             # &lt;--
 "P.jld2"
 "X_stim.jld2"
 "nc0.jld2"
 "ncpIn.jld2"
 "ncpOut.jld2"
 "param.jl"
 "param.jld2"
 "rateX.jld2"
 "rng-init.jld2"
 "rng-train.jld2"
 "utarg.jld2"
 "w0Index.jld2"
 "w0Weights.jld2"
 "wpIndexConvert.jld2"
 "wpIndexIn.jld2"
 "wpIndexOut.jld2"
 "wpWeightIn-ckpt100.jld2"    # &lt;--
 "wpWeightIn.jld2"
 "wpWeightX-ckpt100.jld2"     # &lt;--
 "wpWeightX.jld2"
</code></pre></div>
<p dir="auto">Finally, use <code>test</code> to plot the trained activities:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; activities = test(ntrials=100);
trial #1, task #1: 50.6 sec
trial #2, task #1: 8.94 sec
trial #3, task #1: 8.64 sec
&lt;SNIP&gt;
trial #100, task #1: 8.29 sec

julia&gt; activities.times[1]  # spike times on the first trial for the first 16 neurons
16-element Vector{Vector{UInt16}}:
 [0x00c7, 0x140a, 0x47b7]
 []
 [0x0889, 0x2574, 0x25cd]
 [0x06ce, 0x085e, 0x098c, 0x0bab, 0x0fb9, 0x1053, 0x113d, 0x127d, 0x1643, 0x1f12, 0x2189, 0x21f4, 0x2564, 0x3610, 0x3d2c, 0x3f1f, 0x40ca, 0x433e, 0x455a, 0x4637]
 [0x03d4, 0x0622, 0x0aaf, 0x0cb1, 0x21e2, 0x35ed, 0x393d, 0x404b, 0x4400, 0x4bea, 0x4d02]
 [0x09e3, 0x0bb1, 0x4575]
 []
 [0x093e, 0x0a1e, 0x115a, 0x1296, 0x161c, 0x1865, 0x1b6b, 0x21eb, 0x2257, 0x22d9  …  0x3195, 0x3404, 0x3713, 0x38e1, 0x3dd3, 0x3f89, 0x4310, 0x4685, 0x4a7d, 0x4e04]
 [0x0001, 0x076f, 0x0aca]
 [0x0857]
 [0x0f0f, 0x1183, 0x124b, 0x1389, 0x1e70, 0x32f3, 0x3514, 0x3ed7, 0x42f7]
 [0x117b, 0x4837]
 [0x2340, 0x238e, 0x2401, 0x2624, 0x26c1, 0x308c, 0x31e9, 0x42f2, 0x46e9]
 [0x0001]
 [0x01db, 0x04ab, 0x083f, 0x1633, 0x1fdb, 0x20c9, 0x223a, 0x369d, 0x38d7, 0x3f95, 0x4475]
 [0x06e1, 0x0bdc, 0x1053, 0x13f6, 0x15c1, 0x1706, 0x19ee, 0x1b3b, 0x1ce0, 0x1e16  …  0x3b0e, 0x3e86, 0x3faa, 0x40d4, 0x4356, 0x44fe, 0x48ad, 0x49bb, 0x4bfd, 0x4df1]"><pre class="notranslate"><code>julia&gt; activities = test(ntrials=100);
trial #1, task #1: 50.6 sec
trial #2, task #1: 8.94 sec
trial #3, task #1: 8.64 sec
&lt;SNIP&gt;
trial #100, task #1: 8.29 sec

julia&gt; activities.times[1]  # spike times on the first trial for the first 16 neurons
16-element Vector{Vector{UInt16}}:
 [0x00c7, 0x140a, 0x47b7]
 []
 [0x0889, 0x2574, 0x25cd]
 [0x06ce, 0x085e, 0x098c, 0x0bab, 0x0fb9, 0x1053, 0x113d, 0x127d, 0x1643, 0x1f12, 0x2189, 0x21f4, 0x2564, 0x3610, 0x3d2c, 0x3f1f, 0x40ca, 0x433e, 0x455a, 0x4637]
 [0x03d4, 0x0622, 0x0aaf, 0x0cb1, 0x21e2, 0x35ed, 0x393d, 0x404b, 0x4400, 0x4bea, 0x4d02]
 [0x09e3, 0x0bb1, 0x4575]
 []
 [0x093e, 0x0a1e, 0x115a, 0x1296, 0x161c, 0x1865, 0x1b6b, 0x21eb, 0x2257, 0x22d9  …  0x3195, 0x3404, 0x3713, 0x38e1, 0x3dd3, 0x3f89, 0x4310, 0x4685, 0x4a7d, 0x4e04]
 [0x0001, 0x076f, 0x0aca]
 [0x0857]
 [0x0f0f, 0x1183, 0x124b, 0x1389, 0x1e70, 0x32f3, 0x3514, 0x3ed7, 0x42f7]
 [0x117b, 0x4837]
 [0x2340, 0x238e, 0x2401, 0x2624, 0x26c1, 0x308c, 0x31e9, 0x42f2, 0x46e9]
 [0x0001]
 [0x01db, 0x04ab, 0x083f, 0x1633, 0x1fdb, 0x20c9, 0x223a, 0x369d, 0x38d7, 0x3f95, 0x4475]
 [0x06e1, 0x0bdc, 0x1053, 0x13f6, 0x15c1, 0x1706, 0x19ee, 0x1b3b, 0x1ce0, 0x1e16  …  0x3b0e, 0x3e86, 0x3faa, 0x40d4, 0x4356, 0x44fe, 0x48ad, 0x49bb, 0x4bfd, 0x4df1]
</code></pre></div>
<p dir="auto">The <code>ntrials</code> argument specifies how many iterations to perform, but this time
there is no learning.  We perform multiple iterations so that peri-stimulus
time histograms (PSTHs) with low firing rate neurons can be averaged over
many trials.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="/test-syninput.svg"><img src="/test-syninput.svg" alt="synpatic inputs" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="/test-psth.svg"><img src="/test-psth.svg" alt="PSTH" style="max-width: 100%;"></a></p>
<p dir="auto">The figures above are saved to "test-{syninput,psth}-task1.pdf" and the
underlying data is stored in "test.jld2":</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; readdir(&quot;my-data&quot;)
25-element Vector{String}:
 &quot;P-ckpt100.jld2&quot;
 &quot;P.jld2&quot;
 &quot;X_stim.jld2&quot;
 &quot;nc0.jld2&quot;
 &quot;ncpIn.jld2&quot;
 &quot;ncpOut.jld2&quot;
 &quot;param.jl&quot;
 &quot;param.jld2&quot;
 &quot;rateX.jld2&quot;
 &quot;rng-init.jld2&quot;
 &quot;rng-test.jld2&quot;
 &quot;rng-train.jld2&quot;
 &quot;test-psth-task1.pdf&quot;         # &lt;--
 &quot;test-syninput-task1.pdf&quot;     # &lt;--
 &quot;test.jld2&quot;                   # &lt;--
 &quot;utarg.jld2&quot;
 &quot;w0Index.jld2&quot;
 &quot;w0Weights.jld2&quot;
 &quot;wpIndexConvert.jld2&quot;
 &quot;wpIndexIn.jld2&quot;
 &quot;wpIndexOut.jld2&quot;
 &quot;wpWeightIn-ckpt100.jld2&quot;
 &quot;wpWeightIn.jld2&quot;
 &quot;wpWeightX-ckpt100.jld2&quot;
 &quot;wpWeightX.jld2&quot;"><pre class="notranslate"><code>julia&gt; readdir("my-data")
25-element Vector{String}:
 "P-ckpt100.jld2"
 "P.jld2"
 "X_stim.jld2"
 "nc0.jld2"
 "ncpIn.jld2"
 "ncpOut.jld2"
 "param.jl"
 "param.jld2"
 "rateX.jld2"
 "rng-init.jld2"
 "rng-test.jld2"
 "rng-train.jld2"
 "test-psth-task1.pdf"         # &lt;--
 "test-syninput-task1.pdf"     # &lt;--
 "test.jld2"                   # &lt;--
 "utarg.jld2"
 "w0Index.jld2"
 "w0Weights.jld2"
 "wpIndexConvert.jld2"
 "wpIndexIn.jld2"
 "wpIndexOut.jld2"
 "wpWeightIn-ckpt100.jld2"
 "wpWeightIn.jld2"
 "wpWeightX-ckpt100.jld2"
 "wpWeightX.jld2"
</code></pre></div>
<p dir="auto">Should you wish to replot at a later time, there is no need to call <code>test</code>
again.  <code>test</code> actually only produces the data in "test.jld", and then
immediately calls <code>plot</code> to create the figures.  You can do so directly
yourself, bypassing a lengthy call to <code>test</code>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; plot(&quot;my-data/test.jld2&quot;, ineurons_to_plot=[1,5,9,13])"><pre class="notranslate"><code>julia&gt; plot("my-data/test.jld2", ineurons_to_plot=[1,5,9,13])
</code></pre></div>
<p dir="auto">The <code>ineurons_to_plot</code> argument specifies the desired indices, and must be a
subset of that used in <code>test</code>, which defaults to <code>1:16</code>.  Note that <code>plot</code> (and
<code>test</code>) will overwrite any figures previously generated by <code>test</code> or <code>plot</code>, so
rename or move them accordingly.</p>
<p dir="auto">A few things to note:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>init</code>, for now, can only be done on a CPU,</p>
</li>
<li>
<p dir="auto">for small models, training on the CPU is probably faster than with a GPU,
and using fewer CPU threads might be faster than using more if your model
is particularly small,</p>
</li>
<li>
<p dir="auto">if using GPU(s), start Julia with the number of threads set to the number
of GPUs you have in your workstation.</p>
</li>
</ul>
<h2 dir="auto"><a id="user-content-linux--powershell-command-line-1" class="anchor" aria-hidden="true" href="#linux--powershell-command-line-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Linux / PowerShell command line</h2>
<p dir="auto">The above tutorial can recapitulated on the Linux command line as follows.</p>
<p dir="auto">Copying the parameters file:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ mkdir my-data
$ cp $TSN_DIR/src/param.jl my-data"><pre class="notranslate"><code>$ mkdir my-data
$ cp $TSN_DIR/src/param.jl my-data
</code></pre></div>
<p dir="auto">Picking random synaptic weights and generating artificial synaptic current
targets of sinusoids:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ julia --threads auto $TSN_DIR/src/init.jl $PWD/my-data"><pre class="notranslate"><code>$ julia --threads auto $TSN_DIR/src/init.jl $PWD/my-data
</code></pre></div>
<p dir="auto">Note that <code>init.jl</code> automatically performs the equivalent of <code>param</code> in
the REPL.</p>
<p dir="auto">The <code>--threads</code> flag, which must come immediately after the <code>julia</code> command,
specifies how many CPU threads to use.  The argument after <code>init.jl</code>,
<code>$PWD/my-data</code> here, must be the full (not relative) path to the desired
<code>params.jl</code> file.</p>
<p dir="auto">Iteratively updating the plastic weights with sequential presentations of
the stimulus:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ julia $TSN_DIR/src/train.jl --nloops 100 --gpu $PWD/my-data"><pre class="notranslate"><code>$ julia $TSN_DIR/src/train.jl --nloops 100 --gpu $PWD/my-data
</code></pre></div>
<p dir="auto">Note that <code>train.jl</code> automatically performs the equivalent of <code>config</code>
in the REPL.</p>
<p dir="auto">The <code>--nloops</code> flag, which must come after <code>train.jl</code>, specifies how many
iterations of the training loop to perform.  The <code>--threads</code> flag is not
used here as we are now using the GPU version of <code>train.jl</code> and have just
a single GPU in our workstation.</p>
<p dir="auto">Plotting the trained activities:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ julia $TSN_DIR/src/test.jl --ntrials 100 --gpu $PWD/my-data
$ julia $TSN_DIR/src/plot.jl --ineurons_to_plot=[1,5,9,13] $PWD/my-data/test.jld2"><pre class="notranslate"><code>$ julia $TSN_DIR/src/test.jl --ntrials 100 --gpu $PWD/my-data
$ julia $TSN_DIR/src/plot.jl --ineurons_to_plot=[1,5,9,13] $PWD/my-data/test.jld2
</code></pre></div>
<h2 dir="auto"><a id="user-content-help" class="anchor" aria-hidden="true" href="#help"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Help</h2>
<p dir="auto">TrainSpikingNet.jl is documented not just in this README, but also via comments
in the code, as well as docstrings that are accessible both in the Julia
REPL and the OS command line.  Specifically, at the REPL, entering "?train"
results in:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="help?&gt; train
search: train TrainSpikingNet trailing_ones trailing_zeros AbstractString AbstractUnitRange

  train(; nloops = 1,
          correlation_interval = 1,
          save_best_checkpoint = false,
          restore_from_checkpoint = nothing,
          monitor_resources_used = nothing,
          return_P = false) -&gt; (; weights, P)

  Update the weights using the recursive least squares algorithm nloops times, measuring the
  similarity between the actual and target synaptic currents every correlation_interval
  iterations. To continue training a previous model, specify which of the saved weights to
  start from with restore_from_checkpoint. The learned plastic weights and updated covariance
  matrix are saved as JLD2 files in the data_dir input to the last call to config with the
  checkpoint added as a suffix. The weights and optionally the covariance are also returned as
  a NamedTuple for convenience."><pre class="notranslate"><code>help?&gt; train
search: train TrainSpikingNet trailing_ones trailing_zeros AbstractString AbstractUnitRange

  train(; nloops = 1,
          correlation_interval = 1,
          save_best_checkpoint = false,
          restore_from_checkpoint = nothing,
          monitor_resources_used = nothing,
          return_P = false) -&gt; (; weights, P)

  Update the weights using the recursive least squares algorithm nloops times, measuring the
  similarity between the actual and target synaptic currents every correlation_interval
  iterations. To continue training a previous model, specify which of the saved weights to
  start from with restore_from_checkpoint. The learned plastic weights and updated covariance
  matrix are saved as JLD2 files in the data_dir input to the last call to config with the
  checkpoint added as a suffix. The weights and optionally the covariance are also returned as
  a NamedTuple for convenience.
</code></pre></div>
<p dir="auto">Equivalently, on the OS command line:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="$ julia $TSN_DIR/src/train.jl --help
usage: train.jl [-n NLOOPS] [-c CORRELATION_INTERVAL] [-s]
                [-r RESTORE_FROM_CHECKPOINT]
                [-m MONITOR_RESOURCES_USED] [-g] [-h] data_dir

positional arguments:
  data_dir              full path to the directory containing the
                        parameters file

optional arguments:
  -n, --nloops NLOOPS   number of iterations to train (type: Int64,
                        default: 1)
  -c, --correlation_interval CORRELATION_INTERVAL
                        measure correlation every C training loops.
                        default is every loop (type: Int64, default:
                        1)
  -s, --save_best_checkpoint
                        save the learned weights and covariance
                        matrices with the highest measured correlation
                        too.  default is to only save the last one
  -r, --restore_from_checkpoint RESTORE_FROM_CHECKPOINT
                        continue training from checkpoint R.  default
                        is to start from the beginning (type: Int64)
  -m, --monitor_resources_used MONITOR_RESOURCES_USED
                        measure power, cores, and memory usage every M
                        seconds.  default is never (type: Int64)
  -g, --gpu             use the GPU
  -h, --help            show this help message and exit"><pre class="notranslate"><code>$ julia $TSN_DIR/src/train.jl --help
usage: train.jl [-n NLOOPS] [-c CORRELATION_INTERVAL] [-s]
                [-r RESTORE_FROM_CHECKPOINT]
                [-m MONITOR_RESOURCES_USED] [-g] [-h] data_dir

positional arguments:
  data_dir              full path to the directory containing the
                        parameters file

optional arguments:
  -n, --nloops NLOOPS   number of iterations to train (type: Int64,
                        default: 1)
  -c, --correlation_interval CORRELATION_INTERVAL
                        measure correlation every C training loops.
                        default is every loop (type: Int64, default:
                        1)
  -s, --save_best_checkpoint
                        save the learned weights and covariance
                        matrices with the highest measured correlation
                        too.  default is to only save the last one
  -r, --restore_from_checkpoint RESTORE_FROM_CHECKPOINT
                        continue training from checkpoint R.  default
                        is to start from the beginning (type: Int64)
  -m, --monitor_resources_used MONITOR_RESOURCES_USED
                        measure power, cores, and memory usage every M
                        seconds.  default is never (type: Int64)
  -g, --gpu             use the GPU
  -h, --help            show this help message and exit
</code></pre></div>
<p dir="auto">Docstrings are similarly available for <code>param</code>, <code>config</code>, <code>init</code>, <code>test</code>,
and <code>plot</code>.</p>
<p dir="auto">Don't hesitate to file an issue on Github if you find a bug or have a feature
request.  The best place for usage help is either a GitHub discussion,
<a href="https://discourse.julialang.org/" rel="nofollow">https://discourse.julialang.org/</a>, or to contact one of the authors directly.</p>
<h1 dir="auto"><a id="user-content-custom-usage" class="anchor" aria-hidden="true" href="#custom-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Custom Usage</h1>
<p dir="auto">To train a network for your own purpose you need to specify the neural
architecture, its target activity, and various simulation parameters.
These are done, respectively, through a set of plugin modules that define
adjacency matrices, by supplying a file with the desired synaptic inputs
or PSTHs, and by editing a copy of the default parameters file.</p>
<p dir="auto">There are five plugin modules that define the architecture.  Each consists
of a .jl file containing a function of a specific name that inputs a custom
dictionary of parameters and outputs one or more arrays.  The path to the
.jl file as well as the parameters are both specified in "param.jl" with
variables ending in _file and _args, respectively.  You as the user write
these five functions to return custom adjacency matrices using parameters
of your choosing.  Defaults are supplied for each plugin.</p>
<ul dir="auto">
<li>
<p dir="auto"><code>genPlasticWeights()</code> specifies the connectivity of the learned synapses.
The default is "src/genPlasticWeights-erdos-renyi.jl"</p>
</li>
<li>
<p dir="auto"><code>genStaticWeights()</code> specifies the connectivity of the fixed synapses.
The default is "src/genStaticWeights-erdos-renyi.jl".  This is only used
if K &gt; 0.</p>
</li>
<li>
<p dir="auto"><code>genRateX()</code> specifies the spike thresholds for the feed-forward neurons.
The default is "src/genRateX-ornstein-uhlenbeck.jl".  This is only used
if LX &gt; 0.</p>
</li>
<li>
<p dir="auto"><code>genXStim()</code> specifies the external input applied to each neuron.
The default is "src/genXStim-ornstein-uhlenbeck.jl"</p>
</li>
<li>
<p dir="auto"><code>genUTarget()</code> specifies the desired synaptic currents to learn.
The default is "src/genUTarget-sinusoids.jl".  This is only used if a file
with the targets is not supplied to <code>init</code>.</p>
</li>
</ul>
<p dir="auto">For example, the "genStaticWeights_file" and "genStaticWeights_args"
variables in "src/param.jl" are a string and a dictionary, respectively.
The former specifies the path to a .jl file containing a function called
<code>genStaticWeights()</code> to which the latter is passed when <code>init</code>
is executed.  <code>genStaticWeights()</code> constructs and returns <code>w0Index</code>,
<code>w0Weights</code>, and <code>nc0</code> which together specify the static connections
between neurons based on the parameters <code>Ncells</code>, <code>Ne</code>, <code>pree</code>, <code>prie</code>,
<code>prei</code>, <code>prii</code>, <code>jee</code>, <code>jie</code>, <code>jei</code>, and <code>jii</code>.  Should the default code,
contained in "src/genStaticWeights-erdos-renyi.jl", not do what you want,
you can create your own file (e.g. "genStaticWeights-custom.jl") which
defines an alternative definition of <code>genStaticWeights()</code> (must be the
same name) based on a (possibly) alternative set of parameters.  Simply set
<code>genStaticWeights_file</code> in your custom parameters file to the full path to
your custom function, and <code>getStaticWeights_args</code> to the required parameters
with their values.</p>
<p dir="auto">If your target synaptic inputs are defined algorithmically then you have to
use <code>genUTarget()</code>, but if they are stored in a file then you can also supply
its fullpath to <code>init</code> using the <code>utarg_file</code> argument.  This file will
be copied to <code>utarg.jld2</code>.  If your desired temporal activity patterns are
PSTHs instead of synaptic currents, use the <code>spikerate_file</code> argument instead
and they will be converted to synaptic currents using the method of Ricciardi
(Brunel 2000, J. Comput. Neurosci; Richardson 2007, Phys. Rev. E) and saved
in <code>utarg.jld2</code>.  As this conversion can take awhile, you should subsquently
use the <code>utarg_file</code> flag with this newly generated <code>utarg.jld2</code> file.</p>
<p dir="auto">Simulation parameters, like the learning rate, the duration and time step
of the simulation, the floating point precision to use, the seed value
for random number generation, etc., are specified in "src/param.jl".
Simply make a copy and edit.  See the comments therein for more details.</p>
<p dir="auto">The cell model is also specified in "src/param.jl".  Much like the neural
architecture, this code is pulled out into a plugin, so that it is easy to
use a custom algorithm.  The "cellModel_file" and "cellModel_args" variables
are the path to a .jl file and a dictionary of parameters, respectively.
The former must define six functions, three each for the CPU and GPU:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>cellModel_timestep!</code> updates the state variables at each tick of the clock</p>
</li>
<li>
<p dir="auto"><code>cellModel_spiked[!]</code> returns whether a neuron spiked or not</p>
</li>
<li>
<p dir="auto"><code>cellModel_reset!</code> sets the variables to their initial state</p>
</li>
</ul>
<p dir="auto">The CPU methods each input <code>i</code>, which is the cell index.  The GPU methods
operate on all cells in parallel, and input <code>bnotrefrac</code> and <code>bspike</code> which
are boolean vectors indicating which neurons are not in the refractory
period and which have spiked, respectively.  Other inputs include the
membrane voltage (<code>v</code>), and the external (<code>X</code>) and recurrent (<code>u</code>) currents.</p>
<p dir="auto">Six cell models are supplied, five of which
are the Allen Institute's General Leak Integrate
and Fire (GLIF) models (see <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-017-02717-4/MediaObjects/41467_2017_2717_MOESM1_ESM.pdf" rel="nofollow">Teeter, Iyer, Menon, et al, Mihalas,
2017</a>).
The sixth, "src/cellModel-LIF.jl", is the default, and is a
performance-optimized version of GLIF1.</p>
<p dir="auto">Once you have configured all the above, proceed with <code>init</code>, <code>train</code>,
and <code>test</code> as described in the Tutorial.</p>
<h1 dir="auto"><a id="user-content-file-formats" class="anchor" aria-hidden="true" href="#file-formats"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>File Formats</h1>
<p dir="auto">All data are stored in JLD2 files, which are HDF5 files with a particular
structure inside designed to store Julia objects.  They should be readable
in any programming language that can read HDF5.</p>
<h1 dir="auto"><a id="user-content-intel-math-kernel-library" class="anchor" aria-hidden="true" href="#intel-math-kernel-library"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Intel Math Kernel Library</h1>
<p dir="auto">The CPU code can be sped up by about 10-50% on Intel machines using the
drop-in MKL package to replace the default OpenBLAS.  Install it like
this from the Julia REPL:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; ] add MKL"><pre class="notranslate"><code>julia&gt; ] add MKL
</code></pre></div>
<p dir="auto">Then add <code>using MKL</code> to your startup file:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; edit(joinpath(DEPOT_PATH[1], &quot;config&quot;, &quot;startup.jl&quot;))"><pre class="notranslate"><code>julia&gt; edit(joinpath(DEPOT_PATH[1], "config", "startup.jl"))
</code></pre></div>
<p dir="auto">Or equivalently like this on Linux:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia -e 'using Pkg; Pkg.add(&quot;MKL&quot;)'
echo &quot;using MKL&quot; &gt;&gt; ~/.julia/config/startup.jl"><pre class="notranslate"><code>julia -e 'using Pkg; Pkg.add("MKL")'
echo "using MKL" &gt;&gt; ~/.julia/config/startup.jl
</code></pre></div>
<h1 dir="auto"><a id="user-content-physical-units" class="anchor" aria-hidden="true" href="#physical-units"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Physical Units</h1>
<p dir="auto">The state variables in the parameters file can have dimensions assigned
to them using <a href="https://github.com/PainterQubits/Unitful.jl">Unitful.jl</a>.
For example, <code>dt</code>, the simulation time step, could be set to <code>100μs</code>
instead of the default <code>0.1</code> with a comment that it is in milliseconds.
Doing so makes mixing power-of-ten prefixes easy and serves as a guard
against mixing incompatible units, but incurs a performance cost of about
10% depending on the model size and whether a GPU is used or not.  If this
tradeoff is acceptable, use "src/param-units.jl" as a template for your
parameters file instead of "src/params.jl".</p>
</article></div>