<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-flimsyjl" class="anchor" aria-hidden="true" href="#flimsyjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Flimsy.jl</h1>
<p dir="auto">Flimsy.jl is a Julia package for expressing and training a rich class of machine learning models whose parameters are differentiable with respect to a scalar loss function, i.e., neural networks.</p>
<p dir="auto">Computations are described natively within Julia, making it <em>relatively</em> easy to express complicated models.
For example, models with <a href="http://arxiv.org/abs/1409.0473" rel="nofollow">attentional components</a> or arbitrary size persistent <a href="http://arxiv.org/abs/1503.08895" rel="nofollow">memory structures</a> can be easily expressed using Flimsy.jl</p>
<h2 dir="auto"><a id="user-content-features" class="anchor" aria-hidden="true" href="#features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Features</h2>
<ul dir="auto">
<li>Automatic gradient computation.</li>
<li>Express computation with native control flow structures (<code>for</code>, <code>while</code>, recursion, etc).</li>
<li>Extensible.</li>
<li>Group parameters and functionality via components.</li>
<li>No compilation process (outside of Julia's own JIT compiler).</li>
</ul>
<h2 dir="auto"><a id="user-content-un-features" class="anchor" aria-hidden="true" href="#un-features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Un-Features</h2>
<ul dir="auto">
<li>No GPU support.</li>
<li>No automatic computation graph optimization.</li>
</ul>
<h2 dir="auto"><a id="user-content-why" class="anchor" aria-hidden="true" href="#why"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Why?</h2>
<p dir="auto">Flimsy.jl is primarily an experiment in interface design for neural network centric machine learning libraries.
It aims to overcome what I see as the biggest interface drawbacks of popular libraries such as
<a href="http://deeplearning.net/software/theano/" rel="nofollow">Theano</a>,
<a href="http://torch.ch/" rel="nofollow">Torch</a>, and
<a href="https://www.tensorflow.org/" rel="nofollow">TensorFlow</a>.<br>
<strong>N.B.</strong> I'm much less familiar with the other libararies listed above than Theano, which I have worked with extensively for several years. As such the below criticism may or may not be applicable to all the libraries listed above.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Awkward, limited, and non-native control flow structures</strong><br>
Flimsy.jl sidesteps the need for an explict computational graph structure and the <em>special</em> control flow functions they bring with them, e.g., <code>scan</code>, by implicitly constructing a backward graph at runtime. This is done by pushing a closure (implemented as a type with a <code>call</code> method) onto a shared stack after each function application. Popping and calling the closures until the stack is empty implicitly carries out backpropagation.</p>
</li>
<li>
<p dir="auto"><strong>Lack of reuseable and composable sub-components</strong><br>
Flimsy.jl defines an abstract <code>Component</code> type for coupling parameters and functionality. Using Julia's multiple dispatch and a common set of component functions names allows the creation of a library of <code>Components</code> which can easily be composed to form larger <code>Components</code>. See <a href="https://github.com/thomlake/Flimsy.jl/blob/master/examples/rnn_comparison.jl"><code>examples/rnn_comparison.jl</code></a> for a practical example.</p>
</li>
<li>
<p dir="auto"><strong>Two Language Syndrome</strong><br>
Flimsy.jl is written entirely in Julia, and Julia is fast.
This means new primitive operations can be defined without switching languages and writing wrappers.</p>
</li>
</ul>
<p dir="auto">Unfortunately Flimsy.jl is currently nowhere near performant enough to serve as
a substitute for the libraries mentioned above in many cases.</p>
<h2 dir="auto"><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Disclaimer</h2>
<p dir="auto">Flimsy.jl is experimental software.
There are probably bugs to be fixed and certainly optimizations to be made. Any potential user would be well advised to make liberal use of the <code>check_gradients</code> function to test that gradients are being calculated correctly for their particular model.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">Flimsy.jl is currently unregistered, to install use</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; Pkg.clone(&quot;https://github.com/thomlake/Flimsy.jl.git&quot;)"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/thomlake/Flimsy.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<p dir="auto">An example Logistic Regression implementation is given below.
This and several other builtin components are available in the <code>Flimsy.Components</code> module.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Flimsy
using Synthetic # Data generation: https://github.com/thomlake/Synthetic.jl

# Parameter definition
immutable Params{V&lt;:Variable} &lt;: Component{V}
    w::V
    b::V
end

# Default constructor
Params(m, n) = Params(w=randn(m, n), b=zeros(m, 1))

# Computation the model performs
@comp score(θ::Params, x::Variable) = affine(θ.w, x, θ.b)
@comp predict(θ::Params, x::Variable) = argmax(score(θ, x))
@comp probs(θ::Params, x::Variable) = softmax(score(θ, x))
@comp cost(θ::Params, x::Variable, y) = Cost.categorical_cross_entropy_with_scores(score(θ, x), y)

# Check gradients using finite differences
function check()
    println(&quot;checking gradients....&quot;)
    n_features, n_classes, n_samples = 20, 3, 5
    data = rand(Synthetic.MixtureTask(n_features, n_classes), n_samples)
    X = hcat(map(first, data)...)
    y = vcat(map(last, data)...)
    θ = Runtime(Params(n_classes, n_features))
    check_gradients(cost, θ, Input(X), y)
end

# Train/Test
function main()
    srand(sum(map(Int, collect(&quot;Flimsy&quot;))))
    n_features, n_classes = 20, 3
    n_train, n_test = 50, 50
    D = Synthetic.MixtureTask(n_features, n_classes)
    data_train = rand(D, n_train)
    data_test = rand(D, n_test)
    X_train, y_train = hcat(map(first, data_train)...), vcat(map(last, data_train)...)
    X_test, y_test = hcat(map(first, data_test)...), vcat(map(last, data_test)...)

    θ = Runtime(Params(n_classes, n_features))
    opt = optimizer(RmsProp, θ, learning_rate=0.01, decay=0.9)
    start_time = time()
    for i = 1:100
        nll = cost(θ, Input(X_train), y_train; grad=true)
        update!(opt)
        i % 10 == 0 &amp;&amp; println(&quot;epoch =&gt; $i, nll =&gt; $nll&quot;)
    end
    println(&quot;wall time   =&gt; &quot;, time() - start_time)
    println(&quot;train error =&gt; &quot;, sum(y_train .!= predict(θ, Input(X_train))) / n_train)
    println(&quot;test error  =&gt; &quot;, sum(y_test .!= predict(θ, Input(X_test))) / n_test)
end

(&quot;-c&quot; in ARGS || &quot;--check&quot; in ARGS) &amp;&amp; check()
main()"><pre><span class="pl-k">using</span> Flimsy
<span class="pl-k">using</span> Synthetic <span class="pl-c"><span class="pl-c">#</span> Data generation: https://github.com/thomlake/Synthetic.jl</span>

<span class="pl-c"><span class="pl-c">#</span> Parameter definition</span>
immutable Params{V<span class="pl-k">&lt;:</span><span class="pl-c1">Variable</span>} <span class="pl-k">&lt;:</span> <span class="pl-c1">Component{V}</span>
    w<span class="pl-k">::</span><span class="pl-c1">V</span>
    b<span class="pl-k">::</span><span class="pl-c1">V</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Default constructor</span>
<span class="pl-en">Params</span>(m, n) <span class="pl-k">=</span> <span class="pl-c1">Params</span>(w<span class="pl-k">=</span><span class="pl-c1">randn</span>(m, n), b<span class="pl-k">=</span><span class="pl-c1">zeros</span>(m, <span class="pl-c1">1</span>))

<span class="pl-c"><span class="pl-c">#</span> Computation the model performs</span>
<span class="pl-c1">@comp</span> <span class="pl-en">score</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>) <span class="pl-k">=</span> <span class="pl-c1">affine</span>(θ<span class="pl-k">.</span>w, x, θ<span class="pl-k">.</span>b)
<span class="pl-c1">@comp</span> <span class="pl-en">predict</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>) <span class="pl-k">=</span> <span class="pl-c1">argmax</span>(<span class="pl-c1">score</span>(θ, x))
<span class="pl-c1">@comp</span> <span class="pl-en">probs</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>) <span class="pl-k">=</span> <span class="pl-c1">softmax</span>(<span class="pl-c1">score</span>(θ, x))
<span class="pl-c1">@comp</span> <span class="pl-en">cost</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>, y) <span class="pl-k">=</span> Cost<span class="pl-k">.</span><span class="pl-c1">categorical_cross_entropy_with_scores</span>(<span class="pl-c1">score</span>(θ, x), y)

<span class="pl-c"><span class="pl-c">#</span> Check gradients using finite differences</span>
<span class="pl-k">function</span> <span class="pl-en">check</span>()
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>checking gradients....<span class="pl-pds">"</span></span>)
    n_features, n_classes, n_samples <span class="pl-k">=</span> <span class="pl-c1">20</span>, <span class="pl-c1">3</span>, <span class="pl-c1">5</span>
    data <span class="pl-k">=</span> <span class="pl-c1">rand</span>(Synthetic<span class="pl-k">.</span><span class="pl-c1">MixtureTask</span>(n_features, n_classes), n_samples)
    X <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">map</span>(first, data)<span class="pl-k">...</span>)
    y <span class="pl-k">=</span> <span class="pl-c1">vcat</span>(<span class="pl-c1">map</span>(last, data)<span class="pl-k">...</span>)
    θ <span class="pl-k">=</span> <span class="pl-c1">Runtime</span>(<span class="pl-c1">Params</span>(n_classes, n_features))
    <span class="pl-c1">check_gradients</span>(cost, θ, <span class="pl-c1">Input</span>(X), y)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Train/Test</span>
<span class="pl-k">function</span> <span class="pl-en">main</span>()
    <span class="pl-c1">srand</span>(<span class="pl-c1">sum</span>(<span class="pl-c1">map</span>(Int, <span class="pl-c1">collect</span>(<span class="pl-s"><span class="pl-pds">"</span>Flimsy<span class="pl-pds">"</span></span>))))
    n_features, n_classes <span class="pl-k">=</span> <span class="pl-c1">20</span>, <span class="pl-c1">3</span>
    n_train, n_test <span class="pl-k">=</span> <span class="pl-c1">50</span>, <span class="pl-c1">50</span>
    D <span class="pl-k">=</span> Synthetic<span class="pl-k">.</span><span class="pl-c1">MixtureTask</span>(n_features, n_classes)
    data_train <span class="pl-k">=</span> <span class="pl-c1">rand</span>(D, n_train)
    data_test <span class="pl-k">=</span> <span class="pl-c1">rand</span>(D, n_test)
    X_train, y_train <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">map</span>(first, data_train)<span class="pl-k">...</span>), <span class="pl-c1">vcat</span>(<span class="pl-c1">map</span>(last, data_train)<span class="pl-k">...</span>)
    X_test, y_test <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">map</span>(first, data_test)<span class="pl-k">...</span>), <span class="pl-c1">vcat</span>(<span class="pl-c1">map</span>(last, data_test)<span class="pl-k">...</span>)

    θ <span class="pl-k">=</span> <span class="pl-c1">Runtime</span>(<span class="pl-c1">Params</span>(n_classes, n_features))
    opt <span class="pl-k">=</span> <span class="pl-c1">optimizer</span>(RmsProp, θ, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.01</span>, decay<span class="pl-k">=</span><span class="pl-c1">0.9</span>)
    start_time <span class="pl-k">=</span> <span class="pl-c1">time</span>()
    <span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
        nll <span class="pl-k">=</span> <span class="pl-c1">cost</span>(θ, <span class="pl-c1">Input</span>(X_train), y_train; grad<span class="pl-k">=</span><span class="pl-c1">true</span>)
        <span class="pl-c1">update!</span>(opt)
        i <span class="pl-k">%</span> <span class="pl-c1">10</span> <span class="pl-k">==</span> <span class="pl-c1">0</span> <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>epoch =&gt; <span class="pl-v">$i</span>, nll =&gt; <span class="pl-v">$nll</span><span class="pl-pds">"</span></span>)
    <span class="pl-k">end</span>
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>wall time   =&gt; <span class="pl-pds">"</span></span>, <span class="pl-c1">time</span>() <span class="pl-k">-</span> start_time)
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>train error =&gt; <span class="pl-pds">"</span></span>, <span class="pl-c1">sum</span>(y_train <span class="pl-k">.!=</span> <span class="pl-c1">predict</span>(θ, <span class="pl-c1">Input</span>(X_train))) <span class="pl-k">/</span> n_train)
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>test error  =&gt; <span class="pl-pds">"</span></span>, <span class="pl-c1">sum</span>(y_test <span class="pl-k">.!=</span> <span class="pl-c1">predict</span>(θ, <span class="pl-c1">Input</span>(X_test))) <span class="pl-k">/</span> n_test)
<span class="pl-k">end</span>

(<span class="pl-s"><span class="pl-pds">"</span>-c<span class="pl-pds">"</span></span> <span class="pl-k">in</span> <span class="pl-c1">ARGS</span> <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">"</span>--check<span class="pl-pds">"</span></span> <span class="pl-k">in</span> <span class="pl-c1">ARGS</span>) <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">check</span>()
<span class="pl-c1">main</span>()</pre></div>
<h2 dir="auto"><a id="user-content-directory-layout" class="anchor" aria-hidden="true" href="#directory-layout"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Directory Layout</h2>
<h3 dir="auto"><a id="user-content-src" class="anchor" aria-hidden="true" href="#src"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>src/</code></h3>
<p dir="auto">Core module functionality.</p>
<h3 dir="auto"><a id="user-content-test" class="anchor" aria-hidden="true" href="#test"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>test/</code></h3>
<p dir="auto">Unit tests (uses <a href="https://github.com/JuliaLang/FactCheck.jl">FactCheck.jl</a>).</p>
<h3 dir="auto"><a id="user-content-srcops" class="anchor" aria-hidden="true" href="#srcops"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>src/ops/</code></h3>
<p dir="auto">Differentiable operations.</p>
<h3 dir="auto"><a id="user-content-srccost" class="anchor" aria-hidden="true" href="#srccost"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>src/cost/</code></h3>
<p dir="auto">Module containing common cost functions.</p>
<h3 dir="auto"><a id="user-content-srccomponents" class="anchor" aria-hidden="true" href="#srccomponents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>src/components/</code></h3>
<p dir="auto">Module containing common ML models.</p>
<h3 dir="auto"><a id="user-content-srcctcjl" class="anchor" aria-hidden="true" href="#srcctcjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>src/ctc.jl</code></h3>
<p dir="auto">Module containing utility functions for Connectionist Temporal Classification cost function.</p>
<h3 dir="auto"><a id="user-content-srcextras" class="anchor" aria-hidden="true" href="#srcextras"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>src/extras/</code></h3>
<p dir="auto">Module containing several utility functions.</p>
</article></div>