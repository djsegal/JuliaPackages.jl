<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-flimsyjl" class="anchor" aria-hidden="true" href="#flimsyjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Flimsy.jl</h1>
<p>Flimsy.jl is a Julia package for expressing and training a rich class of machine learning models whose parameters are differentiable with respect to a scalar loss function, i.e., neural networks.</p>
<p>Computations are described natively within Julia, making it <em>relatively</em> easy to express complicated models.
For example, models with <a href="http://arxiv.org/abs/1409.0473" rel="nofollow">attentional components</a> or arbitrary size persistent <a href="http://arxiv.org/abs/1503.08895" rel="nofollow">memory structures</a> can be easily expressed using Flimsy.jl</p>
<h2><a id="user-content-features" class="anchor" aria-hidden="true" href="#features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Features</h2>
<ul>
<li>Automatic gradient computation.</li>
<li>Express computation with native control flow structures (<code>for</code>, <code>while</code>, recursion, etc).</li>
<li>Extensible.</li>
<li>Group parameters and functionality via components.</li>
<li>No compilation process (outside of Julia's own JIT compiler).</li>
</ul>
<h2><a id="user-content-un-features" class="anchor" aria-hidden="true" href="#un-features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Un-Features</h2>
<ul>
<li>No GPU support.</li>
<li>No automatic computation graph optimization.</li>
</ul>
<h2><a id="user-content-why" class="anchor" aria-hidden="true" href="#why"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why?</h2>
<p>Flimsy.jl is primarily an experiment in interface design for neural network centric machine learning libraries.
It aims to overcome what I see as the biggest interface drawbacks of popular libraries such as
<a href="http://deeplearning.net/software/theano/" rel="nofollow">Theano</a>,
<a href="http://torch.ch/" rel="nofollow">Torch</a>, and
<a href="https://www.tensorflow.org/" rel="nofollow">TensorFlow</a>.<br>
<strong>N.B.</strong> I'm much less familiar with the other libararies listed above than Theano, which I have worked with extensively for several years. As such the below criticism may or may not be applicable to all the libraries listed above.</p>
<ul>
<li>
<p><strong>Awkward, limited, and non-native control flow structures</strong><br>
Flimsy.jl sidesteps the need for an explict computational graph structure and the <em>special</em> control flow functions they bring with them, e.g., <code>scan</code>, by implicitly constructing a backward graph at runtime. This is done by pushing a closure (implemented as a type with a <code>call</code> method) onto a shared stack after each function application. Popping and calling the closures until the stack is empty implicitly carries out backpropagation.</p>
</li>
<li>
<p><strong>Lack of reuseable and composable sub-components</strong><br>
Flimsy.jl defines an abstract <code>Component</code> type for coupling parameters and functionality. Using Julia's multiple dispatch and a common set of component functions names allows the creation of a library of <code>Components</code> which can easily be composed to form larger <code>Components</code>. See <a href="https://github.com/thomlake/Flimsy.jl/blob/master/examples/rnn_comparison.jl"><code>examples/rnn_comparison.jl</code></a> for a practical example.</p>
</li>
<li>
<p><strong>Two Language Syndrome</strong><br>
Flimsy.jl is written entirely in Julia, and Julia is fast.
This means new primitive operations can be defined without switching languages and writing wrappers.</p>
</li>
</ul>
<p>Unfortunately Flimsy.jl is currently nowhere near performant enough to serve as
a substitute for the libraries mentioned above in many cases.</p>
<h2><a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Disclaimer</h2>
<p>Flimsy.jl is experimental software.
There are probably bugs to be fixed and certainly optimizations to be made. Any potential user would be well advised to make liberal use of the <code>check_gradients</code> function to test that gradients are being calculated correctly for their particular model.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>Flimsy.jl is currently unregistered, to install use</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/thomlake/Flimsy.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h2>
<p>An example Logistic Regression implementation is given below.
This and several other builtin components are available in the <code>Flimsy.Components</code> module.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Flimsy
<span class="pl-k">using</span> Synthetic <span class="pl-c"><span class="pl-c">#</span> Data generation: https://github.com/thomlake/Synthetic.jl</span>

<span class="pl-c"><span class="pl-c">#</span> Parameter definition</span>
immutable Params{V<span class="pl-k">&lt;:</span><span class="pl-c1">Variable</span>} <span class="pl-k">&lt;:</span> <span class="pl-c1">Component{V}</span>
    w<span class="pl-k">::</span><span class="pl-c1">V</span>
    b<span class="pl-k">::</span><span class="pl-c1">V</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Default constructor</span>
<span class="pl-en">Params</span>(m, n) <span class="pl-k">=</span> <span class="pl-c1">Params</span>(w<span class="pl-k">=</span><span class="pl-c1">randn</span>(m, n), b<span class="pl-k">=</span><span class="pl-c1">zeros</span>(m, <span class="pl-c1">1</span>))

<span class="pl-c"><span class="pl-c">#</span> Computation the model performs</span>
<span class="pl-c1">@comp</span> <span class="pl-en">score</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>) <span class="pl-k">=</span> <span class="pl-c1">affine</span>(θ<span class="pl-k">.</span>w, x, θ<span class="pl-k">.</span>b)
<span class="pl-c1">@comp</span> <span class="pl-en">predict</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>) <span class="pl-k">=</span> <span class="pl-c1">argmax</span>(<span class="pl-c1">score</span>(θ, x))
<span class="pl-c1">@comp</span> <span class="pl-en">probs</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>) <span class="pl-k">=</span> <span class="pl-c1">softmax</span>(<span class="pl-c1">score</span>(θ, x))
<span class="pl-c1">@comp</span> <span class="pl-en">cost</span>(θ<span class="pl-k">::</span><span class="pl-c1">Params</span>, x<span class="pl-k">::</span><span class="pl-c1">Variable</span>, y) <span class="pl-k">=</span> Cost<span class="pl-k">.</span><span class="pl-c1">categorical_cross_entropy_with_scores</span>(<span class="pl-c1">score</span>(θ, x), y)

<span class="pl-c"><span class="pl-c">#</span> Check gradients using finite differences</span>
<span class="pl-k">function</span> <span class="pl-en">check</span>()
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>checking gradients....<span class="pl-pds">"</span></span>)
    n_features, n_classes, n_samples <span class="pl-k">=</span> <span class="pl-c1">20</span>, <span class="pl-c1">3</span>, <span class="pl-c1">5</span>
    data <span class="pl-k">=</span> <span class="pl-c1">rand</span>(Synthetic<span class="pl-k">.</span><span class="pl-c1">MixtureTask</span>(n_features, n_classes), n_samples)
    X <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">map</span>(first, data)<span class="pl-k">...</span>)
    y <span class="pl-k">=</span> <span class="pl-c1">vcat</span>(<span class="pl-c1">map</span>(last, data)<span class="pl-k">...</span>)
    θ <span class="pl-k">=</span> <span class="pl-c1">Runtime</span>(<span class="pl-c1">Params</span>(n_classes, n_features))
    <span class="pl-c1">check_gradients</span>(cost, θ, <span class="pl-c1">Input</span>(X), y)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Train/Test</span>
<span class="pl-k">function</span> <span class="pl-en">main</span>()
    <span class="pl-c1">srand</span>(<span class="pl-c1">sum</span>(<span class="pl-c1">map</span>(Int, <span class="pl-c1">collect</span>(<span class="pl-s"><span class="pl-pds">"</span>Flimsy<span class="pl-pds">"</span></span>))))
    n_features, n_classes <span class="pl-k">=</span> <span class="pl-c1">20</span>, <span class="pl-c1">3</span>
    n_train, n_test <span class="pl-k">=</span> <span class="pl-c1">50</span>, <span class="pl-c1">50</span>
    D <span class="pl-k">=</span> Synthetic<span class="pl-k">.</span><span class="pl-c1">MixtureTask</span>(n_features, n_classes)
    data_train <span class="pl-k">=</span> <span class="pl-c1">rand</span>(D, n_train)
    data_test <span class="pl-k">=</span> <span class="pl-c1">rand</span>(D, n_test)
    X_train, y_train <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">map</span>(first, data_train)<span class="pl-k">...</span>), <span class="pl-c1">vcat</span>(<span class="pl-c1">map</span>(last, data_train)<span class="pl-k">...</span>)
    X_test, y_test <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">map</span>(first, data_test)<span class="pl-k">...</span>), <span class="pl-c1">vcat</span>(<span class="pl-c1">map</span>(last, data_test)<span class="pl-k">...</span>)

    θ <span class="pl-k">=</span> <span class="pl-c1">Runtime</span>(<span class="pl-c1">Params</span>(n_classes, n_features))
    opt <span class="pl-k">=</span> <span class="pl-c1">optimizer</span>(RmsProp, θ, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.01</span>, decay<span class="pl-k">=</span><span class="pl-c1">0.9</span>)
    start_time <span class="pl-k">=</span> <span class="pl-c1">time</span>()
    <span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
        nll <span class="pl-k">=</span> <span class="pl-c1">cost</span>(θ, <span class="pl-c1">Input</span>(X_train), y_train; grad<span class="pl-k">=</span><span class="pl-c1">true</span>)
        <span class="pl-c1">update!</span>(opt)
        i <span class="pl-k">%</span> <span class="pl-c1">10</span> <span class="pl-k">==</span> <span class="pl-c1">0</span> <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>epoch =&gt; <span class="pl-v">$i</span>, nll =&gt; <span class="pl-v">$nll</span><span class="pl-pds">"</span></span>)
    <span class="pl-k">end</span>
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>wall time   =&gt; <span class="pl-pds">"</span></span>, <span class="pl-c1">time</span>() <span class="pl-k">-</span> start_time)
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>train error =&gt; <span class="pl-pds">"</span></span>, <span class="pl-c1">sum</span>(y_train <span class="pl-k">.!=</span> <span class="pl-c1">predict</span>(θ, <span class="pl-c1">Input</span>(X_train))) <span class="pl-k">/</span> n_train)
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>test error  =&gt; <span class="pl-pds">"</span></span>, <span class="pl-c1">sum</span>(y_test <span class="pl-k">.!=</span> <span class="pl-c1">predict</span>(θ, <span class="pl-c1">Input</span>(X_test))) <span class="pl-k">/</span> n_test)
<span class="pl-k">end</span>

(<span class="pl-s"><span class="pl-pds">"</span>-c<span class="pl-pds">"</span></span> <span class="pl-k">in</span> ARGS <span class="pl-k">||</span> <span class="pl-s"><span class="pl-pds">"</span>--check<span class="pl-pds">"</span></span> <span class="pl-k">in</span> ARGS) <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">check</span>()
<span class="pl-c1">main</span>()</pre></div>
<h2><a id="user-content-directory-layout" class="anchor" aria-hidden="true" href="#directory-layout"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Directory Layout</h2>
<h3><a id="user-content-src" class="anchor" aria-hidden="true" href="#src"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>src/</code></h3>
<p>Core module functionality.</p>
<h3><a id="user-content-test" class="anchor" aria-hidden="true" href="#test"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>test/</code></h3>
<p>Unit tests (uses <a href="https://github.com/JuliaLang/FactCheck.jl">FactCheck.jl</a>).</p>
<h3><a id="user-content-srcops" class="anchor" aria-hidden="true" href="#srcops"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>src/ops/</code></h3>
<p>Differentiable operations.</p>
<h3><a id="user-content-srccost" class="anchor" aria-hidden="true" href="#srccost"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>src/cost/</code></h3>
<p>Module containing common cost functions.</p>
<h3><a id="user-content-srccomponents" class="anchor" aria-hidden="true" href="#srccomponents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>src/components/</code></h3>
<p>Module containing common ML models.</p>
<h3><a id="user-content-srcctcjl" class="anchor" aria-hidden="true" href="#srcctcjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>src/ctc.jl</code></h3>
<p>Module containing utility functions for Connectionist Temporal Classification cost function.</p>
<h3><a id="user-content-srcextras" class="anchor" aria-hidden="true" href="#srcextras"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>src/extras/</code></h3>
<p>Module containing several utility functions.</p>
</article></div>