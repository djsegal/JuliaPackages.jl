<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-libsvmjl" class="anchor" aria-hidden="true" href="#libsvmjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LIBSVM.jl</h1>
<p dir="auto"><a href="https://github.com/JuliaML/LIBSVM.jl/actions?query=workflow%3ACI"><img src="https://github.com/JuliaML/LIBSVM.jl/workflows/CI/badge.svg?branch=master" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaML/LIBSVM.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/80d9e2d704597ee68cda3e74b899b1fef98f59bd21fe1494f89a1df600521f70/68747470733a2f2f636f6465636f762e696f2f67682f4a756c69614d4c2f4c494253564d2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d6247777a7954744e506e" alt="codecov" data-canonical-src="https://codecov.io/gh/JuliaML/LIBSVM.jl/branch/master/graph/badge.svg?token=bGwzyTtNPn" style="max-width: 100%;"></a></p>
<p dir="auto">This is a Julia interface for
<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" rel="nofollow">LIBSVM</a> and for the linear
SVM model provided by
<a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" rel="nofollow">LIBLINEAR</a>.</p>
<p dir="auto"><strong>Features:</strong></p>
<ul dir="auto">
<li>Supports all LIBSVM models: classification C-SVC, nu-SVC, regression: epsilon-SVR, nu-SVR
and distribution estimation: one-class SVM</li>
<li>Model objects are represented by Julia type SVM which gives you easy
access to model features and can be saved e.g. as JLD file</li>
<li>Supports ScikitLearn.jl API</li>
</ul>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<h3 dir="auto"><a id="user-content-libsvm-api" class="anchor" aria-hidden="true" href="#libsvm-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LIBSVM API</h3>
<p dir="auto">This provides a lower level API similar to LIBSVM C-interface. See <code>?svmtrain</code>
for options.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using LIBSVM
using RDatasets
using Printf
using Statistics

# Load Fisher's classic iris data
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)

# First four dimension of input data is features
X = Matrix(iris[:, 1:4])'

# LIBSVM handles multi-class data automatically using a one-against-one strategy
y = iris.Species

# Split the dataset into training set and testing set
Xtrain = X[:, 1:2:end]
Xtest  = X[:, 2:2:end]
ytrain = y[1:2:end]
ytest  = y[2:2:end]

# Train SVM on half of the data using default parameters. See documentation
# of svmtrain for options
model = svmtrain(Xtrain, ytrain)

# Test model on the other half of the data.
ŷ, decision_values = svmpredict(model, Xtest);

# Compute accuracy
@printf &quot;Accuracy: %.2f%%\n&quot; mean(ŷ .== ytest) * 100"><pre><span class="pl-k">using</span> LIBSVM
<span class="pl-k">using</span> RDatasets
<span class="pl-k">using</span> Printf
<span class="pl-k">using</span> Statistics

<span class="pl-c"><span class="pl-c">#</span> Load Fisher's classic iris data</span>
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> First four dimension of input data is features</span>
X <span class="pl-k">=</span> <span class="pl-c1">Matrix</span>(iris[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>])<span class="pl-k">'</span>

<span class="pl-c"><span class="pl-c">#</span> LIBSVM handles multi-class data automatically using a one-against-one strategy</span>
y <span class="pl-k">=</span> iris<span class="pl-k">.</span>Species

<span class="pl-c"><span class="pl-c">#</span> Split the dataset into training set and testing set</span>
Xtrain <span class="pl-k">=</span> X[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>]
Xtest  <span class="pl-k">=</span> X[:, <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>]
ytrain <span class="pl-k">=</span> y[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>]
ytest  <span class="pl-k">=</span> y[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>]

<span class="pl-c"><span class="pl-c">#</span> Train SVM on half of the data using default parameters. See documentation</span>
<span class="pl-c"><span class="pl-c">#</span> of svmtrain for options</span>
model <span class="pl-k">=</span> <span class="pl-c1">svmtrain</span>(Xtrain, ytrain)

<span class="pl-c"><span class="pl-c">#</span> Test model on the other half of the data.</span>
ŷ, decision_values <span class="pl-k">=</span> <span class="pl-c1">svmpredict</span>(model, Xtest);

<span class="pl-c"><span class="pl-c">#</span> Compute accuracy</span>
<span class="pl-c1">@printf</span> <span class="pl-s"><span class="pl-pds">"</span>Accuracy: %.2f%%<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> <span class="pl-c1">mean</span>(ŷ <span class="pl-k">.==</span> ytest) <span class="pl-k">*</span> <span class="pl-c1">100</span></pre></div>
<h3 dir="auto"><a id="user-content-precomputed-kernel" class="anchor" aria-hidden="true" href="#precomputed-kernel"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Precomputed kernel</h3>
<p dir="auto">It is possible to use different kernels than those that are provided. In such a
case, it is required to provide a matrix filled with precomputed kernel values.</p>
<p dir="auto">For training, a symmetric matrix is expected:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="K = [k(x_1, x_1)  k(x_1, x_2)  ...  k(x_1, x_l);
     k(x_2, x_1)
         ...                            ...
     k(x_l, x_1)        ...         k(x_l, x_l)]"><pre class="notranslate"><code>K = [k(x_1, x_1)  k(x_1, x_2)  ...  k(x_1, x_l);
     k(x_2, x_1)
         ...                            ...
     k(x_l, x_1)        ...         k(x_l, x_l)]
</code></pre></div>
<p dir="auto">where <code>x_i</code> is <code>i</code>-th training instance and <code>l</code> is the number of training
instances.</p>
<p dir="auto">To predict <code>n</code> instances, a matrix of shape <code>(l, n)</code> is expected:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="KK = [k(x_1, t_1)  k(x_1, t_2)  ...  k(x_1, t_n);
      k(x_2, t_1)
          ...                            ...
      k(x_l, t_1)        ...         k(x_l, t_n)]"><pre class="notranslate"><code>KK = [k(x_1, t_1)  k(x_1, t_2)  ...  k(x_1, t_n);
      k(x_2, t_1)
          ...                            ...
      k(x_l, t_1)        ...         k(x_l, t_n)]
</code></pre></div>
<p dir="auto">where <code>t_i</code> is <code>i</code>-th instance to be predicted.</p>
<h4 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h4>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Training data
X = [-2 -1 -1 1 1 2;
     -1 -1 -2 1 2 1]
y = [1, 1, 1, 2, 2, 2]

# Testing data
T = [-1 2 3;
     -1 2 2]

# Precomputed matrix for training (corresponds to linear kernel)
K = X' * X

model = svmtrain(K, y, kernel=Kernel.Precomputed)

# Precomputed matrix for prediction
KK = X' * T

ỹ, _ = svmpredict(model, KK)"><pre><span class="pl-c"><span class="pl-c">#</span> Training data</span>
X <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">2</span> <span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span> <span class="pl-c1">2</span>;
     <span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-k">-</span><span class="pl-c1">2</span> <span class="pl-c1">1</span> <span class="pl-c1">2</span> <span class="pl-c1">1</span>]
y <span class="pl-k">=</span> [<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>]

<span class="pl-c"><span class="pl-c">#</span> Testing data</span>
T <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-c1">2</span> <span class="pl-c1">3</span>;
     <span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-c1">2</span> <span class="pl-c1">2</span>]

<span class="pl-c"><span class="pl-c">#</span> Precomputed matrix for training (corresponds to linear kernel)</span>
K <span class="pl-k">=</span> X<span class="pl-k">'</span> <span class="pl-k">*</span> X

model <span class="pl-k">=</span> <span class="pl-c1">svmtrain</span>(K, y, kernel<span class="pl-k">=</span>Kernel<span class="pl-k">.</span>Precomputed)

<span class="pl-c"><span class="pl-c">#</span> Precomputed matrix for prediction</span>
KK <span class="pl-k">=</span> X<span class="pl-k">'</span> <span class="pl-k">*</span> T

ỹ, _ <span class="pl-k">=</span> <span class="pl-c1">svmpredict</span>(model, KK)</pre></div>
<h3 dir="auto"><a id="user-content-scikitlearn-api" class="anchor" aria-hidden="true" href="#scikitlearn-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ScikitLearn API</h3>
<p dir="auto">You can alternatively use <code>ScikitLearn.jl</code> API with same options as <code>svmtrain</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using LIBSVM
using RDatasets

# Classification C-SVM
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
X = Matrix(iris[:, 1:4])
y = iris.Species

Xtrain = X[1:2:end, :]
Xtest  = X[2:2:end, :]
ytrain = y[1:2:end]
ytest  = y[2:2:end]

model = fit!(SVC(), Xtrain, ytrain)
ŷ = predict(model, Xtest)"><pre><span class="pl-k">using</span> LIBSVM
<span class="pl-k">using</span> RDatasets

<span class="pl-c"><span class="pl-c">#</span> Classification C-SVM</span>
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)
X <span class="pl-k">=</span> <span class="pl-c1">Matrix</span>(iris[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>])
y <span class="pl-k">=</span> iris<span class="pl-k">.</span>Species

Xtrain <span class="pl-k">=</span> X[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>, :]
Xtest  <span class="pl-k">=</span> X[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>, :]
ytrain <span class="pl-k">=</span> y[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>]
ytest  <span class="pl-k">=</span> y[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>]

model <span class="pl-k">=</span> <span class="pl-c1">fit!</span>(<span class="pl-c1">SVC</span>(), Xtrain, ytrain)
ŷ <span class="pl-k">=</span> <span class="pl-c1">predict</span>(model, Xtest)</pre></div>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Epsilon-Regression

whiteside = RDatasets.dataset(&quot;MASS&quot;, &quot;whiteside&quot;)
X = Matrix(whiteside[:, 3:3])  # the `Gas` column
y = whiteside.Temp

model = fit!(EpsilonSVR(cost = 10., gamma = 1.), X, y)
ŷ = predict(model, X)"><pre><span class="pl-c"><span class="pl-c">#</span> Epsilon-Regression</span>

whiteside <span class="pl-k">=</span> RDatasets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>MASS<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>whiteside<span class="pl-pds">"</span></span>)
X <span class="pl-k">=</span> <span class="pl-c1">Matrix</span>(whiteside[:, <span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">3</span>])  <span class="pl-c"><span class="pl-c">#</span> the `Gas` column</span>
y <span class="pl-k">=</span> whiteside<span class="pl-k">.</span>Temp

model <span class="pl-k">=</span> <span class="pl-c1">fit!</span>(<span class="pl-c1">EpsilonSVR</span>(cost <span class="pl-k">=</span> <span class="pl-c1">10.</span>, gamma <span class="pl-k">=</span> <span class="pl-c1">1.</span>), X, y)
ŷ <span class="pl-k">=</span> <span class="pl-c1">predict</span>(model, X)</pre></div>
<h3 dir="auto"><a id="user-content-mlj-api" class="anchor" aria-hidden="true" href="#mlj-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MLJ API</h3>
<p dir="auto">The <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="nofollow">MLJ</a> interface to LIBSVM.jl consists of the following models:</p>
<ul dir="auto">
<li>classification: <code>LinearSVC</code>, <code>SVC</code>, <code>NuSVC</code></li>
<li>regression: <code>EpsilonSVR</code>, <code>NuSVR</code></li>
<li>outlier detection: <code>OneClassSVM</code></li>
</ul>
<p dir="auto">Each model has a detailed document string, which includes examples of
usage. Document strings can be accessed from MLJ without loading
<code>LIBSVM.jl</code> (or its MLJ interface) as shown in the following example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ     # or MLJModels 
doc(&quot;NuSVC&quot;, pkg=&quot;LIBSVM&quot;)"><pre><span class="pl-k">using</span> MLJ     <span class="pl-c"><span class="pl-c">#</span> or MLJModels </span>
<span class="pl-c1">doc</span>(<span class="pl-s"><span class="pl-pds">"</span>NuSVC<span class="pl-pds">"</span></span>, pkg<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>LIBSVM<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">This assumes the version of MLJModels loaded is 0.15.5 or higher.</p>
<h2 dir="auto"><a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Credits</h2>
<p dir="auto">The LIBSVM.jl library is currently developed and maintained by Matti
Pastell. It was originally developed by Simon Kornblith.</p>
<p dir="auto"><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" rel="nofollow">LIBSVM</a> by Chih-Chung Chang and Chih-Jen Lin</p>
</article></div>