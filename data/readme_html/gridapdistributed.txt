<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gridapdistributed" class="anchor" aria-hidden="true" href="#gridapdistributed"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GridapDistributed</h1>
<p dir="auto"><a href="https://gridap.github.io/GridapDistributed.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Gridap/GridapDistributed.jl/workflows/CI/badge.svg"><img src="https://github.com/Gridap/GridapDistributed.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://zenodo.org/badge/latestdoi/258832236" rel="nofollow"><img src="https://camo.githubusercontent.com/be128c655afbfb10fa3615b3c3311d2a29c469e32e6dd79ee2294f51009480f6/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3235383833323233362e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/258832236.svg" style="max-width: 100%;"></a>
<a href="https://joss.theoj.org/papers/10.21105/joss.04157" rel="nofollow"><img src="https://camo.githubusercontent.com/6d463034fddb623b7872b77509a67989a79963f95944be324b00a64e9938f71e/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f31302e32313130352f6a6f73732e30323532302f7374617475732e737667" alt="DOI" data-canonical-src="https://joss.theoj.org/papers/10.21105/joss.02520/status.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Parallel distributed-memory version of <code>Gridap.jl</code>.</p>
<h2 dir="auto"><a id="user-content-purpose" class="anchor" aria-hidden="true" href="#purpose"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Purpose</h2>
<p dir="auto"><code>GridapDistributed.jl</code> provides a fully-parallel distributed memory extension of the <a href="https://github.com/gridap/Gridap.jl"><code>Gridap.jl</code></a> library. It allows users to approximate PDEs on parallel computers, from multi-core CPU desktop computers to HPC clusters and supercomputers. The sub-package is designed to be as non-intrusive as possible. As a result, sequential Julia scripts written in the high level API of <code>Gridap.jl</code> can be used almost verbatim up to minor adjustments in a parallel context using <code>GridapDistributed.jl</code>.</p>
<p dir="auto">At present, <code>GridapDistributed.jl</code> provides scalable parallel data structures for grid handling,  finite element spaces setup, and distributed linear system assembly. For the latter part, i.e., global distributed sparse matrices and vectors, <code>GridapDistributed.jl</code> relies on <a href="https://github.com/fverdugo/PartitionedArrays.jl"><code>PartitionedArrays.jl</code></a> as distributed linear algebra backend.</p>
<h2 dir="auto"><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Documentation</h2>
<p dir="auto"><code>GridapDistributed.jl</code> and <code>Gridap.jl</code> share almost the same high-level API. We refer to the documentation of <code>Gridap.jl</code> for more details about the API. In the example below, we show the minor differences among the APIs of <code>Gridap.jl</code> and <code>GridapDistributed.jl</code>. We also refer to the following <a href="https://gridap.github.io/Tutorials/dev/pages/t016_poisson_distributed/" rel="nofollow">tutorial</a> and the <a href="https://gridap.github.io/GridapDistributed.jl/dev" rel="nofollow"><code>GridapDistributed.jl</code></a> documentation for additional examples and rationale.</p>
<h2 dir="auto"><a id="user-content-execution-modes-and-how-to-execute-the-program-in-each-mode" class="anchor" aria-hidden="true" href="#execution-modes-and-how-to-execute-the-program-in-each-mode"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Execution modes and how to execute the program in each mode</h2>
<p dir="auto"><code>GridapDistributed.jl</code> driver programs can be either run in sequential execution mode (very useful for developing/debugging parallel programs, see <code>test/sequential/</code> folder for examples) or in message-passing (MPI) execution mode (when you want to deploy the code in the actual parallel computer and perform a fast simulation, see <code>test/mpi/</code> folder for examples). In any case, even if you do no have access to a parallel machine, you should be able to run in both modes in your local desktop/laptop.</p>
<p dir="auto">A <code>GridapDistributed.jl</code> driver program written in sequential execution mode as, e.g., the one available at <code>test/sequential/PoissonTests.jl</code>, is executed from the terminal just as any other Julia script:</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia test/sequential/PoissonTests.jl"><pre>julia test/sequential/PoissonTests.jl</pre></div>
<p dir="auto">On the other hand, a driver program written in MPI execution mode, such as the one shown in the snippet in the next section, involves an invocation of the <code>mpiexecjl</code> script (see <a href="https://github.com/gridap/GridapDistributed.jl/edit/master/README.md#mpi-parallel-julia-script-execution-instructions%5D(https://github.com/gridap/GridapDistributed.jl#mpi-parallel-julia-script-execution-instructions)">below</a>):</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="mpiexecjl -n 4 julia gridap_distributed_mpi_mode_example.jl"><pre class="notranslate"><code>mpiexecjl -n 4 julia gridap_distributed_mpi_mode_example.jl
</code></pre></div>
<p dir="auto">with the appropriate number of MPI tasks, <code>-n 4</code> in this particular example.</p>
<h2 dir="auto"><a id="user-content-simple-example-mpi-parallel-execution-mode" class="anchor" aria-hidden="true" href="#simple-example-mpi-parallel-execution-mode"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Simple example (MPI-parallel execution mode)</h2>
<p dir="auto">The following Julia code snippet solves a 2D Poisson problem in parallel on the unit square. The example follows the MPI-parallel execution mode (note the <code>MPIBackend()</code> argument to the <code>with_backend</code> function call) and thus it must be executed on 4 MPI tasks (note the mesh is partitioned into 4 parts) using the instructions <a href="https://github.com/gridap/GridapDistributed.jl#mpi-parallel-julia-script-execution-instructions">below</a>. If a user wants to use the sequential execution mode, one just replaces <code>MPIBackend()</code> by <code>SequentialBackend()</code> in the call to <code>with_backend</code>. <code>GridapDistributed.jl</code> sequential execution mode scripts are executed as any other julia sequential script.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Gridap
using GridapDistributed
using PartitionedArrays
partition = (2,2)
with_backend(MPIBackend(),partition) do parts
  domain = (0,1,0,1)
  mesh_partition = (4,4)
  model = CartesianDiscreteModel(parts,domain,mesh_partition)
  order = 2
  u((x,y)) = (x+y)^order
  f(x) = -Δ(u,x)
  reffe = ReferenceFE(lagrangian,Float64,order)
  V = TestFESpace(model,reffe,dirichlet_tags=&quot;boundary&quot;)
  U = TrialFESpace(u,V)
  Ω = Triangulation(model)
  dΩ = Measure(Ω,2*order)
  a(u,v) = ∫( ∇(v)⋅∇(u) )dΩ
  l(v) = ∫( v*f )dΩ
  op = AffineFEOperator(a,l,U,V)
  uh = solve(op)
  writevtk(Ω,&quot;results&quot;,cellfields=[&quot;uh&quot;=&gt;uh,&quot;grad_uh&quot;=&gt;∇(uh)])
end"><pre><span class="pl-k">using</span> Gridap
<span class="pl-k">using</span> GridapDistributed
<span class="pl-k">using</span> PartitionedArrays
partition <span class="pl-k">=</span> (<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)
<span class="pl-c1">with_backend</span>(<span class="pl-c1">MPIBackend</span>(),partition) <span class="pl-k">do</span> parts
  domain <span class="pl-k">=</span> (<span class="pl-c1">0</span>,<span class="pl-c1">1</span>,<span class="pl-c1">0</span>,<span class="pl-c1">1</span>)
  mesh_partition <span class="pl-k">=</span> (<span class="pl-c1">4</span>,<span class="pl-c1">4</span>)
  model <span class="pl-k">=</span> <span class="pl-c1">CartesianDiscreteModel</span>(parts,domain,mesh_partition)
  order <span class="pl-k">=</span> <span class="pl-c1">2</span>
  <span class="pl-en">u</span>((x,y)) <span class="pl-k">=</span> (x<span class="pl-k">+</span>y)<span class="pl-k">^</span>order
  <span class="pl-en">f</span>(x) <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">Δ</span>(u,x)
  reffe <span class="pl-k">=</span> <span class="pl-c1">ReferenceFE</span>(lagrangian,Float64,order)
  V <span class="pl-k">=</span> <span class="pl-c1">TestFESpace</span>(model,reffe,dirichlet_tags<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>boundary<span class="pl-pds">"</span></span>)
  U <span class="pl-k">=</span> <span class="pl-c1">TrialFESpace</span>(u,V)
  Ω <span class="pl-k">=</span> <span class="pl-c1">Triangulation</span>(model)
  dΩ <span class="pl-k">=</span> <span class="pl-c1">Measure</span>(Ω,<span class="pl-c1">2</span><span class="pl-k">*</span>order)
  <span class="pl-en">a</span>(u,v) <span class="pl-k">=</span> <span class="pl-c1">∫</span>( <span class="pl-c1">∇</span>(v)<span class="pl-k">⋅</span><span class="pl-c1">∇</span>(u) )dΩ
  <span class="pl-en">l</span>(v) <span class="pl-k">=</span> <span class="pl-c1">∫</span>( v<span class="pl-k">*</span>f )dΩ
  op <span class="pl-k">=</span> <span class="pl-c1">AffineFEOperator</span>(a,l,U,V)
  uh <span class="pl-k">=</span> <span class="pl-c1">solve</span>(op)
  <span class="pl-c1">writevtk</span>(Ω,<span class="pl-s"><span class="pl-pds">"</span>results<span class="pl-pds">"</span></span>,cellfields<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>uh<span class="pl-pds">"</span></span><span class="pl-k">=&gt;</span>uh,<span class="pl-s"><span class="pl-pds">"</span>grad_uh<span class="pl-pds">"</span></span><span class="pl-k">=&gt;</span><span class="pl-c1">∇</span>(uh)])
<span class="pl-k">end</span></pre></div>
<p dir="auto">The domain is discretized using the parallel Cartesian-like mesh generator built-in in <code>GridapDistributed</code>. The only minimal difference with respect to the sequential <code>Gridap</code> script is a call to the <code>with_backend</code> function of <a href="https://github.com/fverdugo/PartitionedArrays.jl"><code>PartitionedArrays.jl</code></a> right at the beginning of the program. With this function, the programer sets up the <code>PartitionedArrays.jl</code> communication backend (i.e., MPI in the example), specifies the number of parts and their layout (i.e., 2x2 partition in the example), and provides a function (using Julia do-block syntax for function arguments in the example) to be run on each part. The function body is equivalent to a sequential <code>Gridap</code> script, except for the <code>CartesianDiscreteModel</code> call, which in <code>GridapDistributed</code> also requires the <code>parts</code> argument passed back by the <code>with_backend</code> function.</p>
<h2 dir="auto"><a id="user-content-using-parallel-solvers" class="anchor" aria-hidden="true" href="#using-parallel-solvers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Using parallel solvers</h2>
<p dir="auto"><code>GridapDistributed.jl</code> is <em>not</em> a library of parallel linear solvers. The linear solver kernel within <code>GridapDistributed.jl</code>, defined with the backslash operator <code>\</code>, is just a sparse LU solver applied to the global system gathered on a master task (not scalable, but very useful for testing and debug purposes).</p>
<p dir="auto">We provide the full set of scalable linear and nonlinear solvers in the <a href="https://petsc.org/release/" rel="nofollow">PETSc</a> library in <a href="https://github.com/gridap/GridapPETSc.jl"><code>GridapPETSc.jl</code></a>. For an example which combines <code>GridapDistributed</code> with <code>GridapPETSc.jl</code>, we refer to the following <a href="https://gridap.github.io/Tutorials/dev/pages/t016_poisson_distributed/" rel="nofollow">tutorial</a>. Additional examples can be found in the <code>test/</code> folder of <code>GridapPETSc</code>. Other linear solver libraries on top of <code>GridapDistributed</code> can be developed in the future.</p>
<h2 dir="auto"><a id="user-content-partitioned-meshes" class="anchor" aria-hidden="true" href="#partitioned-meshes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Partitioned meshes</h2>
<p dir="auto"><code>GridapDistributed.jl</code> provides a built-in parallel generator of Cartesian-like meshes of arbitrary-dimensional, topologically n-cube domains.</p>
<p dir="auto">Distributed unstructured meshes are generated using <a href="https://github.com/gridap/GridapGmsh.jl"><code>GridapGmsh.jl</code></a>. We also refer to <a href="https://github.com/gridap/GridapP4est.jl"><code>GridapP4est.jl</code></a>, for peta-scale handling of meshes which can be decomposed as forest of quadtrees/octrees of the computational domain. Examples of distributed solvers that combine all these building blocks can be found in the following <a href="https://gridap.github.io/Tutorials/dev/pages/t016_poisson_distributed/" rel="nofollow">tutorial</a>.</p>
<h2 dir="auto"><a id="user-content-a-more-complex-example--mpi-parallel-execution-mode" class="anchor" aria-hidden="true" href="#a-more-complex-example--mpi-parallel-execution-mode"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>A more complex example  (MPI-parallel execution mode)</h2>
<p dir="auto">In the following example, we combine <code>GridapDistributed</code> (for the parallel implementation of the PDE discretisation), <code>GridapGmsh</code> (for the distributed unstructured mesh), and <code>GridapPETSc</code> (for the linear solver step). The mesh file can be found <a href="https://github.com/gridap/Tutorials/blob/master/models/demo.msh">here</a>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Gridap
using GridapGmsh
using GridapPETSc
using GridapDistributed
using PartitionedArrays
n = 6
with_backend(MPIBackend(),n) do parts
  options = &quot;-ksp_type cg -pc_type gamg -ksp_monitor&quot;
  GridapPETSc.with(args=split(options)) do
    model = GmshDiscreteModel(parts,&quot;demo.msh&quot;)
    order = 1
    dirichlet_tags = [&quot;boundary1&quot;,&quot;boundary2&quot;]
    u_boundary1(x) = 0.0
    u_boundary2(x) = 1.0
    reffe = ReferenceFE(lagrangian,Float64,order)
    V = TestFESpace(model,reffe,dirichlet_tags=dirichlet_tags)
    U = TrialFESpace(V,[u_boundary1,u_boundary2])
    Ω = Interior(model)
    dΩ = Measure(Ω,2*order)
    a(u,v) = ∫( ∇(u)⋅∇(v) )dΩ
    l(v) = 0
    op = AffineFEOperator(a,l,U,V)
    solver = PETScLinearSolver()
    uh = solve(solver,op)
    writevtk(Ω,&quot;demo&quot;,cellfields=[&quot;uh&quot;=&gt;uh])
  end
end"><pre><span class="pl-k">using</span> Gridap
<span class="pl-k">using</span> GridapGmsh
<span class="pl-k">using</span> GridapPETSc
<span class="pl-k">using</span> GridapDistributed
<span class="pl-k">using</span> PartitionedArrays
n <span class="pl-k">=</span> <span class="pl-c1">6</span>
<span class="pl-c1">with_backend</span>(<span class="pl-c1">MPIBackend</span>(),n) <span class="pl-k">do</span> parts
  options <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>-ksp_type cg -pc_type gamg -ksp_monitor<span class="pl-pds">"</span></span>
  GridapPETSc<span class="pl-k">.</span><span class="pl-c1">with</span>(args<span class="pl-k">=</span><span class="pl-c1">split</span>(options)) <span class="pl-k">do</span>
    model <span class="pl-k">=</span> <span class="pl-c1">GmshDiscreteModel</span>(parts,<span class="pl-s"><span class="pl-pds">"</span>demo.msh<span class="pl-pds">"</span></span>)
    order <span class="pl-k">=</span> <span class="pl-c1">1</span>
    dirichlet_tags <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>boundary1<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>boundary2<span class="pl-pds">"</span></span>]
    <span class="pl-en">u_boundary1</span>(x) <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
    <span class="pl-en">u_boundary2</span>(x) <span class="pl-k">=</span> <span class="pl-c1">1.0</span>
    reffe <span class="pl-k">=</span> <span class="pl-c1">ReferenceFE</span>(lagrangian,Float64,order)
    V <span class="pl-k">=</span> <span class="pl-c1">TestFESpace</span>(model,reffe,dirichlet_tags<span class="pl-k">=</span>dirichlet_tags)
    U <span class="pl-k">=</span> <span class="pl-c1">TrialFESpace</span>(V,[u_boundary1,u_boundary2])
    Ω <span class="pl-k">=</span> <span class="pl-c1">Interior</span>(model)
    dΩ <span class="pl-k">=</span> <span class="pl-c1">Measure</span>(Ω,<span class="pl-c1">2</span><span class="pl-k">*</span>order)
    <span class="pl-en">a</span>(u,v) <span class="pl-k">=</span> <span class="pl-c1">∫</span>( <span class="pl-c1">∇</span>(u)<span class="pl-k">⋅</span><span class="pl-c1">∇</span>(v) )dΩ
    <span class="pl-en">l</span>(v) <span class="pl-k">=</span> <span class="pl-c1">0</span>
    op <span class="pl-k">=</span> <span class="pl-c1">AffineFEOperator</span>(a,l,U,V)
    solver <span class="pl-k">=</span> <span class="pl-c1">PETScLinearSolver</span>()
    uh <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver,op)
    <span class="pl-c1">writevtk</span>(Ω,<span class="pl-s"><span class="pl-pds">"</span>demo<span class="pl-pds">"</span></span>,cellfields<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>uh<span class="pl-pds">"</span></span><span class="pl-k">=&gt;</span>uh])
  <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<h2 dir="auto"><a id="user-content-build" class="anchor" aria-hidden="true" href="#build"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Build</h2>
<p dir="auto">Before using <code>GridapDistributed.jl</code> package, one needs to build the <a href="https://github.com/JuliaParallel/MPI.jl"><code>MPI.jl</code></a> package. We refer to the main documentation of this package for configuration instructions.</p>
<h2 dir="auto"><a id="user-content-mpi-parallel-julia-script-execution-instructions" class="anchor" aria-hidden="true" href="#mpi-parallel-julia-script-execution-instructions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MPI-parallel Julia script execution instructions</h2>
<p dir="auto">In order to execute a MPI-parallel <code>GridapDistributed.jl</code> driver, we can leverage the <code>mpiexecjl</code> script provided by <code>MPI.jl</code>. (Click <a href="https://juliaparallel.github.io/MPI.jl/stable/configuration/#Julia-wrapper-for-mpiexec" rel="nofollow">here</a> for installation instructions). As an example, assuming that we are located on the root directory of <code>GridapDistributed.jl</code>,
an hypothetic MPI-parallel <code>GridapDistributed.jl</code> driver named <code>driver.jl</code> can be executed on 4 MPI tasks as:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="mpiexecjl --project=. -n 4 julia -J sys-image.so driver.jl"><pre class="notranslate"><code>mpiexecjl --project=. -n 4 julia -J sys-image.so driver.jl
</code></pre></div>
<p dir="auto">where <code>-J sys-image.so</code> is optional, but highly recommended in order to reduce JIT compilation times. Here, <code>sys-image.so</code> is assumed to be a Julia system image pre-generated for the driver at hand using the <a href="https://julialang.github.io/PackageCompiler.jl/dev/index.html" rel="nofollow"><code>PackageCompiler.jl</code></a> package. See the <code>test/TestApp/compile</code> folder for example scripts with system image generation along with a test application with source available at <code>test/TestApp/</code>. These scripts are triggered from <code>.github/workflows/ci.yml</code> file on Github CI actions.</p>
<h2 dir="auto"><a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Known issues</h2>
<p dir="auto">A warning when executing MPI-parallel drivers: Data race conditions in the generation of precompiled modules in cache. See <a href="https://juliaparallel.github.io/MPI.jl/stable/knownissues/" rel="nofollow">here</a>.</p>
<h2 dir="auto"><a id="user-content-how-to-cite-gridapdistributed" class="anchor" aria-hidden="true" href="#how-to-cite-gridapdistributed"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to cite GridapDistributed</h2>
<p dir="auto">In order to give credit to the <code>Gridap</code> and <code>GridapDistributed</code> contributors, we simply ask you to cite the <code>Gridap</code> main project as indicated <a href="https://github.com/gridap/Gridap.jl#how-to-cite-gridap">here</a> and the sub-packages you use as indicated in the corresponding repositories. Please, use the reference below in any publication in which you have made use of <code>GridapDistributed</code>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="@article{Badia2022,
  doi = {10.21105/joss.04157},
  url = {https://doi.org/10.21105/joss.04157},
  year = {2022},
  publisher = {The Open Journal},
  volume = {7},
  number = {74},
  pages = {4157},
  author = {Santiago Badia and Alberto F. Martín and Francesc Verdugo},
  title = {GridapDistributed: a massively parallel finite element toolbox in Julia},
  journal = {Journal of Open Source Software}
}"><pre class="notranslate"><code>@article{Badia2022,
  doi = {10.21105/joss.04157},
  url = {https://doi.org/10.21105/joss.04157},
  year = {2022},
  publisher = {The Open Journal},
  volume = {7},
  number = {74},
  pages = {4157},
  author = {Santiago Badia and Alberto F. Martín and Francesc Verdugo},
  title = {GridapDistributed: a massively parallel finite element toolbox in Julia},
  journal = {Journal of Open Source Software}
}
</code></pre></div>
<h2 dir="auto"><a id="user-content-contributing-to-gridapdistributed" class="anchor" aria-hidden="true" href="#contributing-to-gridapdistributed"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing to GridapDistributed</h2>
<p dir="auto">GridapDistributed is a collaborative project open to contributions. If you want to contribute, please take into account:</p>
<ul dir="auto">
<li>Before opening a PR with a significant contribution, contact the project administrators by <a href="https://github.com/gridap/GridapDistributed.jl/issues/new">opening an issue</a> describing what you are willing to implement. Wait for feedback from other community members.</li>
<li>We adhere to the contribution and code-of-conduct instructions of the Gridap.jl project, available <a href="https://github.com/gridap/Gridap.jl/blob/master/CONTRIBUTING.md">here</a> and <a href="https://github.com/gridap/Gridap.jl/blob/master/CODE_OF_CONDUCT.md">here</a>, resp.  Please, carefully read and follow the instructions in these files.</li>
<li>Open a PR with your contribution.</li>
</ul>
<p dir="auto">Want to help? We have <a href="https://github.com/gridap/GridapDistributed.jl/labels/help%20wanted">issues waiting for help</a>. You can start contributing to the GridapDistributed project by solving some of those issues.</p>
</article></div>