<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-gridapdistributed" class="anchor" aria-hidden="true" href="#gridapdistributed"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GridapDistributed</h1>
<p><a href="https://gridap.github.io/GridapDistributed.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://gridap.github.io/GridapDistributed.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Gridap/GridapDistributed.jl/workflows/CI/badge.svg"><img src="https://github.com/Gridap/GridapDistributed.jl/workflows/CI/badge.svg" alt="CI" style="max-width:100%;"></a></p>
<p>Parallel distributed-memory version of <code>Gridap.jl</code>.  <g-emoji class="g-emoji" alias="construction" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a7.png">ðŸš§</g-emoji> work in progress <g-emoji class="g-emoji" alias="construction" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a7.png">ðŸš§</g-emoji></p>
<h2><a id="user-content-purpose" class="anchor" aria-hidden="true" href="#purpose"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Purpose</h2>
<p>This package is currently <strong>experimental, under development</strong>. In any case, the final purpose is to provide programming paradigm-neutral, parallel finite element data structures for distributed computing environments. This feature implies that communication among tasks are not tailored for a particular programming model, and thus can be leveraged with, e.g., MPI or the master-worker programming model built-in in Julia. Whenever one sticks to MPI as the underlying communication layer,  <code>GridapDistributed.jl</code> leverages the suite of tools available in the PETSc software package for the assembly and solution of distributed discrete systems of equations.</p>
<h2><a id="user-content-build" class="anchor" aria-hidden="true" href="#build"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Build</h2>
<p>Before using <code>GridapDistributed.jl</code> package, we have to build <code>MPI.jl</code> and <code>GridapDistributedPETScWrappers.jl</code>. We refer to the main <a href="https://github.com/gridap/GridapDistributedPETScWrappers.jl"><code>README.md</code></a> of the latter for configuration instructions.</p>
<h2><a id="user-content-mpi-parallel-julia-script-execution-instructions" class="anchor" aria-hidden="true" href="#mpi-parallel-julia-script-execution-instructions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MPI-parallel Julia script execution instructions</h2>
<p>In order to execute a MPI-parallel <code>GridapDistributed.jl</code> driver, we have first to figure out the path of the <code>mpirun</code> script corresponding to the MPI library with which <code>MPI.jl</code> was built. In order to do so, we can run the following commands from the root directory of  <code>GridapDistributed.jl</code> git repo:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="$ julia --project=. -e &quot;using MPI;println(MPI.mpiexec_path)&quot; 
/home/amartin/.julia/artifacts/2fcd463fb9498f362be9d1c4ef70a63c920b0e96/bin/mpiexec
"><pre><code>$ julia --project=. -e "using MPI;println(MPI.mpiexec_path)" 
/home/amartin/.julia/artifacts/2fcd463fb9498f362be9d1c4ef70a63c920b0e96/bin/mpiexec
</code></pre></div>
<p>Alternatively, for convenience, one can assign the path of <code>mpirun</code> to an environment variable, i.e.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="$ export MPIRUN=$(julia --project=. -e &quot;using MPI;println(MPI.mpiexec_path)&quot;)
"><pre><code>$ export MPIRUN=$(julia --project=. -e "using MPI;println(MPI.mpiexec_path)")
</code></pre></div>
<p>As an example, the MPI-parallel <code>GridapDistributed.jl</code> driver <code>MPIPETScCommunicatorsTests.jl</code>, located in the <code>test</code> directory, can be executed as:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="$MPIRUN -np 2 julia --project=. -J ../Gridap.jl/compile/Gridapv0.14.1.so test/MPIPETScTests/MPIPETScCommunicatorsTests.jl
"><pre><code>$MPIRUN -np 2 julia --project=. -J ../Gridap.jl/compile/Gridapv0.14.1.so test/MPIPETScTests/MPIPETScCommunicatorsTests.jl
</code></pre></div>
<p>where <code>-J ../Gridap.jl/compile/Gridapv0.14.1.so</code> is optional, but highly recommended in order to reduce JIT compilation times. More details about how to generate this file can be found <a href="https://github.com/gridap/Gridap.jl/tree/julia_script_creation_system_custom_images/compile">here</a>.</p>
<p>Two big warnings when executing MPI-parallel drivers:</p>
<ul>
<li>
<p>Data race conditions associated to the generation of precompiled modules in cache. See <a href="https://juliaparallel.github.io/MPI.jl/stable/knownissues/" rel="nofollow">here</a>.</p>
</li>
<li>
<p>Each time that <code>GridapDistributed.jl</code> is modified, the first time that a parallel driver is executed, the program fails during MPI initialization. But the second, and subsequent times, it works ok. I still do not know the cause of the problem, but it is related to module precompilation as well.</p>
</li>
</ul>
</article></div>