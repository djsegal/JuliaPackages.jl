<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://travis-ci.org/haampie/COSMA.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9897800fb4fa127ba50e568a7e6e6fcbde3ee352443f6ea641f6a3829fae1652/68747470733a2f2f7472617669732d63692e6f72672f6861616d7069652f434f534d412e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/haampie/COSMA.jl.svg?branch=master" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-cosmajl-communication-optimal-matrix-matrix-multiplication-for-distributedarraysjl-over-mpi" class="anchor" aria-hidden="true" href="#cosmajl-communication-optimal-matrix-matrix-multiplication-for-distributedarraysjl-over-mpi"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>COSMA.jl communication optimal matrix-matrix multiplication for DistributedArrays.jl over MPI</h1>
<p dir="auto">COSMA.jl provides wrappers for <a href="https://github.com/eth-cscs/COSMA">eth-cscs/COSMA</a> to do communication-optimal matrix-matrix multiplication for DArray's of element types <code>Float32</code>, <code>Float64</code>, <code>ComplexF32</code> and <code>ComplexF64</code>.</p>
<p dir="auto">Install via the package manager</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;COSMA&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>COSMA<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">A typical prerequisite is to use MPIClusterManager to setup some MPI ranks and to load the package everywhere:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MPIClusterManagers, DistributedArrays, Distributed

manager = MPIManager(np = 6)
addprocs(manager)

@everywhere using COSMA

# Just on the host we have to configure the mapping of Julia's pids to MPI ranks (hopefully this can be removed in a later release)
COSMA.use_manager(manager)"><pre><span class="pl-k">using</span> MPIClusterManagers, DistributedArrays, Distributed

manager <span class="pl-k">=</span> <span class="pl-c1">MPIManager</span>(np <span class="pl-k">=</span> <span class="pl-c1">6</span>)
<span class="pl-c1">addprocs</span>(manager)

<span class="pl-c1">@everywhere</span> <span class="pl-k">using</span> COSMA

<span class="pl-c"><span class="pl-c">#</span> Just on the host we have to configure the mapping of Julia's pids to MPI ranks (hopefully this can be removed in a later release)</span>
COSMA<span class="pl-k">.</span><span class="pl-c1">use_manager</span>(manager)</pre></div>
<p dir="auto">Next create some distributed matrices and multiply them:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using LinearAlgebra

# Float64 matrices, automatically distributed over the MPI ranks
A = drand(100, 100)
B = drand(100, 100)

# Use DistributedArrays to allocate the new matrix C and multiply using COSMA
C = A * B

# Or allocate your own distributed target matrix C:
A_complex = drand(ComplexF32, 100, 100)
B_complex = drand(ComplexF32, 100, 100)
C_complex = dzeros(ComplexF32, 100, 100)

mul!(C_complex, A_complex, B_complex)"><pre><span class="pl-k">using</span> LinearAlgebra

<span class="pl-c"><span class="pl-c">#</span> Float64 matrices, automatically distributed over the MPI ranks</span>
A <span class="pl-k">=</span> <span class="pl-c1">drand</span>(<span class="pl-c1">100</span>, <span class="pl-c1">100</span>)
B <span class="pl-k">=</span> <span class="pl-c1">drand</span>(<span class="pl-c1">100</span>, <span class="pl-c1">100</span>)

<span class="pl-c"><span class="pl-c">#</span> Use DistributedArrays to allocate the new matrix C and multiply using COSMA</span>
C <span class="pl-k">=</span> A <span class="pl-k">*</span> B

<span class="pl-c"><span class="pl-c">#</span> Or allocate your own distributed target matrix C:</span>
A_complex <span class="pl-k">=</span> <span class="pl-c1">drand</span>(ComplexF32, <span class="pl-c1">100</span>, <span class="pl-c1">100</span>)
B_complex <span class="pl-k">=</span> <span class="pl-c1">drand</span>(ComplexF32, <span class="pl-c1">100</span>, <span class="pl-c1">100</span>)
C_complex <span class="pl-k">=</span> <span class="pl-c1">dzeros</span>(ComplexF32, <span class="pl-c1">100</span>, <span class="pl-c1">100</span>)

<span class="pl-c1">mul!</span>(C_complex, A_complex, B_complex)</pre></div>
<h2 dir="auto"><a id="user-content-using-a-custom-mpi-implementation" class="anchor" aria-hidden="true" href="#using-a-custom-mpi-implementation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Using a custom MPI implementation</h2>
<p dir="auto">COSMA.jl depends on MPI.jl, which ships MPICH as a default MPI library. If you need a system-specific version, see the instructions from the <a href="https://juliaparallel.github.io/MPI.jl/latest/configuration/" rel="nofollow">docs of MPI.jl</a>.</p>
<h2 dir="auto"><a id="user-content-notes-about-julias-darray-type" class="anchor" aria-hidden="true" href="#notes-about-julias-darray-type"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notes about Julia's DArray type</h2>
<p dir="auto">COSMA supports Julia's DArray matrix distribution perfectly, and is in fact more powerful: Julia's DArray supports only a single local block per MPI rank, whereas COSMA supports an arbitrary number of them.</p>
</article></div>