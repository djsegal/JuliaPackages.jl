<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-euclideandistancematrices" class="anchor" aria-hidden="true" href="#euclideandistancematrices"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>EuclideanDistanceMatrices</h1>
<p dir="auto"><a href="https://github.com/baggepinnen/EuclideanDistanceMatrices.jl/actions"><img src="https://github.com/baggepinnen/EuclideanDistanceMatrices.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/baggepinnen/EuclideanDistanceMatrices.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a28c181a4fe42cac428660691de52ad0b28e1b7b7467c5378b11f039b78d3b8f/68747470733a2f2f636f6465636f762e696f2f67682f626167676570696e6e656e2f4575636c696465616e44697374616e63654d617472696365732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/baggepinnen/EuclideanDistanceMatrices.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Utilities for working with matrices of squared Euclidean distances.</p>
<ul dir="auto">
<li><code>D̃,S = complete_distmat(D, W)</code>: Fills in missing entries in an incomplete and noisy squared distance matrix. <code>W</code> is a binary mask indicating available values. (Algorithm 5 from the reference below).</li>
<li><code>D̃,E = rankcomplete_distmat(D, W, dim)</code>: Same as above, but works on larger matrices and is less accurate. (Algorithm 2 from the reference below).</li>
<li><code>P = reconstruct_pointset(D, dim)</code> Takes a squared distance matrix or the SVD of one and reconstructs the set of points embedded in dimension <code>dim</code> that generated <code>D</code>; up to a translation and rotation/reflection. See <code>procrustes</code> for help with aligning the result to a collection of anchors.</li>
<li><code>R,t = procrustes(X, Y)</code> Find rotation matrix <code>R</code> and translation vector <code>t</code> such that <code>R*X .+ t ≈ Y</code></li>
<li><code>denoise_distmat(D, dim, p=2)</code> Takes a noisy squared distance matrix and returns a denoised version. <code>p</code> denotes the "norm" used in measuring the error. <code>p=2</code> assumes that the error is Gaussian, whereas <code>p=1</code> assumes that the error is large but sparse. The robust factorization comes from <a href="https://github.com/baggepinnen/TotalLeastSquares.jl/">TotalLeastSquares.jl</a>.</li>
<li><code>posterior</code> Estimate the posterior distribution of locations given both noisy location measurements and distance measurements (not squared), see more details below.</li>
</ul>
<h2 dir="auto"><a id="user-content-bayesian-estimation-of-locations" class="anchor" aria-hidden="true" href="#bayesian-estimation-of-locations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Bayesian estimation of locations</h2>
<h3 dir="auto"><a id="user-content-with-distance-measurements" class="anchor" aria-hidden="true" href="#with-distance-measurements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>With distance measurements</h3>
<p dir="auto">If both noisy position estimates and noisy distance measurements are available, we can estimate the full Bayesian posterior over positions. To this end, the function <code>psoterior</code> is avialable. We demonstrate how it's used with an example, and start by generating some sythetic data:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using EuclideanDistanceMatrices, Turing
N = 10    # Number of points
σL = 0.1  # Location noise std
σD = 0.01 # Distance noise std (measured in the same unit as positions)

P  = randn(2,N)                       # These are the true locations
Pn = P + σL*randn(size(P))            # Noisy locations
D  = pairwise(Euclidean(), P, dims=2) # True distance matrix (this function exoects distances, not squared distances).
Dn = D + σD*randn(size(D))            # Noisy distance matrix
Dn[diagind(Dn)] .= 0 # The diagonal is always 0

# We select a small number of distances to feed the algorithm, this corresponds to only some distances between points being measured
distances = []
p = 0.5 # probability of including a distance
for i = 1:N
    for j = i+1:N
        rand() &lt; p || continue
        push!(distances, (i,j,Dn[i,j]))
    end
end
@show length(distances)
@show expected_number_of_entries = p*((N^2-N)÷2)"><pre><span class="pl-k">using</span> EuclideanDistanceMatrices, Turing
N <span class="pl-k">=</span> <span class="pl-c1">10</span>    <span class="pl-c"><span class="pl-c">#</span> Number of points</span>
σL <span class="pl-k">=</span> <span class="pl-c1">0.1</span>  <span class="pl-c"><span class="pl-c">#</span> Location noise std</span>
σD <span class="pl-k">=</span> <span class="pl-c1">0.01</span> <span class="pl-c"><span class="pl-c">#</span> Distance noise std (measured in the same unit as positions)</span>

P  <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">2</span>,N)                       <span class="pl-c"><span class="pl-c">#</span> These are the true locations</span>
Pn <span class="pl-k">=</span> P <span class="pl-k">+</span> σL<span class="pl-k">*</span><span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(P))            <span class="pl-c"><span class="pl-c">#</span> Noisy locations</span>
D  <span class="pl-k">=</span> <span class="pl-c1">pairwise</span>(<span class="pl-c1">Euclidean</span>(), P, dims<span class="pl-k">=</span><span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> True distance matrix (this function exoects distances, not squared distances).</span>
Dn <span class="pl-k">=</span> D <span class="pl-k">+</span> σD<span class="pl-k">*</span><span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(D))            <span class="pl-c"><span class="pl-c">#</span> Noisy distance matrix</span>
Dn[<span class="pl-c1">diagind</span>(Dn)] <span class="pl-k">.=</span> <span class="pl-c1">0</span> <span class="pl-c"><span class="pl-c">#</span> The diagonal is always 0</span>

<span class="pl-c"><span class="pl-c">#</span> We select a small number of distances to feed the algorithm, this corresponds to only some distances between points being measured</span>
distances <span class="pl-k">=</span> []
p <span class="pl-k">=</span> <span class="pl-c1">0.5</span> <span class="pl-c"><span class="pl-c">#</span> probability of including a distance</span>
<span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span>N
    <span class="pl-k">for</span> j <span class="pl-k">=</span> i<span class="pl-k">+</span><span class="pl-c1">1</span><span class="pl-k">:</span>N
        <span class="pl-c1">rand</span>() <span class="pl-k">&lt;</span> p <span class="pl-k">||</span> <span class="pl-k">continue</span>
        <span class="pl-c1">push!</span>(distances, (i,j,Dn[i,j]))
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c1">@show</span> <span class="pl-c1">length</span>(distances)
<span class="pl-c1">@show</span> expected_number_of_entries <span class="pl-k">=</span> p<span class="pl-k">*</span>((N<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>N)<span class="pl-k">÷</span><span class="pl-c1">2</span>)</pre></div>
<p dir="auto">Given the locations <code>P</code> and <code>distances</code> (vector of tuples with indices and distances), we can now estimate the posterior:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="part, chain = posterior(
    Pn,
    distances;
    nsamples = 2000,
    sampler = NUTS(),
    σL = σL, # This can also be a vector of std:s for each location, see ?MvNormal for alternatives
    σD = σD  # This can also be a vector of std:s for each location, see ?MvNormal for alternatives
)"><pre>part, chain <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(
    Pn,
    distances;
    nsamples <span class="pl-k">=</span> <span class="pl-c1">2000</span>,
    sampler <span class="pl-k">=</span> <span class="pl-c1">NUTS</span>(),
    σL <span class="pl-k">=</span> σL, <span class="pl-c"><span class="pl-c">#</span> This can also be a vector of std:s for each location, see ?MvNormal for alternatives</span>
    σD <span class="pl-k">=</span> σD  <span class="pl-c"><span class="pl-c">#</span> This can also be a vector of std:s for each location, see ?MvNormal for alternatives</span>
)</pre></div>
<p dir="auto">The returned object <code>part</code> is a named tuple containing all the internal variables that were sampled. The fields are of type <code>Particles</code> from <a href="https://github.com/baggepinnen/MonteCarloMeasurements.jl">MonteCarloMeasurements.jl</a>, representing the full posterior distribution of each quantity. The interesting fields are <code>part.P</code> which contains the posterior positions, and <code>part.d</code> which contains the estimated distances. The object <code>chain</code> contains the same information as <code>part</code>, but in the form of a <code>Turing.Chain</code> object.</p>
<p dir="auto">Note that the number of samples in the posterior will not be the same as the number requested by <code>nsamples</code> since Turing automatically drops bad samples etc.</p>
<p dir="auto">We can verify that the estimated locations are closer to the true locations than the ones provided by the measurements alone, and plot the results</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="norm(mean.(part.P) - P) &lt; norm(Pn - P)

scatter(part.P[1,:], part.P[2,:], markersize=6)
scatter!(P[1,:], P[2,:], lab=&quot;True positions&quot;)
scatter!(Pn[1,:], Pn[2,:], lab=&quot;Measured positions&quot;)"><pre><span class="pl-c1">norm</span>(<span class="pl-c1">mean</span>.(part<span class="pl-k">.</span>P) <span class="pl-k">-</span> P) <span class="pl-k">&lt;</span> <span class="pl-c1">norm</span>(Pn <span class="pl-k">-</span> P)

<span class="pl-c1">scatter</span>(part<span class="pl-k">.</span>P[<span class="pl-c1">1</span>,:], part<span class="pl-k">.</span>P[<span class="pl-c1">2</span>,:], markersize<span class="pl-k">=</span><span class="pl-c1">6</span>)
<span class="pl-c1">scatter!</span>(P[<span class="pl-c1">1</span>,:], P[<span class="pl-c1">2</span>,:], lab<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>True positions<span class="pl-pds">"</span></span>)
<span class="pl-c1">scatter!</span>(Pn[<span class="pl-c1">1</span>,:], Pn[<span class="pl-c1">2</span>,:], lab<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Measured positions<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="figs/posterior.svg"><img src="figs/posterior.svg" alt="posterior" style="max-width: 100%;"></a></p>
<p dir="auto">Under the hood, <a href="https://turing.ml/dev/" rel="nofollow">Turing.jl</a> is used to sample from the posterior. If you have a lot of points, it will take a while to run this function. If the sampling takes too long time, you may try estimating an MAP estimate instead. To do this, run <code>using Optim</code> and then pass <code>sampler = MAP()</code>. More docs on MAP estimation is found <a href="https://turing.ml/dev/docs/using-turing/guide#maximum-likelihood-and-maximum-a-posterior-estimates" rel="nofollow">here</a>.</p>
<h3 dir="auto"><a id="user-content-with-tdoa-measurements-time-difference-of-arrival-ie-differences-of-distances" class="anchor" aria-hidden="true" href="#with-tdoa-measurements-time-difference-of-arrival-ie-differences-of-distances"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>With TDOA measurements (time difference of arrival, i.e., differences of distances)</h3>
<p dir="auto">In this setting, we add one location in the matrix of locations, corresponding to the location of the source that generated the ping.
We then set the keyword <code>tdoa=true</code> when calling <code>posterior</code>, and let the vector of <code>(i, j, dist)</code> instead be <code>(i,j,tdoa)</code>. Below is a similar example to the one above, but adapted to this setting.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="N      = 10                      # Number of points
# The standard deviations below can also be supplied as vectors with one element per location
σL     = 0.1                     # Location noise std
σD     = 0.01                    # TDOA noise std (measured in the same unit as positions)
P      = 3randn(2, N)            # These are the true locations
source = randn(2)                # The true source location
Pn     = P + σL * randn(size(P)) # Noisy locations
tdoas  = []
noisy_tdoas = []
p = 0.5 # probability of including a TDOA
for i = 1:N
    for j = i+1:N
        if rand() &lt; p
            di = norm(P[:, i] - source) # Distance from source to i
            dj = norm(P[:, j] - source) # Distance from source to j
            tdoa = di - dj              # This is the predicted TDOA given the posterior locations
            push!(tdoas, (i, j, tdoa))
            push!(noisy_tdoas, (i, j, tdoa + σD * randn()))
        end
    end
end
@show length(tdoas)
@show expected = p * ((N^2 - N) ÷ 2)


part, chain = posterior(
    [Pn source], # We add the source location to the end of this matrix
    noisy_tdoas;
    nsamples = 2000,
    sampler  = NUTS(),
    σL       = σL,
    σD       = σD, # This can also be a vector of std:s for each location, see ?MvNormal for alternatives
    tdoa     = true, # Indicating that we are providing TDOA measurements
)

norm(mean.(part.P[:, 1:end-1]) - P) &lt; norm(Pn - P)"><pre>N      <span class="pl-k">=</span> <span class="pl-c1">10</span>                      <span class="pl-c"><span class="pl-c">#</span> Number of points</span>
<span class="pl-c"><span class="pl-c">#</span> The standard deviations below can also be supplied as vectors with one element per location</span>
σL     <span class="pl-k">=</span> <span class="pl-c1">0.1</span>                     <span class="pl-c"><span class="pl-c">#</span> Location noise std</span>
σD     <span class="pl-k">=</span> <span class="pl-c1">0.01</span>                    <span class="pl-c"><span class="pl-c">#</span> TDOA noise std (measured in the same unit as positions)</span>
P      <span class="pl-k">=</span> <span class="pl-c1">3</span><span class="pl-c1">randn</span>(<span class="pl-c1">2</span>, N)            <span class="pl-c"><span class="pl-c">#</span> These are the true locations</span>
source <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">2</span>)                <span class="pl-c"><span class="pl-c">#</span> The true source location</span>
Pn     <span class="pl-k">=</span> P <span class="pl-k">+</span> σL <span class="pl-k">*</span> <span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(P)) <span class="pl-c"><span class="pl-c">#</span> Noisy locations</span>
tdoas  <span class="pl-k">=</span> []
noisy_tdoas <span class="pl-k">=</span> []
p <span class="pl-k">=</span> <span class="pl-c1">0.5</span> <span class="pl-c"><span class="pl-c">#</span> probability of including a TDOA</span>
<span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span>N
    <span class="pl-k">for</span> j <span class="pl-k">=</span> i<span class="pl-k">+</span><span class="pl-c1">1</span><span class="pl-k">:</span>N
        <span class="pl-k">if</span> <span class="pl-c1">rand</span>() <span class="pl-k">&lt;</span> p
            di <span class="pl-k">=</span> <span class="pl-c1">norm</span>(P[:, i] <span class="pl-k">-</span> source) <span class="pl-c"><span class="pl-c">#</span> Distance from source to i</span>
            dj <span class="pl-k">=</span> <span class="pl-c1">norm</span>(P[:, j] <span class="pl-k">-</span> source) <span class="pl-c"><span class="pl-c">#</span> Distance from source to j</span>
            tdoa <span class="pl-k">=</span> di <span class="pl-k">-</span> dj              <span class="pl-c"><span class="pl-c">#</span> This is the predicted TDOA given the posterior locations</span>
            <span class="pl-c1">push!</span>(tdoas, (i, j, tdoa))
            <span class="pl-c1">push!</span>(noisy_tdoas, (i, j, tdoa <span class="pl-k">+</span> σD <span class="pl-k">*</span> <span class="pl-c1">randn</span>()))
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
<span class="pl-c1">@show</span> <span class="pl-c1">length</span>(tdoas)
<span class="pl-c1">@show</span> expected <span class="pl-k">=</span> p <span class="pl-k">*</span> ((N<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">-</span> N) <span class="pl-k">÷</span> <span class="pl-c1">2</span>)


part, chain <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(
    [Pn source], <span class="pl-c"><span class="pl-c">#</span> We add the source location to the end of this matrix</span>
    noisy_tdoas;
    nsamples <span class="pl-k">=</span> <span class="pl-c1">2000</span>,
    sampler  <span class="pl-k">=</span> <span class="pl-c1">NUTS</span>(),
    σL       <span class="pl-k">=</span> σL,
    σD       <span class="pl-k">=</span> σD, <span class="pl-c"><span class="pl-c">#</span> This can also be a vector of std:s for each location, see ?MvNormal for alternatives</span>
    tdoa     <span class="pl-k">=</span> <span class="pl-c1">true</span>, <span class="pl-c"><span class="pl-c">#</span> Indicating that we are providing TDOA measurements</span>
)

<span class="pl-c1">norm</span>(<span class="pl-c1">mean</span>.(part<span class="pl-k">.</span>P[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>]) <span class="pl-k">-</span> P) <span class="pl-k">&lt;</span> <span class="pl-c1">norm</span>(Pn <span class="pl-k">-</span> P)</pre></div>
<p dir="auto">Once again, we visualize the resulting estimate</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="scatter(part.P[1, 1:end-1], part.P[2, 1:end-1], markersize = 6)
scatter!(P[1, :],  P[2, :],  lab = &quot;True positions&quot;)
scatter!(Pn[1, :], Pn[2, :], lab = &quot;Measured positions&quot;)
scatter!(
    [part.P[1, end]],
    [part.P[2, end]],
    m = (:x, 8),
    lab = &quot;Est. Source&quot;,
)
scatter!([source[1]], [source[2]], m = (:x, 8), lab = &quot;True Source&quot;) |&gt; display"><pre><span class="pl-c1">scatter</span>(part<span class="pl-k">.</span>P[<span class="pl-c1">1</span>, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>], part<span class="pl-k">.</span>P[<span class="pl-c1">2</span>, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>], markersize <span class="pl-k">=</span> <span class="pl-c1">6</span>)
<span class="pl-c1">scatter!</span>(P[<span class="pl-c1">1</span>, :],  P[<span class="pl-c1">2</span>, :],  lab <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>True positions<span class="pl-pds">"</span></span>)
<span class="pl-c1">scatter!</span>(Pn[<span class="pl-c1">1</span>, :], Pn[<span class="pl-c1">2</span>, :], lab <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Measured positions<span class="pl-pds">"</span></span>)
<span class="pl-c1">scatter!</span>(
    [part<span class="pl-k">.</span>P[<span class="pl-c1">1</span>, <span class="pl-c1">end</span>]],
    [part<span class="pl-k">.</span>P[<span class="pl-c1">2</span>, <span class="pl-c1">end</span>]],
    m <span class="pl-k">=</span> (<span class="pl-c1">:x</span>, <span class="pl-c1">8</span>),
    lab <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Est. Source<span class="pl-pds">"</span></span>,
)
<span class="pl-c1">scatter!</span>([source[<span class="pl-c1">1</span>]], [source[<span class="pl-c1">2</span>]], m <span class="pl-k">=</span> (<span class="pl-c1">:x</span>, <span class="pl-c1">8</span>), lab <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>True Source<span class="pl-pds">"</span></span>) <span class="pl-k">|&gt;</span> display</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="figs/posterior_tdoa.svg"><img src="figs/posterior_tdoa.svg" alt="posterior_tdoa" style="max-width: 100%;"></a></p>
<h3 dir="auto"><a id="user-content-relative-vs-absolute-estimates" class="anchor" aria-hidden="true" href="#relative-vs-absolute-estimates"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Relative vs. Absolute estimates</h3>
<p dir="auto">The function <code>posterior</code> estimates the <em>absolute</em> positions of the sensors in the coordinate system used to provide the location measurements. Oftentimes, the relative positions between the sensors are sufficient, and are also easier to estimate. Estimates of the relative positions are available in the resulting samples from the posterior distribution, but hidden within the samples. If we draw 2000 samples from the posterior, the absolute coordinates of each sample can be aligned to the mean of all samples (using <code>procrustes</code>), after which 2000 samples of the relative positions are available. This relative estimate will have lower variance than the absolute estimate. To facilitate this alignment, we have the function</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="P_relative = align_to_mean(part.P)
tr(cov(vec(part.P))) &gt; tr(cov(vec(P_relative))) # Test that the covariance matrix is &quot;smaller&quot;"><pre>P_relative <span class="pl-k">=</span> <span class="pl-c1">align_to_mean</span>(part<span class="pl-k">.</span>P)
<span class="pl-c1">tr</span>(<span class="pl-c1">cov</span>(<span class="pl-c1">vec</span>(part<span class="pl-k">.</span>P))) <span class="pl-k">&gt;</span> <span class="pl-c1">tr</span>(<span class="pl-c1">cov</span>(<span class="pl-c1">vec</span>(P_relative))) <span class="pl-c"><span class="pl-c">#</span> Test that the covariance matrix is "smaller"</span></pre></div>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
pkg&quot;add https://github.com/baggepinnen/EuclideanDistanceMatrices.jl&quot;"><pre><span class="pl-k">using</span> Pkg
<span class="pl-s"><span class="pl-pds"><span class="pl-c1">pkg</span>"</span>add https://github.com/baggepinnen/EuclideanDistanceMatrices.jl<span class="pl-pds">"</span></span></pre></div>
<h3 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h3>
<p dir="auto">Most of the algorithms implemented in this package are described in the excellent paper
"Euclidean Distance Matrices: Essential Theory, Algorithms and Applications"
Ivan Dokmanic, Reza Parhizkar, Juri Ranieri and Martin Vetterli<br>
<a href="https://arxiv.org/pdf/1502.07541.pdf" rel="nofollow">https://arxiv.org/pdf/1502.07541.pdf</a></p>
</article></div>