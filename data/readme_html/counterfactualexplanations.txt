<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="docs/src/assets/wide_logo.png"><img src="docs/src/assets/wide_logo.png" alt="" style="max-width: 100%;"></a></p>
<p dir="auto"><em>Counterfactual Explanations and Algorithmic Recourse in Julia.</em></p>
<p dir="auto"><a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a> <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a> <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status" style="max-width: 100%;"></a> <a href="https://codecov.io/gh/juliatrustworthyai/CounterfactualExplanations.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/273f1b92bfdaa9399425e61b411fe8092b1f0fbd14d22959009426fb18069e4b/68747470733a2f2f636f6465636f762e696f2f67682f6a756c69617472757374776f7274687961692f436f756e7465726661637475616c4578706c616e6174696f6e732e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/juliatrustworthyai/CounterfactualExplanations.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a> <a href="https://github.com/invenia/BlueStyle"><img src="https://camo.githubusercontent.com/c18fbaa52d94d16b90b19701fc90d289b8a5bb920c74c79bab200b14e75420a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c75652d3434393564312e737667" alt="Code Style: Blue" data-canonical-src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" style="max-width: 100%;"></a> <a href="LICENSE"><img src="https://camo.githubusercontent.com/39994881d7518b7a3fbe4ae57bed3e12662b39d1bad309982d536a7875ad309d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a756c69617472757374776f7274687961692f436f756e7465726661637475616c4578706c616e6174696f6e732e6a6c" alt="License" data-canonical-src="https://img.shields.io/github/license/juliatrustworthyai/CounterfactualExplanations.jl" style="max-width: 100%;"></a> <a href="https://pkgs.genieframework.com?packages=CounterfactualExplanations" rel="nofollow"><img src="https://camo.githubusercontent.com/bc6207dc7a3e203b5c450867a78bb8a85078e677591adc4f04f7ac70363d3213/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f706b67732e67656e69656672616d65776f726b2e636f6d2f6170692f76312f62616467652f436f756e7465726661637475616c4578706c616e6174696f6e732f2e706e67" alt="Package Downloads" data-canonical-src="https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/CounterfactualExplanations/.png" style="max-width: 100%;"></a></p>
<p dir="auto"><code>CounterfactualExplanations.jl</code> is a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box algorithms. Both CE and AR are related tools for explainable artificial intelligence (XAI). While the package is written purely in Julia, it can be used to explain machine learning algorithms developed and trained in other popular programming languages like Python and R. See below for a short introduction and other resources or dive straight into the <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/dev" rel="nofollow">docs</a>.</p>
<h2 dir="auto"><a id="user-content--installation" class="anchor" aria-hidden="true" href="#-installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="triangular_flag_on_post" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a9.png">üö©</g-emoji> Installation</h2>
<p dir="auto">You can install the stable release from <a href="https://github.com/JuliaRegistries/General">Julia‚Äôs General Registry</a> as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;CounterfactualExplanations&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>CounterfactualExplanations<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto"><code>CounterfactualExplanations.jl</code> is under active development. To install the development version of the package you can run the following command:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(url=&quot;https://github.com/juliatrustworthyai/CounterfactualExplanations.jl&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(url<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>https://github.com/juliatrustworthyai/CounterfactualExplanations.jl<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content--background-and-motivation" class="anchor" aria-hidden="true" href="#-background-and-motivation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="thinking" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png">ü§î</g-emoji> Background and Motivation</h2>
<p dir="auto">Machine learning models like Deep Neural Networks have become so complex, opaque and underspecified in the data that they are generally considered Black Boxes. Nonetheless, such models often play a key role in data-driven decision-making systems. This creates the following problem: human operators in charge of such systems have to rely on them blindly, while those individuals subject to them generally have no way of challenging an undesirable outcome:</p>
<blockquote>
<p dir="auto">‚ÄúYou cannot appeal to (algorithms). They do not listen. Nor do they bend.‚Äù</p>
<p dir="auto">‚Äî Cathy O‚ÄôNeil in <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction" rel="nofollow"><em>Weapons of Math Destruction</em></a>, 2016</p>
</blockquote>
<h2 dir="auto"><a id="user-content--enter-counterfactual-explanations" class="anchor" aria-hidden="true" href="#-enter-counterfactual-explanations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="crystal_ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52e.png">üîÆ</g-emoji> Enter: Counterfactual Explanations</h2>
<p dir="auto">Counterfactual Explanations can help human stakeholders make sense of the systems they develop, use or endure: they explain how inputs into a system need to change for it to produce different decisions. Explainability benefits internal as well as external quality assurance.</p>
<p dir="auto">Counterfactual Explanations have a few properties that are desirable in the context of Explainable Artificial Intelligence (XAI). These include:</p>
<ul dir="auto">
<li>Full fidelity to the black-box model, since no proxy is involved.</li>
<li>No need for (reasonably) interpretable features as opposed to LIME and SHAP.</li>
<li>Clear link to Algorithmic Recourse and Causal Inference.</li>
<li>Less susceptible to adversarial attacks than LIME and SHAP.</li>
</ul>
<h3 dir="auto"><a id="user-content-example-give-me-some-credit" class="anchor" aria-hidden="true" href="#example-give-me-some-credit"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example: Give Me Some Credit</h3>
<p dir="auto">Consider the following real-world scenario: a retail bank is using a black-box model trained on their clients‚Äô credit history to decide whether they will provide credit to new applicants. To simulate this scenario, we have pre-trained a binary classifier on the publicly available Give Me Some Credit dataset that ships with this package (Kaggle 2011).</p>
<p dir="auto">The figure below shows counterfactuals for 10 randomly chosen individuals that would have been denied credit initially.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="README_files/figure-commonmark/cell-5-output-1.svg"><img src="README_files/figure-commonmark/cell-5-output-1.svg" alt="" style="max-width: 100%;"></a></p>
<h3 dir="auto"><a id="user-content-example-mnist" class="anchor" aria-hidden="true" href="#example-mnist"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example: MNIST</h3>
<p dir="auto">The figure below shows a counterfactual generated for an image classifier trained on MNIST: in particular, it demonstrates which pixels need to change in order for the classifier to predict 4 instead of 9.</p>
<p dir="auto">Since <code>v0.1.9</code> counterfactual generators are fully composable. Here we have composed a generator that combines ideas from Joshi et al. (2019) (REVISE) and Schut et al. (2021):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Compose generator:
generator = Generator()
@chain generator begin
    @objective logitcrossentropy + 0.001distance_l2     
    @with_optimiser JSMADescent(Œ∑=0.5)                  # Greedy (Schut et al. 2021)
    @search_latent_space                                # REVISE (Joshi et al. 2019)
end"><pre><span class="pl-c"><span class="pl-c">#</span> Compose generator:</span>
generator <span class="pl-k">=</span> <span class="pl-c1">Generator</span>()
<span class="pl-c1">@chain</span> generator <span class="pl-k">begin</span>
    <span class="pl-c1">@objective</span> logitcrossentropy <span class="pl-k">+</span> <span class="pl-c1">0.001</span>distance_l2     
    <span class="pl-c1">@with_optimiser</span> <span class="pl-c1">JSMADescent</span>(Œ∑<span class="pl-k">=</span><span class="pl-c1">0.5</span>)                  <span class="pl-c"><span class="pl-c">#</span> Greedy (Schut et al. 2021)</span>
    <span class="pl-c1">@search_latent_space</span>                                <span class="pl-c"><span class="pl-c">#</span> REVISE (Joshi et al. 2019)</span>
<span class="pl-k">end</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="README_files/figure-commonmark/cell-11-output-1.svg"><img src="README_files/figure-commonmark/cell-11-output-1.svg" alt="" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content--usage-example" class="anchor" aria-hidden="true" href="#-usage-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="mag" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png">üîç</g-emoji> Usage example</h2>
<p dir="auto">Generating counterfactuals will typically look like follows.</p>
<p dir="auto">We have some pre-trained model that was fitted to data:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Data and Classifier:
counterfactual_data = load_linearly_separable(1000)
M = fit_model(counterfactual_data, :Linear)"><pre><span class="pl-c"><span class="pl-c">#</span> Data and Classifier:</span>
counterfactual_data <span class="pl-k">=</span> <span class="pl-c1">load_linearly_separable</span>(<span class="pl-c1">1000</span>)
M <span class="pl-k">=</span> <span class="pl-c1">fit_model</span>(counterfactual_data, <span class="pl-c1">:Linear</span>)</pre></div>
<p dir="auto">For some individual and target outcome, we want to understand what a valid counterfactual in the target class looks like:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Randomly selected factual:
x = select_factual(counterfactual_data,rand(1:size(counterfactual_data.X,2)))
y = predict_label(M, counterfactual_data, x)[1]
target = counterfactual_data.y_levels[counterfactual_data.y_levels .!= y][1]"><pre><span class="pl-c"><span class="pl-c">#</span> Randomly selected factual:</span>
x <span class="pl-k">=</span> <span class="pl-c1">select_factual</span>(counterfactual_data,<span class="pl-c1">rand</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(counterfactual_data<span class="pl-k">.</span>X,<span class="pl-c1">2</span>)))
y <span class="pl-k">=</span> <span class="pl-c1">predict_label</span>(M, counterfactual_data, x)[<span class="pl-c1">1</span>]
target <span class="pl-k">=</span> counterfactual_data<span class="pl-k">.</span>y_levels[counterfactual_data<span class="pl-k">.</span>y_levels <span class="pl-k">.!=</span> y][<span class="pl-c1">1</span>]</pre></div>
<p dir="auto">To this end, we specify a counterfactual generator of our choice:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Counterfactual search:
generator = DiCEGenerator(
  opt = Descent(0.01)
)"><pre><span class="pl-c"><span class="pl-c">#</span> Counterfactual search:</span>
generator <span class="pl-k">=</span> <span class="pl-c1">DiCEGenerator</span>(
  opt <span class="pl-k">=</span> <span class="pl-c1">Descent</span>(<span class="pl-c1">0.01</span>)
)</pre></div>
<p dir="auto">Here, we have chosen to use the <code>Generator</code> to move the individual from its factual label 1 to the target label 2.</p>
<p dir="auto">With all of our ingredients specified, we finally generate counterfactuals using a simple API call:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="ce = generate_counterfactual(
  x, target, counterfactual_data, M, generator; 
  num_counterfactuals=3, converge_when=:generator_conditions,
  gradient_tol=1e-3
)"><pre>ce <span class="pl-k">=</span> <span class="pl-c1">generate_counterfactual</span>(
  x, target, counterfactual_data, M, generator; 
  num_counterfactuals<span class="pl-k">=</span><span class="pl-c1">3</span>, converge_when<span class="pl-k">=</span><span class="pl-c1">:generator_conditions</span>,
  gradient_tol<span class="pl-k">=</span><span class="pl-c1">1e-3</span>
)</pre></div>
<p dir="auto">The animation below shows the resulting counterfactual path:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="www/intro.gif"><img src="www/intro.gif" alt="" data-animated-image="" style="max-width: 100%;"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="README_files/intro.gif"><img src="README_files/intro.gif" alt="" data-animated-image="" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-Ô∏è-implemented-counterfactual-generators" class="anchor" aria-hidden="true" href="#Ô∏è-implemented-counterfactual-generators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="ballot_box_with_check" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2611.png">‚òëÔ∏è</g-emoji> Implemented Counterfactual Generators</h2>
<p dir="auto">Currently, the following counterfactual generators are implemented:</p>
<ul dir="auto">
<li>Generic (Wachter, Mittelstadt, and Russell 2017)</li>
<li>Greedy (Schut et al. 2021)</li>
<li>DiCE (Mothilal, Sharma, and Tan 2020)</li>
<li>Latent Space Search as in REVISE (Joshi et al. 2019) and CLUE (Antor√°n et al. 2020)</li>
<li>ClaPROAR (Altmeyer et al. 2023)</li>
<li>GravitationalGenerator (Altmeyer et al. 2023)</li>
</ul>
<h2 dir="auto"><a id="user-content--goals-and-limitations" class="anchor" aria-hidden="true" href="#-goals-and-limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png">üéØ</g-emoji> Goals and limitations</h2>
<p dir="auto">The goal of this library is to contribute to efforts towards trustworthy machine learning in Julia. The Julia language has an edge when it comes to trustworthiness: it is very transparent. Packages like this one are generally written in pure Julia, which makes it easy for users and developers to understand and contribute to open-source code. Eventually, this project aims to offer a one-stop-shop of counterfactual explanations. We want to deliver a package that is at least at par with the <a href="https://github.com/carla-recourse/CARLA">CARLA</a> Python library in terms of its functionality. Currently, the package falls short of this goal in some ways:</p>
<ol dir="auto">
<li>The number of counterfactual generators is still limited.</li>
<li>Mutability constraints are still not supported for Latent Space generators.</li>
</ol>
<p dir="auto">Additionally, our ambition is to enhance the package through the following features:</p>
<ol start="4" dir="auto">
<li>Language interoperability with Python and R: currently still only experimental.</li>
<li>Support for machine learning models trained in <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="nofollow"><code>MLJ.jl</code></a>.</li>
<li>Additional datasets for testing, evaluation and benchmarking.</li>
<li>Support for regression models.</li>
</ol>
<h2 dir="auto"><a id="user-content--contribute" class="anchor" aria-hidden="true" href="#-contribute"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png">üõ†</g-emoji> Contribute</h2>
<p dir="auto">Contributions of any kind are very much welcome! Take a look at the <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/issues">issue</a> to see what things we are currently working on.</p>
<p dir="auto">If any of the below applies to you, this might be the right open-source project for you:</p>
<ul dir="auto">
<li>You‚Äôre an expert in Counterfactual Explanations or Explainable AI more broadly and you are curious about Julia.</li>
<li>You‚Äôre experienced with Julia and are happy to help someone less experienced to up their game. Ideally, you are also curious about Trustworthy AI.</li>
<li>You‚Äôre new to Julia and open-source development and would like to start your learning journey by contributing to a recent and active development. Ideally, you are familiar with machine learning.</li>
</ul>
<p dir="auto"><a href="https://github.com/pat-alt">@pat-alt</a> here: I am still very much at the beginning of my Julia journey, so if you spot any issues or have any suggestions for design improvement, please just open <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/issues">issue</a> or start a <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/discussions">discussion</a>. Our goal is to provide a go-to place for counterfactual explanations in Julia.</p>
<p dir="auto">For more details on how to contribute see <a href="https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/contributing/" rel="nofollow">here</a>. Please follow the <a href="https://github.com/SciML/ColPrac">SciML ColPrac guide</a>.</p>
<h2 dir="auto"><a id="user-content--citation" class="anchor" aria-hidden="true" href="#-citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="mortar_board" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png">üéì</g-emoji> Citation</h2>
<p dir="auto">If you want to use this codebase, please consider citing:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="@software{altmeyer2022CounterfactualExplanations,
  author = {Patrick Altmeyer},
  title = {{CounterfactualExplanations.jl - a Julia package for Counterfactual Explanations and Algorithmic Recourse}},
  url = {https://github.com/juliatrustworthyai/CounterfactualExplanations.jl},
  year = {2022}
}"><pre class="notranslate"><code>@software{altmeyer2022CounterfactualExplanations,
  author = {Patrick Altmeyer},
  title = {{CounterfactualExplanations.jl - a Julia package for Counterfactual Explanations and Algorithmic Recourse}},
  url = {https://github.com/juliatrustworthyai/CounterfactualExplanations.jl},
  year = {2022}
}
</code></pre></div>
<h2 dir="auto"><a id="user-content--references" class="anchor" aria-hidden="true" href="#-references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png">üìö</g-emoji> References</h2>
<p dir="auto">Altmeyer, Patrick, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, and Cynthia Liem. 2023. ‚ÄúEndogenous Macrodynamics in Algorithmic Recourse.‚Äù In <em>First IEEE Conference on Secure and Trustworthy Machine Learning</em>.</p>
<p dir="auto">Antor√°n, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and Jos√© Miguel Hern√°ndez-Lobato. 2020. ‚ÄúGetting a Clue: A Method for Explaining Uncertainty Estimates.‚Äù <a href="https://arxiv.org/abs/2006.06848" rel="nofollow">https://arxiv.org/abs/2006.06848</a>.</p>
<p dir="auto">Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. ‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù <a href="https://arxiv.org/abs/1907.09615" rel="nofollow">https://arxiv.org/abs/1907.09615</a>.</p>
<p dir="auto">Kaggle. 2011. ‚ÄúGive Me Some Credit, Improve on the State of the Art in Credit Scoring by Predicting the Probability That Somebody Will Experience Financial Distress in the Next Two Years.‚Äù Kaggle. <a href="https://www.kaggle.com/c/GiveMeSomeCredit" rel="nofollow">https://www.kaggle.com/c/GiveMeSomeCredit</a>.</p>
<p dir="auto">Mothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. ‚ÄúExplaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.‚Äù In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 607‚Äì17.</p>
<p dir="auto">Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. ‚ÄúGenerating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.‚Äù In <em>International Conference on Artificial Intelligence and Statistics</em>, 1756‚Äì64. PMLR.</p>
<p dir="auto">Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. ‚ÄúCounterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.‚Äù <em>Harv. JL &amp; Tech.</em> 31: 841.</p>
</article></div>