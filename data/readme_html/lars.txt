<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-least-angle-regression" class="anchor" aria-hidden="true" href="#least-angle-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Least angle regression</h1>
<p dir="auto"><a href="https://travis-ci.org/simonster/LARS.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/1742db4bd8d01de02f06aca3601220df4e457b95c998c269efc9e1139facb668/68747470733a2f2f7472617669732d63692e6f72672f73696d6f6e737465722f4c4152532e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/simonster/LARS.jl.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://coveralls.io/r/simonster/LARS.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/2ea14512aa02947522f729fc25138a00e5018144431c4a4e9821db5e05b5083e/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f73696d6f6e737465722f4c4152532e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/simonster/LARS.jl/badge.svg?branch=master" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto"><a href="http://en.wikipedia.org/wiki/Least-angle_regression" rel="nofollow">Least angle regression</a>
is a variable selection/shrinkage procedure for high-dimensional data. It is
also an algorithm for efficiently finding all knots in the solution path for
the aforementioned this regression procedure, as well as for lasso
(L1-regularized) linear regression. Fitting the entire solution path is useful
for selecting the optimal value of the shrinkage parameter λ for a given
dataset, and for the <a href="http://arxiv.org/abs/1301.7161" rel="nofollow">lasso covariance test</a>,
which provides the significance of each variable addition along the lasso path.</p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">LARS solution paths are provided by the <code>lars</code> function:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="lars(X, y; method=:lasso, intercept=true, standardize=true, lambda2=0.0,
     use_gram=true, maxiter=typemax(Int), lambda_min=0.0, verbose=false)"><pre><span class="pl-c1">lars</span>(X, y; method<span class="pl-k">=</span><span class="pl-c1">:lasso</span>, intercept<span class="pl-k">=</span><span class="pl-c1">true</span>, standardize<span class="pl-k">=</span><span class="pl-c1">true</span>, lambda2<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
     use_gram<span class="pl-k">=</span><span class="pl-c1">true</span>, maxiter<span class="pl-k">=</span><span class="pl-c1">typemax</span>(Int), lambda_min<span class="pl-k">=</span><span class="pl-c1">0.0</span>, verbose<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p dir="auto"><code>X</code> is the design matrix and <code>y</code> is the dependent variable. The optional parameters are:</p>
<p dir="auto"><code>method</code> - either <code>:lasso</code> or <code>:lars</code>.</p>
<p dir="auto"><code>intercept</code> - whether to fit an intercept in the model. The intercept is
always unpenalized.</p>
<p dir="auto"><code>standardize</code> - whether to standardize the predictor matrix. In contrast to
linear regression, this affects the algorithm's results. The returned
coefficients are always unstandardized.</p>
<p dir="auto"><code>lambda2</code> - the elastic net ridge penalty. Zero for pure lasso. Note that the
returned coefficients are the "naive" elastic net coefficients. They can be
adjusted as recommended by Zhou and Hastie (2005) by scaling by <code>1 + lambda2</code>.</p>
<p dir="auto"><code>use_gram</code> - whether to use a precomputed Gram matrix in computation.</p>
<p dir="auto"><code>maxiter</code> - maximum number of iterations of the algorithm. If this is
exceeded, an incomplete path is returned. <code>lambda_min</code> - value of λ at which
the algorithm should stop.</p>
<p dir="auto"><code>verbose</code> - if true, prints information at each step.</p>
<p dir="auto">The <code>covtest</code> function computes the lasso covariance test based on a LARS path:</p>
<p dir="auto"><code>covtest(path, X, y; errorvar)</code></p>
<p dir="auto"><code>path</code> is the output of the LARS function above, and <code>X</code> and <code>y</code> are the
independent and dependent variables used in fitting the path. If specified,
<code>errorvar</code> is the variance of the error. If not specified, the error variance
is computed based on the least squares fit of the full model.</p>
<h2 dir="auto"><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notes</h2>
<p dir="auto">The output of <code>covtest</code> has minor discrepancies with that of the <a href="http://cran.r-project.org/web/packages/covTest/index.html" rel="nofollow">covTest
package</a>. This is
because the covTest package does not take into account the intercept in the
least squares model fit when computing the error variance, which I believe is
incorrect. I have emailed the authors but have yet to receive a response.</p>
<h2 dir="auto"><a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Benchmarks</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="/benchmark/performance.png"><img src="/benchmark/performance.png" alt="scikit-learn Performance Comparison" style="max-width: 100%;"></a></p>
<p dir="auto">LARS.jl is substantially faster than scikit-learn for cases where the number
of samples exceeds the number of features, particularly when using a Gram
matrix. For cases where the number of features greatly exceeds the number of
samples, scikit-learn is still occasionally faster. I am still tracking down
the cause.</p>
<h2 dir="auto"><a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>See also</h2>
<p dir="auto"><a href="https://github.com/simonster/GLMNet.jl">GLMNet</a> fits the lasso solution path
using coordinate descent and supports fitting L1-regularized generalized
linear models.</p>
<h2 dir="auto"><a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Credits</h2>
<p dir="auto">This package is written and maintained by Simon Kornblith <a href="mailto:simon@simonster.com">simon@simonster.com</a>.</p>
<p dir="auto">The <code>lars</code> function is derived from code from scikit-learn written by:</p>
<ul dir="auto">
<li>Alexandre Gramfort <a href="mailto:alexandre.gramfort@inria.fr">alexandre.gramfort@inria.fr</a></li>
<li>Fabian Pedregosa <a href="mailto:fabian.pedregosa@inria.fr">fabian.pedregosa@inria.fr</a></li>
<li>Olivier Grisel <a href="mailto:olivier.grisel@ensta.org">olivier.grisel@ensta.org</a></li>
<li>Vincent Michel <a href="mailto:vincent.michel@inria.fr">vincent.michel@inria.fr</a></li>
<li>Peter Prettenhofer <a href="mailto:peter.prettenhofer@gmail.com">peter.prettenhofer@gmail.com</a></li>
<li>Mathieu Blondel <a href="mailto:mathieu@mblondel.org">mathieu@mblondel.org</a></li>
<li>Lars Buitinck <a href="mailto:L.J.Buitinck@uva.nl">L.J.Buitinck@uva.nl</a></li>
</ul>
</article></div>