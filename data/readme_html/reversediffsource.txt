<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-reversediffsourcejl" class="anchor" aria-hidden="true" href="#reversediffsourcejl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ReverseDiffSource.jl</h1>
<p><em>Reverse automated differentiation from an expression or a function</em></p>
<table>
<thead>
<tr>
<th align="center">Julia 0.3</th>
<th align="center">Julia 0.4</th>
<th align="center">Julia 0.5</th>
<th align="center">master (on nightly + release)</th>
<th align="center">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://pkg.julialang.org/?pkg=ReverseDiffSource&amp;ver=0.3" rel="nofollow"><img src="https://camo.githubusercontent.com/4f4e2e61ba5dec1aa57126fb7661e4c932149609/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f5265766572736544696666536f757263655f302e332e737667" alt="ReverseDiffSource" data-canonical-src="http://pkg.julialang.org/badges/ReverseDiffSource_0.3.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="http://pkg.julialang.org/?pkg=ReverseDiffSource&amp;ver=0.4" rel="nofollow"><img src="https://camo.githubusercontent.com/b2ce3c80fc652ac353a7b64cd30cee2334e456a5/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f5265766572736544696666536f757263655f302e342e737667" alt="ReverseDiffSource" data-canonical-src="http://pkg.julialang.org/badges/ReverseDiffSource_0.4.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="http://pkg.julialang.org/?pkg=ReverseDiffSource&amp;ver=0.5" rel="nofollow"><img src="https://camo.githubusercontent.com/71dd5342e6eae5b62d273f99d025fa76d1ba3763/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f5265766572736544696666536f757263655f302e352e737667" alt="ReverseDiffSource" data-canonical-src="http://pkg.julialang.org/badges/ReverseDiffSource_0.5.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://travis-ci.org/JuliaDiff/ReverseDiffSource.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dfecf35a79b8fa9be84b8696678d69692fe40910/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961446966662f5265766572736544696666536f757263652e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaDiff/ReverseDiffSource.jl.svg?branch=master" style="max-width:100%;"></a></td>
<td align="center"><a href="https://coveralls.io/r/JuliaDiff/ReverseDiffSource.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/273b9346ae71523363dc18e903441ebfbbcd3a48/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f4a756c6961446966662f5265766572736544696666536f757263652e6a6c2f62616467652e706e673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/JuliaDiff/ReverseDiffSource.jl/badge.png?branch=master" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>This package provides a function <code>rdiff()</code> that generates valid Julia code for the calculation of derivatives up to any order for a user supplied expression or generic function. Install with <code>Pkg.add("ReverseDiffSource")</code>. Package documentation and examples can be found <a href="http://reversediffsourcejl.readthedocs.org/en/master/index.html" rel="nofollow">here</a>.</p>
<p>This version of automated differentiation operates at the source level (provided either in an expression or a generic function) to output Julia code calculating the derivatives (in a expression or a function respectively). Compared to other automated differentiation methods it does not rely on method overloading or new types and should, in principle, produce fast code.</p>
<p>Usage examples:</p>
<ul>
<li>derivative of xÂ³</li>
</ul>
<pre><code>    julia&gt; rdiff( :(x^3) , x=Float64)  # 'x=Float64' indicates the type of x to rdiff
    :(begin
        (x^3,3 * x^2.0)  # expression calculates a tuple of (value, derivate)
        end)
</code></pre>
<ul>
<li>first 10 derivatives of <code>sin(x)</code>  (notice the simplifications)</li>
</ul>
<pre><code>    julia&gt; rdiff( :(sin(x)) , order=10, x=Float64)  # derivatives up to order 10
    :(begin
            _tmp1 = sin(x)
            _tmp2 = cos(x)
            _tmp3 = -_tmp1
            _tmp4 = -_tmp2
            _tmp5 = -_tmp3
            (_tmp1,_tmp2,_tmp3,_tmp4,_tmp5,_tmp2,_tmp3,_tmp4,_tmp5,_tmp2,_tmp3)
        end)
</code></pre>
<ul>
<li>works on functions too</li>
</ul>
<pre><code>	julia&gt; rosenbrock(x) = (1 - x[1])^2 + 100(x[2] - x[1]^2)^2   # function to be derived
	julia&gt; rosen2 = rdiff(rosenbrock, (Vector{Float64},), order=2)       # orders up to 2
		(anonymous function)
</code></pre>
<ul>
<li>gradient calculation of a 3 hidden layer neural network for backpropagation</li>
</ul>
<pre><code>    # w1-w3 are the hidden layer weight matrices, x1 the input vector
    function ann(w1, w2, w3, x1)
        x2 = w1 * x1
        x2 = log(1. + exp(x2))   # soft RELU unit
        x3 = w2 * x2
        x3 = log(1. + exp(x3))   # soft RELU unit
        x4 = w3 * x3
        1. / (1. + exp(-x4[1]))  # sigmoid output
    end

    w1, w2, w3 = randn(10,10), randn(10,10), randn(1,10)
    x1 = randn(10)
    dann = m.rdiff(ann, (Matrix{Float64}, Matrix{Float64}, Matrix{Float64}, Vector{Float64}))
    dann(w1, w2, w3, x1) # network output + gradient on w1, w2, w3 and x1
</code></pre>
</article></div>