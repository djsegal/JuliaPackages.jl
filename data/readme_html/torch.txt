<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-torchjl" class="anchor" aria-hidden="true" href="#torchjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Torch.jl</h1>
<p dir="auto">Sensible extensions for exposing torch in Julia.</p>
<p dir="auto">This package is aimed at providing the <code>Tensor</code> type, which offloads all computations over to <a href="https://pytorch.org/cppdocs/" rel="nofollow">ATen</a>, the foundational tensor library for PyTorch, written in C++.</p>
<p dir="auto"><strong>Note:</strong></p>
<ul dir="auto">
<li>Needs a machine with a CUDA GPU (CUDA 10.1 or above)
<ul dir="auto">
<li>will need lazy artifacts function without a GPU</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto">To add the package, from the Julia REPL, enter the Pkg prompt by typing <code>]</code> and execute the following:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pkg&gt; add Torch"><pre>pkg<span class="pl-k">&gt;</span> add Torch</pre></div>
<p dir="auto">Or via Julia's package manager Pkg.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using Pkg; Pkg.add(&quot;Torch&quot;);"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Torch<span class="pl-pds">"</span></span>);</pre></div>
<h2 dir="auto"><a id="user-content-usage-example" class="anchor" aria-hidden="true" href="#usage-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage Example</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Metalhead, Metalhead.Flux, Torch
using Torch: torch

resnet = ResNet()"><pre><span class="pl-k">using</span> Metalhead, Metalhead<span class="pl-k">.</span>Flux, Torch
<span class="pl-k">using</span> Torch<span class="pl-k">:</span> torch

resnet <span class="pl-k">=</span> <span class="pl-c1">ResNet</span>()</pre></div>
<p dir="auto">We can move our object over to Torch via a simple call to <code>torch</code></p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="tresnet = resnet.layers |&gt; torch"><pre>tresnet <span class="pl-k">=</span> resnet<span class="pl-k">.</span>layers <span class="pl-k">|&gt;</span> torch</pre></div>
<p dir="auto">Or if we need more control over the device to be used like so:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="ip = rand(Float32, 224, 224, 3, 1) # An RGB Image
tip = tensor(ip, dev = 0) # 0 =&gt; GPU:0 in Torch
cpu_tensor = tensor(ip, dev = -1) # -1 =&gt; CPU:0"><pre>ip <span class="pl-k">=</span> <span class="pl-c1">rand</span>(Float32, <span class="pl-c1">224</span>, <span class="pl-c1">224</span>, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> An RGB Image</span>
tip <span class="pl-k">=</span> <span class="pl-c1">tensor</span>(ip, dev <span class="pl-k">=</span> <span class="pl-c1">0</span>) <span class="pl-c"><span class="pl-c">#</span> 0 =&gt; GPU:0 in Torch</span>
cpu_tensor <span class="pl-k">=</span> <span class="pl-c1">tensor</span>(ip, dev <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> -1 =&gt; CPU:0</span></pre></div>
<p dir="auto">Calling into the model is done via the usual Flux mechanism.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="tresnet(tip);"><pre><span class="pl-c1">tresnet</span>(tip);</pre></div>
<p dir="auto">We can take gradients using Zygote as well</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="gs = gradient(x -&gt; sum(tresnet(x)), tip);

# Or

ps = Flux.params(tresnet);
gs = gradient(ps) do
  sum(tresnet(tip))
end"><pre>gs <span class="pl-k">=</span> <span class="pl-c1">gradient</span>(x <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(<span class="pl-c1">tresnet</span>(x)), tip);

<span class="pl-c"><span class="pl-c">#</span> Or</span>

ps <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">params</span>(tresnet);
gs <span class="pl-k">=</span> <span class="pl-c1">gradient</span>(ps) <span class="pl-k">do</span>
  <span class="pl-c1">sum</span>(<span class="pl-c1">tresnet</span>(tip))
<span class="pl-k">end</span></pre></div>
<h2 dir="auto"><a id="user-content-contributing-and-issues" class="anchor" aria-hidden="true" href="#contributing-and-issues"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing and Issues</h2>
<p dir="auto">Please feel free to open issues you might encounter in the issue tracker.
I would also appreciate contributions through PRs toward corrections, increased
coverage, docs, etc. Testing currently runs on Linux, but that can be expanded
as need arises.</p>
<h2 dir="auto"><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">Takes a lot of inspiration from existing such projects - ocaml-torch for generating the wrappers.</p>
</article></div>