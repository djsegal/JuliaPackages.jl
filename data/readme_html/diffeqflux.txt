<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-diffeqfluxjl" class="anchor" aria-hidden="true" href="#diffeqfluxjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DiffEqFlux.jl</h1>
<p><a href="https://gitter.im/JuliaDiffEq/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge" rel="nofollow"><img src="https://camo.githubusercontent.com/063a520f1733d1b53d1e2fdb37b70a8016dd36f6/68747470733a2f2f6261646765732e6769747465722e696d2f4a756c69614469666645712f4c6f6262792e737667" alt="Join the chat at https://gitter.im/JuliaDiffEq/Lobby" data-canonical-src="https://badges.gitter.im/JuliaDiffEq/Lobby.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.org/JuliaDiffEq/DiffEqFlux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ae2585d9f35142dc7ba2bd71e02dd50b25f2c01c/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614469666645712f446966664571466c75782e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaDiffEq/DiffEqFlux.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/ChrisRackauckas/diffeqflux-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a2671b2202b69ed4703c8d9feadace0d222e4317/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6535613970616435386f6a6f323669723f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/e5a9pad58ojo26ir?svg=true" style="max-width:100%;"></a>
<a href="https://gitlab.com/juliadiffeq/DiffEqFlux-jl/pipelines" rel="nofollow"><img src="https://camo.githubusercontent.com/2e6d4a82bf2c782e0a278d281d95129a444a3a66/68747470733a2f2f6769746c61622e636f6d2f6a756c69616469666665712f446966664571466c75782d6a6c2f6261646765732f6d61737465722f706970656c696e652e737667" alt="GitlabCI" data-canonical-src="https://gitlab.com/juliadiffeq/DiffEqFlux-jl/badges/master/pipeline.svg" style="max-width:100%;"></a></p>
<p>DiffEqFlux.jl fuses the world of differential equations with machine learning
by helping users put diffeq solvers into neural networks. This package utilizes
<a href="http://docs.juliadiffeq.org/dev/" rel="nofollow">DifferentialEquations.jl</a> and
<a href="https://fluxml.ai/" rel="nofollow">Flux.jl</a> as its building blocks to support research in
<a href="http://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/" rel="nofollow">Scientific Machine Learning</a>
and neural differential equations in traditional machine learning.</p>
<h2><a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contents</h2>
<ul>
<li><a href="#problem-domain">Problem Domain</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#example-usage">Example Usage</a>
<ul>
<li><a href="#optimizing-parameters-of-an-ode-for-an-optimal-control-problem">Optimizing parameters of an ODE for an Optimal Control problem</a></li>
<li><a href="#using-other-differential-equations">Using Other Differential Equations</a></li>
<li><a href="#neural-ordinary-differential-equations">Neural Ordinary Differential Equations</a></li>
<li><a href="#training-a-neural-ordinary-differential-equation">Training a Neural Ordinary Differential Equation</a></li>
</ul>
</li>
<li><a href="#use-with-gpus">Use with GPUs</a></li>
<li><a href="#universal-differential-equations">Universal Differential Equations</a>
<ul>
<li><a href="#universal-differential-equations-for-neural-optimal-control">Universal Differential Equations for Neural Optimal Control</a></li>
</ul>
</li>
<li>Neural Differential Equations for Non-ODEs: Neural SDEs, Neural DDEs, etc.</li>
<li><a href="#api-documentation">API Documentation</a>
<ul>
<li><a href="#Neural-DE-Layer-Functions">Neural DE Layer Functions</a></li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a></li>
</ul>
<h2><a id="user-content-problem-domain" class="anchor" aria-hidden="true" href="#problem-domain"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Problem Domain</h2>
<p>DiffEqFlux.jl is not just for neural ordinary differential equations.
DiffEqFlux.jl is for universal differential equations. For an overview of the topic
with applications, consult the paper <a href="https://arxiv.org/abs/2001.04385" rel="nofollow">Universal Differential Equations for Scientific Machine Learning</a></p>
<p>As such, it is the first package to support and demonstrate:</p>
<ul>
<li>Stiff universal ordinary differential equations (universal ODEs)</li>
<li>Universal stochastic differential equations (universal SDEs)</li>
<li>Universal delay differential equations (universal DDEs)</li>
<li>Universal partial differential equations (universal PDEs)</li>
<li>Universal jump stochastic differential equations (universal jump diffusions)</li>
<li>Hybrid universal differential equations (universal DEs with event handling)</li>
</ul>
<p>with high order, adaptive, implicit, GPU-accelerated, Newton-Krylov, etc.
methods. For examples, please refer to
<a href="https://julialang.org/blog/2019/01/fluxdiffeq" rel="nofollow">the release blog post</a>.
Additional demonstrations, like neural
PDEs and neural jump SDEs, can be found
<a href="http://www.stochasticlifestyle.com/neural-jump-sdes-jump-diffusions-and-neural-pdes/" rel="nofollow">at this blog post</a>
(among many others!).</p>
<p>Do not limit yourself to the current neuralization. With this package, you can
explore various ways to integrate the two methodologies:</p>
<ul>
<li>Neural networks can be defined where the “activations” are nonlinear functions
described by differential equations.</li>
<li>Neural networks can be defined where some layers are ODE solves</li>
<li>ODEs can be defined where some terms are neural networks</li>
<li>Cost functions on ODEs can define neural networks</li>
</ul>
<h2><a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Citation</h2>
<p>If you use DiffEqFlux.jl or are influenced by its ideas for expanding beyond
neural ODEs, please cite:</p>
<pre><code>@article{DBLP:journals/corr/abs-1902-02376,
  author    = {Christopher Rackauckas and
               Mike Innes and
               Yingbo Ma and
               Jesse Bettencourt and
               Lyndon White and
               Vaibhav Dixit},
  title     = {DiffEqFlux.jl - {A} Julia Library for Neural Differential Equations},
  journal   = {CoRR},
  volume    = {abs/1902.02376},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02376},
  archivePrefix = {arXiv},
  eprint    = {1902.02376},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-02376},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</code></pre>
<h2><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example Usage</h2>
<p>For an overview of what this package is for,
<a href="https://julialang.org/blog/2019/01/fluxdiffeq" rel="nofollow">see this blog post</a>.</p>
<h3><a id="user-content-optimizing-parameters-of-an-ode-for-an-optimal-control-problem" class="anchor" aria-hidden="true" href="#optimizing-parameters-of-an-ode-for-an-optimal-control-problem"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Optimizing parameters of an ODE for an Optimal Control problem</h3>
<p>First let's create a Lotka-Volterra ODE using DifferentialEquations.jl. For
more details, <a href="http://docs.juliadiffeq.org/dev/" rel="nofollow">see the DifferentialEquations.jl documentation</a></p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DifferentialEquations, Flux, Optim, DiffEqFlux
<span class="pl-k">function</span> <span class="pl-en">lotka_volterra</span>(du,u,p,t)
  x, y <span class="pl-k">=</span> u
  α, β, δ, γ <span class="pl-k">=</span> p
  du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> dx <span class="pl-k">=</span> α<span class="pl-k">*</span>x <span class="pl-k">-</span> β<span class="pl-k">*</span>x<span class="pl-k">*</span>y
  du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> dy <span class="pl-k">=</span> <span class="pl-k">-</span>δ<span class="pl-k">*</span>y <span class="pl-k">+</span> γ<span class="pl-k">*</span>x<span class="pl-k">*</span>y
<span class="pl-k">end</span>
u0 <span class="pl-k">=</span> [<span class="pl-c1">1.0</span>,<span class="pl-c1">1.0</span>]
tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0</span>,<span class="pl-c1">10.0</span>)
p <span class="pl-k">=</span> [<span class="pl-c1">1.5</span>,<span class="pl-c1">1.0</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">1.0</span>]
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(lotka_volterra,u0,tspan,p)
sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(prob,<span class="pl-c1">Tsit5</span>())
<span class="pl-k">using</span> Plots
<span class="pl-c1">plot</span>(sol)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1814174/51388169-9a07f300-1af6-11e9-8c6c-83c41e81d11c.png"><img src="https://user-images.githubusercontent.com/1814174/51388169-9a07f300-1af6-11e9-8c6c-83c41e81d11c.png" alt="LV Solution Plot" style="max-width:100%;"></a></p>
<p>Next we define a single layer neural network that using the
<a href="https://docs.juliadiffeq.org/latest/analysis/sensitivity/" rel="nofollow">AD-compatible <code>concrete_solve</code> function</a>
function that takes the parameters and an initial condition and returns the
solution of the differential equation as a
<a href="https://github.com/JuliaDiffEq/RecursiveArrayTools.jl"><code>DiffEqArray</code></a> (same
array semantics as the standard differential equation solution object but without
the interpolations).</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">predict_adjoint</span>(p) <span class="pl-c"><span class="pl-c">#</span> Our 1-layer neural network</span>
  <span class="pl-c1">Array</span>(<span class="pl-c1">concrete_solve</span>(prob,<span class="pl-c1">Tsit5</span>(),u0,p,saveat<span class="pl-k">=</span><span class="pl-c1">0.0</span><span class="pl-k">:</span><span class="pl-c1">0.1</span><span class="pl-k">:</span><span class="pl-c1">10.0</span>))
<span class="pl-k">end</span></pre></div>
<p>Next we choose a loss function. Our goal will be to find parameter that make
the Lotka-Volterra solution constant <code>x(t)=1</code>, so we defined our loss as the
squared distance from 1. Note that when using <code>sciml_train</code>, the first return
is the loss value, and the other returns are sent to the callback for monitoring
convergence.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">loss_adjoint</span>(p)
  prediction <span class="pl-k">=</span> <span class="pl-c1">predict_adjoint</span>(p)
  loss <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,x<span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-k">for</span> x <span class="pl-k">in</span> prediction)
  loss,prediction
<span class="pl-k">end</span></pre></div>
<p>Lastly, we use the <code>sciml_train</code> function to train the parameters using BFGS to
arrive at parameters which optimize for our goal. <code>sciml_train</code> allows defining
a callback that will be called at each step of our training loop. It takes in the
current parameter vector and the returns of the last call to the loss function.
We will display the current loss and make a plot of the current situation:</p>
<div class="highlight highlight-source-julia"><pre>cb <span class="pl-k">=</span> <span class="pl-k">function</span> (p,l,pred) <span class="pl-c"><span class="pl-c">#</span>callback function to observe training</span>
  <span class="pl-c1">display</span>(l)
  <span class="pl-c"><span class="pl-c">#</span> using `remake` to re-create our `prob` with current parameters `p`</span>
  <span class="pl-c1">display</span>(<span class="pl-c1">plot</span>(<span class="pl-c1">solve</span>(<span class="pl-c1">remake</span>(prob,p<span class="pl-k">=</span>p),<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.0</span><span class="pl-k">:</span><span class="pl-c1">0.1</span><span class="pl-k">:</span><span class="pl-c1">10.0</span>),ylim<span class="pl-k">=</span>(<span class="pl-c1">0</span>,<span class="pl-c1">6</span>)))
  <span class="pl-k">return</span> <span class="pl-c1">false</span> <span class="pl-c"><span class="pl-c">#</span> Tell it to not halt the optimization. If return true, then optimization stops</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Display the ODE with the initial parameter values.</span>
<span class="pl-c1">cb</span>(p,<span class="pl-c1">loss_adjoint</span>(p)<span class="pl-k">...</span>)

res <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>(loss_adjoint, p, <span class="pl-c1">BFGS</span>(initial_stepnorm <span class="pl-k">=</span> <span class="pl-c1">0.0001</span>), cb <span class="pl-k">=</span> cb)</pre></div>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">*</span> Status<span class="pl-k">:</span> failure (objective increased between iterations) (line search failed)

<span class="pl-k">*</span> Candidate solution
   Minimizer<span class="pl-k">:</span> [<span class="pl-c1">1.44e+00</span>, <span class="pl-c1">1.44e+00</span>, <span class="pl-c1">2.54e+00</span>,  <span class="pl-k">...</span>]
   Minimum<span class="pl-k">:</span>   <span class="pl-c1">1.993849e-06</span>

<span class="pl-k">*</span> Found with
   Algorithm<span class="pl-k">:</span>     BFGS
   Initial Point<span class="pl-k">:</span> [<span class="pl-c1">1.50e+00</span>, <span class="pl-c1">1.00e+00</span>, <span class="pl-c1">3.00e+00</span>,  <span class="pl-k">...</span>]

<span class="pl-k">*</span> Convergence measures
   <span class="pl-k">|</span>x <span class="pl-k">-</span> x<span class="pl-k">'</span><span class="pl-k">|</span>               <span class="pl-k">=</span> <span class="pl-c1">1.27e-09</span> ≰ <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span>x <span class="pl-k">-</span> x<span class="pl-k">'</span><span class="pl-k">|</span><span class="pl-k">/</span><span class="pl-k">|</span>x<span class="pl-k">'</span><span class="pl-k">|</span>          <span class="pl-k">=</span> <span class="pl-c1">4.98e-10</span> ≰ <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span><span class="pl-c1">f</span>(x) <span class="pl-k">-</span> <span class="pl-c1">f</span>(x<span class="pl-k">'</span>)<span class="pl-k">|</span>         <span class="pl-k">=</span> <span class="pl-c1">1.99e-12</span> ≰ <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span><span class="pl-c1">f</span>(x) <span class="pl-k">-</span> <span class="pl-c1">f</span>(x<span class="pl-k">'</span>)<span class="pl-k">|</span><span class="pl-k">/</span><span class="pl-k">|</span><span class="pl-c1">f</span>(x<span class="pl-k">'</span>)<span class="pl-k">|</span> <span class="pl-k">=</span> <span class="pl-c1">1.00e-06</span> ≰ <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span><span class="pl-c1">g</span>(x)<span class="pl-k">|</span>                 <span class="pl-k">=</span> <span class="pl-c1">5.74e-03</span> ≰ <span class="pl-c1">1.0e-08</span>

<span class="pl-k">*</span> Work counters
   Seconds run<span class="pl-k">:</span>   <span class="pl-c1">6</span>  (vs limit <span class="pl-c1">Inf</span>)
   Iterations<span class="pl-k">:</span>    <span class="pl-c1">6</span>
   <span class="pl-c1">f</span>(x) calls<span class="pl-k">:</span>    <span class="pl-c1">83</span>
   <span class="pl-c1">∇f</span>(x) calls<span class="pl-k">:</span>   <span class="pl-c1">83</span></pre></div>
<p>In just seconds we found parameters which give a loss of <code>1e-6</code>! We can get the
final loss with <code>res.minimum</code>, and get the optimal parameters with <code>res.minimizer</code>.
For example, we can plot the final outcome and show that we solved the control
problem and successfully found parameters to make the ODE solution constant:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">plot</span>(<span class="pl-c1">solve</span>(<span class="pl-c1">remake</span>(prob,p<span class="pl-k">=</span>res<span class="pl-k">.</span>minimizer),<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.0</span><span class="pl-k">:</span><span class="pl-c1">0.1</span><span class="pl-k">:</span><span class="pl-c1">10.0</span>),ylim<span class="pl-k">=</span>(<span class="pl-c1">0</span>,<span class="pl-c1">6</span>))</pre></div>
<p>Here's an old video showing the use of the ADAM optimizer:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1814174/51399500-1f4dd080-1b14-11e9-8c9d-144f93b6eac2.gif"><img src="https://user-images.githubusercontent.com/1814174/51399500-1f4dd080-1b14-11e9-8c9d-144f93b6eac2.gif" alt="Flux ODE Training Animation" style="max-width:100%;"></a></p>
<h3><a id="user-content-using-other-differential-equations" class="anchor" aria-hidden="true" href="#using-other-differential-equations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using Other Differential Equations</h3>
<p>Other differential equation problem types from DifferentialEquations.jl are
supported. For example, we can build a layer with a delay differential equation
like:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">delay_lotka_volterra</span>(du,u,h,p,t)
  x, y <span class="pl-k">=</span> u
  α, β, δ, γ <span class="pl-k">=</span> p
  du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> dx <span class="pl-k">=</span> (α <span class="pl-k">-</span> β<span class="pl-k">*</span>y)<span class="pl-k">*</span><span class="pl-c1">h</span>(p,t<span class="pl-k">-</span><span class="pl-c1">0.1</span>)[<span class="pl-c1">1</span>]
  du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> dy <span class="pl-k">=</span> (δ<span class="pl-k">*</span>x <span class="pl-k">-</span> γ)<span class="pl-k">*</span>y
<span class="pl-k">end</span>
<span class="pl-en">h</span>(p,t) <span class="pl-k">=</span> <span class="pl-c1">ones</span>(<span class="pl-c1">eltype</span>(p),<span class="pl-c1">2</span>)
u0 <span class="pl-k">=</span> [<span class="pl-c1">1.0</span>,<span class="pl-c1">1.0</span>]
prob <span class="pl-k">=</span> <span class="pl-c1">DDEProblem</span>(delay_lotka_volterra,u0,h,(<span class="pl-c1">0.0</span>,<span class="pl-c1">10.0</span>),constant_lags<span class="pl-k">=</span>[<span class="pl-c1">0.1</span>])

p <span class="pl-k">=</span> [<span class="pl-c1">2.2</span>, <span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">0.4</span>]
<span class="pl-k">function</span> <span class="pl-en">predict_dde</span>(p)
  <span class="pl-c1">Array</span>(<span class="pl-c1">concrete_solve</span>(prob,<span class="pl-c1">MethodOfSteps</span>(<span class="pl-c1">Tsit5</span>()),u0,p,saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>,sensealg<span class="pl-k">=</span><span class="pl-c1">TrackerAdjoint</span>())
<span class="pl-k">end</span>
<span class="pl-en">loss_dde</span>(p) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,x<span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-k">for</span> x <span class="pl-k">in</span> <span class="pl-c1">predict_dde</span>(p))
<span class="pl-c1">loss_dde</span>(p)</pre></div>
<p>Notice that we chose <code>sensealg=TrackerAdjoint()</code> to utilize the Tracker.jl
reverse-mode to handle the delay differential equation.</p>
<p>Or we can use a stochastic differential equation. Here we demonstrate
<code>sensealg=ForwardDiffSensitivity()</code> for forward-mode automatic differentiation
of a small stochastic differential equation:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">lotka_volterra_noise</span>(du,u,p,t)
  du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-c1">0.1</span>u[<span class="pl-c1">1</span>]
  du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> <span class="pl-c1">0.1</span>u[<span class="pl-c1">2</span>]
<span class="pl-k">end</span>
u0 <span class="pl-k">=</span> [<span class="pl-c1">1.0</span>,<span class="pl-c1">1.0</span>]
prob <span class="pl-k">=</span> <span class="pl-c1">SDEProblem</span>(lotka_volterra,lotka_volterra_noise,u0,(<span class="pl-c1">0.0</span>,<span class="pl-c1">10.0</span>))

p <span class="pl-k">=</span> [<span class="pl-c1">2.2</span>, <span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">0.4</span>]
<span class="pl-k">function</span> <span class="pl-en">predict_sde</span>(p)
  <span class="pl-c1">Array</span>(<span class="pl-c1">concrete_solve</span>(prob,<span class="pl-c1">SOSRI</span>(),u0,p,sensealg<span class="pl-k">=</span><span class="pl-c1">ForwardDiffSensitivity</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>))
<span class="pl-k">end</span>
<span class="pl-en">loss_sde</span>(p) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,x<span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-k">for</span> x <span class="pl-k">in</span> <span class="pl-c1">predict_sde</span>(p))
<span class="pl-c1">loss_sde</span>(p)</pre></div>
<p>For this training process, because the loss function is stochastic, we will use
the <code>ADAM</code> optimizer from Flux.jl. The <code>sciml_train</code> function is the same as
before. However, to speed up the training process, we will use a global counter
so that way we only plot the current results every 10 iterations. This looks like:</p>
<div class="highlight highlight-source-julia"><pre>iter <span class="pl-k">=</span> <span class="pl-c1">0</span>
cb <span class="pl-k">=</span> <span class="pl-k">function</span> (p,l)
  <span class="pl-c1">display</span>(l)
  <span class="pl-k">global</span> iter
  <span class="pl-k">if</span> iter<span class="pl-k">%</span><span class="pl-c1">10</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>
      <span class="pl-c1">display</span>(<span class="pl-c1">plot</span>(<span class="pl-c1">solve</span>(<span class="pl-c1">remake</span>(prob,p<span class="pl-k">=</span>p),<span class="pl-c1">SOSRI</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>),ylim<span class="pl-k">=</span>(<span class="pl-c1">0</span>,<span class="pl-c1">6</span>)))
  <span class="pl-k">end</span>
  iter <span class="pl-k">+=</span> <span class="pl-c1">1</span>
  <span class="pl-k">return</span> <span class="pl-c1">false</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Display the ODE with the current parameter values.</span>
<span class="pl-c1">cb</span>(p,<span class="pl-c1">loss_sde</span>(p))

DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>(loss_sde, p, <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.1</span>), cb <span class="pl-k">=</span> cb, maxiters <span class="pl-k">=</span> <span class="pl-c1">100</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1814174/51399524-2c6abf80-1b14-11e9-96ae-0192f7debd03.gif"><img src="https://user-images.githubusercontent.com/1814174/51399524-2c6abf80-1b14-11e9-96ae-0192f7debd03.gif" alt="SDE NN Animation" style="max-width:100%;"></a></p>
<h3><a id="user-content-neural-ordinary-differential-equations" class="anchor" aria-hidden="true" href="#neural-ordinary-differential-equations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural Ordinary Differential Equations</h3>
<p>We can use DiffEqFlux.jl to define, solve, and train neural ordinary differential
equations. A neural ODE is an ODE where a neural network defines its derivative
function. Thus for example, with the multilayer perceptron neural network
<code>Chain(Dense(2,50,tanh),Dense(50,2))</code>, the best way to define a neural ODE by hand
would be to use non-mutating adjoints, which looks like:</p>
<div class="highlight highlight-source-julia"><pre>p,re <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">destructure</span>(model)
<span class="pl-en">dudt_</span>(u,p,t) <span class="pl-k">=</span> <span class="pl-c1">re</span>(p)(u)
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(dudt_,x,tspan,p)
my_neural_ode_prob <span class="pl-k">=</span> <span class="pl-c1">concrete_solve</span>(prob,<span class="pl-c1">Tsit5</span>(),u0,p,args<span class="pl-k">...</span>;kwargs<span class="pl-k">...</span>)</pre></div>
<p>(<code>Flux.restructure</code> and <code>Flux.destructure</code> are helper functions which transform
the neural network to use parameters <code>p</code>)</p>
<p>A convenience function which handles all of the details is <code>NeuralODE</code>. To
use <code>NeuralODE</code>, you give it the initial condition, the internal neural
network model to use, the timespan to solve on, and any ODE solver arguments.
For example, this neural ODE would be defined as:</p>
<div class="highlight highlight-source-julia"><pre>tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0</span>f0,<span class="pl-c1">25.0</span>f0)
n_ode <span class="pl-k">=</span> <span class="pl-c1">NeuralODE</span>(model,tspan,<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>)</pre></div>
<p>where here we made it a layer that takes in the initial condition and spits
out an array for the time series saved at every 0.1 time steps.</p>
<h3><a id="user-content-training-a-neural-ordinary-differential-equation" class="anchor" aria-hidden="true" href="#training-a-neural-ordinary-differential-equation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training a Neural Ordinary Differential Equation</h3>
<p>Let's get a time series array from the Lotka-Volterra equation as data:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots

u0 <span class="pl-k">=</span> Float32[<span class="pl-c1">2.</span>; <span class="pl-c1">0.</span>]
datasize <span class="pl-k">=</span> <span class="pl-c1">30</span>
tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0</span>f0,<span class="pl-c1">1.5</span>f0)

<span class="pl-k">function</span> <span class="pl-en">trueODEfunc</span>(du,u,p,t)
    true_A <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">0.1</span> <span class="pl-c1">2.0</span>; <span class="pl-k">-</span><span class="pl-c1">2.0</span> <span class="pl-k">-</span><span class="pl-c1">0.1</span>]
    du <span class="pl-k">.=</span> ((u<span class="pl-k">.^</span><span class="pl-c1">3</span>)<span class="pl-k">'</span>true_A)<span class="pl-k">'</span>
<span class="pl-k">end</span>
t <span class="pl-k">=</span> <span class="pl-c1">range</span>(tspan[<span class="pl-c1">1</span>],tspan[<span class="pl-c1">2</span>],length<span class="pl-k">=</span>datasize)
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(trueODEfunc,u0,tspan)
ode_data <span class="pl-k">=</span> <span class="pl-c1">Array</span>(<span class="pl-c1">solve</span>(prob,<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span>t))</pre></div>
<p>Now let's define a neural network with a <code>NeuralODE</code> layer. First we define
the layer. Here we're going to use <code>FastChain</code>, which is a faster neural network
structure for NeuralODEs:</p>
<div class="highlight highlight-source-julia"><pre>dudt2 <span class="pl-k">=</span> <span class="pl-c1">FastChain</span>((x,p) <span class="pl-k">-&gt;</span> x<span class="pl-k">.^</span><span class="pl-c1">3</span>,
            <span class="pl-c1">FastDense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">50</span>,tanh),
            <span class="pl-c1">FastDense</span>(<span class="pl-c1">50</span>,<span class="pl-c1">2</span>))
n_ode <span class="pl-k">=</span> <span class="pl-c1">NeuralODE</span>(dudt2,tspan,<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span>t)</pre></div>
<p>Note that we can directly use <code>Chain</code>s from Flux.jl as well, for example:</p>
<div class="highlight highlight-source-julia"><pre>dudt2 <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(x <span class="pl-k">-&gt;</span> x<span class="pl-k">.^</span><span class="pl-c1">3</span>,
             <span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">50</span>,tanh),
             <span class="pl-c1">Dense</span>(<span class="pl-c1">50</span>,<span class="pl-c1">2</span>))
n_ode <span class="pl-k">=</span> <span class="pl-c1">NeuralODE</span>(dudt2,tspan,<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span>t)</pre></div>
<p>In our model we used the <code>x -&gt; x.^3</code> assumption in the model. By incorporating structure
into our equations, we can reduce the required size and training time for the
neural network, but a good guess needs to be known!</p>
<p>From here we build a loss function around it. The <code>NeuralODE</code> has an optional
second argument for new parameters which we will use to iteratively change the
neural network in our training loop. We will use the L2 loss of the network's
output against the time series data:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">predict_n_ode</span>(p)
  <span class="pl-c1">n_ode</span>(u0,p)
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">loss_n_ode</span>(p)
    pred <span class="pl-k">=</span> <span class="pl-c1">predict_n_ode</span>(p)
    loss <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,ode_data <span class="pl-k">.-</span> pred)
    loss,pred
<span class="pl-k">end</span>

<span class="pl-c1">loss_n_ode</span>(n_ode<span class="pl-k">.</span>p) <span class="pl-c"><span class="pl-c">#</span> n_ode.p stores the initial parameters of the neural ODE</span></pre></div>
<p>and then train the neural network to learn the ODE:</p>
<div class="highlight highlight-source-julia"><pre>cb <span class="pl-k">=</span> <span class="pl-k">function</span> (p,l,pred;doplot<span class="pl-k">=</span><span class="pl-c1">false</span>) <span class="pl-c"><span class="pl-c">#</span>callback function to observe training</span>
  <span class="pl-c1">display</span>(l)
  <span class="pl-c"><span class="pl-c">#</span> plot current prediction against data</span>
  <span class="pl-k">if</span> doplot
    pl <span class="pl-k">=</span> <span class="pl-c1">scatter</span>(t,ode_data[<span class="pl-c1">1</span>,:],label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>)
    <span class="pl-c1">scatter!</span>(pl,t,pred[<span class="pl-c1">1</span>,:],label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>prediction<span class="pl-pds">"</span></span>)
    <span class="pl-c1">display</span>(<span class="pl-c1">plot</span>(pl))
  <span class="pl-k">end</span>
  <span class="pl-k">return</span> <span class="pl-c1">false</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Display the ODE with the initial parameter values.</span>
<span class="pl-c1">cb</span>(n_ode<span class="pl-k">.</span>p,<span class="pl-c1">loss_n_ode</span>(n_ode<span class="pl-k">.</span>p)<span class="pl-k">...</span>)

res1 <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>(loss_n_ode, n_ode<span class="pl-k">.</span>p, <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.05</span>), cb <span class="pl-k">=</span> cb, maxiters <span class="pl-k">=</span> <span class="pl-c1">300</span>)
<span class="pl-c1">cb</span>(res1<span class="pl-k">.</span>minimizer,<span class="pl-c1">loss_n_ode</span>(res1<span class="pl-k">.</span>minimizer)<span class="pl-k">...</span>;doplot<span class="pl-k">=</span><span class="pl-c1">true</span>)
res2 <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>(loss_n_ode, res1<span class="pl-k">.</span>minimizer, <span class="pl-c1">LBFGS</span>(), cb <span class="pl-k">=</span> cb)
<span class="pl-c1">cb</span>(res2<span class="pl-k">.</span>minimizer,<span class="pl-c1">loss_n_ode</span>(res2<span class="pl-k">.</span>minimizer)<span class="pl-k">...</span>;doplot<span class="pl-k">=</span><span class="pl-c1">true</span>)

<span class="pl-c"><span class="pl-c">#</span> result is res2 as an Optim.jl object</span>
<span class="pl-c"><span class="pl-c">#</span> res2.minimizer are the best parameters</span>
<span class="pl-c"><span class="pl-c">#</span> res2.minimum is the best loss</span></pre></div>
<pre><code>* Status: failure (reached maximum number of iterations)

* Candidate solution
   Minimizer: [4.38e-01, -6.02e-01, 4.98e-01,  ...]
   Minimum:   8.691715e-02

* Found with
   Algorithm:     ADAM
   Initial Point: [-3.02e-02, -5.40e-02, 2.78e-01,  ...]

* Convergence measures
   |x - x'|               = NaN ≰ 0.0e+00
   |x - x'|/|x'|          = NaN ≰ 0.0e+00
   |f(x) - f(x')|         = NaN ≰ 0.0e+00
   |f(x) - f(x')|/|f(x')| = NaN ≰ 0.0e+00
   |g(x)|                 = NaN ≰ 0.0e+00

* Work counters
   Seconds run:   5  (vs limit Inf)
   Iterations:    300
   f(x) calls:    300
   ∇f(x) calls:   300


* Status: success

* Candidate solution
   Minimizer: [4.23e-01, -6.24e-01, 4.41e-01,  ...]
   Minimum:   1.429496e-02

* Found with
   Algorithm:     L-BFGS
   Initial Point: [4.38e-01, -6.02e-01, 4.98e-01,  ...]

* Convergence measures
   |x - x'|               = 1.46e-11 ≰ 0.0e+00
   |x - x'|/|x'|          = 1.26e-11 ≰ 0.0e+00
   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00
   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00
   |g(x)|                 = 4.28e-02 ≰ 1.0e-08

* Work counters
   Seconds run:   4  (vs limit Inf)
   Iterations:    35
   f(x) calls:    336
   ∇f(x) calls:   336

</code></pre>
<p>Here we showcase starting the optimization with <code>ADAM</code> to more quickly find a
minimum, and then honing in on the minimum by using <code>LBFGS</code>. By using the two
together, we are able to fit the neural ODE in 9 seconds! (Note, the timing
commented out the plotting).</p>
<h2><a id="user-content-use-with-gpus" class="anchor" aria-hidden="true" href="#use-with-gpus"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use with GPUs</h2>
<p>Note that the differential equation solvers will run on the GPU if the initial
condition is a GPU array. Thus for example, we can define a neural ODE by hand
that runs on the GPU:</p>
<div class="highlight highlight-source-julia"><pre>u0 <span class="pl-k">=</span> Float32[<span class="pl-c1">2.</span>; <span class="pl-c1">0.</span>] <span class="pl-k">|&gt;</span> gpu
dudt <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">50</span>,tanh),<span class="pl-c1">Dense</span>(<span class="pl-c1">50</span>,<span class="pl-c1">2</span>)) <span class="pl-k">|&gt;</span> gpu

p,re <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">destructure</span>(model)
<span class="pl-en">dudt_</span>(u,p,t) <span class="pl-k">=</span> <span class="pl-c1">re</span>(p)(u)
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(ODEfunc, u0,tspan, p)

<span class="pl-c"><span class="pl-c">#</span> Runs on a GPU</span>
sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(prob,<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>)</pre></div>
<p>and <code>concrete_solve</code> works similarly. Or we can directly use the neural ODE
layer function, like:</p>
<div class="highlight highlight-source-julia"><pre>n_ode <span class="pl-k">=</span> <span class="pl-c1">NeuralODE</span>(<span class="pl-c1">gpu</span>(dudt2),tspan,<span class="pl-c1">Tsit5</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>)</pre></div>
<h2><a id="user-content-universal-differential-equations" class="anchor" aria-hidden="true" href="#universal-differential-equations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Universal Differential Equations</h2>
<p>You can also mix a known differential equation and a neural differential equation, so that
the parameters and the neural network are estimated simultaneously!</p>
<h3><a id="user-content-universal-differential-equations-for-neural-optimal-control" class="anchor" aria-hidden="true" href="#universal-differential-equations-for-neural-optimal-control"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Universal Differential Equations for Neural Optimal Control</h3>
<p>Here's an example of doing this with both reverse-mode autodifferentiation and
with adjoints. We will assume that we know the dynamics of the second equation
(linear dynamics), and our goal is to find a neural network that will control
the second equation to stay close to 1.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DiffEqFlux, Flux, Optim, OrdinaryDiffEq

u0 <span class="pl-k">=</span> <span class="pl-c1">Float32</span>(<span class="pl-c1">1.1</span>)
tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0</span>f0,<span class="pl-c1">25.0</span>f0)

ann <span class="pl-k">=</span> <span class="pl-c1">FastChain</span>(<span class="pl-c1">FastDense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">16</span>,tanh), <span class="pl-c1">FastDense</span>(<span class="pl-c1">16</span>,<span class="pl-c1">16</span>,tanh), <span class="pl-c1">FastDense</span>(<span class="pl-c1">16</span>,<span class="pl-c1">1</span>))
p1 <span class="pl-k">=</span> <span class="pl-c1">initial_params</span>(ann)
p2 <span class="pl-k">=</span> Float32[<span class="pl-c1">0.5</span>,<span class="pl-k">-</span><span class="pl-c1">0.5</span>]
p3 <span class="pl-k">=</span> [p1;p2]
θ <span class="pl-k">=</span> Float32[u0;p3]

<span class="pl-k">function</span> <span class="pl-en">dudt_</span>(du,u,p,t)
    x, y <span class="pl-k">=</span> u
    du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-c1">ann</span>(u,p[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(p1)])[<span class="pl-c1">1</span>]
    du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> p[<span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>]<span class="pl-k">*</span>y <span class="pl-k">+</span> p[<span class="pl-c1">end</span>]<span class="pl-k">*</span>x
<span class="pl-k">end</span>
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(dudt_,u0,tspan,p3)
<span class="pl-c1">concrete_solve</span>(prob,<span class="pl-c1">Tsit5</span>(),[<span class="pl-c1">0</span>f0,u0],p3,abstol<span class="pl-k">=</span><span class="pl-c1">1e-8</span>,reltol<span class="pl-k">=</span><span class="pl-c1">1e-6</span>)

<span class="pl-k">function</span> <span class="pl-en">predict_adjoint</span>(θ)
  <span class="pl-c1">Array</span>(<span class="pl-c1">concrete_solve</span>(prob,<span class="pl-c1">Tsit5</span>(),[<span class="pl-c1">0</span>f0,θ[<span class="pl-c1">1</span>]],θ[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>],saveat<span class="pl-k">=</span><span class="pl-c1">0.0</span><span class="pl-k">:</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">25.0</span>))
<span class="pl-k">end</span>
<span class="pl-en">loss_adjoint</span>(θ) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,<span class="pl-c1">predict_adjoint</span>(θ)[<span class="pl-c1">2</span>,:]<span class="pl-k">.</span><span class="pl-k">-</span><span class="pl-c1">1</span>)
l <span class="pl-k">=</span> <span class="pl-c1">loss_adjoint</span>(θ)

cb <span class="pl-k">=</span> <span class="pl-k">function</span> (θ,l)
  <span class="pl-c1">println</span>(l)
  <span class="pl-c"><span class="pl-c">#</span>display(plot(solve(remake(prob,p=Flux.data(p3),u0=Flux.data(u0)),Tsit5(),saveat=0.1),ylim=(0,6)))</span>
  <span class="pl-k">return</span> <span class="pl-c1">false</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Display the ODE with the current parameter values.</span>
<span class="pl-c1">cb</span>(θ,l)

loss1 <span class="pl-k">=</span> <span class="pl-c1">loss_adjoint</span>(θ)
res <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>(loss_adjoint, θ, <span class="pl-c1">BFGS</span>(initial_stepnorm<span class="pl-k">=</span><span class="pl-c1">0.01</span>), cb <span class="pl-k">=</span> cb)</pre></div>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">*</span> Status<span class="pl-k">:</span> success

<span class="pl-k">*</span> Candidate solution
   Minimizer<span class="pl-k">:</span> [<span class="pl-c1">1.00e+00</span>, <span class="pl-c1">4.33e-02</span>, <span class="pl-c1">3.72e-01</span>,  <span class="pl-k">...</span>]
   Minimum<span class="pl-k">:</span>   <span class="pl-c1">6.572520e-13</span>

<span class="pl-k">*</span> Found with
   Algorithm<span class="pl-k">:</span>     BFGS
   Initial Point<span class="pl-k">:</span> [<span class="pl-c1">1.10e+00</span>, <span class="pl-c1">4.18e-02</span>, <span class="pl-c1">3.64e-01</span>,  <span class="pl-k">...</span>]

<span class="pl-k">*</span> Convergence measures
   <span class="pl-k">|</span>x <span class="pl-k">-</span> x<span class="pl-k">'</span><span class="pl-k">|</span>               <span class="pl-k">=</span> <span class="pl-c1">0.00e+00</span> <span class="pl-k">≤</span> <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span>x <span class="pl-k">-</span> x<span class="pl-k">'</span><span class="pl-k">|</span><span class="pl-k">/</span><span class="pl-k">|</span>x<span class="pl-k">'</span><span class="pl-k">|</span>          <span class="pl-k">=</span> <span class="pl-c1">0.00e+00</span> <span class="pl-k">≤</span> <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span><span class="pl-c1">f</span>(x) <span class="pl-k">-</span> <span class="pl-c1">f</span>(x<span class="pl-k">'</span>)<span class="pl-k">|</span>         <span class="pl-k">=</span> <span class="pl-c1">0.00e+00</span> <span class="pl-k">≤</span> <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span><span class="pl-c1">f</span>(x) <span class="pl-k">-</span> <span class="pl-c1">f</span>(x<span class="pl-k">'</span>)<span class="pl-k">|</span><span class="pl-k">/</span><span class="pl-k">|</span><span class="pl-c1">f</span>(x<span class="pl-k">'</span>)<span class="pl-k">|</span> <span class="pl-k">=</span> <span class="pl-c1">0.00e+00</span> <span class="pl-k">≤</span> <span class="pl-c1">0.0e+00</span>
   <span class="pl-k">|</span><span class="pl-c1">g</span>(x)<span class="pl-k">|</span>                 <span class="pl-k">=</span> <span class="pl-c1">5.45e-06</span> ≰ <span class="pl-c1">1.0e-08</span>

<span class="pl-k">*</span> Work counters
   Seconds run<span class="pl-k">:</span>   <span class="pl-c1">8</span>  (vs limit <span class="pl-c1">Inf</span>)
   Iterations<span class="pl-k">:</span>    <span class="pl-c1">23</span>
   <span class="pl-c1">f</span>(x) calls<span class="pl-k">:</span>    <span class="pl-c1">172</span>
   <span class="pl-c1">∇f</span>(x) calls<span class="pl-k">:</span>   <span class="pl-c1">172</span></pre></div>
<p>Notice that in just 23 iterations or 8 seconds we get to a minimum of <code>7e-13</code>,
successfully solving the nonlinear optimal control problem.</p>
<h2><a id="user-content-neural-differential-equations-for-non-odes-neural-sdes-neural-ddes-etc" class="anchor" aria-hidden="true" href="#neural-differential-equations-for-non-odes-neural-sdes-neural-ddes-etc"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural Differential Equations for Non-ODEs: Neural SDEs, Neural DDEs, etc.</h2>
<p>With neural stochastic differential equations, there is once again a helper form <code>neural_dmsde</code> which can
be used for the multiplicative noise case (consult the layers API documentation, or
<a href="https://github.com/MikeInnes/zygote-paper/blob/master/neural_sde/neural_sde.jl">this full example using the layer function</a>).</p>
<p>However, since there are far too many possible combinations for the API to
support, in many cases you will want to performantly define neural differential
equations for non-ODE systems from scratch. For these systems, it is generally
best to use <code>TrackerAdjoint</code> with non-mutating (out-of-place) forms. For example,
the following defines a neural SDE with neural networks for both the drift and
diffusion terms:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">dudt_</span>(u,p,t) <span class="pl-k">=</span> <span class="pl-c1">model</span>(u)
<span class="pl-en">g</span>(u,p,t) <span class="pl-k">=</span> <span class="pl-c1">model2</span>(u)
prob <span class="pl-k">=</span> <span class="pl-c1">SDEProblem</span>(dudt_,g,x,tspan,<span class="pl-c1">nothing</span>)</pre></div>
<p>where <code>model</code> and <code>model2</code> are different neural networks. The same can apply to a neural delay differential equation.
Its out-of-place formulation is <code>f(u,h,p,t)</code>. Thus for example, if we want to define a neural delay differential equation
which uses the history value at <code>p.tau</code> in the past, we can define:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">dudt_</span>(u,h,p,t) <span class="pl-k">=</span> <span class="pl-c1">model</span>([u;<span class="pl-c1">h</span>(t<span class="pl-k">-</span>p<span class="pl-k">.</span>tau)])
prob <span class="pl-k">=</span> <span class="pl-c1">DDEProblem</span>(dudt_,u0,h,tspan,<span class="pl-c1">nothing</span>)</pre></div>
<h3><a id="user-content-neural-sde-example" class="anchor" aria-hidden="true" href="#neural-sde-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural SDE Example</h3>
<p>First let's build training data from the same example as the neural ODE:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Flux, DiffEqFlux, StochasticDiffEq, Plots, DiffEqBase<span class="pl-k">.</span>EnsembleAnalysis, Statistics

u0 <span class="pl-k">=</span> Float32[<span class="pl-c1">2.</span>; <span class="pl-c1">0.</span>]
datasize <span class="pl-k">=</span> <span class="pl-c1">30</span>
tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0</span>f0,<span class="pl-c1">1.0</span>f0)

<span class="pl-k">function</span> <span class="pl-en">trueSDEfunc</span>(du,u,p,t)
    true_A <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">0.1</span> <span class="pl-c1">2.0</span>; <span class="pl-k">-</span><span class="pl-c1">2.0</span> <span class="pl-k">-</span><span class="pl-c1">0.1</span>]
    du <span class="pl-k">.=</span> ((u<span class="pl-k">.^</span><span class="pl-c1">3</span>)<span class="pl-k">'</span>true_A)<span class="pl-k">'</span>
<span class="pl-k">end</span>
t <span class="pl-k">=</span> <span class="pl-c1">range</span>(tspan[<span class="pl-c1">1</span>],tspan[<span class="pl-c1">2</span>],length<span class="pl-k">=</span>datasize)
mp <span class="pl-k">=</span> Float32[<span class="pl-c1">0.2</span>,<span class="pl-c1">0.2</span>]
<span class="pl-k">function</span> <span class="pl-en">true_noise_func</span>(du,u,p,t)
    du <span class="pl-k">.=</span> mp<span class="pl-k">.*</span>u
<span class="pl-k">end</span>
prob <span class="pl-k">=</span> <span class="pl-c1">SDEProblem</span>(trueSDEfunc,true_noise_func,u0,tspan)</pre></div>
<p>For our dataset we will use DifferentialEquations.jl's <a href="http://docs.juliadiffeq.org/dev/features/ensemble.html" rel="nofollow">parallel ensemble interface</a>
to generate data from the average of 10000 runs of the SDE:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Take a typical sample from the mean</span>
ensemble_prob <span class="pl-k">=</span> <span class="pl-c1">EnsembleProblem</span>(prob)
ensemble_sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(ensemble_prob,<span class="pl-c1">SOSRI</span>(),trajectories <span class="pl-k">=</span> <span class="pl-c1">10000</span>)
ensemble_sum <span class="pl-k">=</span> <span class="pl-c1">EnsembleSummary</span>(ensemble_sol)
sde_data,sde_data_vars <span class="pl-k">=</span> <span class="pl-c1">Array</span>.(<span class="pl-c1">timeseries_point_meanvar</span>(ensemble_sol,t))</pre></div>
<p>Now we build a neural SDE. For simplicity we will use the <code>NeuralDSDE</code>
neural SDE with diagonal noise layer function:</p>
<div class="highlight highlight-source-julia"><pre>drift_dudt <span class="pl-k">=</span> <span class="pl-c1">FastChain</span>((x,p) <span class="pl-k">-&gt;</span> x<span class="pl-k">.^</span><span class="pl-c1">3</span>,
             <span class="pl-c1">FastDense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">50</span>,tanh),
             <span class="pl-c1">FastDense</span>(<span class="pl-c1">50</span>,<span class="pl-c1">2</span>))
diffusion_dudt <span class="pl-k">=</span> <span class="pl-c1">FastChain</span>(<span class="pl-c1">FastDense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>))
n_sde <span class="pl-k">=</span> <span class="pl-c1">NeuralDSDE</span>(drift_dudt,diffusion_dudt,tspan,<span class="pl-c1">SOSRI</span>(),saveat<span class="pl-k">=</span>t,reltol<span class="pl-k">=</span><span class="pl-c1">1e-1</span>,abstol<span class="pl-k">=</span><span class="pl-c1">1e-1</span>)</pre></div>
<p>Let's see what that looks like:</p>
<div class="highlight highlight-source-julia"><pre>pred <span class="pl-k">=</span> <span class="pl-c1">n_sde</span>(u0) <span class="pl-c"><span class="pl-c">#</span> Get the prediction using the correct initial condition</span>
<span class="pl-en">drift_</span>(u,p,t) <span class="pl-k">=</span> <span class="pl-c1">drift_dudt</span>(u,p[<span class="pl-c1">1</span><span class="pl-k">:</span>n_sde<span class="pl-k">.</span>len])
<span class="pl-en">diffusion_</span>(u,p,t) <span class="pl-k">=</span> <span class="pl-c1">diffusion_dudt</span>(u,p[(n_sde<span class="pl-k">.</span>len<span class="pl-k">+</span><span class="pl-c1">1</span>)<span class="pl-k">:</span><span class="pl-c1">end</span>])
nprob <span class="pl-k">=</span> <span class="pl-c1">SDEProblem</span>(drift_,diffusion_,u0,(<span class="pl-c1">0.0</span>f0,<span class="pl-c1">1.2</span>f0),n_sde<span class="pl-k">.</span>p)

ensemble_nprob <span class="pl-k">=</span> <span class="pl-c1">EnsembleProblem</span>(nprob)
ensemble_nsol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(ensemble_nprob,<span class="pl-c1">SOSRI</span>(),trajectories <span class="pl-k">=</span> <span class="pl-c1">100</span>, saveat <span class="pl-k">=</span> t)
ensemble_nsum <span class="pl-k">=</span> <span class="pl-c1">EnsembleSummary</span>(ensemble_nsol)
p1 <span class="pl-k">=</span> <span class="pl-c1">plot</span>(ensemble_nsum, title <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Neural SDE: Before Training<span class="pl-pds">"</span></span>)
<span class="pl-c1">scatter!</span>(p1,t,sde_data<span class="pl-k">'</span>,lw<span class="pl-k">=</span><span class="pl-c1">3</span>)
<span class="pl-c1">scatter</span>(t,sde_data[<span class="pl-c1">1</span>,:],label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>)
<span class="pl-c1">scatter!</span>(t,pred[<span class="pl-c1">1</span>,:],label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>prediction<span class="pl-pds">"</span></span>)</pre></div>
<p>Now just as with the neural ODE we define a loss function that calculates the
mean and variance from <code>n</code> runs at each time point and uses the distance
from the data values:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">predict_n_sde</span>(p)
  <span class="pl-c1">Array</span>(<span class="pl-c1">n_sde</span>(u0,p))
<span class="pl-k">end</span>
<span class="pl-k">function</span> <span class="pl-en">loss_n_sde</span>(p;n<span class="pl-k">=</span><span class="pl-c1">100</span>)
  samples <span class="pl-k">=</span> [<span class="pl-c1">predict_n_sde</span>(p) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n]
  means <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">mean</span>.([[samples[i][j] <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples)] <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples[<span class="pl-c1">1</span>])]),<span class="pl-c1">size</span>(samples[<span class="pl-c1">1</span>])<span class="pl-k">...</span>)
  vars <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">var</span>.([[samples[i][j] <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples)] <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples[<span class="pl-c1">1</span>])]),<span class="pl-c1">size</span>(samples[<span class="pl-c1">1</span>])<span class="pl-k">...</span>)
  loss <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,sde_data <span class="pl-k">-</span> means) <span class="pl-k">+</span> <span class="pl-c1">sum</span>(abs2,sde_data_vars <span class="pl-k">-</span> vars)
  loss,means,vars
<span class="pl-k">end</span>

cb <span class="pl-k">=</span> <span class="pl-k">function</span> (p,loss,means,vars) <span class="pl-c"><span class="pl-c">#</span>callback function to observe training</span>
  <span class="pl-c"><span class="pl-c">#</span> loss against current data</span>
  <span class="pl-c1">display</span>(loss)
  <span class="pl-c"><span class="pl-c">#</span> plot current prediction against data</span>
  pl <span class="pl-k">=</span> <span class="pl-c1">scatter</span>(t,sde_data[<span class="pl-c1">1</span>,:],yerror <span class="pl-k">=</span> sde_data_vars[<span class="pl-c1">1</span>,:],label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>)
  <span class="pl-c1">scatter!</span>(pl,t,means[<span class="pl-c1">1</span>,:],ribbon <span class="pl-k">=</span> vars[<span class="pl-c1">1</span>,:], label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>prediction<span class="pl-pds">"</span></span>)
  <span class="pl-c1">display</span>(<span class="pl-c1">plot</span>(pl))
  <span class="pl-k">return</span> <span class="pl-c1">false</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Display the SDE with the initial parameter values.</span>
<span class="pl-c1">cb</span>(n_sde<span class="pl-k">.</span>p,<span class="pl-c1">loss_n_sde</span>(n_sde<span class="pl-k">.</span>p)<span class="pl-k">...</span>)</pre></div>
<p>Now we train using this loss function. We can pre-train a little bit using
a smaller <code>n</code> and then decrease it after it has had some time to adjust towards
the right mean behavior:</p>
<div class="highlight highlight-source-julia"><pre>opt <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.025</span>)
res1 <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>((p)<span class="pl-k">-&gt;</span><span class="pl-c1">loss_n_sde</span>(p,n<span class="pl-k">=</span><span class="pl-c1">10</span>),  n_sde<span class="pl-k">.</span>p, opt, cb <span class="pl-k">=</span> cb, maxiters <span class="pl-k">=</span> <span class="pl-c1">100</span>)
res2 <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>((p)<span class="pl-k">-&gt;</span><span class="pl-c1">loss_n_sde</span>(p,n<span class="pl-k">=</span><span class="pl-c1">100</span>), res1<span class="pl-k">.</span>minimizer, opt, cb <span class="pl-k">=</span> cb, maxiters <span class="pl-k">=</span> <span class="pl-c1">100</span>)</pre></div>
<p>And now we plot the solution to an ensemble of the trained neural SDE:</p>
<div class="highlight highlight-source-julia"><pre>samples <span class="pl-k">=</span> [<span class="pl-c1">predict_n_sde</span>(res2<span class="pl-k">.</span>minimizer) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1000</span>]
means <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">mean</span>.([[samples[i][j] <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples)] <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples[<span class="pl-c1">1</span>])]),<span class="pl-c1">size</span>(samples[<span class="pl-c1">1</span>])<span class="pl-k">...</span>)
vars <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">var</span>.([[samples[i][j] <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples)] <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(samples[<span class="pl-c1">1</span>])]),<span class="pl-c1">size</span>(samples[<span class="pl-c1">1</span>])<span class="pl-k">...</span>)

p2 <span class="pl-k">=</span> <span class="pl-c1">scatter</span>(t,sde_data<span class="pl-k">'</span>,yerror <span class="pl-k">=</span> sde_data_vars<span class="pl-k">'</span>,label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>, title <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Neural SDE: After Training<span class="pl-pds">"</span></span>, xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Time<span class="pl-pds">"</span></span>)
<span class="pl-c1">plot!</span>(p2,t,means<span class="pl-k">'</span>,lw <span class="pl-k">=</span> <span class="pl-c1">8</span>,ribbon <span class="pl-k">=</span> vars<span class="pl-k">'</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>prediction<span class="pl-pds">"</span></span>)

<span class="pl-c1">plot</span>(p1,p2,layout<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">1</span>))</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1814174/76975872-88dc9100-6909-11ea-80f7-242f661ebad1.png"><img src="https://user-images.githubusercontent.com/1814174/76975872-88dc9100-6909-11ea-80f7-242f661ebad1.png" alt="neural_sde" style="max-width:100%;"></a></p>
<p>Try this with GPUs as well!</p>
<h3><a id="user-content-enforcing-physical-constraints-with-singular-mass-matrices" class="anchor" aria-hidden="true" href="#enforcing-physical-constraints-with-singular-mass-matrices"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enforcing Physical Constraints with Singular Mass Matrices</h3>
<p>As shown in the <a href="https://docs.juliadiffeq.org/latest/tutorials/advanced_ode_example/#Handling-Mass-Matrices-1" rel="nofollow">stiff ODE tutorial</a>, differential-algebraic equations
(DAEs) can be used to impose physical constraints. One way to define a DAE is
through an ODE with a singular mass matrix. For example, if we make <code>Mu = f(u)</code>
where the last row of <code>M</code> is all zeros, then we have a constraint defined by
the right hand side. Using <code>NeuralODEMM</code>, we can use this to define a neural
ODE where the sum of all 3 terms must add to one. An example of this is as
follows:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Flux, DiffEqFlux, OrdinaryDiffEq, Optim, Test
<span class="pl-c"><span class="pl-c">#</span>A desired MWE for now, not a test yet.</span>
<span class="pl-k">function</span> <span class="pl-en">f</span>(du,u,p,t)
    y₁,y₂,y₃ <span class="pl-k">=</span> u
    k₁,k₂,k₃ <span class="pl-k">=</span> p
    du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-k">-</span>k₁<span class="pl-k">*</span>y₁ <span class="pl-k">+</span> k₃<span class="pl-k">*</span>y₂<span class="pl-k">*</span>y₃
    du[<span class="pl-c1">2</span>] <span class="pl-k">=</span>  k₁<span class="pl-k">*</span>y₁ <span class="pl-k">-</span> k₃<span class="pl-k">*</span>y₂<span class="pl-k">*</span>y₃ <span class="pl-k">-</span> k₂<span class="pl-k">*</span>y₂<span class="pl-k">^</span><span class="pl-c1">2</span>
    du[<span class="pl-c1">3</span>] <span class="pl-k">=</span>  y₁ <span class="pl-k">+</span> y₂ <span class="pl-k">+</span> y₃ <span class="pl-k">-</span> <span class="pl-c1">1</span>
    <span class="pl-c1">nothing</span>
<span class="pl-k">end</span>
u₀ <span class="pl-k">=</span> [<span class="pl-c1">1.0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>]
M <span class="pl-k">=</span> [<span class="pl-c1">1.</span> <span class="pl-c1">0</span>  <span class="pl-c1">0</span>
     <span class="pl-c1">0</span>  <span class="pl-c1">1.</span> <span class="pl-c1">0</span>
     <span class="pl-c1">0</span>  <span class="pl-c1">0</span>  <span class="pl-c1">0</span>]
tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0</span>,<span class="pl-c1">1.0</span>)
p <span class="pl-k">=</span> [<span class="pl-c1">0.04</span>,<span class="pl-c1">3e7</span>,<span class="pl-c1">1e4</span>]
func <span class="pl-k">=</span> <span class="pl-c1">ODEFunction</span>(f,mass_matrix<span class="pl-k">=</span>M)
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(func,u₀,tspan,p)
sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(prob,<span class="pl-c1">Rodas5</span>(),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>)

dudt2 <span class="pl-k">=</span> <span class="pl-c1">FastChain</span>(<span class="pl-c1">FastDense</span>(<span class="pl-c1">3</span>,<span class="pl-c1">64</span>,tanh),<span class="pl-c1">FastDense</span>(<span class="pl-c1">64</span>,<span class="pl-c1">2</span>))
ndae <span class="pl-k">=</span> <span class="pl-c1">NeuralODEMM</span>(dudt2, (u,p,t) <span class="pl-k">-&gt;</span> [u[<span class="pl-c1">1</span>] <span class="pl-k">+</span> u[<span class="pl-c1">2</span>] <span class="pl-k">+</span> u[<span class="pl-c1">3</span>] <span class="pl-k">-</span> <span class="pl-c1">1</span>], tspan, M, <span class="pl-c1">Rodas5</span>(autodiff<span class="pl-k">=</span><span class="pl-c1">false</span>),saveat<span class="pl-k">=</span><span class="pl-c1">0.1</span>)
<span class="pl-c1">ndae</span>(u₀)

<span class="pl-k">function</span> <span class="pl-en">predict_n_dae</span>(p)
    <span class="pl-c1">ndae</span>(u₀,p)
<span class="pl-k">end</span>
<span class="pl-k">function</span> <span class="pl-en">loss</span>(p)
    pred <span class="pl-k">=</span> <span class="pl-c1">predict_n_dae</span>(p)
    loss <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2,sol <span class="pl-k">.-</span> pred)
    loss,pred
<span class="pl-k">end</span>

cb <span class="pl-k">=</span> <span class="pl-k">function</span> (p,l,pred) <span class="pl-c"><span class="pl-c">#</span>callback function to observe training</span>
  <span class="pl-c1">display</span>(l)
  <span class="pl-k">return</span> <span class="pl-c1">false</span>
<span class="pl-k">end</span>

l1 <span class="pl-k">=</span> <span class="pl-c1">first</span>(<span class="pl-c1">loss</span>(ndae<span class="pl-k">.</span>p))
res <span class="pl-k">=</span> DiffEqFlux<span class="pl-k">.</span><span class="pl-c1">sciml_train</span>(loss, ndae<span class="pl-k">.</span>p, <span class="pl-c1">BFGS</span>(initial_stepnorm <span class="pl-k">=</span> <span class="pl-c1">0.001</span>),
                             cb <span class="pl-k">=</span> cb, maxiters <span class="pl-k">=</span> <span class="pl-c1">100</span>)
<span class="pl-c1">@test</span> res<span class="pl-k">.</span>minimum <span class="pl-k">&lt;</span> l1</pre></div>
<p>This is a highly stiff problem, making the fitting difficult, but we have manually
imposed that sum constraint via <code>(u,p,t) -&gt; [u[1] + u[2] + u[3] - 1]</code>, making
the fitting easier.</p>
<h3><a id="user-content-neural-jump-diffusions-neural-jump-sde-and-neural-partial-differential-equations-neural-pdes" class="anchor" aria-hidden="true" href="#neural-jump-diffusions-neural-jump-sde-and-neural-partial-differential-equations-neural-pdes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</h3>
<p>For the sake of not having a never-ending documentation of every single combination of CPU/GPU with
every layer and every neural differential equation, we will end here. But you may want to consult
<a href="http://www.stochasticlifestyle.com/neural-jump-sdes-jump-diffusions-and-neural-pdes/" rel="nofollow">this blog post</a> which
showcases defining neural jump diffusions and neural partial differential equations.</p>
<h2><a id="user-content-api-documentation" class="anchor" aria-hidden="true" href="#api-documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>API Documentation</h2>
<h3><a id="user-content-neural-de-layer-functions" class="anchor" aria-hidden="true" href="#neural-de-layer-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural DE Layer Functions</h3>
<ul>
<li><code>NeuralODE(model,tspan,solver,args...;kwargs...)</code> defines a neural ODE
layer where <code>model</code> is a Flux.jl model, <code>tspan</code> is the
time span to integrate, and the rest of the arguments are passed to the ODE
solver.</li>
<li><code>NeuralODEMM(model,constraints,tspan,mass_matrix,args...;kwargs...)</code>
defines a neural ODE layer with a mass matrix, i.e. <code>Mu=[model(u);constraints(u,p,t)]</code>
where the constraints cover the rank-deficient area of the mass matrix (i.e.,
the constraints of a differential-algebraic equation). Thus the mass matrix
is allowed to be singular.</li>
<li><code>NeuralDSDE(model1,model2,tspan,solver,args...;kwargs...)</code> defines a neural
SDE layer where <code>model1</code> is a Flux.jl for the drift equation, <code>model2</code> is a
Flux.jl model for the diffusion equation, <code>tspan</code> is the time span to
integrate, and the rest of the arguments are passed to the SDE solver.
The noise is diagonal, i.e. it assumes a vector output and performs
<code>model2(u) .* dW</code> against a dW matching the number of states.</li>
<li><code>NeuralSDE(model1,model2,tspan,nbrown,solver,args...;kwargs...)</code> defines a neural
SDE layer where <code>model1</code> is a Flux.jl for the drift equation, <code>model2</code> is a
Flux.jl model for the diffusion equation, <code>tspan</code> is the time span to
integrate, <code>nbrown</code> is the number of Brownian motions, and the rest of the
arguments are passed to the SDE solver. The model is multiplicative,
i.e. it's interpreted as <code>model2(u) * dW</code>, and so the return of <code>model2</code> should
be an appropriate matrix for performing this multiplication, i.e. the size of
its output should be <code>length(x) x nbrown</code>.</li>
<li><code>NeuralCDDE(model,tspan,lags,solver,args...;kwargs...)</code>defines a neural DDE
layer where <code>model</code> is a Flux.jl model, <code>tspan</code> is the
time span to integrate, lags is the lagged values to use in the predictor,
and the rest of the arguments are passed to the ODE solver. The model should
take in a vector that concatenates the lagged states, i.e.
<code>[u(t);u(t-lags[1]);...;u(t-lags[end])]</code></li>
</ul>
<h2><a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmarks</h2>
<p>A raw ODE solver benchmark showcases <a href="https://gist.github.com/ChrisRackauckas/cc6ac746e2dfd285c28e0584a2bfd320">a 50,000x performance advantage over torchdiffeq on small ODEs</a>.</p>
</article></div>