<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-pencilarrays" class="anchor" aria-hidden="true" href="#pencilarrays"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>PencilArrays</h1>
<p dir="auto"><a href="https://jipolanco.github.io/PencilArrays.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://jipolanco.github.io/PencilArrays.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://doi.org/10.5281/zenodo.5148035" rel="nofollow"><img src="https://camo.githubusercontent.com/c6df610fcbc36a7c87dc8aafd1bc8bb55efcfc65653c22075e83cf107a011664/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e353134383033352e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.5148035.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><a href="https://github.com/jipolanco/PencilArrays.jl/actions"><img src="https://github.com/jipolanco/PencilArrays.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/jipolanco/PencilArrays.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/774d614271432c339bae5b78c05aa597d35dc3de88fd669359935853dc86e241/68747470733a2f2f636f6465636f762e696f2f67682f6a69706f6c616e636f2f50656e63696c4172726179732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/jipolanco/PencilArrays.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Distributed Julia arrays using the MPI protocol.</p>
<p dir="auto">This package provides a convenient framework for working with multidimensional
Julia arrays distributed among MPI processes.</p>
<p dir="auto">The name of this package originates from the decomposition of 3D domains along
two out of three dimensions, sometimes called <em>pencil</em> decomposition.
This is illustrated by the figure below, which represents a distributed 3D array.
Each coloured block is managed by a different MPI process.</p>
<p align="center" dir="auto">
  <br>
  <a target="_blank" rel="noopener noreferrer" href="docs/src/img/pencils.svg"><img width="85%" alt="Pencil decomposition of 3D domains" src="docs/src/img/pencils.svg" style="max-width: 100%;"></a>
</p>
<p dir="auto">More generally, PencilArrays can decompose arrays of arbitrary dimension <code>N</code>,
along an arbitrary number of subdimensions <code>M ≤ N</code>.
(In the image above, <code>N = 3</code> and <code>M = 2</code>.)</p>
<p dir="auto">PencilArrays is the basis for the
<a href="https://github.com/jipolanco/PencilFFTs.jl">PencilFFTs</a> package, which
provides efficient and highly scalable distributed FFTs.</p>
<h2 dir="auto"><a id="user-content-features" class="anchor" aria-hidden="true" href="#features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Features</h2>
<ul dir="auto">
<li>
<p dir="auto">distribution of <code>N</code>-dimensional arrays among MPI processes;</p>
</li>
<li>
<p dir="auto">decomposition of arrays along all or a subset of dimensions;</p>
</li>
<li>
<p dir="auto">tools for conveniently and efficiently iterating over the coordinates of distributed multidimensional geometries;</p>
</li>
<li>
<p dir="auto">transpositions between different decomposition configurations, using
point-to-point and collective MPI communications;</p>
</li>
<li>
<p dir="auto">zero-cost, convenient dimension permutations using the <a href="https://github.com/jipolanco/StaticPermutations.jl">StaticPermutations.jl</a> package;</p>
</li>
<li>
<p dir="auto">convenient parallel I/O using either MPI-IO or the <a href="https://portal.hdfgroup.org/display/HDF5/Parallel+HDF5" rel="nofollow">Parallel
HDF5</a> libraries;</p>
</li>
<li>
<p dir="auto">distributed FFTs and related transforms via the
<a href="https://github.com/jipolanco/PencilFFTs.jl">PencilFFTs.jl</a> package.</p>
</li>
</ul>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">PencilArrays can be installed using the Julia package manager:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; ] add PencilArrays"><pre class="notranslate"><code>julia&gt; ] add PencilArrays
</code></pre></div>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick start</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MPI
using PencilArrays

MPI.Init()
comm = MPI.COMM_WORLD       # MPI communicator
rank = MPI.Comm_rank(comm)  # rank of local process

# Let's decompose a 3D grid across all MPI processes.
# The resulting configuration is described by a Pencil object.
dims_global = (42, 31, 29)  # global dimensions of the array
pen_x = Pencil(dims_global, comm)

# By default the 3D grid is decomposed along the two last dimensions, similarly
# to the &quot;x-pencil&quot; configuration in the figure above:
println(pen_x)
# Decomposition of 3D data
#   Data dimensions: (42, 31, 29)
#   Decomposed dimensions: (2, 3)
#   Data permutation: NoPermutation()
#   Array type: Array

# We can now allocate distributed arrays in the x-pencil configuration.
Ax = PencilArray{Float64}(undef, pen_x)
fill!(Ax, rank * π)  # each process locally fills its part of the array
parent(Ax)           # parent array holding the local data (here, an Array{Float64,3})
size(Ax)             # total size of the array = (42, 31, 29)
size_local(Ax)       # size of local part, e.g. (42, 8, 10) for a given process
range_local(Ax)      # range of local part on global grid, e.g. (1:42, 16:23, 20:29)

# Let's associate the dimensions to a global grid of coordinates (x_i, y_j, z_k)
xs_global = range(0, 1;  length = dims_global[1])
ys_global = range(0, 2;  length = dims_global[2])
zs_global = range(0, 2π; length = dims_global[3])

# Part of the grid associated to the local MPI process:
grid = localgrid(pen_x, (xs_global, ys_global, zs_global))

# This is convenient for example if we want to initialise the `Ax` array as
# a function of the grid coordinates (x, y, z):
@. Ax = grid.x + (2 * grid.y * cos(grid.z))

# Alternatively (useful in higher dimensions):
@. Ax = grid[1] + (2 * grid[2] * cos(grid[3]))

# Create another pencil configuration, decomposing along dimensions (1, 3).
# We could use the same constructor as before, but it's recommended to reuse the
# previous Pencil instead to reduce memory usage.
pen_y = Pencil(pen_x; decomp_dims = (1, 3))

# Now transpose from the x-pencil to the y-pencil configuration, redistributing
# the data initially in Ax.
Ay = PencilArray{Float64}(undef, pen_y)
transpose!(Ay, Ax)

# We can check that Ax and Ay have the same data (but distributed differently)
# by combining the data from all different processes onto a single process
# (this should never be used for large datasets!)
gather(Ax) == gather(Ay)  # true"><pre><span class="pl-k">using</span> MPI
<span class="pl-k">using</span> PencilArrays

MPI<span class="pl-k">.</span><span class="pl-c1">Init</span>()
comm <span class="pl-k">=</span> MPI<span class="pl-k">.</span>COMM_WORLD       <span class="pl-c"><span class="pl-c">#</span> MPI communicator</span>
rank <span class="pl-k">=</span> MPI<span class="pl-k">.</span><span class="pl-c1">Comm_rank</span>(comm)  <span class="pl-c"><span class="pl-c">#</span> rank of local process</span>

<span class="pl-c"><span class="pl-c">#</span> Let's decompose a 3D grid across all MPI processes.</span>
<span class="pl-c"><span class="pl-c">#</span> The resulting configuration is described by a Pencil object.</span>
dims_global <span class="pl-k">=</span> (<span class="pl-c1">42</span>, <span class="pl-c1">31</span>, <span class="pl-c1">29</span>)  <span class="pl-c"><span class="pl-c">#</span> global dimensions of the array</span>
pen_x <span class="pl-k">=</span> <span class="pl-c1">Pencil</span>(dims_global, comm)

<span class="pl-c"><span class="pl-c">#</span> By default the 3D grid is decomposed along the two last dimensions, similarly</span>
<span class="pl-c"><span class="pl-c">#</span> to the "x-pencil" configuration in the figure above:</span>
<span class="pl-c1">println</span>(pen_x)
<span class="pl-c"><span class="pl-c">#</span> Decomposition of 3D data</span>
<span class="pl-c"><span class="pl-c">#</span>   Data dimensions: (42, 31, 29)</span>
<span class="pl-c"><span class="pl-c">#</span>   Decomposed dimensions: (2, 3)</span>
<span class="pl-c"><span class="pl-c">#</span>   Data permutation: NoPermutation()</span>
<span class="pl-c"><span class="pl-c">#</span>   Array type: Array</span>

<span class="pl-c"><span class="pl-c">#</span> We can now allocate distributed arrays in the x-pencil configuration.</span>
Ax <span class="pl-k">=</span> <span class="pl-c1">PencilArray</span><span class="pl-c1">{Float64}</span>(undef, pen_x)
<span class="pl-c1">fill!</span>(Ax, rank <span class="pl-k">*</span> π)  <span class="pl-c"><span class="pl-c">#</span> each process locally fills its part of the array</span>
<span class="pl-c1">parent</span>(Ax)           <span class="pl-c"><span class="pl-c">#</span> parent array holding the local data (here, an Array{Float64,3})</span>
<span class="pl-c1">size</span>(Ax)             <span class="pl-c"><span class="pl-c">#</span> total size of the array = (42, 31, 29)</span>
<span class="pl-c1">size_local</span>(Ax)       <span class="pl-c"><span class="pl-c">#</span> size of local part, e.g. (42, 8, 10) for a given process</span>
<span class="pl-c1">range_local</span>(Ax)      <span class="pl-c"><span class="pl-c">#</span> range of local part on global grid, e.g. (1:42, 16:23, 20:29)</span>

<span class="pl-c"><span class="pl-c">#</span> Let's associate the dimensions to a global grid of coordinates (x_i, y_j, z_k)</span>
xs_global <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>;  length <span class="pl-k">=</span> dims_global[<span class="pl-c1">1</span>])
ys_global <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">0</span>, <span class="pl-c1">2</span>;  length <span class="pl-k">=</span> dims_global[<span class="pl-c1">2</span>])
zs_global <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">0</span>, <span class="pl-c1">2</span>π; length <span class="pl-k">=</span> dims_global[<span class="pl-c1">3</span>])

<span class="pl-c"><span class="pl-c">#</span> Part of the grid associated to the local MPI process:</span>
grid <span class="pl-k">=</span> <span class="pl-c1">localgrid</span>(pen_x, (xs_global, ys_global, zs_global))

<span class="pl-c"><span class="pl-c">#</span> This is convenient for example if we want to initialise the `Ax` array as</span>
<span class="pl-c"><span class="pl-c">#</span> a function of the grid coordinates (x, y, z):</span>
<span class="pl-c1">@.</span> Ax <span class="pl-k">=</span> grid<span class="pl-k">.</span>x <span class="pl-k">+</span> (<span class="pl-c1">2</span> <span class="pl-k">*</span> grid<span class="pl-k">.</span>y <span class="pl-k">*</span> <span class="pl-c1">cos</span>(grid<span class="pl-k">.</span>z))

<span class="pl-c"><span class="pl-c">#</span> Alternatively (useful in higher dimensions):</span>
<span class="pl-c1">@.</span> Ax <span class="pl-k">=</span> grid[<span class="pl-c1">1</span>] <span class="pl-k">+</span> (<span class="pl-c1">2</span> <span class="pl-k">*</span> grid[<span class="pl-c1">2</span>] <span class="pl-k">*</span> <span class="pl-c1">cos</span>(grid[<span class="pl-c1">3</span>]))

<span class="pl-c"><span class="pl-c">#</span> Create another pencil configuration, decomposing along dimensions (1, 3).</span>
<span class="pl-c"><span class="pl-c">#</span> We could use the same constructor as before, but it's recommended to reuse the</span>
<span class="pl-c"><span class="pl-c">#</span> previous Pencil instead to reduce memory usage.</span>
pen_y <span class="pl-k">=</span> <span class="pl-c1">Pencil</span>(pen_x; decomp_dims <span class="pl-k">=</span> (<span class="pl-c1">1</span>, <span class="pl-c1">3</span>))

<span class="pl-c"><span class="pl-c">#</span> Now transpose from the x-pencil to the y-pencil configuration, redistributing</span>
<span class="pl-c"><span class="pl-c">#</span> the data initially in Ax.</span>
Ay <span class="pl-k">=</span> <span class="pl-c1">PencilArray</span><span class="pl-c1">{Float64}</span>(undef, pen_y)
<span class="pl-c1">transpose!</span>(Ay, Ax)

<span class="pl-c"><span class="pl-c">#</span> We can check that Ax and Ay have the same data (but distributed differently)</span>
<span class="pl-c"><span class="pl-c">#</span> by combining the data from all different processes onto a single process</span>
<span class="pl-c"><span class="pl-c">#</span> (this should never be used for large datasets!)</span>
<span class="pl-c1">gather</span>(Ax) <span class="pl-k">==</span> <span class="pl-c1">gather</span>(Ay)  <span class="pl-c"><span class="pl-c">#</span> true</span></pre></div>
</article></div>