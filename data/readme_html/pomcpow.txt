<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-pomcpow" class="anchor" aria-hidden="true" href="#pomcpow"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>POMCPOW</h1>
<p dir="auto"><a href="https://github.com/JuliaPOMDP/POMCPOW.jl/actions/workflows/CI.yml"><img src="https://github.com/JuliaPOMDP/POMCPOW.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaPOMDP/POMCPOW.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/35ed9997d404cb91e5e3913955a4751272f40110cf50e0123d178efb7b02ab6f/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961504f4d44502f504f4d43504f572e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d6462396c557044696a4b" alt="codecov" data-canonical-src="https://codecov.io/gh/JuliaPOMDP/POMCPOW.jl/branch/master/graph/badge.svg?token=db9lUpDijK" style="max-width: 100%;"></a></p>
<p dir="auto">POMCPOW is an online solver based on Monte Carlo tree search for POMDPs with continuous state, action, and observation spaces. For more information, see <a href="https://arxiv.org/abs/1709.06196" rel="nofollow">https://arxiv.org/abs/1709.06196</a> (code to reproduce the experiments in this paper can be found <a href="https://github.com/zsunberg/ContinuousPOMDPTreeSearchExperiments.jl">here</a>).</p>
<p dir="auto">This POMCPOW implementation solves problems specified using the <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a> interface. The requirements are the same as for an importance-sampling particle filter - a generative model for the dynamics and an explicit observation model.</p>
<h1 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h1>
<p dir="auto">For Julia 1.0 and above, use the JuliaPOMDP registry:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Pkg
Pkg.add(&quot;POMDPs&quot;)
import POMDPs
POMDPs.add_registry()
Pkg.add(&quot;POMCPOW&quot;)"><pre><span class="pl-k">import</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>POMDPs<span class="pl-pds">"</span></span>)
<span class="pl-k">import</span> POMDPs
POMDPs<span class="pl-k">.</span><span class="pl-c1">add_registry</span>()
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>POMCPOW<span class="pl-pds">"</span></span>)</pre></div>
<h1 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h1>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using POMDPs
using POMCPOW
using POMDPModels
using POMDPTools

solver = POMCPOWSolver(criterion=MaxUCB(20.0))
pomdp = BabyPOMDP() # from POMDPModels
planner = solve(solver, pomdp)

hr = HistoryRecorder(max_steps=100)
hist = simulate(hr, pomdp, planner)
for (s, b, a, r, sp, o) in hist
    @show s, a, r, sp
end

rhist = simulate(hr, pomdp, RandomPolicy(pomdp))
println(&quot;&quot;&quot;
    Cumulative Discounted Reward (for 1 simulation)
        Random: $(discounted_reward(rhist))
        POMCPOW: $(discounted_reward(hist))
    &quot;&quot;&quot;)"><pre><span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> POMCPOW
<span class="pl-k">using</span> POMDPModels
<span class="pl-k">using</span> POMDPTools

solver <span class="pl-k">=</span> <span class="pl-c1">POMCPOWSolver</span>(criterion<span class="pl-k">=</span><span class="pl-c1">MaxUCB</span>(<span class="pl-c1">20.0</span>))
pomdp <span class="pl-k">=</span> <span class="pl-c1">BabyPOMDP</span>() <span class="pl-c"><span class="pl-c">#</span> from POMDPModels</span>
planner <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, pomdp)

hr <span class="pl-k">=</span> <span class="pl-c1">HistoryRecorder</span>(max_steps<span class="pl-k">=</span><span class="pl-c1">100</span>)
hist <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(hr, pomdp, planner)
<span class="pl-k">for</span> (s, b, a, r, sp, o) <span class="pl-k">in</span> hist
    <span class="pl-c1">@show</span> s, a, r, sp
<span class="pl-k">end</span>

rhist <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(hr, pomdp, <span class="pl-c1">RandomPolicy</span>(pomdp))
<span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"""</span></span>
<span class="pl-s">    Cumulative Discounted Reward (for 1 simulation)</span>
<span class="pl-s">        Random: <span class="pl-v">$(<span class="pl-c1">discounted_reward</span>(rhist))</span></span>
<span class="pl-s">        POMCPOW: <span class="pl-v">$(<span class="pl-c1">discounted_reward</span>(hist))</span></span>
<span class="pl-s">    <span class="pl-pds">"""</span></span>)</pre></div>
<p dir="auto">Algorithm options are controlled with keyword arguments to the constructor. Use <code>?POMCPOWSolver</code> to see a list of options. It should output the following:</p>
<p dir="auto">Fields:</p>
<ul dir="auto">
<li><code>eps::Float64</code>:
Rollouts and tree expansion will stop when discount^depth is less than this.
default: <code>0.01</code></li>
<li><code>max_depth::Int</code>:
Rollouts and tree expension will stop when this depth is reached.
default: <code>10</code></li>
<li><code>criterion::Any</code>:
Criterion to decide which action to take at each node. e.g. <code>MaxUCB(c)</code>, <code>MaxQ</code>, or <code>MaxTries</code>.
default: <code>MaxUCB(1.0)</code></li>
<li><code>final_criterion::Any</code>:
Criterion for choosing the action to take after the tree is constructed.
default: <code>MaxQ()</code></li>
<li><code>tree_queries::Int</code>:
Number of iterations during each action() call.
default: <code>1000</code></li>
<li><code>max_time::Float64</code>:
Time limit for planning at each steps (seconds).
default: <code>Inf</code></li>
<li><code>rng::AbstractRNG</code>:
Random number generator.
default: <code>Base.GLOBAL_RNG</code></li>
<li><code>node_sr_belief_updater::Updater</code>:
Updater for state-reward distribution at the nodes.
default: <code>POWNodeFilter()</code></li>
<li><code>estimate_value::Any</code>: (rollout policy can be specified by setting this to RolloutEstimator(policy))
Function, object, or number used to estimate the value at the leaf nodes.
If this is a function <code>f</code>, <code>f(pomdp, s, h::BeliefNode, steps)</code> will be called to estimate the value.
If this is an object <code>o</code>, <code>estimate_value(o, pomdp, s, h::BeliefNode, steps)</code> will be called.
If this is a number, the value will be set to that number.
default: <code>RolloutEstimator(RandomSolver(rng))</code></li>
<li><code>enable_action_pw::Bool</code>:
Controls whether progressive widening is done on actions; if <code>false</code>, the entire action space is used.
default: <code>true</code></li>
<li><code>check_repeat_obs::Bool</code>:
Check if an observation was sampled multiple times. This has some dictionary maintenance overhead, but prevents multiple nodes with the same observation from being created. If the observation space is discrete, this should probably be used, but can be turned off for speed.
default: <code>true</code></li>
<li><code>check_repeat_act::Bool</code>:
Check if an action was sampled multiple times. This has some dictionary maintenance overhead, but prevents multiple nodes with the same action from being created. If the action space is discrete, this should probably be used, but can be turned off for speed.
default: <code>true</code></li>
<li><code>k_action::Float64</code>, <code>alpha_action::Float64</code>, <code>k_observation::Float64</code>, <code>alpha_observation::Float64</code>:
These constants control the double progressive widening. A new observation
or action will be added if the number of children is less than or equal to kN^alpha.
defaults: k: <code>10</code>, alpha: <code>0.5</code></li>
<li><code>init_V::Any</code>:
Function, object, or number used to set the initial V(h,a) value at a new node.
If this is a function <code>f</code>, <code>f(pomdp, h, a)</code> will be called to set the value.
If this is an object <code>o</code>, <code>init_V(o, pomdp, h, a)</code> will be called.
If this is a number, V will be set to that number
default: <code>0.0</code></li>
<li><code>init_N::Any</code>:
Function, object, or number used to set the initial N(s,a) value at a new node.
If this is a function <code>f</code>, <code>f(pomdp, h, a)</code> will be called to set the value.
If this is an object <code>o</code>, <code>init_N(o, pomdp, h, a)</code> will be called.
If this is a number, N will be set to that number
default: <code>0</code></li>
<li><code>next_action::Any</code>
Function or object used to choose the next action to be considered for progressive widening.
The next action is determined based on the POMDP, the belief, <code>b</code>, and the current <code>BeliefNode</code>, <code>h</code>.
If this is a function <code>f</code>, <code>f(pomdp, b, h)</code> will be called to set the value.
If this is an object <code>o</code>, <code>next_action(o, pomdp, b, h)</code> will be called.
default: <code>RandomActionGenerator(rng)</code></li>
<li><code>default_action::Any</code>:
Function, action, or Policy used to determine the action if POMCP fails with exception <code>ex</code>.
If this is a Function <code>f</code>, <code>f(belief, ex)</code> will be called.
If this is a Policy <code>p</code>, <code>action(p, belief)</code> will be called.
If it is an object <code>a</code>, <code>default_action(a, belief, ex)</code> will be called, and
if this method is not implemented, <code>a</code> will be returned directly.</li>
</ul>
<p dir="auto">Check out <a href="https://github.com/zsunberg/VDPTag2.jl/blob/master/README.md">VDPTag2.jl</a> for an additional problem that is solved by POMCPOW.</p>
</article></div>