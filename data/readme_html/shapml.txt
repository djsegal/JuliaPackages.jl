<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><p><a href="https://travis-ci.org/nredell/ShapML.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a2a92fed5e4aec457d0819cc3935490a6bb0e312/68747470733a2f2f7472617669732d63692e6f72672f6e726564656c6c2f536861704d4c2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/nredell/ShapML.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/nredell/ShapML.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/8597a45eea78b76f5af9f94fbded6eec7bdfad48/68747470733a2f2f636f6465636f762e696f2f67682f6e726564656c6c2f536861704d4c2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/nredell/ShapML.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a>
<a href="https://nredell.github.io/ShapML.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3e353c26ddfe819150acbc732248f4f2a37f5175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></p>
<h1><a id="user-content-shapml-" class="anchor" aria-hidden="true" href="#shapml-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ShapML <a target="_blank" rel="noopener noreferrer" href="./tools/ShapML_logo.png"><img src="./tools/ShapML_logo.png" alt="ShapML logo" align="right" height="138.5" style="max-width:100%;"></a></h1>
<p>The purpose of <code>ShapML</code> is to compute stochastic feature-level Shapley values which
can be used to (a) interpret and/or (b) assess the fairness of any machine learning model.
<strong><a href="https://christophm.github.io/interpretable-ml-book/shapley.html" rel="nofollow">Shapley values</a></strong>
are an intuitive and theoretically sound model-agnostic diagnostic tool to understand both <strong>global feature importance</strong> across all instances in a data set and instance/row-level <strong>local feature importance</strong> in black-box machine learning models.</p>
<p>This package implements the algorithm described in
<a href="https://link.springer.com/article/10.1007%2Fs10115-013-0679-x" rel="nofollow">Å trumbelj and Kononenko's (2014) sampling-based Shapley approximation algorithm</a>
to compute the stochastic Shapley values for a given instance and model feature.</p>
<ul>
<li>
<p><strong>Flexibility</strong>:</p>
<ul>
<li>Shapley values can be estimated for any machine learning model using a simple user-defined <code>predict()</code> wrapper function.</li>
</ul>
</li>
<li>
<p><strong>Speed</strong>:</p>
<ul>
<li>The speed advantage of <code>ShapML</code> comes in the form of giving the user the ability to select 1 or more target features of interest and avoid having to compute Shapley values for all model features (i.e., a subset of target features from a trained model will return the same feature-level Shapley values as the full model with all features). This is especially useful in high-dimensional models as the computation of a Shapley value is exponential in the number of features.</li>
</ul>
</li>
</ul>
<h2><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Install</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>ShapML<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-documentation-and-vignettes" class="anchor" aria-hidden="true" href="#documentation-and-vignettes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Documentation and Vignettes</h2>
<ul>
<li><strong><a href="https://nredell.github.io/ShapML.jl/dev/" rel="nofollow">Docs</a></strong></li>
</ul>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Examples</h2>
<h3><a id="user-content-random-forest-regression-model---non-parallel" class="anchor" aria-hidden="true" href="#random-forest-regression-model---non-parallel"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Random Forest regression model - Non-parallel</h3>
<ul>
<li>
<p>We'll explain the impact of 13 features from the Boston Housing dataset on the
predicted outcome <code>MedV</code>--or the median value of owner-occupied homes in $1000's--using predictions
from a trained Random Forest regression model and stochastic Shapley values.</p>
</li>
<li>
<p>We'll explain a subset of 300 instances and then assess global feature importance
by aggregating the unique feature importances for each of these instances.</p>
</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ShapML
<span class="pl-k">using</span> RDatasets
<span class="pl-k">using</span> DataFrames
<span class="pl-k">using</span> MLJ  <span class="pl-c"><span class="pl-c">#</span> Machine learning</span>
<span class="pl-k">using</span> Gadfly  <span class="pl-c"><span class="pl-c">#</span> Plotting</span>

<span class="pl-c"><span class="pl-c">#</span> Load data.</span>
boston <span class="pl-k">=</span> RDatasets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>MASS<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Boston<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">#</span>------------------------------------------------------------------------------</span>
<span class="pl-c"><span class="pl-c">#</span> Train a machine learning model; currently limited to single outcome regression and binary classification.</span>
outcome_name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>MedV<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Data prep.</span>
y, X <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">unpack</span>(boston, <span class="pl-k">==</span>(<span class="pl-c1">Symbol</span>(outcome_name)), colname <span class="pl-k">-&gt;</span> <span class="pl-c1">true</span>)

<span class="pl-c"><span class="pl-c">#</span> Instantiate an ML model; choose any single-outcome ML model from any package.</span>
random_forest <span class="pl-k">=</span> <span class="pl-c1">@load</span> RandomForestRegressor pkg <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>DecisionTree<span class="pl-pds">"</span></span>
model <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">machine</span>(random_forest, X, y)

<span class="pl-c"><span class="pl-c">#</span> Train the model.</span>
<span class="pl-c1">fit!</span>(model)

<span class="pl-c"><span class="pl-c">#</span> Create a wrapper function that takes the following positional arguments: (1) a</span>
<span class="pl-c"><span class="pl-c">#</span> trained ML model from any Julia package, (2) a DataFrame of model features. The</span>
<span class="pl-c"><span class="pl-c">#</span> function should return a 1-column DataFrame of predictions--column names do not matter.</span>
<span class="pl-k">function</span> <span class="pl-en">predict_function</span>(model, data)
  data_pred <span class="pl-k">=</span> <span class="pl-c1">DataFrame</span>(y_pred <span class="pl-k">=</span> <span class="pl-c1">predict</span>(model, data))
  <span class="pl-k">return</span> data_pred
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span>------------------------------------------------------------------------------</span>
<span class="pl-c"><span class="pl-c">#</span> ShapML setup.</span>
explain <span class="pl-k">=</span> <span class="pl-c1">copy</span>(boston[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">300</span>, :]) <span class="pl-c"><span class="pl-c">#</span> Compute Shapley feature-level predictions for 300 instances.</span>
explain <span class="pl-k">=</span> <span class="pl-c1">select</span>(explain, <span class="pl-c1">Not</span>(<span class="pl-c1">Symbol</span>(outcome_name)))  <span class="pl-c"><span class="pl-c">#</span> Remove the outcome column.</span>

reference <span class="pl-k">=</span> <span class="pl-c1">copy</span>(boston)  <span class="pl-c"><span class="pl-c">#</span> An optional reference population to compute the baseline prediction.</span>
reference <span class="pl-k">=</span> <span class="pl-c1">select</span>(reference, <span class="pl-c1">Not</span>(<span class="pl-c1">Symbol</span>(outcome_name)))

sample_size <span class="pl-k">=</span> <span class="pl-c1">60</span>  <span class="pl-c"><span class="pl-c">#</span> Number of Monte Carlo samples.</span>
<span class="pl-c"><span class="pl-c">#</span>------------------------------------------------------------------------------</span>
<span class="pl-c"><span class="pl-c">#</span> Compute stochastic Shapley values.</span>
data_shap <span class="pl-k">=</span> ShapML<span class="pl-k">.</span><span class="pl-c1">shap</span>(explain <span class="pl-k">=</span> explain,
                        reference <span class="pl-k">=</span> reference,
                        model <span class="pl-k">=</span> model,
                        predict_function <span class="pl-k">=</span> predict_function,
                        sample_size <span class="pl-k">=</span> sample_size,
                        seed <span class="pl-k">=</span> <span class="pl-c1">1</span>
                        )

<span class="pl-c1">show</span>(data_shap, allcols <span class="pl-k">=</span> <span class="pl-c1">true</span>)</pre></div>
<p align="center">
    <a target="_blank" rel="noopener noreferrer" href="./tools/shap_output.PNG"><img src="./tools/shap_output.PNG" alt="shap_output" style="max-width:100%;"></a>
</p>
<ul>
<li>
<p>Now we'll create several plots that summarize the Shapley results for our Random Forest model.
These plots will eventually be refined and incorporated into <code>ShapML</code>.</p>
</li>
<li>
<p><strong>Global feature importance</strong></p>
<ul>
<li>Because Shapley values represent deviations from the average or baseline prediction,
plotting their average absolute value for each feature gives a sense of the magnitude with which
they affect model predictions across all explained instances.</li>
</ul>
</li>
</ul>
<div class="highlight highlight-source-julia"><pre>data_plot <span class="pl-k">=</span> DataFrames<span class="pl-k">.</span><span class="pl-c1">by</span>(data_shap, [<span class="pl-c1">:feature_name</span>],
                          mean_effect <span class="pl-k">=</span> [<span class="pl-c1">:shap_effect</span>] <span class="pl-k">=&gt;</span> x <span class="pl-k">-&gt;</span> <span class="pl-c1">mean</span>(<span class="pl-c1">abs</span>.(x<span class="pl-k">.</span>shap_effect)))

data_plot <span class="pl-k">=</span> <span class="pl-c1">sort</span>(data_plot, <span class="pl-c1">order</span>(<span class="pl-c1">:mean_effect</span>, rev <span class="pl-k">=</span> <span class="pl-c1">true</span>))

baseline <span class="pl-k">=</span> <span class="pl-c1">round</span>(data_shap<span class="pl-k">.</span>intercept[<span class="pl-c1">1</span>], digits <span class="pl-k">=</span> <span class="pl-c1">1</span>)

p <span class="pl-k">=</span> <span class="pl-c1">plot</span>(data_plot, y <span class="pl-k">=</span> <span class="pl-c1">:feature_name</span>, x <span class="pl-k">=</span> <span class="pl-c1">:mean_effect</span>, Coord<span class="pl-k">.</span><span class="pl-c1">cartesian</span>(yflip <span class="pl-k">=</span> <span class="pl-c1">true</span>),
         Scale<span class="pl-k">.</span>y_discrete, Geom<span class="pl-k">.</span><span class="pl-c1">bar</span>(position <span class="pl-k">=</span> <span class="pl-c1">:dodge</span>, orientation <span class="pl-k">=</span> <span class="pl-c1">:horizontal</span>),
         <span class="pl-c1">Theme</span>(bar_spacing <span class="pl-k">=</span> <span class="pl-c1">1</span>mm),
         Guide<span class="pl-k">.</span><span class="pl-c1">xlabel</span>(<span class="pl-s"><span class="pl-pds">"</span>|Shapley effect| (baseline = <span class="pl-v">$baseline</span>)<span class="pl-pds">"</span></span>), Guide<span class="pl-k">.</span><span class="pl-c1">ylabel</span>(<span class="pl-c1">nothing</span>),
         Guide<span class="pl-k">.</span><span class="pl-c1">title</span>(<span class="pl-s"><span class="pl-pds">"</span>Feature Importance - Mean Absolute Shapley Value<span class="pl-pds">"</span></span>))
p</pre></div>
<p align="center">
    <a target="_blank" rel="noopener noreferrer" href="./tools/feature_importance_example.png"><img src="./tools/feature_importance_example.png" alt="feature_importance" style="max-width:100%;"></a>
</p>
<ul>
<li><strong>Global feature effects</strong>
<ul>
<li>The plot below shows how changing the value of the <code>Rm</code> feature--the most influential feature overall--affects
model predictions (holding the other features constant). Each point represents 1 of our 300 explained instances.
The black line is a loess line of best fit to summarize the effect.</li>
</ul>
</li>
</ul>
<div class="highlight highlight-source-julia"><pre>data_plot <span class="pl-k">=</span> data_shap[data_shap<span class="pl-k">.</span>feature_name <span class="pl-k">.==</span> <span class="pl-s"><span class="pl-pds">"</span>Rm<span class="pl-pds">"</span></span>, :]  <span class="pl-c"><span class="pl-c">#</span> Selecting 1 feature for ease of plotting.</span>

baseline <span class="pl-k">=</span> <span class="pl-c1">round</span>(data_shap<span class="pl-k">.</span>intercept[<span class="pl-c1">1</span>], digits <span class="pl-k">=</span> <span class="pl-c1">1</span>)

p_points <span class="pl-k">=</span> <span class="pl-c1">layer</span>(data_plot, x <span class="pl-k">=</span> <span class="pl-c1">:feature_value</span>, y <span class="pl-k">=</span> <span class="pl-c1">:shap_effect</span>, Geom<span class="pl-k">.</span><span class="pl-c1">point</span>())
p_line <span class="pl-k">=</span> <span class="pl-c1">layer</span>(data_plot, x <span class="pl-k">=</span> <span class="pl-c1">:feature_value</span>, y <span class="pl-k">=</span> <span class="pl-c1">:shap_effect</span>, Geom<span class="pl-k">.</span><span class="pl-c1">smooth</span>(method <span class="pl-k">=</span> <span class="pl-c1">:loess</span>, smoothing <span class="pl-k">=</span> <span class="pl-c1">0.5</span>),
               <span class="pl-c1">style</span>(line_width <span class="pl-k">=</span> <span class="pl-c1">0.75</span>mm,), <span class="pl-c1">Theme</span>(default_color <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>black<span class="pl-pds">"</span></span>))
p <span class="pl-k">=</span> <span class="pl-c1">plot</span>(p_line, p_points, Guide<span class="pl-k">.</span><span class="pl-c1">xlabel</span>(<span class="pl-s"><span class="pl-pds">"</span>Feature value<span class="pl-pds">"</span></span>), Guide<span class="pl-k">.</span><span class="pl-c1">ylabel</span>(<span class="pl-s"><span class="pl-pds">"</span>Shapley effect (baseline = <span class="pl-v">$baseline</span>)<span class="pl-pds">"</span></span>),
         Guide<span class="pl-k">.</span><span class="pl-c1">title</span>(<span class="pl-s"><span class="pl-pds">"</span>Feature Effect - <span class="pl-v">$(data_plot<span class="pl-k">.</span>feature_name[<span class="pl-c1">1</span>])</span><span class="pl-pds">"</span></span>))
p</pre></div>
<p align="center">
    <a target="_blank" rel="noopener noreferrer" href="./tools/feature_effects.png"><img src="./tools/feature_effects.png" alt="feature_effects" style="max-width:100%;"></a>
</p>
<hr>
<h3><a id="user-content-random-forest-regression-model---parallel" class="anchor" aria-hidden="true" href="#random-forest-regression-model---parallel"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Random Forest regression model - Parallel</h3>
<ul>
<li>
<p>We'll explain the same dataset with the same model, but this time we'll compute
the Shapley values in parallel across cores using the built-in distributed computing
in <code>ShapML</code> which implements <code>Distributed.pmap()</code> internally.</p>
</li>
<li>
<p>The stochastic Shapley values will be computed in parallel over 6 cores on the same machine.</p>
</li>
<li>
<p>With the same seed set, <strong>non-parallel and parallel computation will return the same results</strong>.</p>
</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributed
<span class="pl-c1">addprocs</span>(<span class="pl-c1">6</span>)  <span class="pl-c"><span class="pl-c">#</span> 6 cores.</span></pre></div>
<ul>
<li>The <code>@everywhere</code> block of code will load the relevant packages on each core. If
you use another ML package, you would swap it in for <code>using MLJ</code>.</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@everywhere</span> <span class="pl-k">begin</span>
  <span class="pl-k">using</span> ShapML
  <span class="pl-k">using</span> DataFrames
  <span class="pl-k">using</span> MLJ
<span class="pl-k">end</span></pre></div>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> RDatasets

<span class="pl-c"><span class="pl-c">#</span> Load data.</span>
boston <span class="pl-k">=</span> RDatasets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>MASS<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Boston<span class="pl-pds">"</span></span>)
<span class="pl-c"><span class="pl-c">#</span>------------------------------------------------------------------------------</span>
<span class="pl-c"><span class="pl-c">#</span> Train a machine learning model; currently limited to single outcome regression and binary classification.</span>
outcome_name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>MedV<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Data prep.</span>
y, X <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">unpack</span>(boston, <span class="pl-k">==</span>(<span class="pl-c1">Symbol</span>(outcome_name)), colname <span class="pl-k">-&gt;</span> <span class="pl-c1">true</span>)

<span class="pl-c"><span class="pl-c">#</span> Instantiate an ML model; choose any single-outcome ML model from any package.</span>
random_forest <span class="pl-k">=</span> <span class="pl-c1">@load</span> RandomForestRegressor pkg <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>DecisionTree<span class="pl-pds">"</span></span>
model <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">machine</span>(random_forest, X, y)

<span class="pl-c"><span class="pl-c">#</span> Train the model.</span>
<span class="pl-c1">fit!</span>(model)</pre></div>
<ul>
<li><code>@everywhere</code> is needed to properly initialize the <code>predict()</code> wrapper function.</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Create a wrapper function that takes the following positional arguments: (1) a</span>
<span class="pl-c"><span class="pl-c">#</span> trained ML model from any Julia package, (2) a DataFrame of model features. The</span>
<span class="pl-c"><span class="pl-c">#</span> function should return a 1-column DataFrame of predictions--column names do not matter.</span>
<span class="pl-c1">@everywhere</span> <span class="pl-k">function</span> <span class="pl-en">predict_function</span>(model, data)
  data_pred <span class="pl-k">=</span> <span class="pl-c1">DataFrame</span>(y_pred <span class="pl-k">=</span> <span class="pl-c1">predict</span>(model, data))
  <span class="pl-k">return</span> data_pred
<span class="pl-k">end</span></pre></div>
<ul>
<li>Notice that we've set <code>ShapML.shap(parallel = :samples)</code> to perform the computation
in parallel across our 60 Monte Carlo samples.</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> ShapML setup.</span>
explain <span class="pl-k">=</span> <span class="pl-c1">copy</span>(boston[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">300</span>, :]) <span class="pl-c"><span class="pl-c">#</span> Compute Shapley feature-level predictions for 300 instances.</span>
explain <span class="pl-k">=</span> <span class="pl-c1">select</span>(explain, <span class="pl-c1">Not</span>(<span class="pl-c1">Symbol</span>(outcome_name)))  <span class="pl-c"><span class="pl-c">#</span> Remove the outcome column.</span>

reference <span class="pl-k">=</span> <span class="pl-c1">copy</span>(boston)  <span class="pl-c"><span class="pl-c">#</span> An optional reference population to compute the baseline prediction.</span>
reference <span class="pl-k">=</span> <span class="pl-c1">select</span>(reference, <span class="pl-c1">Not</span>(<span class="pl-c1">Symbol</span>(outcome_name)))

sample_size <span class="pl-k">=</span> <span class="pl-c1">60</span>  <span class="pl-c"><span class="pl-c">#</span> Number of Monte Carlo samples.</span>
<span class="pl-c"><span class="pl-c">#</span>------------------------------------------------------------------------------</span>
<span class="pl-c"><span class="pl-c">#</span> Compute stochastic Shapley values.</span>
data_shap <span class="pl-k">=</span> ShapML<span class="pl-k">.</span><span class="pl-c1">shap</span>(explain <span class="pl-k">=</span> explain,
                        reference <span class="pl-k">=</span> reference,
                        model <span class="pl-k">=</span> model,
                        predict_function <span class="pl-k">=</span> predict_function,
                        sample_size <span class="pl-k">=</span> sample_size,
                        parallel <span class="pl-k">=</span> <span class="pl-c1">:samples</span>,  <span class="pl-c"><span class="pl-c">#</span> Parallel computation over "sample_size".</span>
                        seed <span class="pl-k">=</span> <span class="pl-c1">1</span>
                        )

<span class="pl-c1">show</span>(data_shap, allcols <span class="pl-k">=</span> <span class="pl-c1">true</span>)</pre></div>
<p align="center">
    <a target="_blank" rel="noopener noreferrer" href="./tools/shap_output.PNG"><img src="./tools/shap_output.PNG" alt="shap_output" style="max-width:100%;"></a>
</p>
</article></div>