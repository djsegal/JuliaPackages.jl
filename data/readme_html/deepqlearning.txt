<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-deepqlearning" class="anchor" aria-hidden="true" href="#deepqlearning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DeepQLearning</h1>
<p><a href="https://travis-ci.org/JuliaPOMDP/DeepQLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/25592a0d482323d0a048099374b6f51a6567da2f28f0a5d60bad3dd6a1af8a7b/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961504f4d44502f44656570514c6561726e696e672e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaPOMDP/DeepQLearning.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p><a href="https://coveralls.io/github/JuliaPOMDP/DeepQLearning.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/575fef866cc488d8e8fd1fd61fa4f94f155a35b7414408009b7b6d0b233f2b3c/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f4a756c6961504f4d44502f44656570514c6561726e696e672e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/JuliaPOMDP/DeepQLearning.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a></p>
<p><a href="http://codecov.io/github/JuliaPOMDP/DeepQLearning.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/8171381dfea4faed9a33925e1dd6384720b9fd7806bb880f4bcb81d141479894/687474703a2f2f636f6465636f762e696f2f6769746875622f4a756c6961504f4d44502f44656570514c6561726e696e672e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/JuliaPOMDP/DeepQLearning.jl/coverage.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package provides an implementation of the Deep Q learning algorithm for solving MDPs. For more information see <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">https://arxiv.org/pdf/1312.5602.pdf</a>.
It uses POMDPs.jl and Flux.jl</p>
<p>It supports the following innovations:</p>
<ul>
<li>Target network</li>
<li>Prioritized replay <a href="https://arxiv.org/pdf/1511.05952.pdf" rel="nofollow">https://arxiv.org/pdf/1511.05952.pdf</a></li>
<li>Dueling <a href="https://arxiv.org/pdf/1511.06581.pdf" rel="nofollow">https://arxiv.org/pdf/1511.06581.pdf</a></li>
<li>Double Q <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847" rel="nofollow">http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847</a></li>
<li>Recurrent Q Learning</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg
# Pkg.Registry.add(&quot;https://github.com/JuliaPOMDP/Registry) # for julia 1.1+

# for julia 1.0 add the registry throught the POMDP package
# Pkg.add(&quot;POMDPs&quot;)
# using POMDPs
# POMDPs.add_registry() 
Pkg.add(&quot;DeepQLearning&quot;)
"><pre><span class="pl-k">using</span> Pkg
<span class="pl-c"><span class="pl-c">#</span> Pkg.Registry.add("https://github.com/JuliaPOMDP/Registry) # for julia 1.1+</span>

<span class="pl-c"><span class="pl-c">#</span> for julia 1.0 add the registry throught the POMDP package</span>
<span class="pl-c"><span class="pl-c">#</span> Pkg.add("POMDPs")</span>
<span class="pl-c"><span class="pl-c">#</span> using POMDPs</span>
<span class="pl-c"><span class="pl-c">#</span> POMDPs.add_registry() </span>
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>DeepQLearning<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using DeepQLearning
using POMDPs
using Flux
using POMDPModels
using POMDPSimulators
using POMDPPolicies

# load MDP model from POMDPModels or define your own!
mdp = SimpleGridWorld();

# Define the Q network (see Flux.jl documentation)
# the gridworld state is represented by a 2 dimensional vector.
model = Chain(Dense(2, 32), Dense(32, length(actions(mdp))))

exploration = EpsGreedyPolicy(mdp, LinearDecaySchedule(start=1.0, stop=0.01, steps=10000/2))

solver = DeepQLearningSolver(qnetwork = model, max_steps=10000, 
                             exploration_policy = exploration,
                             learning_rate=0.005,log_freq=500,
                             recurrence=false,double_q=true, dueling=true, prioritized_replay=true)
policy = solve(solver, mdp)

sim = RolloutSimulator(max_steps=30)
r_tot = simulate(sim, mdp, policy)
println(&quot;Total discounted reward for 1 simulation: $r_tot&quot;)
"><pre><span class="pl-k">using</span> DeepQLearning
<span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> Flux
<span class="pl-k">using</span> POMDPModels
<span class="pl-k">using</span> POMDPSimulators
<span class="pl-k">using</span> POMDPPolicies

<span class="pl-c"><span class="pl-c">#</span> load MDP model from POMDPModels or define your own!</span>
mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>();

<span class="pl-c"><span class="pl-c">#</span> Define the Q network (see Flux.jl documentation)</span>
<span class="pl-c"><span class="pl-c">#</span> the gridworld state is represented by a 2 dimensional vector.</span>
model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>, <span class="pl-c1">32</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">32</span>, <span class="pl-c1">length</span>(<span class="pl-c1">actions</span>(mdp))))

exploration <span class="pl-k">=</span> <span class="pl-c1">EpsGreedyPolicy</span>(mdp, <span class="pl-c1">LinearDecaySchedule</span>(start<span class="pl-k">=</span><span class="pl-c1">1.0</span>, stop<span class="pl-k">=</span><span class="pl-c1">0.01</span>, steps<span class="pl-k">=</span><span class="pl-c1">10000</span><span class="pl-k">/</span><span class="pl-c1">2</span>))

solver <span class="pl-k">=</span> <span class="pl-c1">DeepQLearningSolver</span>(qnetwork <span class="pl-k">=</span> model, max_steps<span class="pl-k">=</span><span class="pl-c1">10000</span>, 
                             exploration_policy <span class="pl-k">=</span> exploration,
                             learning_rate<span class="pl-k">=</span><span class="pl-c1">0.005</span>,log_freq<span class="pl-k">=</span><span class="pl-c1">500</span>,
                             recurrence<span class="pl-k">=</span><span class="pl-c1">false</span>,double_q<span class="pl-k">=</span><span class="pl-c1">true</span>, dueling<span class="pl-k">=</span><span class="pl-c1">true</span>, prioritized_replay<span class="pl-k">=</span><span class="pl-c1">true</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)

sim <span class="pl-k">=</span> <span class="pl-c1">RolloutSimulator</span>(max_steps<span class="pl-k">=</span><span class="pl-c1">30</span>)
r_tot <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(sim, mdp, policy)
<span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Total discounted reward for 1 simulation: <span class="pl-v">$r_tot</span><span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-specifying-exploration--evaluation-policy" class="anchor" aria-hidden="true" href="#specifying-exploration--evaluation-policy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Specifying exploration / evaluation policy</h2>
<p>An exploration policy and evaluation policy can be specified in the solver parameters.</p>
<p>An <strong>exploration policy</strong> can be provided in the form of a function that must return an action. The function provided will be called as follows: <code>f(policy, env, obs, global_step, rng)</code> where <code>policy</code> is the NN policy being trained, <code>env</code> the environment, <code>obs</code> the observation at which to take the action, <code>global_step</code> the interaction step of the solver, and <code>rng</code> a random number generator. This package provides by default an epsilon greedy policy with linear decrease of epsilon with <code>global_step</code>.</p>
<p>An <strong>evaluation policy</strong> can be provided in a similar manner. The function will be called as follows: <code>f(policy, env, n_eval, max_episode_length, verbose)</code> where <code>policy</code> is the NN policy being trained, <code>env</code> the environment, <code>n_eval</code> the number of evaluation episode, <code>max_episode_length</code> the maximum number of steps in one episode, and <code>verbose</code> a boolean to enable printing or not. The evaluation function must returns three elements:</p>
<ul>
<li>Average total reward (Float), the average score per episode</li>
<li>Average number of steps (Float), the average number of steps taken per episode</li>
<li>Info, a dictionary mapping <code>String</code> to <code>Float</code> that can be used to log custom scalar values.</li>
</ul>
<h2><a id="user-content-q-network" class="anchor" aria-hidden="true" href="#q-network"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Q-Network</h2>
<p>The <code>qnetwork</code> options of the solver should accept any <code>Chain</code> object. It is expected that they will be multi-layer perceptrons or convolutional layers followed by dense layer. If the network is ending with dense layers, the <code>dueling</code> option will split all the dense layers at the end of the network.</p>
<p>If the observation is a multi-dimensional array (e.g. an image), one can use the <code>flattenbatch</code> function to flatten all the dimensions of the image. It is useful to connect convolutional layers and dense layers for example. <code>flattenbatch</code> will flatten all the dimensions but the batch size.</p>
<p>The input size of the network is problem dependent and must be specified when you create the q network.</p>
<p>This package exports the type <code>AbstractNNPolicy</code> which represents neural network based policy. In addition to the functions from <code>POMDPs.jl</code>, <code>AbstractNNPolicy</code> objects supports the following:
- <code>getnetwork(policy)</code>: returns the value network of the policy
- <code>resetstate!(policy)</code>: reset the hidden states of a policy (does nothing if it is not an RNN)</p>
<h2><a id="user-content-savingreloading-model" class="anchor" aria-hidden="true" href="#savingreloading-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Saving/Reloading model</h2>
<p>See <a href="http://fluxml.ai/Flux.jl/stable/saving.html" rel="nofollow">Flux.jl documentation</a> for saving and loading models. The DeepQLearning solver saves the weights of the Q-network as a <code>bson</code> file in <code>solver.logdir/"qnetwork.bson"</code>.</p>
<h2><a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Logging</h2>
<p>Logging is done through <a href="https://github.com/PhilipVinc/TensorBoardLogger.jl">TensorBoardLogger.jl</a>. A log directory can be specified in the solver options, to disable logging you can set the <code>logdir</code> option to <code>nothing</code>.</p>
<h2><a id="user-content-gpu-support" class="anchor" aria-hidden="true" href="#gpu-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPU Support</h2>
<p><code>DeepQLearning.jl</code> should support running the calculations on GPUs through the package <a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays.jl</a>.
You must checkout the branch <code>gpu-support</code>. Note that it has not been tested thoroughly.
To run the solver on GPU you must first load <code>CuArrays</code> and then proceed as usual.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using CuArrays
using DeepQLearning
using POMDPs
using Flux
using POMDPModels

mdp = SimpleGridWorld();

# the model weights will be send to the gpu in the call to solve
model = Chain(Dense(2, 32), Dense(32, length(actions(mdp))))

solver = DeepQLearningSolver(qnetwork = model, max_steps=10000, 
                             learning_rate=0.005,log_freq=500,
                             recurrence=false,double_q=true, dueling=true, prioritized_replay=true)
policy = solve(solver, mdp)
"><pre><span class="pl-k">using</span> CuArrays
<span class="pl-k">using</span> DeepQLearning
<span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> Flux
<span class="pl-k">using</span> POMDPModels

mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>();

<span class="pl-c"><span class="pl-c">#</span> the model weights will be send to the gpu in the call to solve</span>
model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>, <span class="pl-c1">32</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">32</span>, <span class="pl-c1">length</span>(<span class="pl-c1">actions</span>(mdp))))

solver <span class="pl-k">=</span> <span class="pl-c1">DeepQLearningSolver</span>(qnetwork <span class="pl-k">=</span> model, max_steps<span class="pl-k">=</span><span class="pl-c1">10000</span>, 
                             learning_rate<span class="pl-k">=</span><span class="pl-c1">0.005</span>,log_freq<span class="pl-k">=</span><span class="pl-c1">500</span>,
                             recurrence<span class="pl-k">=</span><span class="pl-c1">false</span>,double_q<span class="pl-k">=</span><span class="pl-c1">true</span>, dueling<span class="pl-k">=</span><span class="pl-c1">true</span>, prioritized_replay<span class="pl-k">=</span><span class="pl-c1">true</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)</pre></div>
<h2><a id="user-content-solver-options" class="anchor" aria-hidden="true" href="#solver-options"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Solver Options</h2>
<p><strong>Fields of the Q Learning solver:</strong></p>
<ul>
<li><code>qnetwork::Any = nothing</code> Specify the architecture of the Q network</li>
<li><code>learning_rate::Float64 = 1e-4</code> learning rate</li>
<li><code>max_steps::Int64</code> total number of training step default = 1000</li>
<li><code>target_update_freq::Int64</code> frequency at which the target network is updated default = 500</li>
<li><code>batch_size::Int64</code> batch size sampled from the replay buffer default = 32</li>
<li><code>train_freq::Int64</code> frequency at which the active network is updated default  = 4</li>
<li><code>log_freq::Int64</code> frequency at which to logg info default = 100</li>
<li><code>eval_freq::Int64</code> frequency at which to eval the network default = 100</li>
<li><code>num_ep_eval::Int64</code> number of episodes to evaluate the policy default = 100</li>
<li><code>eps_fraction::Float64</code> fraction of the training set used to explore default = 0.5</li>
<li><code>eps_end::Float64</code> value of epsilon at the end of the exploration phase default = 0.01</li>
<li><code>double_q::Bool</code> double q learning udpate default = true</li>
<li><code>dueling::Bool</code> dueling structure for the q network default = true</li>
<li><code>recurrence::Bool = false</code> set to true to use DRQN, it will throw an error if you set it to false and pass a recurrent model.</li>
<li><code>prioritized_replay::Bool</code> enable prioritized experience replay default = true</li>
<li><code>prioritized_replay_alpha::Float64</code> default = 0.6</li>
<li><code>prioritized_replay_epsilon::Float64</code> default = 1e-6</li>
<li><code>prioritized_replay_beta::Float64</code> default = 0.4</li>
<li><code>buffer_size::Int64</code> size of the experience replay buffer default = 1000</li>
<li><code>max_episode_length::Int64</code> maximum length of a training episode default = 100</li>
<li><code>train_start::Int64</code> number of steps used to fill in the replay buffer initially default = 200</li>
<li><code>save_freq::Int64</code> save the model every <code>save_freq</code> steps, default = 1000</li>
<li><code>evaluation_policy::Function = basic_evaluation</code> function use to evaluate the policy every <code>eval_freq</code> steps, the default is a rollout that return the undiscounted average reward</li>
<li><code>exploration_policy::Any = linear_epsilon_greedy(max_steps, eps_fraction, eps_end)</code> exploration strategy (default is epsilon greedy with linear decay)</li>
<li><code>rng::AbstractRNG</code> random number generator default = MersenneTwister(0)</li>
<li><code>logdir::String = ""</code> folder in which to save the model</li>
<li><code>verbose::Bool</code> default = true</li>
</ul>
</article></div>