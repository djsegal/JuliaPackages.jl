<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-deepqlearning" class="anchor" aria-hidden="true" href="#deepqlearning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DeepQLearning</h1>
<p><a href="https://travis-ci.org/JuliaPOMDP/DeepQLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/24c5b7670deae417a2f8772255f9076c8db3c3d0/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961504f4d44502f44656570514c6561726e696e672e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaPOMDP/DeepQLearning.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p><a href="https://coveralls.io/github/JuliaPOMDP/DeepQLearning.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/0cfbc6063f07b6d8dd6426e91016dbd71d49d53b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f4a756c6961504f4d44502f44656570514c6561726e696e672e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/JuliaPOMDP/DeepQLearning.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a></p>
<p><a href="http://codecov.io/github/JuliaPOMDP/DeepQLearning.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/2016b44d0a8eb252dd124d941843acbfb7e44013/687474703a2f2f636f6465636f762e696f2f6769746875622f4a756c6961504f4d44502f44656570514c6561726e696e672e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/JuliaPOMDP/DeepQLearning.jl/coverage.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package provides an implementation of the Deep Q learning algorithm for solving MDPs. For more information see <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">https://arxiv.org/pdf/1312.5602.pdf</a>.
It uses POMDPs.jl and Flux.jl</p>
<p>It supports the following innovations:</p>
<ul>
<li>Target network</li>
<li>Prioritized replay <a href="https://arxiv.org/pdf/1511.05952.pdf" rel="nofollow">https://arxiv.org/pdf/1511.05952.pdf</a></li>
<li>Dueling <a href="https://arxiv.org/pdf/1511.06581.pdf" rel="nofollow">https://arxiv.org/pdf/1511.06581.pdf</a></li>
<li>Double Q <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847" rel="nofollow">http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847</a></li>
<li>Recurrent Q Learning</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Pkg
<span class="pl-c"><span class="pl-c">#</span> Pkg.Registry.add("https://github.com/JuliaPOMDP/Registry) # for julia 1.1+</span>

<span class="pl-c"><span class="pl-c">#</span> for julia 1.0 add the registry throught the POMDP package</span>
<span class="pl-c"><span class="pl-c">#</span> Pkg.add("POMDPs")</span>
<span class="pl-c"><span class="pl-c">#</span> using POMDPs</span>
<span class="pl-c"><span class="pl-c">#</span> POMDPs.add_registry() </span>
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>DeepQLearning<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DeepQLearning
<span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> Flux
<span class="pl-k">using</span> POMDPModels
<span class="pl-k">using</span> POMDPSimulators

<span class="pl-c"><span class="pl-c">#</span> load MDP model from POMDPModels or define your own!</span>
mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>();

<span class="pl-c"><span class="pl-c">#</span> Define the Q network (see Flux.jl documentation)</span>
<span class="pl-c"><span class="pl-c">#</span> the gridworld state is represented by a 2 dimensional vector.</span>
model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>, <span class="pl-c1">32</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">32</span>, <span class="pl-c1">length</span>(<span class="pl-c1">actions</span>(mdp))))

solver <span class="pl-k">=</span> <span class="pl-c1">DeepQLearningSolver</span>(qnetwork <span class="pl-k">=</span> model, max_steps<span class="pl-k">=</span><span class="pl-c1">10000</span>, 
                             learning_rate<span class="pl-k">=</span><span class="pl-c1">0.005</span>,log_freq<span class="pl-k">=</span><span class="pl-c1">500</span>,
                             recurrence<span class="pl-k">=</span><span class="pl-c1">false</span>,double_q<span class="pl-k">=</span><span class="pl-c1">true</span>, dueling<span class="pl-k">=</span><span class="pl-c1">true</span>, prioritized_replay<span class="pl-k">=</span><span class="pl-c1">true</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)

sim <span class="pl-k">=</span> <span class="pl-c1">RolloutSimulator</span>(max_steps<span class="pl-k">=</span><span class="pl-c1">30</span>)
r_tot <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(sim, mdp, policy)
<span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Total discounted reward for 1 simulation: <span class="pl-v">$r_tot</span><span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-specifying-exploration--evaluation-policy" class="anchor" aria-hidden="true" href="#specifying-exploration--evaluation-policy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Specifying exploration / evaluation policy</h2>
<p>An exploration policy and evaluation policy can be specified in the solver parameters.</p>
<p>An <strong>exploration policy</strong> can be provided in the form of a function that must return an action. The function provided will be called as follows: <code>f(policy, env, obs, global_step, rng)</code> where <code>policy</code> is the NN policy being trained, <code>env</code> the environment, <code>obs</code> the observation at which to take the action, <code>global_step</code> the interaction step of the solver, and <code>rng</code> a random number generator. This package provides by default an epsilon greedy policy with linear decrease of epsilon with <code>global_step</code>.</p>
<p>An <strong>evaluation policy</strong> can be provided in a similar manner. The function will be called as follows: <code>f(policy, env, n_eval, max_episode_length, verbose)</code> where <code>policy</code> is the NN policy being trained, <code>env</code> the environment, <code>n_eval</code> the number of evaluation episode, <code>max_episode_length</code> the maximum number of steps in one episode, and <code>verbose</code> a boolean to enable printing or not. The evaluation function must returns three elements:</p>
<ul>
<li>Average total reward (Float), the average score per episode</li>
<li>Average number of steps (Float), the average number of steps taken per episode</li>
<li>Info, a dictionary mapping <code>String</code> to <code>Float</code> that can be used to log custom scalar values.</li>
</ul>
<h2><a id="user-content-q-network" class="anchor" aria-hidden="true" href="#q-network"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Q-Network</h2>
<p>The <code>qnetwork</code> options of the solver should accept any <code>Chain</code> object. It is expected that they will be multi-layer perceptrons or convolutional layers followed by dense layer. If the network is ending with dense layers, the <code>dueling</code> option will split all the dense layers at the end of the network.</p>
<p>If the observation is a multi-dimensional array (e.g. an image), one can use the <code>flattenbatch</code> function to flatten all the dimensions of the image. It is useful to connect convolutional layers and dense layers for example. <code>flattenbatch</code> will flatten all the dimensions but the batch size.</p>
<p>The input size of the network is problem dependent and must be specified when you create the q network.</p>
<p>This package exports the type <code>AbstractNNPolicy</code> which represents neural network based policy. In addition to the functions from <code>POMDPs.jl</code>, <code>AbstractNNPolicy</code> objects supports the following:
- <code>getnetwork(policy)</code>: returns the value network of the policy
- <code>resetstate!(policy)</code>: reset the hidden states of a policy (does nothing if it is not an RNN)</p>
<h2><a id="user-content-savingreloading-model" class="anchor" aria-hidden="true" href="#savingreloading-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Saving/Reloading model</h2>
<p>See <a href="http://fluxml.ai/Flux.jl/stable/saving.html" rel="nofollow">Flux.jl documentation</a> for saving and loading models. The DeepQLearning solver saves the weights of the Q-network as a <code>bson</code> file in <code>solver.logdir/"qnetwork.bson"</code>.</p>
<h2><a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Logging</h2>
<p>Logging is done through <a href="https://github.com/PhilipVinc/TensorBoardLogger.jl">TensorBoardLogger.jl</a>. A log directory can be specified in the solver options, to disable logging you can set the <code>logdir</code> option to <code>nothing</code>.</p>
<h2><a id="user-content-gpu-support" class="anchor" aria-hidden="true" href="#gpu-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GPU Support</h2>
<p><code>DeepQLearning.jl</code> should support running the calculations on GPUs through the package <a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays.jl</a>.
You must checkout the branch <code>gpu-support</code>. Note that it has not been tested thoroughly.
To run the solver on GPU you must first load <code>CuArrays</code> and then proceed as usual.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> CuArrays
<span class="pl-k">using</span> DeepQLearning
<span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> Flux
<span class="pl-k">using</span> POMDPModels

mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>();

<span class="pl-c"><span class="pl-c">#</span> the model weights will be send to the gpu in the call to solve</span>
model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>, <span class="pl-c1">32</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">32</span>, <span class="pl-c1">length</span>(<span class="pl-c1">actions</span>(mdp))))

solver <span class="pl-k">=</span> <span class="pl-c1">DeepQLearningSolver</span>(qnetwork <span class="pl-k">=</span> model, max_steps<span class="pl-k">=</span><span class="pl-c1">10000</span>, 
                             learning_rate<span class="pl-k">=</span><span class="pl-c1">0.005</span>,log_freq<span class="pl-k">=</span><span class="pl-c1">500</span>,
                             recurrence<span class="pl-k">=</span><span class="pl-c1">false</span>,double_q<span class="pl-k">=</span><span class="pl-c1">true</span>, dueling<span class="pl-k">=</span><span class="pl-c1">true</span>, prioritized_replay<span class="pl-k">=</span><span class="pl-c1">true</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)</pre></div>
<h2><a id="user-content-solver-options" class="anchor" aria-hidden="true" href="#solver-options"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Solver Options</h2>
<p><strong>Fields of the Q Learning solver:</strong></p>
<ul>
<li><code>qnetwork::Any = nothing</code> Specify the architecture of the Q network</li>
<li><code>learning_rate::Float64 = 1e-4</code> learning rate</li>
<li><code>max_steps::Int64</code> total number of training step default = 1000</li>
<li><code>target_update_freq::Int64</code> frequency at which the target network is updated default = 500</li>
<li><code>batch_size::Int64</code> batch size sampled from the replay buffer default = 32</li>
<li><code>train_freq::Int64</code> frequency at which the active network is updated default  = 4</li>
<li><code>log_freq::Int64</code> frequency at which to logg info default = 100</li>
<li><code>eval_freq::Int64</code> frequency at which to eval the network default = 100</li>
<li><code>num_ep_eval::Int64</code> number of episodes to evaluate the policy default = 100</li>
<li><code>eps_fraction::Float64</code> fraction of the training set used to explore default = 0.5</li>
<li><code>eps_end::Float64</code> value of epsilon at the end of the exploration phase default = 0.01</li>
<li><code>double_q::Bool</code> double q learning udpate default = true</li>
<li><code>dueling::Bool</code> dueling structure for the q network default = true</li>
<li><code>recurrence::Bool = false</code> set to true to use DRQN, it will throw an error if you set it to false and pass a recurrent model.</li>
<li><code>prioritized_replay::Bool</code> enable prioritized experience replay default = true</li>
<li><code>prioritized_replay_alpha::Float64</code> default = 0.6</li>
<li><code>prioritized_replay_epsilon::Float64</code> default = 1e-6</li>
<li><code>prioritized_replay_beta::Float64</code> default = 0.4</li>
<li><code>buffer_size::Int64</code> size of the experience replay buffer default = 1000</li>
<li><code>max_episode_length::Int64</code> maximum length of a training episode default = 100</li>
<li><code>train_start::Int64</code> number of steps used to fill in the replay buffer initially default = 200</li>
<li><code>save_freq::Int64</code> save the model every <code>save_freq</code> steps, default = 1000</li>
<li><code>evaluation_policy::Function = basic_evaluation</code> function use to evaluate the policy every <code>eval_freq</code> steps, the default is a rollout that return the undiscounted average reward</li>
<li><code>exploration_policy::Any = linear_epsilon_greedy(max_steps, eps_fraction, eps_end)</code> exploration strategy (default is epsilon greedy with linear decay)</li>
<li><code>rng::AbstractRNG</code> random number generator default = MersenneTwister(0)</li>
<li><code>logdir::String = ""</code> folder in which to save the model</li>
<li><code>verbose::Bool</code> default = true</li>
</ul>
</article></div>