<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-commoncrawljl" class="anchor" aria-hidden="true" href="#commoncrawljl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CommonCrawl.jl</h1>
<p dir="auto"><a href="https://travis-ci.org/tanmaykm/CommonCrawl.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/fdaedef3a425d97a0cd9e6a9cba4f98fc77a200fa5f8fc2400adb053acae20fe/68747470733a2f2f7472617669732d63692e6f72672f74616e6d61796b6d2f436f6d6d6f6e437261776c2e6a6c2e706e67" alt="Build Status" data-canonical-src="https://travis-ci.org/tanmaykm/CommonCrawl.jl.png" style="max-width: 100%;"></a></p>
<p dir="auto">Interface to the <a href="http://aws.amazon.com/datasets/41740" rel="nofollow">common crawl dataset on Amazon S3</a></p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">An instance of the corpus is obtained as:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="cc = CrawlCorpus(cache_location::String, debug::Bool=false)"><pre class="notranslate"><code>cc = CrawlCorpus(cache_location::String, debug::Bool=false)
</code></pre></div>
<p dir="auto">Since the crawl corpus files are large, they are cached locally by default at <code>cache_location</code>. The first time a file is accessed, it is downloaded in full into the cache location. Subsequent calls to read are served locally.</p>
<p dir="auto">All cached files, or a particular cached archive file can be deleted:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="clear_cache(cc::CrawlCorpus)
clear_cache(cc::CrawlCorpus, archive::URI)"><pre class="notranslate"><code>clear_cache(cc::CrawlCorpus)
clear_cache(cc::CrawlCorpus, archive::URI)
</code></pre></div>
<p dir="auto">Segments and archive files in a segment can be listed as:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="segment_names = segments(cc::CrawlCorpus)
archive_uris = archives(cc::CrawlCorpus, segment::String)"><pre class="notranslate"><code>segment_names = segments(cc::CrawlCorpus)
archive_uris = archives(cc::CrawlCorpus, segment::String)
</code></pre></div>
<p dir="auto">Archive files across all segments can be accessed easily as:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="archive_uris = archives(cc::CrawlCorpus, count::Int=0)"><pre class="notranslate"><code>archive_uris = archives(cc::CrawlCorpus, count::Int=0)
</code></pre></div>
<p dir="auto">Passing count as <code>0</code> lists all available archive files (which can be large).</p>
<p dir="auto">A particular archive file can be opened as:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="open(cc::CrawlCorpus, archive::URI)"><pre class="notranslate"><code>open(cc::CrawlCorpus, archive::URI)
</code></pre></div>
<p dir="auto">And crawl entries can be read from an opened archive as:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entry = read_entry(cc::CrawlCorpus, f::IO, mime_part::String=&quot;&quot;, metadata_only::Bool=false)
entries = read_entries(cc::CrawlCorpus, f::IO, mime_part::String=&quot;&quot;, num_entries::Int=0, metadata_only::Bool=false)"><pre class="notranslate"><code>entry = read_entry(cc::CrawlCorpus, f::IO, mime_part::String="", metadata_only::Bool=false)
entries = read_entries(cc::CrawlCorpus, f::IO, mime_part::String="", num_entries::Int=0, metadata_only::Bool=false)
</code></pre></div>
<p dir="auto">Method <code>read_entry</code> returns an <code>ArchiveEntry</code> instance corresponding to the next entry in the file with mime type beginning with <code>mime_part</code>. Method <code>read_entries</code> returns an array of <code>ArchiveEntry</code> objects. If <code>num_entries</code> is <code>0</code>, all matching entries in the archive file are returned. If <code>metadata_only</code> is true, only the file metadata (url and mime type) is populated in the entries.</p>
</article></div>