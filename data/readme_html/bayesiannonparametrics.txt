<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-bayesiannonparametricsjl" class="anchor" aria-hidden="true" href="#bayesiannonparametricsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BayesianNonparametrics.jl</h1>
<p><a href="https://travis-ci.org/OFAI/BayesianNonparametrics.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e081b634ff90b0e8b4f88bc1ea3a2430140b7d8b8292acf786ed479f5c6d3bad/68747470733a2f2f7472617669732d63692e6f72672f4f4641492f426179657369616e4e6f6e706172616d6574726963732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/OFAI/BayesianNonparametrics.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/OFAI/BayesianNonparametrics.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/0dc42a8d1de0707af4642f1f61f82abed7fc20bd79d224897d20d915c911eae7/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4f4641492f426179657369616e4e6f6e706172616d6574726963732e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/OFAI/BayesianNonparametrics.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p><em>BayesianNonparametrics</em> is a Julia package implementing state-of-the-art Bayesian nonparametric models for medium-sized unsupervised problems. The software package brings Bayesian nonparametrics to non-specialists allowing the widespread use of Bayesian nonparametric models. Emphasis is put on consistency, performance and ease of use allowing easy access to Bayesian nonparametric models inside Julia.</p>
<p><em>BayesianNonparametrics</em> allows you to</p>
<ul>
<li>explain discrete or continous data using: Dirichlet Process Mixtures or Hierarchical Dirichlet Process Mixtures</li>
<li>analyse variable dependencies using: Variable Clustering Model</li>
<li>fit multivariate or univariate distributions for discrete or continous data with conjugate priors</li>
<li>compute point estimtates of Dirichlet Process Mixtures posterior samples</li>
</ul>
<h4><a id="user-content-news" class="anchor" aria-hidden="true" href="#news"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>News</h4>
<p><em>BayesianNonparametrics</em> is Julia 0.7  / 1.0 compatible</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>You can install the package into your running Julia installation using Julia's package manager, i.e.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="pkg&gt; add BayesianNonparametrics
"><pre>pkg<span class="pl-k">&gt;</span> add BayesianNonparametrics</pre></div>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h2>
<p>Documentation is available in Markdown:
<a href="docs/README.md">documentation</a></p>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h2>
<p>The following example illustrates the use of <em>BayesianNonparametrics</em> for clustering of continuous observations using a Dirichlet Process Mixture of Gaussians.</p>
<p>After loading the package:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using BayesianNonparametrics
"><pre><span class="pl-k">using</span> BayesianNonparametrics</pre></div>
<p>we can generate a 2D synthetic dataset (or use a multivariate continuous dataset of interest)</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="(X, Y) = bloobs(randomize = false)
"><pre>(X, Y) <span class="pl-k">=</span> <span class="pl-c1">bloobs</span>(randomize <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<p>and construct the parameters of our base distribution:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="μ0 = vec(mean(X, dims = 1))
κ0 = 5.0
ν0 = 9.0
Σ0 = cov(X)
H = WishartGaussian(μ0, κ0, ν0, Σ0)
"><pre>μ0 <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">mean</span>(X, dims <span class="pl-k">=</span> <span class="pl-c1">1</span>))
κ0 <span class="pl-k">=</span> <span class="pl-c1">5.0</span>
ν0 <span class="pl-k">=</span> <span class="pl-c1">9.0</span>
Σ0 <span class="pl-k">=</span> <span class="pl-c1">cov</span>(X)
H <span class="pl-k">=</span> <span class="pl-c1">WishartGaussian</span>(μ0, κ0, ν0, Σ0)</pre></div>
<p>After defining the base distribution we can specify the model:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model = DPM(H)
"><pre>model <span class="pl-k">=</span> <span class="pl-c1">DPM</span>(H)</pre></div>
<p>which is in this case a Dirichlet Process Mixture. Each model has to be initialised, one possible initialisation approach for Dirichlet Process Mixtures is a k-Means initialisation:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="modelBuffer = init(X, model, KMeansInitialisation(k = 10))
"><pre>modelBuffer <span class="pl-k">=</span> <span class="pl-c1">init</span>(X, model, <span class="pl-c1">KMeansInitialisation</span>(k <span class="pl-k">=</span> <span class="pl-c1">10</span>))</pre></div>
<p>The resulting buffer object can now be used to apply posterior inference on the model given <code>X</code>. In the following we apply Gibbs sampling for 500 iterations without burn in or thining:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="models = train(modelBuffer, DPMHyperparam(), Gibbs(maxiter = 500))
"><pre>models <span class="pl-k">=</span> <span class="pl-c1">train</span>(modelBuffer, <span class="pl-c1">DPMHyperparam</span>(), <span class="pl-c1">Gibbs</span>(maxiter <span class="pl-k">=</span> <span class="pl-c1">500</span>))</pre></div>
<p>You shoud see the progress of the sampling process in the command line. After applying Gibbs sampling, it is possible explore the posterior based on their posterior densities,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="densities = map(m -&gt; m.energy, models)
"><pre>densities <span class="pl-k">=</span> <span class="pl-c1">map</span>(m <span class="pl-k">-&gt;</span> m<span class="pl-k">.</span>energy, models)</pre></div>
<p>number of active components</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="activeComponents = map(m -&gt; sum(m.weights .&gt; 0), models)
"><pre>activeComponents <span class="pl-k">=</span> <span class="pl-c1">map</span>(m <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(m<span class="pl-k">.</span>weights <span class="pl-k">.&gt;</span> <span class="pl-c1">0</span>), models)</pre></div>
<p>or the groupings of the observations:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="assignments = map(m -&gt; m.assignments, models)
"><pre>assignments <span class="pl-k">=</span> <span class="pl-c1">map</span>(m <span class="pl-k">-&gt;</span> m<span class="pl-k">.</span>assignments, models)</pre></div>
<p>The following animation illustrates posterior samples obtained by a Dirichlet Process Mixture:</p>
<p><a target="_blank" rel="noopener noreferrer" href="posteriorSamples.gif"><img src="posteriorSamples.gif" alt="alt text" title="Posterior Sample" style="max-width:100%;"></a></p>
<p>Alternatively, one can compute a point estimate based on the posterior similarity matrix:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="A = reduce(hcat, assignments)
(N, D) = size(X)
PSM = ones(N, N)
M = size(A, 2)
for i in 1:N
  for j in 1:i-1
    PSM[i, j] = sum(A[i,:] .== A[j,:]) / M
    PSM[j, i] = PSM[i, j]
  end
end
"><pre>A <span class="pl-k">=</span> <span class="pl-c1">reduce</span>(hcat, assignments)
(N, D) <span class="pl-k">=</span> <span class="pl-c1">size</span>(X)
PSM <span class="pl-k">=</span> <span class="pl-c1">ones</span>(N, N)
M <span class="pl-k">=</span> <span class="pl-c1">size</span>(A, <span class="pl-c1">2</span>)
<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>N
  <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>i<span class="pl-k">-</span><span class="pl-c1">1</span>
    PSM[i, j] <span class="pl-k">=</span> <span class="pl-c1">sum</span>(A[i,:] <span class="pl-k">.==</span> A[j,:]) <span class="pl-k">/</span> M
    PSM[j, i] <span class="pl-k">=</span> PSM[i, j]
  <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p>and find the optimal partition which minimizes the lower bound of the variation of information:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="mink = minimum(length(m.weights) for m in models)
maxk = maximum(length(m.weights) for m in models)
(peassignments, _) = pointestimate(PSM, method = :average, mink = mink, maxk = maxk)
"><pre>mink <span class="pl-k">=</span> <span class="pl-c1">minimum</span>(<span class="pl-c1">length</span>(m<span class="pl-k">.</span>weights) <span class="pl-k">for</span> m <span class="pl-k">in</span> models)
maxk <span class="pl-k">=</span> <span class="pl-c1">maximum</span>(<span class="pl-c1">length</span>(m<span class="pl-k">.</span>weights) <span class="pl-k">for</span> m <span class="pl-k">in</span> models)
(peassignments, _) <span class="pl-k">=</span> <span class="pl-c1">pointestimate</span>(PSM, method <span class="pl-k">=</span> <span class="pl-c1">:average</span>, mink <span class="pl-k">=</span> mink, maxk <span class="pl-k">=</span> maxk)</pre></div>
<p>The grouping wich minimizes the lower bound of the variation of information is illustrated in the following image:
<a target="_blank" rel="noopener noreferrer" href="pointestimate.png"><img src="pointestimate.png" alt="alt text" title="Point Estimate" style="max-width:100%;"></a></p>
</article></div>