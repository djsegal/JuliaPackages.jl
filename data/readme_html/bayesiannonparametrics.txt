<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-bayesiannonparametricsjl" class="anchor" aria-hidden="true" href="#bayesiannonparametricsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>BayesianNonparametrics.jl</h1>
<p><a href="https://travis-ci.org/OFAI/BayesianNonparametrics.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/37b19ab2aee9e578165f1f56516efda0d250bd04/68747470733a2f2f7472617669732d63692e6f72672f4f4641492f426179657369616e4e6f6e706172616d6574726963732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/OFAI/BayesianNonparametrics.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/OFAI/BayesianNonparametrics.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/85e7e7b933379c4153403c4191e8123f0dfc7575/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4f4641492f426179657369616e4e6f6e706172616d6574726963732e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/OFAI/BayesianNonparametrics.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p><em>BayesianNonparametrics</em> is a Julia package implementing state-of-the-art Bayesian nonparametric models for medium-sized unsupervised problems. The software package brings Bayesian nonparametrics to non-specialists allowing the widespread use of Bayesian nonparametric models. Emphasis is put on consistency, performance and ease of use allowing easy access to Bayesian nonparametric models inside Julia.</p>
<p><em>BayesianNonparametrics</em> allows you to</p>
<ul>
<li>explain discrete or continous data using: Dirichlet Process Mixtures or Hierarchical Dirichlet Process Mixtures</li>
<li>analyse variable dependencies using: Variable Clustering Model</li>
<li>fit multivariate or univariate distributions for discrete or continous data with conjugate priors</li>
<li>compute point estimtates of Dirichlet Process Mixtures posterior samples</li>
</ul>
<h4><a id="user-content-news" class="anchor" aria-hidden="true" href="#news"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>News</h4>
<p><em>BayesianNonparametrics</em> is Julia 0.7  / 1.0 compatible</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>You can install the package into your running Julia installation using Julia's package manager, i.e.</p>
<div class="highlight highlight-source-julia"><pre>pkg<span class="pl-k">&gt;</span> add BayesianNonparametrics</pre></div>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Documentation</h2>
<p>Documentation is available in Markdown:
<a href="docs/README.md">documentation</a></p>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example</h2>
<p>The following example illustrates the use of <em>BayesianNonparametrics</em> for clustering of continuous observations using a Dirichlet Process Mixture of Gaussians.</p>
<p>After loading the package:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> BayesianNonparametrics</pre></div>
<p>we can generate a 2D synthetic dataset (or use a multivariate continuous dataset of interest)</p>
<div class="highlight highlight-source-julia"><pre>(X, Y) <span class="pl-k">=</span> <span class="pl-c1">bloobs</span>(randomize <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<p>and construct the parameters of our base distribution:</p>
<div class="highlight highlight-source-julia"><pre>μ<span class="pl-c1">0</span> <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">mean</span>(X, dims <span class="pl-k">=</span> <span class="pl-c1">1</span>))
κ<span class="pl-c1">0</span> <span class="pl-k">=</span> <span class="pl-c1">5.0</span>
ν<span class="pl-c1">0</span> <span class="pl-k">=</span> <span class="pl-c1">9.0</span>
Σ<span class="pl-c1">0</span> <span class="pl-k">=</span> <span class="pl-c1">cov</span>(X)
H <span class="pl-k">=</span> <span class="pl-c1">WishartGaussian</span>(μ<span class="pl-c1">0</span>, κ<span class="pl-c1">0</span>, ν<span class="pl-c1">0</span>, Σ<span class="pl-c1">0</span>)</pre></div>
<p>After defining the base distribution we can specify the model:</p>
<div class="highlight highlight-source-julia"><pre>model <span class="pl-k">=</span> <span class="pl-c1">DPM</span>(H)</pre></div>
<p>which is in this case a Dirichlet Process Mixture. Each model has to be initialised, one possible initialisation approach for Dirichlet Process Mixtures is a k-Means initialisation:</p>
<div class="highlight highlight-source-julia"><pre>modelBuffer <span class="pl-k">=</span> <span class="pl-c1">init</span>(X, model, <span class="pl-c1">KMeansInitialisation</span>(k <span class="pl-k">=</span> <span class="pl-c1">10</span>))</pre></div>
<p>The resulting buffer object can now be used to apply posterior inference on the model given <code>X</code>. In the following we apply Gibbs sampling for 500 iterations without burn in or thining:</p>
<div class="highlight highlight-source-julia"><pre>models <span class="pl-k">=</span> <span class="pl-c1">train</span>(modelBuffer, <span class="pl-c1">DPMHyperparam</span>(), <span class="pl-c1">Gibbs</span>(maxiter <span class="pl-k">=</span> <span class="pl-c1">500</span>))</pre></div>
<p>You shoud see the progress of the sampling process in the command line. After applying Gibbs sampling, it is possible explore the posterior based on their posterior densities,</p>
<div class="highlight highlight-source-julia"><pre>densities <span class="pl-k">=</span> <span class="pl-c1">map</span>(m <span class="pl-k">-&gt;</span> m<span class="pl-k">.</span>energy, models)</pre></div>
<p>number of active components</p>
<div class="highlight highlight-source-julia"><pre>activeComponents <span class="pl-k">=</span> <span class="pl-c1">map</span>(m <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(m<span class="pl-k">.</span>weights <span class="pl-k">.&gt;</span> <span class="pl-c1">0</span>), models)</pre></div>
<p>or the groupings of the observations:</p>
<div class="highlight highlight-source-julia"><pre>assignments <span class="pl-k">=</span> <span class="pl-c1">map</span>(m <span class="pl-k">-&gt;</span> m<span class="pl-k">.</span>assignments, models)</pre></div>
<p>The following animation illustrates posterior samples obtained by a Dirichlet Process Mixture:</p>
<p><a target="_blank" rel="noopener noreferrer" href="posteriorSamples.gif"><img src="posteriorSamples.gif" alt="alt text" title="Posterior Sample" style="max-width:100%;"></a></p>
<p>Alternatively, one can compute a point estimate based on the posterior similarity matrix:</p>
<div class="highlight highlight-source-julia"><pre>A <span class="pl-k">=</span> <span class="pl-c1">reduce</span>(hcat, assignments)
(N, D) <span class="pl-k">=</span> <span class="pl-c1">size</span>(X)
PSM <span class="pl-k">=</span> <span class="pl-c1">ones</span>(N, N)
M <span class="pl-k">=</span> <span class="pl-c1">size</span>(A, <span class="pl-c1">2</span>)
<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>N
  <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>i<span class="pl-k">-</span><span class="pl-c1">1</span>
    PSM[i, j] <span class="pl-k">=</span> <span class="pl-c1">sum</span>(A[i,:] <span class="pl-k">.==</span> A[j,:]) <span class="pl-k">/</span> M
    PSM[j, i] <span class="pl-k">=</span> PSM[i, j]
  <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p>and find the optimal partition which minimizes the lower bound of the variation of information:</p>
<div class="highlight highlight-source-julia"><pre>mink <span class="pl-k">=</span> <span class="pl-c1">minimum</span>(<span class="pl-c1">length</span>(m<span class="pl-k">.</span>weights) <span class="pl-k">for</span> m <span class="pl-k">in</span> models)
maxk <span class="pl-k">=</span> <span class="pl-c1">maximum</span>(<span class="pl-c1">length</span>(m<span class="pl-k">.</span>weights) <span class="pl-k">for</span> m <span class="pl-k">in</span> models)
(peassignments, _) <span class="pl-k">=</span> <span class="pl-c1">pointestimate</span>(PSM, method <span class="pl-k">=</span> <span class="pl-c1">:average</span>, mink <span class="pl-k">=</span> mink, maxk <span class="pl-k">=</span> maxk)</pre></div>
<p>The grouping wich minimizes the lower bound of the variation of information is illustrated in the following image:
<a target="_blank" rel="noopener noreferrer" href="pointestimate.png"><img src="pointestimate.png" alt="alt text" title="Point Estimate" style="max-width:100%;"></a></p>
</article></div>