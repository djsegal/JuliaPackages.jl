<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-koala-" class="anchor" aria-hidden="true" href="#koala-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Koala <a target="_blank" rel="noopener noreferrer" href="logo.png"><img src="logo.png" alt="" style="max-width:100%;"></a></h1>
<p>A Julia machine learning environment combining convenience and control,
through a combination of high and low-level interfaces.</p>
<p>This experimental framework is no longer being maintained. The Koala
project has been merged with
the <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine
learning framework being developed at the Alan Turing Institute.</p>
<p>Koala's high-level interface is provided through the systematic
implementation of default pretransformations of training and testing
data. In particular, all learning algorithms receive learning data in
a common format (a <code>DataFrame</code> object) with all algorithm-specific
transformations occurring under the hood. This allows for the quick and
efficient comparison of several learning models. To mitigate against
data leakage, data transformations are "fit" on training data
only. However, there is a provision for dealing automatically in
testing or (cross) validation with classes of categorical variables
not seen during the fit, a common annoyance.</p>
<h3><a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation and usage</h3>
<p>For an introductory tour, clone the repositories into directories that your
Julia installation can find and run <a href="docs/tour.jl">docs/tour.jl</a> in your
Julia REPL. Note that Koala and associated libraries require Julia 0.6.</p>
<h3><a id="user-content-current-koala-machine-learning-library" class="anchor" aria-hidden="true" href="#current-koala-machine-learning-library"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Current Koala machine learning library</h3>
<p><a href="https://github.com/ablaom/KoalaTrees.jl">KoalaTrees</a>: Supervised learning with single regularized trees</p>
<p><a href="https://github.com/ablaom/KoalaEnsembles.jl">KoalaEnsembles</a>: Build weighted ensemble learners (e.g., random forests, extreme random forests)</p>
<p><a href="https://github.com/ablaom/KoalaLightGBM.jl">KoalaLightGBM</a>: A Koala
wrap of Microsoft's gradient tree boosting
<a href="https://github.com/Microsoft/LightGBM">algorithm</a></p>
<p><a href="https://github.com/ablaom/KoalaElasticNet.jl">KoalaElasticNet</a>: The elastic net and lasso linear predictors</p>
<p><a href="https://github.com/ablaom/KoalaRidge.jl">KoalaRidge</a>: Ridge regression and classification</p>
<p><a href="https://github.com/ablaom/KoalaKNN.jl">KoalaKNN</a>: K-nearest neighbor</p>
<p><a href="https://github.com/ablaom/KoalaTransforms.jl">KoalaTransforms</a>: A library of common data transformations (and dependency of several of the other libraries)</p>
<p>KoalaFlux (coming soon): A wrap of Mike Innes' beautiful Julia
implementation of neural networks, including a facility to learn
categorical feature embeddings.</p>
<p>At present the above implement supervised regression (todo:
classification). To learn how to wrap your favorite machine learning
code for use in Koala, refer to:</p>
<p><a href="https://github.com/ablaom/KoalaLow.jl">KoalaLow</a>: To expose Koala's low-level interface</p>
</article></div>