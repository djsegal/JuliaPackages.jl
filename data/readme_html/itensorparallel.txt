<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-itensorparallel" class="anchor" aria-hidden="true" href="#itensorparallel"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ITensorParallel</h1>
<p dir="auto"><a href="https://mtfishman.github.io/ITensorParallel.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://mtfishman.github.io/ITensorParallel.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/mtfishman/ITensorParallel.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/mtfishman/ITensorParallel.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/mtfishman/ITensorParallel.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/37b5ea7039cf580daebf1b215c99b06f4fbcf95b796a4f7652f0d183025ef552/68747470733a2f2f636f6465636f762e696f2f67682f6d74666973686d616e2f4954656e736f72506172616c6c656c2e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/mtfishman/ITensorParallel.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://github.com/invenia/BlueStyle"><img src="https://camo.githubusercontent.com/c18fbaa52d94d16b90b19701fc90d289b8a5bb920c74c79bab200b14e75420a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c75652d3434393564312e737667" alt="Code Style: Blue" data-canonical-src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h1>
<p dir="auto">This package adds more shared and distributed memory parallelism to <a href="https://github.com/ITensor/ITensors.jl">ITensors.jl</a>, with the goal of implementing the techniques for nested parallelization in DMRG laid out in by Zhai and Chan in <a href="https://arxiv.org/abs/2103.09976" rel="nofollow">arXiv:2103.09976</a>. So far, it focuses on parallelizing over optimizing or evolving sums of tensor networks (in particular, parallelized sums of MPOs), which can be composed with dense or block sparse threaded tensor operations that are implemented in ITensors.jl. We plan to add real-space parallel DMRG, TDVP, and TEBD based on <a href="https://arxiv.org/abs/1301.3494" rel="nofollow">arXiv:1301.3494</a> as well.</p>
<p dir="auto">For multithreading, we are using Julia's standard <a href="https://docs.julialang.org/en/v1/manual/multi-threading/" rel="nofollow">Threads.jl</a> library, as well as convenient abstractions on top of that for parallelizing over maps and reductions provided by <a href="https://github.com/juliafolds/folds.jl">Folds.jl</a> and <a href="https://github.com/JuliaFolds/FLoops.jl">FLoops.jl</a>. See <a href="https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/" rel="nofollow">here</a> for a nice overview of parallelization in Julia.</p>
<p dir="auto">For distributed computing, we make use of Julia's standard <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/" rel="nofollow">Distributed.jl</a> library along with it's interface through <a href="https://github.com/juliafolds/folds.jl">Folds.jl</a> and <a href="https://github.com/JuliaFolds/FLoops.jl">FLoops.jl</a>, as well as <a href="https://juliaparallel.github.io/MPI.jl/latest/" rel="nofollow">MPI.jl</a>. Take a look at Julia'd documentation on <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/" rel="nofollow">distributed computing</a> for more information and background. We may investigate other parallelization abstractions such as <a href="https://github.com/JuliaParallel/Dagger.jl">Dagger.jl</a> as well.</p>
<p dir="auto">To run Distributed.jl-based computations on clusters, we recommend using Julia's cluster manager tools like <a href="https://github.com/JuliaParallel/ClusterManagers.jl">ClusterManagers.jl</a>, <a href="https://github.com/kleinhenz/SlurmClusterManager.jl">SlurmClusterManager.jl</a>, and <a href="https://github.com/JuliaParallel/MPIClusterManagers.jl">MPIClusterManagers.jl</a>.</p>
<p dir="auto">See the <a href="https://github.com/ITensor/ITensorParallel.jl/tree/main/examples">examples folder</a> for examples of running DMRG parallelized over sums of MPOs, using Threads.jl, Distributed.jl, and MPI.jl.</p>
</article></div>