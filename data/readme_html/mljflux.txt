<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-mljflux" class="anchor" aria-hidden="true" href="#mljflux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MLJFlux</h1>
<p>An interface to the Flux deep learning models for the
<a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine
learning framework</p>
<p><a href="https://travis-ci.com/alan-turing-institute/MLJFlux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/7ad1d68bc0ec636a89dceb9e8251379474fee786/68747470733a2f2f7472617669732d63692e636f6d2f616c616e2d747572696e672d696e737469747574652f4d4c4a466c75782e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/alan-turing-institute/MLJFlux.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://coveralls.io/github/alan-turing-institute/MLJFlux.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/f6a2fcb71e7d0bab3b4cf9d398db371966488a61/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f616c616e2d747572696e672d696e737469747574652f4d4c4a466c75782e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/alan-turing-institute/MLJFlux.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p>MLJFlux makes it possible to apply the machine learning
meta-algorithms provided by MLJ - such as out-of-sample performance
evaluation and hyper-parameter optimization - to some classes of
supervised deep learning models. It does this by providing an
interface to the <a href="https://fluxml.ai/Flux.jl/stable/" rel="nofollow">Flux</a>
framework.</p>
<h3><a id="user-content-basic-idea" class="anchor" aria-hidden="true" href="#basic-idea"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic idea</h3>
<p>Each MLJFlux model has a <em>builder</em> hyperparameter, an object encoding
instructions for creating a neural network given the data that the
model eventually sees (e.g., the number of classes in a classification
problem). While each MLJ model has a simple default builder, users
will generally need to define their own builders to get good results,
and this will require familiarity with the <a href="https://fluxml.ai/Flux.jl/stable/" rel="nofollow">Flux
API</a> for defining a neural network
chain.</p>
<p>In the future MLJFlux may provided an assortment of more sophisticated
canned builders.</p>
<h3><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h3>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">activate</span>(<span class="pl-s"><span class="pl-pds">"</span>my_environment<span class="pl-pds">"</span></span>, shared<span class="pl-k">=</span><span class="pl-c1">true</span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MLJFlux<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MLJ<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>RDatasets<span class="pl-pds">"</span></span>)  <span class="pl-c"><span class="pl-c">#</span> for the demo below</span></pre></div>
<h3><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h3>
<p>Following is an introductory example using a default builder and no
standardization of input features.</p>
<h4><a id="user-content-loading-some-data-and-instantiating-a-model" class="anchor" aria-hidden="true" href="#loading-some-data-and-instantiating-a-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Loading some data and instantiating a model</h4>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> MLJ
<span class="pl-k">import</span> RDatasets
iris <span class="pl-k">=</span> RDatasets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>);
y, X <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(iris, <span class="pl-k">==</span>(<span class="pl-c1">:Species</span>), colname <span class="pl-k">-&gt;</span> <span class="pl-c1">true</span>, rng<span class="pl-k">=</span><span class="pl-c1">123</span>);
<span class="pl-c1">@load</span> NeuralNetworkClassifier

julia<span class="pl-k">&gt;</span> clf <span class="pl-k">=</span> <span class="pl-c1">NeuralNetworkClassifier</span>()
<span class="pl-c1">NeuralNetworkClassifier</span>(
    builder <span class="pl-k">=</span> <span class="pl-c1">Short</span>(
            n_hidden <span class="pl-k">=</span> <span class="pl-c1">0</span>,
            dropout <span class="pl-k">=</span> <span class="pl-c1">0.5</span>,
            σ <span class="pl-k">=</span> NNlib<span class="pl-k">.</span>σ),
    finaliser <span class="pl-k">=</span> NNlib<span class="pl-k">.</span>softmax,
    optimiser <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.001</span>, (<span class="pl-c1">0.9</span>, <span class="pl-c1">0.999</span>), <span class="pl-c1">IdDict</span><span class="pl-c1">{Any,Any}</span>()),
    loss <span class="pl-k">=</span> Flux<span class="pl-k">.</span>crossentropy,
    epochs <span class="pl-k">=</span> <span class="pl-c1">10</span>,
    batch_size <span class="pl-k">=</span> <span class="pl-c1">1</span>,
    lambda <span class="pl-k">=</span> <span class="pl-c1">0.0</span>,
    alpha <span class="pl-k">=</span> <span class="pl-c1">0.0</span>,
    optimiser_changes_trigger_retraining <span class="pl-k">=</span> <span class="pl-c1">false</span>) @ <span class="pl-c1">1</span>…<span class="pl-c1">60</span></pre></div>
<h4><a id="user-content-incremental-training" class="anchor" aria-hidden="true" href="#incremental-training"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Incremental training</h4>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> Random<span class="pl-k">.</span>seed!; <span class="pl-c1">seed!</span>(<span class="pl-c1">123</span>)
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(clf, X, y)
<span class="pl-c1">fit!</span>(mach)

julia<span class="pl-k">&gt;</span> training_loss <span class="pl-k">=</span> <span class="pl-c1">cross_entropy</span>(<span class="pl-c1">predict</span>(mach, X), y) <span class="pl-k">|&gt;</span> mean
<span class="pl-c1">0.89526004</span>f0

<span class="pl-c"><span class="pl-c">#</span> increase learning rate and add iterations:</span>
clf<span class="pl-k">.</span>optimiser<span class="pl-k">.</span>eta <span class="pl-k">=</span> clf<span class="pl-k">.</span>optimiser<span class="pl-k">.</span>eta <span class="pl-k">*</span> <span class="pl-c1">2</span>
clf<span class="pl-k">.</span>epochs <span class="pl-k">=</span> clf<span class="pl-k">.</span>epochs <span class="pl-k">+</span> <span class="pl-c1">5</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">fit!</span>(mach, verbosity<span class="pl-k">=</span><span class="pl-c1">2</span>)
[ Info<span class="pl-k">:</span> Updating Machine{NeuralNetworkClassifier{Short,…}} @<span class="pl-c1">240.</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.853</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.8207</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.8072</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.752</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.7077</span>
Machine{NeuralNetworkClassifier{Short,…}} @ <span class="pl-c1">1</span>…<span class="pl-c1">42</span>

julia<span class="pl-k">&gt;</span> training_loss <span class="pl-k">=</span> <span class="pl-c1">cross_entropy</span>(<span class="pl-c1">predict</span>(mach, X), y) <span class="pl-k">|&gt;</span> mean
<span class="pl-c1">0.7076618</span>f0</pre></div>
<h4><a id="user-content-accessing-the-flux-chain-model" class="anchor" aria-hidden="true" href="#accessing-the-flux-chain-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Accessing the Flux chain (model)</h4>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">fitted_params</span>(mach)<span class="pl-k">.</span>chain
<span class="pl-c1">Chain</span>(<span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">4</span>, <span class="pl-c1">3</span>, σ), Flux<span class="pl-k">.</span><span class="pl-c1">Dropout</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">0.5</span>, <span class="pl-c1">false</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">3</span>, <span class="pl-c1">3</span>)), softmax)</pre></div>
<h4><a id="user-content-evolution-of-out-of-sample-performance" class="anchor" aria-hidden="true" href="#evolution-of-out-of-sample-performance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evolution of out-of-sample performance</h4>
<div class="highlight highlight-source-julia"><pre>r <span class="pl-k">=</span> <span class="pl-c1">range</span>(clf, <span class="pl-c1">:epochs</span>, lower<span class="pl-k">=</span><span class="pl-c1">1</span>, upper<span class="pl-k">=</span><span class="pl-c1">200</span>, scale<span class="pl-k">=</span><span class="pl-c1">:log10</span>)
curve <span class="pl-k">=</span> <span class="pl-c1">learning_curve</span>(clf, X, y,
                       range<span class="pl-k">=</span>r,
                       resampling<span class="pl-k">=</span><span class="pl-c1">Holdout</span>(fraction_train<span class="pl-k">=</span><span class="pl-c1">0.7</span>),
                       measure<span class="pl-k">=</span>cross_entropy)
<span class="pl-k">using</span> Plots
<span class="pl-c1">plot</span>(curve<span class="pl-k">.</span>parameter_values,
       curve<span class="pl-k">.</span>measurements,
       xlab<span class="pl-k">=</span>curve<span class="pl-k">.</span>parameter_name,
       xscale<span class="pl-k">=</span>curve<span class="pl-k">.</span>parameter_scale,
       ylab <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Cross Entropy<span class="pl-pds">"</span></span>)
</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="learning_curve.png"><img src="learning_curve.png" alt="learning_curve.png" style="max-width:100%;"></a></p>
<h3><a id="user-content-models" class="anchor" aria-hidden="true" href="#models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Models</h3>
<p>In MLJ a <em>model</em> is a mutable struct storing hyperparameters for some
learning algorithm indicated by the model name, and that's all. In
particular, an MLJ model does not store learned parameters.</p>
<p><em>Warning:</em> In Flux the term "model" has another meaning. However, as all
Flux "models" used in MLJFLux are <code>Flux.Chain</code> objects, we call them
<em>chains</em>, and restrict use of "model" to models in the MLJ sense.</p>
<p>MLJFlux provides four model types, for use with input features <code>X</code> and
targets <code>y</code> of the <a href="https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/" rel="nofollow">scientific
type</a>
indicated in the table below. The parameters <code>n_in</code> and <code>n_out</code>
refer to information passed to the builder, as described under
<a href="defining-a-new-builder">Defining a new builder</a> below.</p>
<table>
<thead>
<tr>
<th>model type</th>
<th>prediction type</th>
<th><code>scitype(X) &lt;: _</code></th>
<th><code>scitype(y) &lt;: _</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralNetworkRegressor</code></td>
<td><code>Deterministic</code></td>
<td><code>Table(Continuous)</code> with <code>n_in</code> columns</td>
<td><code>AbstractVector{&lt;:Continuous)</code> (<code>n_out = 1</code>)</td>
</tr>
<tr>
<td><code>MultitargetNeuralNetworkRegressor</code></td>
<td><code>Deterministic</code></td>
<td><code>Table(Continuous)</code> with <code>n_in</code> columns</td>
<td><code>&lt;: Table(Continuous)</code> with <code>n_out</code> columns</td>
</tr>
<tr>
<td><code>NeuralNetworkClassifier</code></td>
<td><code>Probabilistic</code></td>
<td><code>&lt;:Table(Continuous)</code> with <code>n_in</code> columns</td>
<td><code>AbstractVector{&lt;:Finite}</code> with <code>n_out</code> classes</td>
</tr>
<tr>
<td><code>ImageClassifier</code></td>
<td><code>Probabilistic</code></td>
<td><code>AbstractVector(&lt;:Image{W,H})</code> with <code>n_in = (W, H)</code></td>
<td><code>AbstractVector{&lt;:Finite}</code> with <code>n_out</code> classes</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Table 1. Input and output types for MLJFlux models</p>
</blockquote>
<h4><a id="user-content-non-tabular-input" class="anchor" aria-hidden="true" href="#non-tabular-input"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Non-tabular input</h4>
<p>Any <code>AbstractMatrix{&lt;:AbstractFloat}</code> object <code>Xmat</code> can be forced to
have scitype <code>Table(Continuous)</code> by replacing it with <code> X = MLJ.table(Xmat)</code>. Furthermore, this wrapping, and subsequent
unwrapping under the hood, will compile to a no-op. At present this
includes support for sparse matrix data, but the implementation has
not been optimized for sparse data at this time and so should be used
with caution.</p>
<p>Instructions for coercing common image formats into some
<code>AbstractVector{&lt;:Image}</code> are
<a href="https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/#Type-coercion-for-image-data-1" rel="nofollow">here</a>.</p>
<h4><a id="user-content-built-in-builders" class="anchor" aria-hidden="true" href="#built-in-builders"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Built-in builders</h4>
<p>MLJ provides two simple builders out of the box:</p>
<ul>
<li>
<p><code>MLJFlux.Linear(σ=...)</code> builds a fully connected two layer
network with <code>n_in</code> inputs and <code>n_out</code> outputs, with activation
function <code>σ</code>, defaulting to a <code>MLJFlux.relu</code>.</p>
</li>
<li>
<p><code>MLJFlux.Short(n_hidden=..., dropout=..., σ=...)</code> builds a
full-connected three-layer network with <code>n_in</code> inputs and <code>n_out</code>
outputs using <code>n_hidden</code> nodes in the hidden layer and the specified
<code>dropout</code> (defaulting to 0.5). An activation function <code>σ</code> is applied
between the hidden and final layers. If <code>n_hidden=0</code> (the default)
then <code>n_hidden</code> is the geometric mean of the number of input and
output nodes.</p>
</li>
</ul>
<p>See Table 1 above to see how <code>n_in</code> and <code>n_out</code> relate to the data.</p>
<h3><a id="user-content-model-hyperparameters" class="anchor" aria-hidden="true" href="#model-hyperparameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Model hyperparameters.</h3>
<p>All models share the following hyper-parameters:</p>
<ol>
<li>
<p><code>builder</code>: Default = <code>MLJFlux.Linear(σ=Flux.relu)</code> (regressors) or
<code>MLJFlux.Short(n_hidden=0, dropout=0.5, σ=Flux.σ)</code> (classifiers)</p>
</li>
<li>
<p><code>optimiser</code>: The optimiser to use for training. Default =
<code>Flux.ADAM()</code></p>
</li>
<li>
<p><code>loss</code>: The loss function used for training. Default = <code>Flux.mse</code> (regressors) and <code>Flux.crossentropy</code> (classifiers)</p>
</li>
<li>
<p><code>n_epochs</code>: Number of epochs to train for. Default = <code>10</code></p>
</li>
<li>
<p><code>batch_size</code>: The batch_size for the data. Default = 1</p>
</li>
<li>
<p><code>lambda</code>: The regularization strength. Default = 0. Range = [0, ∞)</p>
</li>
<li>
<p><code>alpha</code>: The L2/L1 mix of regularization. Default = 0. Range = [0, 1]</p>
</li>
<li>
<p><code>optimiser_changes_trigger_retraining</code>: True if fitting an
associated machine should trigger retraining from scratch whenever
the optimiser changes. Default = <code>false</code></p>
</li>
</ol>
<p>The classifiers have an additional hyperparameter <code>finaliser</code> (default
= <code>Flux.softmax</code>) which is the operation applied to the unnormalized
output of the final layer to obtain probabilities (outputs summing to
one). Default = <code>Flux.softmax</code>. It should return a vector of the same
length as its input.</p>







<h3><a id="user-content-defining-a-new-builder" class="anchor" aria-hidden="true" href="#defining-a-new-builder"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Defining a new builder</h3>
<p>Following is an example defining a new builder for creating a simple
fully-connected neural network with two hidden layers, with <code>n1</code> nodes
in the first hidden layer, and <code>n2</code> nodes in the second, for use in
any of the first three models in Table 1. The definition includes one
mutable struct and one method:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">mutable struct</span> MyNetwork <span class="pl-k">&lt;:</span> <span class="pl-c1">MLJFlux.Builder</span>
    n1 <span class="pl-k">::</span> <span class="pl-c1">Int</span>
    n2 <span class="pl-k">::</span> <span class="pl-c1">Int</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> MLJFlux<span class="pl-k">.</span><span class="pl-en">build</span>(nn<span class="pl-k">::</span><span class="pl-c1">MyNetwork</span>, n_in, n_out)
    <span class="pl-k">return</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(n_in, nn<span class="pl-k">.</span>n1), <span class="pl-c1">Dense</span>(nn<span class="pl-k">.</span>n1, nn<span class="pl-k">.</span>n2), <span class="pl-c1">Dense</span>(nn<span class="pl-k">.</span>n2, n_out))
<span class="pl-k">end</span></pre></div>
<p>Note here that <code>n_in</code> and <code>n_out</code> depend on the size of the data (see
Table 1).</p>
<p>More generally, defining a new builder means defining a new struct
(sub-typing <code>MLJFlux.Builder</code> to get pretty printing) <code>MyNetwork</code>, say,
and defining a new <code>MLJFlux.fit</code> method with one of these signatures:</p>
<div class="highlight highlight-source-julia"><pre>MLJFlux<span class="pl-k">.</span><span class="pl-c1">build</span>(builder<span class="pl-k">::</span><span class="pl-c1">MyNetwork</span>, n_in, n_out)
MLJFlux<span class="pl-k">.</span><span class="pl-c1">build</span>(builder<span class="pl-k">::</span><span class="pl-c1">MyNetwork</span>, n_in, n_out, n_channels) <span class="pl-c"><span class="pl-c">#</span> for use with `ImageClassifier`</span></pre></div>
<p>This method must return a <code>Flux.Chain</code> instance, <code>chain</code>, subject to the
following conditions:</p>
<ul>
<li>
<p><code>chain(x)</code> must make sense:</p>
<ul>
<li>
<p>for any <code>x &lt;: Vector{&lt;:AbstractFloat}</code> of length <code>n_in</code> (for use
with one of the first three model types); or</p>
</li>
<li>
<p>for any <code>x &lt;: Array{&lt;:Float32, 3}</code> of size
<code>(W, H, n_channels)</code>, where <code>n_in = (W, H)</code> and <code>n_channels</code> is
1 or 3 (for use with <code>ImageClassifier</code>)</p>
</li>
</ul>
</li>
<li>
<p>The object returned by <code>chain(x)</code> must be an <code>AbstractFloat</code> vector
of length <code>n_out</code>.</p>
</li>
</ul>
<h3><a id="user-content-loss-functions" class="anchor" aria-hidden="true" href="#loss-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Loss functions</h3>
<p>Currently, the loss function specified by <code>loss=...</code> is applied
internally by Flux and needs to conform to the Flux API. You cannot,
for example, supply one of MLJ's probabilistic loss functions, such as
<code>MLJ.cross_entropy</code> to one of the classifiers constructors, although
you <em>should</em> use MLJ loss functions in MLJ meta-algorithms.</p>









</article></div>