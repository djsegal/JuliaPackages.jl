<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-mljflux" class="anchor" aria-hidden="true" href="#mljflux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MLJFlux</h1>
<p>An interface to Flux deep learning models for the <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine learning framework</p>
<p><a href="https://travis-ci.com/alan-turing-institute/MLJFlux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/7ad1d68bc0ec636a89dceb9e8251379474fee786/68747470733a2f2f7472617669732d63692e636f6d2f616c616e2d747572696e672d696e737469747574652f4d4c4a466c75782e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/alan-turing-institute/MLJFlux.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://coveralls.io/github/alan-turing-institute/MLJFlux.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/f6a2fcb71e7d0bab3b4cf9d398db371966488a61/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f616c616e2d747572696e672d696e737469747574652f4d4c4a466c75782e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/alan-turing-institute/MLJFlux.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p>MLJFlux.jl makes a variety of deep learning models available to users
of the MLJ machine learning toolbox by providing an interface to
<a href="https://github.com/FluxML/Flux.jl">Flux</a> framework.</p>
<p>This package is a work-in-progess and does not have a stable
API. Presently, the user should be familiar with building a Flux
chain.</p>
<h3><a id="user-content-models" class="anchor" aria-hidden="true" href="#models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models</h3>
<p>In MLJ a <em>model</em> is a mutable struct storing hyperparameters for some learning algorithm indicated by the model name. MLJFlux provides three such models:</p>
<ul>
<li><code>NeuralNetworkRegressor</code></li>
<li><code>MultivariateNeuralNetworkRegressor</code></li>
<li><code>NeuralNetworkClassifier</code></li>
<li><code>ImageClassifier</code></li>
</ul>
<p><em>Warning:</em> In Flux the term "model" has another meaning. However, as all
Flux "models" used in MLJFLux are <code>Flux.Chain</code> objects, we call them
<em>chains</em>, and restrict use of "model" to models in the MLJ sense.</p>
<h3><a id="user-content-constructing-a-model" class="anchor" aria-hidden="true" href="#constructing-a-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Constructing a model</h3>
<p>Construction begins by defining an auxiliary struct called a
<em>builder</em>, and an associated <code>fit</code> method, for generating a
<code>Flux.Chain</code> object compatible with the data (bound later to the MLJ
model). The struct must be derived from MLJFlux.Builder, as in this
example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">mutable struct</span> MyNetwork <span class="pl-k">&lt;:</span> <span class="pl-c1">MLJFlux.Builder</span>
    n1 <span class="pl-k">::</span> <span class="pl-c1">Int</span>
    n2 <span class="pl-k">::</span> <span class="pl-c1">Int</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> MLJFlux<span class="pl-k">.</span><span class="pl-en">fit</span>(nn<span class="pl-k">::</span><span class="pl-c1">MyNetwork</span>, a, b)
    <span class="pl-k">return</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(a, nn<span class="pl-k">.</span>n1), <span class="pl-c1">Dense</span>(nn<span class="pl-k">.</span>n1, nn<span class="pl-k">.</span>n2), <span class="pl-c1">Dense</span>(nn<span class="pl-k">.</span>n2, b))
<span class="pl-k">end</span></pre></div>
<p><em>Notes:</em></p>
<ul>
<li>The attributes of the MyNetwork struct <code>n1</code>, <code>n2</code> can be anything. What matters is the result of the <code>fit</code> function.</li>
<li>Here <code>a</code> is the the number of input features, inferred from
the data by MLJ when the model is trained. (It may be this argument is ignored, as in an
initial convolution layer for image classification).</li>
<li>Here <code>b</code> is the dimension of the target variable
(<code>NeuralNetworkRegressor</code>) or the number of (univariate) target
levels (<code>NeuralNetworkClassifier</code> or <code>ImageClassifier</code>) - again inferred from the data.</li>
</ul>
<p>Now that we have a builder, we can instantiate an MLJ model. For example:</p>
<div class="highlight highlight-source-julia"><pre>nn_regressor <span class="pl-k">=</span> <span class="pl-c1">NeuralNetworkRegressor</span>(builder<span class="pl-k">=</span><span class="pl-c1">MyNetwork</span>(<span class="pl-c1">32</span>, <span class="pl-c1">16</span>), 
loss<span class="pl-k">=</span>Flux<span class="pl-k">.</span>mse, epochs<span class="pl-k">=</span><span class="pl-c1">5</span>)</pre></div>
<p>The object <code>nn_regressor</code> behaves like any other MLJ model. It can be wrapped inside an MLJ <code>machine</code>, and you can do anything you'd do with
an MLJ machine.</p>
<div class="highlight highlight-source-julia"><pre>mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(nn_regressor, X, y)
<span class="pl-c1">fit!</span>(mach, verbosity<span class="pl-k">=</span><span class="pl-c1">2</span>)
yhat <span class="pl-k">=</span> <span class="pl-c1">predict</span>(mach, rows <span class="pl-k">=</span> train)</pre></div>
<p>and so on.</p>
<h3><a id="user-content-loss-functions" class="anchor" aria-hidden="true" href="#loss-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Loss functions.</h3>
<p>The loss function specified by <code>loss=...</code> is applied internally by
Flux and needs to conform to the Flux API. You cannot, for example,
supply one of MLJ's probablistic loss functions, such as
<code>MLJ.cross_entropy</code> to one of the classifiers.  Unless, you are
familiar with this API, it is recommended you use one of the <a href="https://github.com/FluxML/Flux.jl/blob/v0.8.3/src/layers/stateless.jl">loss
functions provided by
Flux</a>
or leave <code>loss</code> unspecified to invoke the default. For a binary classification problem you might also consider <code>Flux.binarycrossentropy</code>, while for a classification problem with more than two classes (most image problems) consider <code>Flux.logitbinarycrossentropy</code>, as these have better numerical stability than vanilla <code>Flux.crossentropy</code>.</p>
<h3><a id="user-content-hyperparameters" class="anchor" aria-hidden="true" href="#hyperparameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hyperparameters.</h3>
<p><code>NeuralNetworkRegressor</code> and <code>NeuralNetworkClassifier</code> have the following hyperparameters:</p>
<ol>
<li>
<p><code>builder</code>: An instance of some concrete subtype of
<code>MLJFlux.Builder</code>, as in the above example</p>
</li>
<li>
<p><code>optimiser</code>: The optimiser to use for training. Default =
<code>Flux.ADAM()</code></p>
</li>
<li>
<p><code>loss</code>: The loss function used for training. Default = <code>Flux.mse</code> (regressors) and <code>Flux.crossentropy</code> (classifiers)</p>
</li>
<li>
<p><code>n_epochs</code>: Number of epochs to train for. Default = <code>10</code></p>
</li>
<li>
<p><code>batch_size</code>: The batch_size for the data. Default = 1</p>
</li>
<li>
<p><code>lambda</code>: The regularization strength. Default = 0. Range = [0, âˆž)</p>
</li>
<li>
<p><code>alpha</code>: The L2/L1 mix of regularization. Default = 0. Range = [0, 1]</p>
</li>
<li>
<p><code>optimiser_changes_trigger_retraining</code>: True if fitting an
associated machine should trigger retraining from scratch whenever
the optimiser changes. Default = false</p>
</li>
</ol>







</article></div>