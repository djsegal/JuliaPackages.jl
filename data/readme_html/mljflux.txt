<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-mljflux" class="anchor" aria-hidden="true" href="#mljflux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MLJFlux</h1>
<p dir="auto">An interface to the Flux deep learning models for the
<a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine
learning framework.</p>
<table>
<thead>
<tr>
<th>Branch</th>
<th>Julia</th>
<th>CPU CI</th>
<th>GPU CI</th>
<th>Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>master</code></td>
<td>v1</td>
<td><a href="https://github.com/FluxML/MLJFlux.jl/actions/workflows/ci.yml"><img src="https://github.com/FluxML/MLJFlux.jl/workflows/CI/badge.svg?branch=master" alt="Continuous Integration (CPU)" title="Continuous Integration (CPU)" style="max-width: 100%;"></a></td>
<td><a href="https://buildkite.com/julialang/mljflux-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a623223709971cb47e0fca596b7b0adc23c900dcbced838e27f78d4fffa6173d/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f61653433396531663665643666313738333432613065643136366430393833646536656331623732333235653465336537652e7376673f6272616e63683d6d617374657226737465703d4a756c69612532307631" alt="Continuous Integration (GPU)" title="Continuous Integration (GPU)" data-canonical-src="https://badge.buildkite.com/ae439e1f6ed6f178342a0ed166d0983de6ec1b72325e4e3e7e.svg?branch=master&amp;step=Julia%20v1" style="max-width: 100%;"></a></td>
<td><a href="https://github.com/FluxML/MLJFlux.jl/actions/workflows/ci.yml"><img src="https://camo.githubusercontent.com/c54ff6060bc95a337eb45f25285f43a38f4023badef591a8de5a857a4b85a57e/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f616c616e2d747572696e672d696e737469747574652f4d4c4a466c75782e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Code Coverage" title="Code Coverage" data-canonical-src="https://coveralls.io/repos/github/alan-turing-institute/MLJFlux.jl/badge.svg?branch=master" style="max-width: 100%;"></a></td>
</tr>
<tr>
<td><code>dev</code></td>
<td>v1</td>
<td><a href="https://github.com/FluxML/MLJFlux.jl/actions/workflows/ci.yml"><img src="https://github.com/FluxML/MLJFlux.jl/workflows/CI/badge.svg?branch=dev" alt="Continuous Integration (CPU)" title="Continuous Integration (CPU)" style="max-width: 100%;"></a></td>
<td><a href="https://buildkite.com/julialang/mljflux-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f90dcb29945b0dbeba5eb13c649cfa1428acd2333fe74e6a5e5bb196248e75af/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f61653433396531663665643666313738333432613065643136366430393833646536656331623732333235653465336537652e7376673f6272616e63683d64657626737465703d4a756c69612532307631" alt="Continuous Integration (GPU)" title="Continuous Integration (GPU)" data-canonical-src="https://badge.buildkite.com/ae439e1f6ed6f178342a0ed166d0983de6ec1b72325e4e3e7e.svg?branch=dev&amp;step=Julia%20v1" style="max-width: 100%;"></a></td>
<td><a href="https://github.com/FluxML/MLJFlux.jl/actions/workflows/ci.yml"><img src="https://camo.githubusercontent.com/37c7be19984533c8eef92c3010c5dca5b54572f4911b7602c51302e1cbdad756/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f616c616e2d747572696e672d696e737469747574652f4d4c4a466c75782e6a6c2f62616467652e7376673f6272616e63683d646576" alt="Code Coverage" title="Code Coverage" data-canonical-src="https://coveralls.io/repos/github/alan-turing-institute/MLJFlux.jl/badge.svg?branch=dev" style="max-width: 100%;"></a></td>
</tr>
</tbody>
</table>
<p dir="auto">MLJFlux makes it possible to apply the machine learning
meta-algorithms provided by MLJ - such as out-of-sample performance
evaluation, hyper-parameter optimization, and iteration control - to some classes of
<strong>supervised deep learning models</strong>. It does this by providing an
interface to the <a href="https://fluxml.ai/Flux.jl/stable/" rel="nofollow">Flux</a>
framework.</p>
<p dir="auto">The guiding vision of this package is to make evaluating and
optimizing basic Flux models more convenient to users already familiar
with the MLJ workflow. This goal will likely place restrictions of the
class of Flux models that can used, at least in the medium term. For
example, online learning, re-enforcement learning, and adversarial
networks are currently out of scope.</p>
<p dir="auto">Currently MLJFlux is also limited to training models in the case that all
training data fits into memory.</p>
<h3 dir="auto"><a id="user-content-basic-idea" class="anchor" aria-hidden="true" href="#basic-idea"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basic idea</h3>
<p dir="auto">Each MLJFlux model has a <em>builder</em> hyperparameter, an object encoding
instructions for creating a neural network given the data that the
model eventually sees (e.g., the number of classes in a classification
problem). While each MLJ model has a simple default builder, users
will generally need to define their own builders to get good results,
and this will require familiarity with the <a href="https://fluxml.ai/Flux.jl/stable/" rel="nofollow">Flux
API</a> for defining a neural network
chain.</p>
<p dir="auto">In the future MLJFlux may provide a larger assortment of canned
builders. Pull requests introducing new ones are most welcome.</p>
<h3 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.activate(&quot;my_environment&quot;, shared=true)
Pkg.add(&quot;MLJ&quot;)
Pkg.add(&quot;MLJFlux&quot;)
Pkg.add(&quot;RDatasets&quot;)  # for the demo below
Pkg.add(&quot;Plots&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">activate</span>(<span class="pl-s"><span class="pl-pds">"</span>my_environment<span class="pl-pds">"</span></span>, shared<span class="pl-k">=</span><span class="pl-c1">true</span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MLJ<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MLJFlux<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>RDatasets<span class="pl-pds">"</span></span>)  <span class="pl-c"><span class="pl-c">#</span> for the demo below</span>
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Plots<span class="pl-pds">"</span></span>)</pre></div>
<h3 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h3>
<p dir="auto">Following is an introductory example using a default builder and no
standardization of input features (<a href="/examples/iris">notebook/script</a>).</p>
<p dir="auto">For an example implementing early stopping and snapshots, using MLJ's
<a href="https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/" rel="nofollow"><code>IteratedModel</code>
wrapper</a>,
see the <a href="https://github.com/FluxML/MLJFlux.jl/blob/dev/examples/mnist">MNIST dataset
example</a>.</p>
<h4 dir="auto"><a id="user-content-loading-some-data-and-instantiating-a-model" class="anchor" aria-hidden="true" href="#loading-some-data-and-instantiating-a-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Loading some data and instantiating a model</h4>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ
import RDatasets
iris = RDatasets.dataset(&quot;datasets&quot;, &quot;iris&quot;);
y, X = unpack(iris, ==(:Species), colname -&gt; true, rng=123);
NeuralNetworkClassifier = @load NeuralNetworkClassifier

julia&gt; clf = NeuralNetworkClassifier()
NeuralNetworkClassifier(
	builder = Short(
			n_hidden = 0,
			dropout = 0.5,
			σ = NNlib.σ),
	finaliser = NNlib.softmax,
	optimiser = ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}()),
	loss = Flux.crossentropy,
	epochs = 10,
	batch_size = 1,
	lambda = 0.0,
	alpha = 0.0,
	optimiser_changes_trigger_retraining = false) @ 1…60"><pre><span class="pl-k">using</span> MLJ
<span class="pl-k">import</span> RDatasets
iris <span class="pl-k">=</span> RDatasets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>);
y, X <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(iris, <span class="pl-k">==</span>(<span class="pl-c1">:Species</span>), colname <span class="pl-k">-&gt;</span> <span class="pl-c1">true</span>, rng<span class="pl-k">=</span><span class="pl-c1">123</span>);
NeuralNetworkClassifier <span class="pl-k">=</span> <span class="pl-c1">@load</span> NeuralNetworkClassifier

julia<span class="pl-k">&gt;</span> clf <span class="pl-k">=</span> <span class="pl-c1">NeuralNetworkClassifier</span>()
<span class="pl-c1">NeuralNetworkClassifier</span>(
	builder <span class="pl-k">=</span> <span class="pl-c1">Short</span>(
			n_hidden <span class="pl-k">=</span> <span class="pl-c1">0</span>,
			dropout <span class="pl-k">=</span> <span class="pl-c1">0.5</span>,
			σ <span class="pl-k">=</span> NNlib<span class="pl-k">.</span>σ),
	finaliser <span class="pl-k">=</span> NNlib<span class="pl-k">.</span>softmax,
	optimiser <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.001</span>, (<span class="pl-c1">0.9</span>, <span class="pl-c1">0.999</span>), <span class="pl-c1">IdDict</span><span class="pl-c1">{Any,Any}</span>()),
	loss <span class="pl-k">=</span> Flux<span class="pl-k">.</span>crossentropy,
	epochs <span class="pl-k">=</span> <span class="pl-c1">10</span>,
	batch_size <span class="pl-k">=</span> <span class="pl-c1">1</span>,
	lambda <span class="pl-k">=</span> <span class="pl-c1">0.0</span>,
	alpha <span class="pl-k">=</span> <span class="pl-c1">0.0</span>,
	optimiser_changes_trigger_retraining <span class="pl-k">=</span> <span class="pl-c1">false</span>) @ <span class="pl-c1">1</span>…<span class="pl-c1">60</span></pre></div>
<h4 dir="auto"><a id="user-content-incremental-training" class="anchor" aria-hidden="true" href="#incremental-training"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Incremental training</h4>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Random.seed!; seed!(123)
mach = machine(clf, X, y)
fit!(mach)

julia&gt; training_loss = cross_entropy(predict(mach, X), y) |&gt; mean
0.9064070459118777

# Increasing learning rate and adding iterations:
clf.optimiser.eta = clf.optimiser.eta * 2
clf.epochs = clf.epochs + 5

julia&gt; fit!(mach, verbosity=2)
[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…} @804.
[ Info: Loss is 0.8686
[ Info: Loss is 0.8228
[ Info: Loss is 0.7706
[ Info: Loss is 0.7565
[ Info: Loss is 0.7347
Machine{NeuralNetworkClassifier{Short,…},…} @804 trained 2 times; caches data
  args:
	1:  Source @985 ⏎ `Table{AbstractVector{Continuous}}`
	2:  Source @367 ⏎ `AbstractVector{Multiclass{3}}`

julia&gt; training_loss = cross_entropy(predict(mach, X), y) |&gt; mean
0.7347092796453824"><pre><span class="pl-k">import</span> Random<span class="pl-k">.</span>seed!; <span class="pl-c1">seed!</span>(<span class="pl-c1">123</span>)
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(clf, X, y)
<span class="pl-c1">fit!</span>(mach)

julia<span class="pl-k">&gt;</span> training_loss <span class="pl-k">=</span> <span class="pl-c1">cross_entropy</span>(<span class="pl-c1">predict</span>(mach, X), y) <span class="pl-k">|&gt;</span> mean
<span class="pl-c1">0.9064070459118777</span>

<span class="pl-c"><span class="pl-c">#</span> Increasing learning rate and adding iterations:</span>
clf<span class="pl-k">.</span>optimiser<span class="pl-k">.</span>eta <span class="pl-k">=</span> clf<span class="pl-k">.</span>optimiser<span class="pl-k">.</span>eta <span class="pl-k">*</span> <span class="pl-c1">2</span>
clf<span class="pl-k">.</span>epochs <span class="pl-k">=</span> clf<span class="pl-k">.</span>epochs <span class="pl-k">+</span> <span class="pl-c1">5</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">fit!</span>(mach, verbosity<span class="pl-k">=</span><span class="pl-c1">2</span>)
[ Info<span class="pl-k">:</span> Updating Machine{NeuralNetworkClassifier{Short,…},…} @<span class="pl-c1">804.</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.8686</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.8228</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.7706</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.7565</span>
[ Info<span class="pl-k">:</span> Loss is <span class="pl-c1">0.7347</span>
Machine{NeuralNetworkClassifier{Short,…},…} @<span class="pl-c1">804</span> trained <span class="pl-c1">2</span> times; caches data
  args<span class="pl-k">:</span>
	<span class="pl-c1">1</span><span class="pl-k">:</span>  Source @<span class="pl-c1">985</span> ⏎ <span class="pl-s"><span class="pl-pds">`</span>Table{AbstractVector{Continuous}}<span class="pl-pds">`</span></span>
	<span class="pl-c1">2</span><span class="pl-k">:</span>  Source @<span class="pl-c1">367</span> ⏎ <span class="pl-s"><span class="pl-pds">`</span>AbstractVector{Multiclass{3}}<span class="pl-pds">`</span></span>

julia<span class="pl-k">&gt;</span> training_loss <span class="pl-k">=</span> <span class="pl-c1">cross_entropy</span>(<span class="pl-c1">predict</span>(mach, X), y) <span class="pl-k">|&gt;</span> mean
<span class="pl-c1">0.7347092796453824</span></pre></div>
<h4 dir="auto"><a id="user-content-accessing-the-flux-chain-model" class="anchor" aria-hidden="true" href="#accessing-the-flux-chain-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Accessing the Flux chain (model)</h4>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; fitted_params(mach).chain
Chain(Chain(Dense(4, 3, σ), Flux.Dropout{Float64}(0.5, false), Dense(3, 3)), softmax)"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">fitted_params</span>(mach)<span class="pl-k">.</span>chain
<span class="pl-c1">Chain</span>(<span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">4</span>, <span class="pl-c1">3</span>, σ), Flux<span class="pl-k">.</span><span class="pl-c1">Dropout</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">0.5</span>, <span class="pl-c1">false</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">3</span>, <span class="pl-c1">3</span>)), softmax)</pre></div>
<h4 dir="auto"><a id="user-content-evolution-of-out-of-sample-performance" class="anchor" aria-hidden="true" href="#evolution-of-out-of-sample-performance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Evolution of out-of-sample performance</h4>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="r = range(clf, :epochs, lower=1, upper=200, scale=:log10)
curve = learning_curve(clf, X, y,
					   range=r,
					   resampling=Holdout(fraction_train=0.7),
					   measure=cross_entropy)
using Plots
plot(curve.parameter_values,
	   curve.measurements,
	   xlab=curve.parameter_name,
	   xscale=curve.parameter_scale,
	   ylab = &quot;Cross Entropy&quot;)
"><pre>r <span class="pl-k">=</span> <span class="pl-c1">range</span>(clf, <span class="pl-c1">:epochs</span>, lower<span class="pl-k">=</span><span class="pl-c1">1</span>, upper<span class="pl-k">=</span><span class="pl-c1">200</span>, scale<span class="pl-k">=</span><span class="pl-c1">:log10</span>)
curve <span class="pl-k">=</span> <span class="pl-c1">learning_curve</span>(clf, X, y,
					   range<span class="pl-k">=</span>r,
					   resampling<span class="pl-k">=</span><span class="pl-c1">Holdout</span>(fraction_train<span class="pl-k">=</span><span class="pl-c1">0.7</span>),
					   measure<span class="pl-k">=</span>cross_entropy)
<span class="pl-k">using</span> Plots
<span class="pl-c1">plot</span>(curve<span class="pl-k">.</span>parameter_values,
	   curve<span class="pl-k">.</span>measurements,
	   xlab<span class="pl-k">=</span>curve<span class="pl-k">.</span>parameter_name,
	   xscale<span class="pl-k">=</span>curve<span class="pl-k">.</span>parameter_scale,
	   ylab <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Cross Entropy<span class="pl-pds">"</span></span>)
</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="examples/iris/iris_history.png"><img src="examples/iris/iris_history.png" alt="" style="max-width: 100%;"></a></p>
<h3 dir="auto"><a id="user-content-models" class="anchor" aria-hidden="true" href="#models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Models</h3>
<p dir="auto">In MLJ a <em>model</em> is a mutable struct storing hyperparameters for some
learning algorithm indicated by the model name, and that's all. In
particular, an MLJ model does not store learned parameters.</p>
<p dir="auto"><em>Warning:</em> In Flux the term "model" has another meaning. However, as all
Flux "models" used in MLJFLux are <code>Flux.Chain</code> objects, we call them
<em>chains</em>, and restrict use of "model" to models in the MLJ sense.</p>
<p dir="auto">MLJFlux provides four model types, for use with input features <code>X</code> and
targets <code>y</code> of the <a href="https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/" rel="nofollow">scientific
type</a>
indicated in the table below. The parameters <code>n_in</code>, <code>n_out</code> and <code>n_channels</code>
refer to information passed to the builder, as described under
<a href="defining-a-new-builder">Defining a new builder</a> below.</p>
<table>
<thead>
<tr>
<th>model type</th>
<th>prediction type</th>
<th><code>scitype(X) &lt;: _</code></th>
<th><code>scitype(y) &lt;: _</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>NeuralNetworkRegressor</code></td>
<td><code>Deterministic</code></td>
<td><code>Table(Continuous)</code> with <code>n_in</code> columns</td>
<td><code>AbstractVector{&lt;:Continuous)</code> (<code>n_out = 1</code>)</td>
</tr>
<tr>
<td><code>MultitargetNeuralNetworkRegressor</code></td>
<td><code>Deterministic</code></td>
<td><code>Table(Continuous)</code> with <code>n_in</code> columns</td>
<td><code>&lt;: Table(Continuous)</code> with <code>n_out</code> columns</td>
</tr>
<tr>
<td><code>NeuralNetworkClassifier</code></td>
<td><code>Probabilistic</code></td>
<td><code>&lt;:Table(Continuous)</code> with <code>n_in</code> columns</td>
<td><code>AbstractVector{&lt;:Finite}</code> with <code>n_out</code> classes</td>
</tr>
<tr>
<td><code>ImageClassifier</code></td>
<td><code>Probabilistic</code></td>
<td><code>AbstractVector(&lt;:Image{W,H})</code> with <code>n_in = (W, H)</code></td>
<td><code>AbstractVector{&lt;:Finite}</code> with <code>n_out</code> classes</td>
</tr>
</tbody>
</table>
<blockquote>
<p dir="auto">Table 1. Input and output types for MLJFlux models</p>
</blockquote>
<h4 dir="auto"><a id="user-content-non-tabular-input" class="anchor" aria-hidden="true" href="#non-tabular-input"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Non-tabular input</h4>
<p dir="auto">Any <code>AbstractMatrix{&lt;:AbstractFloat}</code> object <code>Xmat</code> can be forced to
have scitype <code>Table(Continuous)</code> by replacing it with <code> X = MLJ.table(Xmat)</code>. Furthermore, this wrapping, and subsequent
unwrapping under the hood, will compile to a no-op. At present this
includes support for sparse matrix data, but the implementation has
not been optimized for sparse data at this time and so should be used
with caution.</p>
<p dir="auto">Instructions for coercing common image formats into some
<code>AbstractVector{&lt;:Image}</code> are
<a href="https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/#Type-coercion-for-image-data-1" rel="nofollow">here</a>.</p>
<h3 dir="auto"><a id="user-content-warm-restart" class="anchor" aria-hidden="true" href="#warm-restart"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Warm restart</h3>
<p dir="auto">MLJ machines cache state enabling the "warm restart" of model
training, as demonstrated in the example above. In the case of MLJFlux
models, <code>fit!(mach)</code> will use a warm restart if:</p>
<ul dir="auto">
<li>
<p dir="auto">only <code>model.epochs</code> has changed since the last call; or</p>
</li>
<li>
<p dir="auto">only <code>model.epochs</code> or <code>model.optimiser</code> have changed since the last
call and <code>model.optimiser_changes_trigger_retraining == false</code> (the
default) (the "state" part of the optimiser is ignored in this
comparison). This allows one to dynamically modify learning rates,
for example.</p>
</li>
</ul>
<p dir="auto">Here <code>model=mach.model</code> is the associated MLJ model.</p>
<p dir="auto">The warm restart feature makes it possible to apply early stopping
criteria, as defined in
<a href="https://github.com/ablaom/EarlyStopping.jl">EarlyStopping.jl</a>. For an
example, see <a href="/examples/mnist/">/examples/mnist/</a>. (Eventually, this
will be handled by an MLJ model wrapper for controlling arbitrary
iterative models.)</p>
<h3 dir="auto"><a id="user-content-training-on-a-gpu" class="anchor" aria-hidden="true" href="#training-on-a-gpu"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training on a GPU</h3>
<p dir="auto">When instantiating a model for training on a GPU, specify
<code>acceleration=CUDALibs()</code>, as in</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ
ImageClassifier = @load ImageClassifier
model = ImageClassifier(epochs=10, acceleration=CUDALibs())
mach = machine(model, X, y) |&gt; fit!"><pre><span class="pl-k">using</span> MLJ
ImageClassifier <span class="pl-k">=</span> <span class="pl-c1">@load</span> ImageClassifier
model <span class="pl-k">=</span> <span class="pl-c1">ImageClassifier</span>(epochs<span class="pl-k">=</span><span class="pl-c1">10</span>, acceleration<span class="pl-k">=</span><span class="pl-c1">CUDALibs</span>())
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(model, X, y) <span class="pl-k">|&gt;</span> fit!</pre></div>
<p dir="auto">In this example, the data <code>X, y</code> is copied onto the GPU under the hood
on the call to <code>fit!</code> and cached for use in any warm restart (see
above). The Flux chain used in training is always copied back to the
CPU at then conclusion of <code>fit!</code>, and made available as
<code>fitted_params(mach)</code>.</p>
<h3 dir="auto"><a id="user-content-random-number-generators-and-reproducibility" class="anchor" aria-hidden="true" href="#random-number-generators-and-reproducibility"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Random number generators and reproducibility</h3>
<p dir="auto">Every MLJFlux model includes an <code>rng</code> hyper-parameter that is passed
to builders for the purposes of weight initialization. This can be
any <code>AbstractRNG</code> or the seed (integer) for a <code>MersenneTwister</code> that
will be reset on every cold restart of model (machine) training.</p>
<p dir="auto">Until there is a <a href="https://github.com/FluxML/Flux.jl/issues/1617" data-hovercard-type="issue" data-hovercard-url="/FluxML/Flux.jl/issues/1617/hovercard">mechanism for
doing so</a> <code>rng</code> is <em>not</em>
passed to dropout layers and one must manually seed the <code>GLOBAL_RNG</code>
for reproducibility purposes, when using a builder that includes
<code>Dropout</code> (such as <code>MLJFlux.Short</code>). If training models on a
GPU (i.e., <code>acceleration isa CUDALibs</code>) one must additionally call
<code>CUDA.seed!(...)</code>.</p>
<h3 dir="auto"><a id="user-content-built-in-builders" class="anchor" aria-hidden="true" href="#built-in-builders"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Built-in builders</h3>
<p dir="auto">The following builders are provided out-of-the-box. Query their
doc-strings for advanced options and further details.</p>
<table>
<thead>
<tr>
<th align="left">builder</th>
<th align="left">description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>MLJFlux.Linear(σ=relu)</code></td>
<td align="left">vanilla linear network with activation function <code>σ</code></td>
</tr>
<tr>
<td align="left"><code>MLJFlux.Short(n_hidden=0, dropout=0.5, σ=sigmoid)</code></td>
<td align="left">fully connected network with one hidden layer and dropout</td>
</tr>
<tr>
<td align="left"><code>MLJFlux.MLP(hidden=(10,))</code></td>
<td align="left">general multi-layer perceptron</td>
</tr>
</tbody>
</table>
<h3 dir="auto"><a id="user-content-model-hyperparameters" class="anchor" aria-hidden="true" href="#model-hyperparameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model hyperparameters.</h3>
<p dir="auto">All models share the following hyper-parameters:</p>
<ol dir="auto">
<li>
<p dir="auto"><code>builder</code>: Default = <code>MLJFlux.Linear(σ=Flux.relu)</code> (regressors) or
<code>MLJFlux.Short(n_hidden=0, dropout=0.5, σ=Flux.σ)</code> (classifiers)</p>
</li>
<li>
<p dir="auto"><code>optimiser</code>: The optimiser to use for training. Default =
<code>Flux.ADAM()</code></p>
</li>
<li>
<p dir="auto"><code>loss</code>: The loss function used for training. Default = <code>Flux.mse</code>
(regressors) and <code>Flux.crossentropy</code> (classifiers)</p>
</li>
<li>
<p dir="auto"><code>n_epochs</code>: Number of epochs to train for. Default = <code>10</code></p>
</li>
<li>
<p dir="auto"><code>batch_size</code>: The batch_size for the data. Default = 1</p>
</li>
<li>
<p dir="auto"><code>lambda</code>: The regularization strength. Default = 0. Range = [0, ∞)</p>
</li>
<li>
<p dir="auto"><code>alpha</code>: The L2/L1 mix of regularization. Default = 0. Range = [0, 1]</p>
</li>
<li>
<p dir="auto"><code>rng</code>: The random number generator (RNG) passed to builders, for
weight intitialization, for example. Can be any <code>AbstractRNG</code> or
the seed (integer) for a <code>MersenneTwister</code> that is reset on every
cold restart of model (machine) training. Default =
<code>GLOBAL_RNG</code>.</p>
</li>
<li>
<p dir="auto"><code>acceleration</code>: Use <code>CUDALibs()</code> for training on GPU; default is <code>CPU1()</code>.</p>
</li>
<li>
<p dir="auto"><code>optimiser_changes_trigger_retraining</code>: True if fitting an
associated machine should trigger retraining from scratch whenever
the optimiser changes. Default = <code>false</code></p>
</li>
</ol>
<p dir="auto">The classifiers have an additional hyperparameter <code>finaliser</code> (default
= <code>Flux.softmax</code>) which is the operation applied to the unnormalized
output of the final layer to obtain probabilities (outputs summing to
one). Default = <code>Flux.softmax</code>. It should return a vector of the same
length as its input.</p>







<h3 dir="auto"><a id="user-content-defining-a-new-builder" class="anchor" aria-hidden="true" href="#defining-a-new-builder"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Defining a new builder</h3>
<p dir="auto">Following is an example defining a new builder for creating a simple
fully-connected neural network with two hidden layers, with <code>n1</code> nodes
in the first hidden layer, and <code>n2</code> nodes in the second, for use in
any of the first three models in Table 1. The definition includes one
mutable struct and one method:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="mutable struct MyBuilder &lt;: MLJFlux.Builder
	n1 :: Int
	n2 :: Int
end

function MLJFlux.build(nn::MyBuilder, rng, n_in, n_out)
	init = Flux.glorot_uniform(rng)
	return Chain(Dense(n_in, nn.n1, init=init),
				 Dense(nn.n1, nn.n2, init=init),
				 Dense(nn.n2, n_out, init=init))
end"><pre><span class="pl-k">mutable struct</span> MyBuilder <span class="pl-k">&lt;:</span> <span class="pl-c1">MLJFlux.Builder</span>
	n1 <span class="pl-k">::</span> <span class="pl-c1">Int</span>
	n2 <span class="pl-k">::</span> <span class="pl-c1">Int</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> MLJFlux<span class="pl-k">.</span><span class="pl-en">build</span>(nn<span class="pl-k">::</span><span class="pl-c1">MyBuilder</span>, rng, n_in, n_out)
	init <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">glorot_uniform</span>(rng)
	<span class="pl-k">return</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(n_in, nn<span class="pl-k">.</span>n1, init<span class="pl-k">=</span>init),
				 <span class="pl-c1">Dense</span>(nn<span class="pl-k">.</span>n1, nn<span class="pl-k">.</span>n2, init<span class="pl-k">=</span>init),
				 <span class="pl-c1">Dense</span>(nn<span class="pl-k">.</span>n2, n_out, init<span class="pl-k">=</span>init))
<span class="pl-k">end</span></pre></div>
<p dir="auto">Note here that <code>n_in</code> and <code>n_out</code> depend on the size of the data (see
Table 1).</p>
<p dir="auto">For a concrete image classification example, see
<a href="examples/mnist">examples/mnist</a>.</p>
<p dir="auto">More generally, defining a new builder means defining a new struct
sub-typing <code>MLJFlux.Builder</code> and defining a new <code>MLJFlux.build</code> method
with one of these signatures:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="MLJFlux.build(builder::MyBuilder, rng, n_in, n_out)
MLJFlux.build(builder::MyBuilder, rng, n_in, n_out, n_channels) # for use with `ImageClassifier`"><pre>MLJFlux<span class="pl-k">.</span><span class="pl-c1">build</span>(builder<span class="pl-k">::</span><span class="pl-c1">MyBuilder</span>, rng, n_in, n_out)
MLJFlux<span class="pl-k">.</span><span class="pl-c1">build</span>(builder<span class="pl-k">::</span><span class="pl-c1">MyBuilder</span>, rng, n_in, n_out, n_channels) <span class="pl-c"><span class="pl-c">#</span> for use with `ImageClassifier`</span></pre></div>
<p dir="auto">This method must return a <code>Flux.Chain</code> instance, <code>chain</code>, subject to the
following conditions:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>chain(x)</code> must make sense:</p>
<ul dir="auto">
<li>
<p dir="auto">for any <code>x &lt;: Array{&lt;:AbstractFloat, 2}</code> of size <code>(n_in, batch_size)</code> where <code>batch_size</code> is any integer (for use with one
of the first three model types); or</p>
</li>
<li>
<p dir="auto">for any <code>x &lt;: Array{&lt;:Float32, 4}</code> of size <code>(W, H, n_channels, batch_size)</code>, where <code>(W, H) = n_in</code>, <code>n_channels</code> is 1 or 3, and
<code>batch_size</code> is any integer (for use with <code>ImageClassifier</code>)</p>
</li>
</ul>
</li>
<li>
<p dir="auto">The object returned by <code>chain(x)</code> must be an <code>AbstractFloat</code> vector
of length <code>n_out</code>.</p>
</li>
</ul>
<p dir="auto">Alternatively, use <code>MLJFlux.@builder(neural_net)</code> to automatically create a builder for
any valid Flux chain expression <code>neural_net</code>, where the symbols <code>n_in</code>, <code>n_out</code>,
<code>n_channels</code> and <code>rng</code> can appear literally, with the interpretations explained above. For
example,</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="builder = MLJFlux.@builder Chain(Dense(n_in, 128), Dense(128, n_out, tanh))"><pre class="notranslate"><code>builder = MLJFlux.@builder Chain(Dense(n_in, 128), Dense(128, n_out, tanh))
</code></pre></div>
<h3 dir="auto"><a id="user-content-loss-functions" class="anchor" aria-hidden="true" href="#loss-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Loss functions</h3>
<p dir="auto">Currently, the loss function specified by <code>loss=...</code> is applied
internally by Flux and needs to conform to the Flux API. You cannot,
for example, supply one of MLJ's probabilistic loss functions, such as
<code>MLJ.cross_entropy</code> to one of the classifier constructors, although
you <em>should</em> use MLJ loss functions in MLJ meta-algorithms.</p>









<h3 dir="auto"><a id="user-content-an-image-classification-example" class="anchor" aria-hidden="true" href="#an-image-classification-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>An image classification example</h3>
<p dir="auto">An expanded version of this example, with early stopping and
snapshots, is available <a href="/examples/mnist">here</a>.</p>
<p dir="auto">We define a builder that builds a chain with six alternating
convolution and max-pool layers, and a final dense layer, which we
apply to the MNIST image dataset.</p>
<p dir="auto">First we define a generic builder (working for any image size, color
or gray):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ
using Flux
using MLDatasets

# helper function
function flatten(x::AbstractArray)
	return reshape(x, :, size(x)[end])
end

import MLJFlux
mutable struct MyConvBuilder
	filter_size::Int
	channels1::Int
	channels2::Int
	channels3::Int
end

function MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)

	k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3

	mod(k, 2) == 1 || error(&quot;`filter_size` must be odd. &quot;)

	# padding to preserve image size on convolution:
	p = div(k - 1, 2)

	front = Chain(
			   Conv((k, k), n_channels =&gt; c1, pad=(p, p), relu),
			   MaxPool((2, 2)),
			   Conv((k, k), c1 =&gt; c2, pad=(p, p), relu),
			   MaxPool((2, 2)),
			   Conv((k, k), c2 =&gt; c3, pad=(p, p), relu),
			   MaxPool((2 ,2)),
			   flatten)
	d = Flux.outputsize(front, (n_in..., n_channels, 1)) |&gt; first
	return Chain(front, Dense(d, n_out))
end"><pre><span class="pl-k">using</span> MLJ
<span class="pl-k">using</span> Flux
<span class="pl-k">using</span> MLDatasets

<span class="pl-c"><span class="pl-c">#</span> helper function</span>
<span class="pl-k">function</span> <span class="pl-en">flatten</span>(x<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>)
	<span class="pl-k">return</span> <span class="pl-c1">reshape</span>(x, :, <span class="pl-c1">size</span>(x)[<span class="pl-c1">end</span>])
<span class="pl-k">end</span>

<span class="pl-k">import</span> MLJFlux
<span class="pl-k">mutable struct</span> MyConvBuilder
	filter_size<span class="pl-k">::</span><span class="pl-c1">Int</span>
	channels1<span class="pl-k">::</span><span class="pl-c1">Int</span>
	channels2<span class="pl-k">::</span><span class="pl-c1">Int</span>
	channels3<span class="pl-k">::</span><span class="pl-c1">Int</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> MLJFlux<span class="pl-k">.</span><span class="pl-en">build</span>(b<span class="pl-k">::</span><span class="pl-c1">MyConvBuilder</span>, rng, n_in, n_out, n_channels)

	k, c1, c2, c3 <span class="pl-k">=</span> b<span class="pl-k">.</span>filter_size, b<span class="pl-k">.</span>channels1, b<span class="pl-k">.</span>channels2, b<span class="pl-k">.</span>channels3

	<span class="pl-c1">mod</span>(k, <span class="pl-c1">2</span>) <span class="pl-k">==</span> <span class="pl-c1">1</span> <span class="pl-k">||</span> <span class="pl-c1">error</span>(<span class="pl-s"><span class="pl-pds">"</span>`filter_size` must be odd. <span class="pl-pds">"</span></span>)

	<span class="pl-c"><span class="pl-c">#</span> padding to preserve image size on convolution:</span>
	p <span class="pl-k">=</span> <span class="pl-c1">div</span>(k <span class="pl-k">-</span> <span class="pl-c1">1</span>, <span class="pl-c1">2</span>)

	front <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
			   <span class="pl-c1">Conv</span>((k, k), n_channels <span class="pl-k">=&gt;</span> c1, pad<span class="pl-k">=</span>(p, p), relu),
			   <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span>, <span class="pl-c1">2</span>)),
			   <span class="pl-c1">Conv</span>((k, k), c1 <span class="pl-k">=&gt;</span> c2, pad<span class="pl-k">=</span>(p, p), relu),
			   <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span>, <span class="pl-c1">2</span>)),
			   <span class="pl-c1">Conv</span>((k, k), c2 <span class="pl-k">=&gt;</span> c3, pad<span class="pl-k">=</span>(p, p), relu),
			   <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span> ,<span class="pl-c1">2</span>)),
			   flatten)
	d <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">outputsize</span>(front, (n_in<span class="pl-k">...</span>, n_channels, <span class="pl-c1">1</span>)) <span class="pl-k">|&gt;</span> first
	<span class="pl-k">return</span> <span class="pl-c1">Chain</span>(front, <span class="pl-c1">Dense</span>(d, n_out))
<span class="pl-k">end</span></pre></div>
<p dir="auto">Next, we load some of the MNIST data and check scientific types
conform to those is the table above:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="N = 500
Xraw, yraw = MNIST.traindata();
Xraw = Xraw[:,:,1:N];
yraw = yraw[1:N];

julia&gt; scitype(Xraw)
AbstractArray{Unknown, 3}

julia&gt; scitype(yraw)
AbstractArray{Count,1}"><pre>N <span class="pl-k">=</span> <span class="pl-c1">500</span>
Xraw, yraw <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>();
Xraw <span class="pl-k">=</span> Xraw[:,:,<span class="pl-c1">1</span><span class="pl-k">:</span>N];
yraw <span class="pl-k">=</span> yraw[<span class="pl-c1">1</span><span class="pl-k">:</span>N];

julia<span class="pl-k">&gt;</span> <span class="pl-c1">scitype</span>(Xraw)
AbstractArray{Unknown, <span class="pl-c1">3</span>}

julia<span class="pl-k">&gt;</span> <span class="pl-c1">scitype</span>(yraw)
AbstractArray{Count,<span class="pl-c1">1</span>}</pre></div>
<p dir="auto">Inputs should have element scitype <code>GrayImage</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="X = coerce(Xraw, GrayImage);"><pre>X <span class="pl-k">=</span> <span class="pl-c1">coerce</span>(Xraw, GrayImage);</pre></div>
<p dir="auto">For classifiers, target must have element scitype <code>&lt;: Finite</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="y = coerce(yraw, Multiclass);"><pre>y <span class="pl-k">=</span> <span class="pl-c1">coerce</span>(yraw, Multiclass);</pre></div>
<p dir="auto">Instantiating an image classifier model:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="ImageClassifier = @load ImageClassifier
clf = ImageClassifier(builder=MyConvBuilder(3, 16, 32, 32),
					  epochs=10,
					  loss=Flux.crossentropy)"><pre>ImageClassifier <span class="pl-k">=</span> <span class="pl-c1">@load</span> ImageClassifier
clf <span class="pl-k">=</span> <span class="pl-c1">ImageClassifier</span>(builder<span class="pl-k">=</span><span class="pl-c1">MyConvBuilder</span>(<span class="pl-c1">3</span>, <span class="pl-c1">16</span>, <span class="pl-c1">32</span>, <span class="pl-c1">32</span>),
					  epochs<span class="pl-k">=</span><span class="pl-c1">10</span>,
					  loss<span class="pl-k">=</span>Flux<span class="pl-k">.</span>crossentropy)</pre></div>
<p dir="auto">And evaluating the accuracy of the model on a 30% holdout set:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="mach = machine(clf, X, y)

julia&gt; evaluate!(mach,
				 resampling=Holdout(rng=123, fraction_train=0.7),
				 operation=predict_mode,
				 measure=misclassification_rate)
┌────────────────────────┬───────────────┬────────────┐
│ _.measure              │ _.measurement │ _.per_fold │
├────────────────────────┼───────────────┼────────────┤
│ misclassification_rate │ 0.0467        │ [0.0467]   │
└────────────────────────┴───────────────┴────────────┘"><pre>mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(clf, X, y)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">evaluate!</span>(mach,
				 resampling<span class="pl-k">=</span><span class="pl-c1">Holdout</span>(rng<span class="pl-k">=</span><span class="pl-c1">123</span>, fraction_train<span class="pl-k">=</span><span class="pl-c1">0.7</span>),
				 operation<span class="pl-k">=</span>predict_mode,
				 measure<span class="pl-k">=</span>misclassification_rate)
┌────────────────────────┬───────────────┬────────────┐
│ _<span class="pl-k">.</span>measure              │ _<span class="pl-k">.</span>measurement │ _<span class="pl-k">.</span>per_fold │
├────────────────────────┼───────────────┼────────────┤
│ misclassification_rate │ <span class="pl-c1">0.0467</span>        │ [<span class="pl-c1">0.0467</span>]   │
└────────────────────────┴───────────────┴────────────┘</pre></div>
<h3 dir="auto"><a id="user-content-adding-new-models-to-mljflux-advanced" class="anchor" aria-hidden="true" href="#adding-new-models-to-mljflux-advanced"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Adding new models to MLJFlux (advanced)</h3>
<p dir="auto">This section is mainly for MLJFlux developers. It assumes familiarity
with the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/" rel="nofollow">MLJ model
API</a></p>
<p dir="auto">If one subtypes a new model type as either
<code>MLJFlux.MLJFluxProbabilistic</code> or <code>MLJFlux.MLJFluxDeterministic</code>, then
instead of defining new methods for <code>MLJModelInterface.fit</code> and
<code>MLJModelInterface.update</code> one can make use of fallbacks by
implementing the lower level methods <code>shape</code>, <code>build</code>, and
<code>fitresult</code>. See the <a href="/src/classifier.jl">classifier source code</a> for
an example.</p>
<p dir="auto">One still needs to implement a new <code>predict</code> method.</p>
</article></div>