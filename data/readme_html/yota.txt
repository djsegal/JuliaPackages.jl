<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-yötä" class="anchor" aria-hidden="true" href="#yötä"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Yötä</h1>
<p><a href="https://travis-ci.org/dfdx/Yota.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6bf5d6d0ec3428e4d423ee73a869050a232e2860a3ba9cc2166e21d6fcfef521/68747470733a2f2f7472617669732d63692e6f72672f646664782f596f74612e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/dfdx/Yota.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>Reverse-mode automatic differentiation for static and dynamic graphs.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="mutable struct Linear{T}
    W::AbstractArray{T,2}
    b::AbstractArray{T}
end

forward(m::Linear, X) = m.W * X

loss(m::Linear, X) = sum(forward(m, X))

m = Linear(rand(3,4), rand(3))
X = rand(4,5)

val, g = grad(loss, m, X)
"><pre><span class="pl-k">mutable struct</span> Linear{T}
    W<span class="pl-k">::</span><span class="pl-c1">AbstractArray{T,2}</span>
    b<span class="pl-k">::</span><span class="pl-c1">AbstractArray{T}</span>
<span class="pl-k">end</span>

<span class="pl-en">forward</span>(m<span class="pl-k">::</span><span class="pl-c1">Linear</span>, X) <span class="pl-k">=</span> m<span class="pl-k">.</span>W <span class="pl-k">*</span> X

<span class="pl-en">loss</span>(m<span class="pl-k">::</span><span class="pl-c1">Linear</span>, X) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">forward</span>(m, X))

m <span class="pl-k">=</span> <span class="pl-c1">Linear</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">4</span>), <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>))
X <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4</span>,<span class="pl-c1">5</span>)

val, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(loss, m, X)</pre></div>
<p><code>g</code> is an object of type <code>GradientResult</code> holding gradients w.r.t. input variables. For scalars
and tensors it returns gradient value, for structs it returns dictionary of
(field path → gradient) pairs:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; g[1]
Dict{Tuple{Symbol},Array{Float64,2}} with 1 entry:
  (:W,) =&gt; [3.38128 2.97142 2.39706 1.55525; 3.38128 2.97142 2.39706 1.55525; 3.38128 2.97142 2.39706 1.55525]   # gradient w.r.t. m.W

julia&gt; g[2]  # gradient w.r.t. X
4×5 Array{Float64,2}:
 0.910691  0.910691  0.910691  0.910691  0.910691
 1.64994   1.64994   1.64994   1.64994   1.64994
 1.81215   1.81215   1.81215   1.81215   1.81215
 2.31594   2.31594   2.31594   2.31594   2.31594
"><pre>julia<span class="pl-k">&gt;</span> g[<span class="pl-c1">1</span>]
Dict{Tuple{Symbol},Array{Float64,<span class="pl-c1">2</span>}} with <span class="pl-c1">1</span> entry<span class="pl-k">:</span>
  (<span class="pl-c1">:W</span>,) <span class="pl-k">=&gt;</span> [<span class="pl-c1">3.38128</span> <span class="pl-c1">2.97142</span> <span class="pl-c1">2.39706</span> <span class="pl-c1">1.55525</span>; <span class="pl-c1">3.38128</span> <span class="pl-c1">2.97142</span> <span class="pl-c1">2.39706</span> <span class="pl-c1">1.55525</span>; <span class="pl-c1">3.38128</span> <span class="pl-c1">2.97142</span> <span class="pl-c1">2.39706</span> <span class="pl-c1">1.55525</span>]   <span class="pl-c"><span class="pl-c">#</span> gradient w.r.t. m.W</span>

julia<span class="pl-k">&gt;</span> g[<span class="pl-c1">2</span>]  <span class="pl-c"><span class="pl-c">#</span> gradient w.r.t. X</span>
<span class="pl-c1">4</span><span class="pl-k">×</span><span class="pl-c1">5</span> Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>
 <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>
 <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>
 <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span></pre></div>
<p><code>GradientResult</code> can be used in conjunction with <code>update!()</code> function to modify tensors and fields of (mutable) structs. To continue out previous example:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="for i=1:100
    val, g = grad(loss, m, X)
    println(&quot;Loss value in $(i)th epoch: $val&quot;)
    update!(m, g[1], (x, gx) -&gt; x .- 0.01gx)
end
"><pre><span class="pl-k">for</span> i<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
    val, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(loss, m, X)
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Loss value in <span class="pl-v">$(i)</span>th epoch: <span class="pl-v">$val</span><span class="pl-pds">"</span></span>)
    <span class="pl-c1">update!</span>(m, g[<span class="pl-c1">1</span>], (x, gx) <span class="pl-k">-&gt;</span> x <span class="pl-k">.-</span> <span class="pl-c1">0.01</span>gx)
<span class="pl-k">end</span></pre></div>
<p>(Note that our simplified loss function doesn't actually represent an error to be minimized, so loss value quickly goes below zero. For more realistic and much more complex examples see <a href="https://github.com/dfdx/Yota.jl/blob/master/examples/vae.jl">vae.jl</a>.)</p>
<h2><a id="user-content-custom-derivatives" class="anchor" aria-hidden="true" href="#custom-derivatives"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Custom derivatives</h2>
<p>You can add custom derivatives using <code>@diffrule</code> macro (see list of allowed variable names below).</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="logistic(x) = 1 / (1 + exp(-x))
# for an expression like `y = logistic(x)` where x is a Number
# gradient w.r.t. x
# is `(logistic(x) * (1 - logistic(x)) * dy)` where &quot;dy&quot; stands for derivative &quot;dL/dy&quot;
@diffrule logistic(x::Number) x (logistic(x) * (1 - logistic(x)) * dy)

L(x) = sum(logistic.(x))
val, g = grad(L, rand(5))
"><pre><span class="pl-en">logistic</span>(x) <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-k">/</span> (<span class="pl-c1">1</span> <span class="pl-k">+</span> <span class="pl-c1">exp</span>(<span class="pl-k">-</span>x))
<span class="pl-c"><span class="pl-c">#</span> for an expression like `y = logistic(x)` where x is a Number</span>
<span class="pl-c"><span class="pl-c">#</span> gradient w.r.t. x</span>
<span class="pl-c"><span class="pl-c">#</span> is `(logistic(x) * (1 - logistic(x)) * dy)` where "dy" stands for derivative "dL/dy"</span>
<span class="pl-c1">@diffrule</span> <span class="pl-c1">logistic</span>(x<span class="pl-k">::</span><span class="pl-c1">Number</span>) x (<span class="pl-c1">logistic</span>(x) <span class="pl-k">*</span> (<span class="pl-c1">1</span> <span class="pl-k">-</span> <span class="pl-c1">logistic</span>(x)) <span class="pl-k">*</span> dy)

<span class="pl-en">L</span>(x) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">logistic</span>.(x))
val, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(L, <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>))</pre></div>
<p>For functions accepting keyword arguments use <code>@diffrule_kw</code> instead:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="import NNlib: conv, ∇conv_data, ∇conv_filter

@diffrule_kw conv(x, w) x ∇conv_data(dy, w)
@diffrule_kw conv(x, w) w ∇conv_filter(dy, x)
"><pre><span class="pl-k">import</span> NNlib<span class="pl-k">:</span> conv, ∇conv_data, ∇conv_filter

<span class="pl-c1">@diffrule_kw</span> <span class="pl-c1">conv</span>(x, w) x <span class="pl-c1">∇conv_data</span>(dy, w)
<span class="pl-c1">@diffrule_kw</span> <span class="pl-c1">conv</span>(x, w) w <span class="pl-c1">∇conv_filter</span>(dy, x)</pre></div>
<p>During reverse pass Yota will generate call to derivative function with the same keyword arguments that were
passed to the original one. For example, if you have:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="conv(A, W; pad=1)
"><pre><span class="pl-c1">conv</span>(A, W; pad<span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>
<p>corresponding derivative will be:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="∇conv_data(dy, w; pad=1)
"><pre><span class="pl-c1">∇conv_data</span>(dy, w; pad<span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>
<p>There's also <code>@nodiff(call_pattern, variable)</code> macro which stops Yota from backpropagating through that variable.</p>
<h3><a id="user-content-allowed-variable-names" class="anchor" aria-hidden="true" href="#allowed-variable-names"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Allowed variable names</h3>
<p>To distinguish between variable names that can be matched to (a.k.a. placeholders) and fixed symbols (e.g. function names), <code>@diffrule</code> uses several rules:</p>
<ul>
<li><code>y</code> means return value of a primal expression, e.g. <code>y = f(x)</code></li>
<li><code>dy</code> means derivative of a loss function w.r.t. to <code>y</code></li>
<li><code>t</code>, <code>u</code>, <code>v</code>, <code>w</code>, <code>x</code>, as well as <code>i</code>, <code>j</code>, <code>k</code> (all listed in <code>Yota.DIFF_PHS</code>) are "placeholders" and can be used as names of variables, e.g. <code>@diffrule foo(u, v) u ∇foo(dy, u, v)</code></li>
<li>anything starting with <code>_</code> is also considered a placeholder, e.g. <code>@diffrule bar(u, _state) _state ∇bar(dy, u, _state)</code></li>
</ul>
<h2><a id="user-content-tracer-and-the-tape" class="anchor" aria-hidden="true" href="#tracer-and-the-tape"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tracer and the Tape</h2>
<p>Being a reverse-mode AD package, Yota works in 2 steps:</p>
<ol>
<li>Record all primitive operations onto a "tape".</li>
<li>Go back trough the tape, recording derivatives for each operation.</li>
</ol>
<p>"Tape" here is simply a list of operations. You can get the tape as a <code>.tape</code> field of <code>GradientResult</code> or construct it directly using <code>trace</code> function:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="import Yota: trace

val, tape = trace(L, rand(5))
print(tape)

# Tape
#   inp %1::Array{Float64,1}
#   const %2 = logistic::typeof(logistic)
#   %3 = broadcast(%2, %1)::Array{Float64,1}
#   %4 = sum(%3)::Float64
"><pre><span class="pl-k">import</span> Yota<span class="pl-k">:</span> trace

val, tape <span class="pl-k">=</span> <span class="pl-c1">trace</span>(L, <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>))
<span class="pl-c1">print</span>(tape)

<span class="pl-c"><span class="pl-c">#</span> Tape</span>
<span class="pl-c"><span class="pl-c">#</span>   inp %1::Array{Float64,1}</span>
<span class="pl-c"><span class="pl-c">#</span>   const %2 = logistic::typeof(logistic)</span>
<span class="pl-c"><span class="pl-c">#</span>   %3 = broadcast(%2, %1)::Array{Float64,1}</span>
<span class="pl-c"><span class="pl-c">#</span>   %4 = sum(%3)::Float64</span></pre></div>
<p><code>trace</code> uses <a href="https://github.com/FluxML/IRTools.jl">IRTools.jl</a> to collect function calls during execution. Functions are divided into 2 groups:</p>
<ul>
<li>primitive, which are recorded to the tape;</li>
<li>non-primitive, which are traced-through down to primitive ones.</li>
</ul>
<p>By default, set of primitive functions is defined in <code>Yota.PRIMITIVES</code> and includes such beasts as <code>*</code>, <code>broadcast</code>, <code>getproperty</code> as well as all functions for which <code>@diffrule</code> is defined. You can also specify custom primitives using <code>primitive=Set([...])</code> keyword to <code>trace()</code>.</p>
<p>Also note that <code>broadcast</code>'s first argument is always considered a primitive and recorded to the tape as is, so backpropagation will only work if there's a <code>@diffrule</code> for it.</p>
<p>Tape can also be executed and compiled:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using BenchmarkTools
import Yota: play!, compile!

x = rand(5)

@btime play!(tape, x)
# 3.526 μs (13 allocations: 640 bytes)

compile!(tape)
@btime play!(tape, x)
# 492.063 ns (2 allocations: 144 bytes)
"><pre><span class="pl-k">using</span> BenchmarkTools
<span class="pl-k">import</span> Yota<span class="pl-k">:</span> play!, compile!

x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>)

<span class="pl-c1">@btime</span> <span class="pl-c1">play!</span>(tape, x)
<span class="pl-c"><span class="pl-c">#</span> 3.526 μs (13 allocations: 640 bytes)</span>

<span class="pl-c1">compile!</span>(tape)
<span class="pl-c1">@btime</span> <span class="pl-c1">play!</span>(tape, x)
<span class="pl-c"><span class="pl-c">#</span> 492.063 ns (2 allocations: 144 bytes)</span></pre></div>
<h2><a id="user-content-cuda-support" class="anchor" aria-hidden="true" href="#cuda-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CUDA support</h2>
<p><code>CuArray</code> is fully supported. If you encounter an issue with CUDA arrays which you don't have with ordinary arrays, please file a bug.</p>
<h2><a id="user-content-static-vs-dynamic-experimental" class="anchor" aria-hidden="true" href="#static-vs-dynamic-experimental"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Static vs. dynamic (experimental)</h2>
<p>Tracer records operations as they are executed the first time with given arguments. For example, for a loop like this:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function iterative(x, n)
    for i=1:n
        x = 2 .* x
    end
    return sum(x)
end
"><pre><span class="pl-k">function</span> <span class="pl-en">iterative</span>(x, n)
    <span class="pl-k">for</span> i<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span>n
        x <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-k">.*</span> x
    <span class="pl-k">end</span>
    <span class="pl-k">return</span> <span class="pl-c1">sum</span>(x)
<span class="pl-k">end</span></pre></div>
<p>exactly <code>n</code> iterations will be recorded to the tape and replaying tape with any other values of <code>n</code> will make no effect. This also applies to a standard <code>grad()</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="x = rand(4)
_, g = grad(iterative, x, 1)   # g[1] == [2.0, 2.0, 2.0, 2.0]
_, g = grad(iterative, x, 2)   # g[1] == [2.0, 2.0, 2.0, 2.0]
_, g = grad(iterative, x, 3)   # g[1] == [2.0, 2.0, 2.0, 2.0]
"><pre>x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4</span>)
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">1</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">2</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">3</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span></pre></div>
<p>Nevertheless, Yota provides pseudo-dynamic capabilities by caching gradient results for all ever generated tapes. This doesn't eliminate cost of re-tracing, but avoids repeated backpropagation and tape optimization. You can tell <code>grad()</code> to use dynamic caching using <code>dynamic=true</code> keyword argument:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="x = rand(4)
_, g = grad(iterative, x, 1; dynamic=true)   # g[1] == [2.0, 2.0, 2.0, 2.0]
_, g = grad(iterative, x, 2; dynamic=true)   # g[1] == [4.0, 4.0, 4.0, 4.0]
_, g = grad(iterative, x, 3; dynamic=true)   # g[1] == [8.0, 8.0, 8.0, 8.0]
"><pre>x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4</span>)
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">1</span>; dynamic<span class="pl-k">=</span><span class="pl-c1">true</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">2</span>; dynamic<span class="pl-k">=</span><span class="pl-c1">true</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [4.0, 4.0, 4.0, 4.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">3</span>; dynamic<span class="pl-k">=</span><span class="pl-c1">true</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [8.0, 8.0, 8.0, 8.0]</span></pre></div>
<p>Note that this feature is experimental and may be removed in future versions.</p>
</article></div>