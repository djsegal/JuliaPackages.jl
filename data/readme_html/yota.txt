<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-yötä" class="anchor" aria-hidden="true" href="#yötä"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Yötä</h1>
<p><a href="https://travis-ci.org/dfdx/Yota.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/b9ba403617d1eefa45ee72ab5438a47b37ee7e24/68747470733a2f2f7472617669732d63692e6f72672f646664782f596f74612e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/dfdx/Yota.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>Reverse-mode automatic differentiation for static and dynamic graphs.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">mutable struct</span> Linear{T}
    W<span class="pl-k">::</span><span class="pl-c1">AbstractArray{T,2}</span>
    b<span class="pl-k">::</span><span class="pl-c1">AbstractArray{T}</span>
<span class="pl-k">end</span>

<span class="pl-en">forward</span>(m<span class="pl-k">::</span><span class="pl-c1">Linear</span>, X) <span class="pl-k">=</span> m<span class="pl-k">.</span>W <span class="pl-k">*</span> X

<span class="pl-en">loss</span>(m<span class="pl-k">::</span><span class="pl-c1">Linear</span>, X) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">forward</span>(m, X))

m <span class="pl-k">=</span> <span class="pl-c1">Linear</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">4</span>), <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>))
X <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4</span>,<span class="pl-c1">5</span>)

val, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(loss, m, X)</pre></div>
<p><code>g</code> is an object of type <code>GradientResult</code> holding gradients w.r.t. input variables. For scalars
and tensors it returns gradient value, for structs it returns dictionary of
(field path → gradient) pairs:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> g[<span class="pl-c1">1</span>]
Dict{Tuple{Symbol},Array{Float64,<span class="pl-c1">2</span>}} with <span class="pl-c1">1</span> entry<span class="pl-k">:</span>
  (<span class="pl-c1">:W</span>,) <span class="pl-k">=&gt;</span> [<span class="pl-c1">3.38128</span> <span class="pl-c1">2.97142</span> <span class="pl-c1">2.39706</span> <span class="pl-c1">1.55525</span>; <span class="pl-c1">3.38128</span> <span class="pl-c1">2.97142</span> <span class="pl-c1">2.39706</span> <span class="pl-c1">1.55525</span>; <span class="pl-c1">3.38128</span> <span class="pl-c1">2.97142</span> <span class="pl-c1">2.39706</span> <span class="pl-c1">1.55525</span>]   <span class="pl-c"><span class="pl-c">#</span> gradient w.r.t. m.W</span>

julia<span class="pl-k">&gt;</span> g[<span class="pl-c1">2</span>]  <span class="pl-c"><span class="pl-c">#</span> gradient w.r.t. X</span>
<span class="pl-c1">4</span><span class="pl-k">×</span><span class="pl-c1">5</span> Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>  <span class="pl-c1">0.910691</span>
 <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>   <span class="pl-c1">1.64994</span>
 <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>   <span class="pl-c1">1.81215</span>
 <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span>   <span class="pl-c1">2.31594</span></pre></div>
<p><code>GradientResult</code> can be used in conjunction with <code>update!()</code> function to modify tensors and fields of (mutable) structs. To continue out previous example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">for</span> i<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
    val, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(loss, m, X)
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Loss value in <span class="pl-v">$(i)</span>th epoch: <span class="pl-v">$val</span><span class="pl-pds">"</span></span>)
    <span class="pl-c1">update!</span>(m, g[<span class="pl-c1">1</span>], (x, gx) <span class="pl-k">-&gt;</span> x <span class="pl-k">.-</span> <span class="pl-c1">0.01</span>gx)
<span class="pl-k">end</span></pre></div>
<p>(Note that our simplified loss function doesn't actually represent an error to be minimized, so loss value quickly goes below zero. For more realistic and much more complex examples see <a href="https://github.com/dfdx/Yota.jl/blob/master/examples/vae.jl">vae.jl</a>.)</p>
<h2><a id="user-content-custom-derivatives" class="anchor" aria-hidden="true" href="#custom-derivatives"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Custom derivatives</h2>
<p>You can add custom derivatives using <code>@diffrule</code> macro.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">logistic</span>(x) <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-k">/</span> (<span class="pl-c1">1</span> <span class="pl-k">+</span> <span class="pl-c1">exp</span>(<span class="pl-k">-</span>x))
<span class="pl-c"><span class="pl-c">#</span> for an expression like `y = logistic(x)` where x is a Number</span>
<span class="pl-c"><span class="pl-c">#</span> gradient w.r.t. x</span>
<span class="pl-c"><span class="pl-c">#</span> is `(logistic(x) * (1 - logistic(x)) * dy)` where "dy" stands for derivative "dL/dy"</span>
<span class="pl-c1">@diffrule</span> <span class="pl-c1">logistic</span>(x<span class="pl-k">::</span><span class="pl-c1">Number</span>) x (<span class="pl-c1">logistic</span>(x) <span class="pl-k">*</span> (<span class="pl-c1">1</span> <span class="pl-k">-</span> <span class="pl-c1">logistic</span>(x)) <span class="pl-k">*</span> dy)

<span class="pl-en">L</span>(x) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">logistic</span>.(x))
val, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(L, <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>))</pre></div>
<p>For functions accepting keyword arguments use <code>@diffrule_kw</code> instead:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> NNlib<span class="pl-k">:</span> conv, ∇conv_data, ∇conv_filter

<span class="pl-c1">@diffrule_kw</span> <span class="pl-c1">conv</span>(x, w) x <span class="pl-c1">∇conv_data</span>(dy, w)
<span class="pl-c1">@diffrule_kw</span> <span class="pl-c1">conv</span>(x, w) w <span class="pl-c1">∇conv_filter</span>(dy, x)</pre></div>
<p>During reverse pass Yota will generate call to derivative function with the same keyword arguments that were
passed to the original one. For example, if you have:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">conv</span>(A, W; pad<span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>
<p>corresponding derivative will be:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">∇conv_data</span>(dy, w; pad<span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>
<p>There's also <code>@nodiff(call_pattern, variable)</code> macro which stops Yota from backpropagating through that variable.</p>
<h2><a id="user-content-tracer-and-the-tape" class="anchor" aria-hidden="true" href="#tracer-and-the-tape"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tracer and the Tape</h2>
<p>Being a reverse-mode AD package, Yota works in 2 steps:</p>
<ol>
<li>Record all primitive operations onto a "tape".</li>
<li>Go back trough the tape, recording derivatives for each operation.</li>
</ol>
<p>"Tape" here is simply a list of operations. You can get the tape as a <code>.tape</code> field of <code>GradientResult</code> or construct it directly using <code>trace</code> function:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> Yota<span class="pl-k">:</span> trace

val, tape <span class="pl-k">=</span> <span class="pl-c1">trace</span>(L, <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>))
<span class="pl-c1">print</span>(tape)

<span class="pl-c"><span class="pl-c">#</span> Tape</span>
<span class="pl-c"><span class="pl-c">#</span>   inp %1::Array{Float64,1}</span>
<span class="pl-c"><span class="pl-c">#</span>   const %2 = logistic::typeof(logistic)</span>
<span class="pl-c"><span class="pl-c">#</span>   %3 = broadcast(%2, %1)::Array{Float64,1}</span>
<span class="pl-c"><span class="pl-c">#</span>   %4 = sum(%3)::Float64</span></pre></div>
<p><code>trace</code> uses <a href="https://github.com/jrevels/Cassette.jl">Cassette.jl</a> to collect function calls during execution. Functions are divided into 2 groups:</p>
<ul>
<li>primitive, which are recorded to the tape;</li>
<li>non-primitive, which are traced-through down to primitive ones.</li>
</ul>
<p>By default, set of primitive functions is defined in <code>Yota.PRIMITIVES</code> and includes such beasts as <code>*</code>, <code>broadcast</code>, <code>getproperty</code> as well as all functions for which <code>@diffrule</code> is defined. You can also specify custom primitives using <code>primitive=Set([...])</code> keyword to <code>trace()</code>.</p>
<p>Also note that <code>broadcast</code>'s first argument is always considered a primitive and recorded to the tape as is, so backpropagation will only work if there's a <code>@diffrule</code> for it.</p>
<p>Tape can also be executed and compiled:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> BenchmarkTools
<span class="pl-k">import</span> Yota<span class="pl-k">:</span> play!, compile!

x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>)

<span class="pl-c1">@btime</span> <span class="pl-c1">play!</span>(tape, x)
<span class="pl-c"><span class="pl-c">#</span> 3.526 μs (13 allocations: 640 bytes)</span>

<span class="pl-c1">compile!</span>(tape)
<span class="pl-c1">@btime</span> <span class="pl-c1">play!</span>(tape, x)
<span class="pl-c"><span class="pl-c">#</span> 492.063 ns (2 allocations: 144 bytes)</span></pre></div>
<p>Note that <code>trace()</code> is an alias to <code>ctrace()</code> - Cassette-based tracer. There's also an alternative implementation with identical interface and capabilities, but based on <a href="https://github.com/JuliaDebug/JuliaInterpreter.jl">JuliaInterpreter.jl</a>. This implementation is available by name <code>itrace()</code> and is currently supported on Julia 1.0-1.3.</p>
<h2><a id="user-content-cuarrays-support-experimental" class="anchor" aria-hidden="true" href="#cuarrays-support-experimental"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CuArrays support (experimental)</h2>
<p>Yota should work with CuArrays out of the box, although integration is not well tested yet.</p>
<p>In addition, you can use function <code>to_cuda()</code> to transform arrays and structs into CUDA-compatible, see <a href="https://github.com/dfdx/Yota.jl/blob/master/examples/cu_vae.jl">cu_vae.jl</a> for an example. Note that this API is highly experimental and will most likely change to something more device-agnostic.</p>
<h2><a id="user-content-static-vs-dynamic-experimental" class="anchor" aria-hidden="true" href="#static-vs-dynamic-experimental"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Static vs. dynamic (experimental)</h2>
<p>Tracer records operations as they are executed the first time with given arguments. For example, for a loop like this:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">iterative</span>(x, n)
    <span class="pl-k">for</span> i<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span>n
        x <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-k">.*</span> x
    <span class="pl-k">end</span>
    <span class="pl-k">return</span> <span class="pl-c1">sum</span>(x)
<span class="pl-k">end</span></pre></div>
<p>exactly <code>n</code> iterations will be recorded to the tape and replaying tape with any other values of <code>n</code> will make no effect. This also applies to a standard <code>grad()</code>:</p>
<div class="highlight highlight-source-julia"><pre>x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4</span>)
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">1</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">2</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">3</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span></pre></div>
<p>Nevertheless, Yota provides pseudo-dynamic capabilities by caching gradient results for all ever generated tapes. This doesn't eliminate cost of re-tracing, but avoids repeated backpropagation and tape optimization. You can tell <code>grad()</code> to use dynamic caching using <code>dynamic=true</code> keyword argument:</p>
<div class="highlight highlight-source-julia"><pre>x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">4</span>)
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">1</span>; dynamic<span class="pl-k">=</span><span class="pl-c1">true</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [2.0, 2.0, 2.0, 2.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">2</span>; dynamic<span class="pl-k">=</span><span class="pl-c1">true</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [4.0, 4.0, 4.0, 4.0]</span>
_, g <span class="pl-k">=</span> <span class="pl-c1">grad</span>(iterative, x, <span class="pl-c1">3</span>; dynamic<span class="pl-k">=</span><span class="pl-c1">true</span>)   <span class="pl-c"><span class="pl-c">#</span> g[1] == [8.0, 8.0, 8.0, 8.0]</span></pre></div>
<p>Note that this feature is experimental and may be removed in future versions.</p>
<h2><a id="user-content-simple-grad-experimental" class="anchor" aria-hidden="true" href="#simple-grad-experimental"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Simple grad (experimental)</h2>
<p><code>grad()</code> uses a number of optimizations and buffering to make gradient calculation as fast as possible. Sometimes, however, all we need is a simple gradient function that accepts all the same argument as the original one and returns gradients of its arguments, without attached buffers, additional data structures or whatever. You can create such a function using <code>simplegrad()</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> Yota<span class="pl-k">:</span> simplegrad

<span class="pl-en">loss</span>(W<span class="pl-k">::</span><span class="pl-c1">AbstractMatrix</span>, b<span class="pl-k">::</span><span class="pl-c1">AbstractVector</span>, x<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(W <span class="pl-k">*</span> x <span class="pl-k">.+</span> b)

W, b, x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">128</span>, <span class="pl-c1">784</span>), <span class="pl-c1">rand</span>(<span class="pl-c1">128</span>), <span class="pl-c1">rand</span>(<span class="pl-c1">784</span>, <span class="pl-c1">100</span>)
∇loss <span class="pl-k">=</span> <span class="pl-c1">simplegrad</span>(loss, W, b, x)   <span class="pl-c"><span class="pl-c">#</span> note: ∇loss is a new _function_, world age concerns apply</span>

val, dW, db, dx <span class="pl-k">=</span> <span class="pl-c1">∇loss</span>(W, b, x)

<span class="pl-c1">@code_lowered</span> <span class="pl-c1">∇loss</span>(W, b, x)
<span class="pl-c"><span class="pl-c">#</span> CodeInfo(</span>
<span class="pl-c"><span class="pl-c">#</span> 1 ─       %4 = (*)(%1, %3)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %5 = +</span>
<span class="pl-c"><span class="pl-c">#</span> │         %6 = (broadcast)(%5, %4, %2)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %7 = (sum)(%6)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %8 = 1.0</span>
<span class="pl-c"><span class="pl-c">#</span> │         %9 = (Yota.sum_grad)(%6, %8)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %10 = (Yota.unbroadcast)(%4, %9)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %11 = (Yota.unbroadcast)(%2, %9)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %12 = (transpose)(%3)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %13 = (*)(%10, %12)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %14 = (transpose)(%1)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %15 = (*)(%14, %10)</span>
<span class="pl-c"><span class="pl-c">#</span> │         %13 = (Core.tuple)(%7, %13, %11, %15)</span>
<span class="pl-c"><span class="pl-c">#</span> └──       return %13</span>
<span class="pl-c"><span class="pl-c">#</span> )</span></pre></div>
</article></div>