<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-cudajl" class="anchor" aria-hidden="true" href="#cudajl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CUDA.jl</h1>
<p><em>CUDA programming in Julia</em></p>
<table>
<thead>
<tr>
<th align="center"><strong>Documentation</strong></th>
<th align="center"><strong>Build Status</strong></th>
<th align="center"><strong>Performance</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://juliagpu.github.io/CUDA.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a> <a href="https://juliagpu.github.io/CUDA.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://buildkite.com/julialang/cuda-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/634ffbeacdf6ad332503cb119ecf4a3bebd6c6c50883b11cdf393c24c2dcdda3/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f32376161656233353261393432303239376564326433306362303535616333383361333939656138663132313539393931322e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://badge.buildkite.com/27aaeb352a9420297ed2d30cb055ac383a399ea8f121599912.svg?branch=master" style="max-width:100%;"></a> <a href="https://codecov.io/gh/JuliaGPU/CUDA.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ed150790794ae5b2d4680e9fdba1b2802703c79d7f98d0d06985d3b1ca16405a/68747470733a2f2f636f6465636f762e696f2f67682f4a756c69614750552f435544412e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/JuliaGPU/CUDA.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://speed.juliagpu.org/changes/?exe=6&amp;env=1&amp;tre=50" rel="nofollow"><img src="https://camo.githubusercontent.com/3f484cc8968731e307d443170784d0f4abd1c67fccd07e15d8e6ab811f845561/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f62656e63686d61726b732d5472656e642d696e666f726d6174696f6e616c" alt="" data-canonical-src="https://img.shields.io/badge/benchmarks-Trend-informational" style="max-width:100%;"></a> <a href="https://speed.juliagpu.org/timeline/#/?exe=6&amp;env=1&amp;base=none&amp;ben=grid&amp;revs=50" rel="nofollow"><img src="https://camo.githubusercontent.com/e5c0488cc74a8ddbe068bfc2442a44b60614595f051c1c3b5a90e8eef9aefdf1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f62656e63686d61726b732d43686172742d696e666f726d6174696f6e616c" alt="" data-canonical-src="https://img.shields.io/badge/benchmarks-Chart-informational" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>The CUDA.jl package is the main programming interface for working with NVIDIA CUDA GPUs
using Julia. It features a user-friendly array abstraction, a compiler for writing CUDA
kernels in Julia, and wrappers for various CUDA libraries.</p>
<h2><a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Requirements</h2>
<p>The latest development version of CUDA.jl requires <strong>Julia 1.6</strong> or higher. If you are using
an older version of Julia, you need to use a released version of CUDA.jl. This will happen
automatically when you install the package using Julia's package manager.</p>
<p>CUDA.jl currently also requires a CUDA-capable GPU with <strong>compute capability 5.0</strong> (Maxwell)
or higher, and an accompanying NVIDIA driver with support for <strong>CUDA 10.1</strong> or newer. These
requirements are not enforced by the Julia package manager when installing CUDA.jl.
Depending on your system and GPU, you may need to install an older version of the package.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick start</h2>
<p>Before all, make sure you have a recent NVIDIA driver. On Windows, make sure you have the
<a href="https://aka.ms/vs/16/release/vc_redist.x64.exe" rel="nofollow">Visual C++ redistributable</a> installed.
You do not need to install the CUDA Toolkit.</p>
<p>CUDA.jl can be installed with the Julia package manager. From the Julia REPL, type <code>]</code> to
enter the Pkg REPL mode and run:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="pkg&gt; add CUDA
"><pre><code>pkg&gt; add CUDA
</code></pre></div>
<p>Or, equivalently, via the <code>Pkg</code> API:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; import Pkg; Pkg.add(&quot;CUDA&quot;)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">import</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>CUDA<span class="pl-pds">"</span></span>)</pre></div>
<p>For an overview of the CUDA toolchain in use, you can run the following command after
importing the package:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using CUDA

julia&gt; CUDA.versioninfo()
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> CUDA

julia<span class="pl-k">&gt;</span> CUDA<span class="pl-k">.</span><span class="pl-c1">versioninfo</span>()</pre></div>
<p>This may take a while, as it will precompile the package and download a suitable version of
the CUDA toolkit. If you prefer to use your own (not recommended), set the
<code>JULIA_CUDA_USE_BINARYBUILDER</code> environment variable to <code>false</code> before importing the package.</p>
<p>If your GPU is not fully supported, the above command (or any other command that initializes
the toolkit) will issue a warning. Your devices' compute capability will be listed as part of
the <code>versioninfo()</code> output, but you can always query it explicitly:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; [CUDA.capability(dev) for dev in CUDA.devices()]
1-element Vector{VersionNumber}:
 v&quot;5.0.0&quot;
"><pre>julia<span class="pl-k">&gt;</span> [CUDA<span class="pl-k">.</span><span class="pl-c1">capability</span>(dev) <span class="pl-k">for</span> dev <span class="pl-k">in</span> CUDA<span class="pl-k">.</span><span class="pl-c1">devices</span>()]
<span class="pl-c1">1</span><span class="pl-k">-</span>element Vector{VersionNumber}<span class="pl-k">:</span>
 <span class="pl-s"><span class="pl-pds"><span class="pl-c1">v</span>"</span>5.0.0<span class="pl-pds">"</span></span></pre></div>
<p>For more usage instructions and other information, please refer to <a href="https://juliagpu.github.io/CUDA.jl/stable/" rel="nofollow">the
documentation</a>.</p>
<h2><a id="user-content-supporting-and-citing" class="anchor" aria-hidden="true" href="#supporting-and-citing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supporting and Citing</h2>
<p>Much of the software in this ecosystem was developed as part of academic research. If you
would like to help support it, please star the repository as such metrics may help us secure
funding in the future. If you use our software as part of your research, teaching, or other
activities, we would be grateful if you could cite our work. The
<a href="https://github.com/JuliaGPU/CUDA.jl/blob/master/CITATION.bib">CITATION.bib</a> file in the
root of this repository lists the relevant papers.</p>
<h2><a id="user-content-project-status" class="anchor" aria-hidden="true" href="#project-status"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Project Status</h2>
<p>The package is tested against, and being developed for, Julia 1.3 and above. Main
development and testing happens on Linux, but the package is expected to work on macOS and
Windows as well.</p>
<h2><a id="user-content-questions-and-contributions" class="anchor" aria-hidden="true" href="#questions-and-contributions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Questions and Contributions</h2>
<p>Usage questions can be posted on the <a href="https://discourse.julialang.org/c/domain/gpu" rel="nofollow">Julia Discourse
forum</a> under the GPU domain and/or in the #gpu
channel of the <a href="https://julialang.org/community/" rel="nofollow">Julia Slack</a>.</p>
<p>Contributions are very welcome, as are feature requests and suggestions. Please open an
<a href="https://github.com/JuliaGPU/CUDA.jl/issues">issue</a> if you encounter any problems.</p>
</article></div>