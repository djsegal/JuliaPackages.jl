<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-bridgesamplingjl" class="anchor" aria-hidden="true" href="#bridgesamplingjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BridgeSampling.jl</h1>
<p dir="auto"><a href="https://github.com/sqwayer/BridgeSampling.jl/actions"><img src="https://github.com/sqwayer/BridgeSampling.jl/workflows/CI/badge.svg" alt="Build status (Github Actions)" style="max-width: 100%;"></a>
<a href="http://codecov.io/github/sqwayer/BridgeSampling.jl?branch=main" rel="nofollow"><img src="https://camo.githubusercontent.com/456d48daaaba39b92ba1a18874638638c0623de43b034fe7998b186dacde46da/687474703a2f2f636f6465636f762e696f2f6769746875622f737177617965722f42726964676553616d706c696e672e6a6c2f636f7665726167652e7376673f6272616e63683d6d61696e" alt="codecov.io" data-canonical-src="http://codecov.io/github/sqwayer/BridgeSampling.jl/coverage.svg?branch=main" style="max-width: 100%;"></a>
<a href="https://sqwayer.github.io/BridgeSampling.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://sqwayer.github.io/BridgeSampling.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a></p>
<p dir="auto">[WIP] A Julia package for computing Normalizing Constants (aka Marginal Likelihoods), Bayes Factors and Posterior Model Probabilities.</p>
<p dir="auto">This is a Julia version of the <a href="https://github.com/quentingronau/bridgesampling">R package</a> <code>bridgesampling</code>, rewrote from scratch.</p>
<p dir="auto">Quentin F. Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S. Leslie, Jonathan J. Forster, Eric-Jan Wagenmakers, Helen Steingroever, A tutorial on bridge sampling, Journal of Mathematical Psychology, Volume 81, 2017, Pages 80-97, ISSN 0022-2496, <a href="https://doi.org/10.1016/j.jmp.2017.09.005" rel="nofollow">https://doi.org/10.1016/j.jmp.2017.09.005</a>.</p>
<h2 dir="auto"><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basic usage</h2>
<p dir="auto">The function <code>bridgesampling</code> estimates the log marginal likelihood with bridge sampling, given a matrix of posterior samples and an unnormalized log posterior function.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using BridgeSampling
 # Bivariate standard normal distribution
samples = rand(MvNormal(ones(2)), 10_000)
log_posterior(x) = -0.5 * x' * x
logML = bridgesampling(samples, log_posterior, [-Inf, -Inf], [Inf, Inf])"><pre><span class="pl-k">using</span> BridgeSampling
 <span class="pl-c"><span class="pl-c">#</span> Bivariate standard normal distribution</span>
samples <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">MvNormal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>)), <span class="pl-c1">10_000</span>)
<span class="pl-en">log_posterior</span>(x) <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">0.5</span> <span class="pl-k">*</span> x<span class="pl-k">'</span> <span class="pl-k">*</span> x
logML <span class="pl-k">=</span> <span class="pl-c1">bridgesampling</span>(samples, log_posterior, [<span class="pl-k">-</span><span class="pl-c1">Inf</span>, <span class="pl-k">-</span><span class="pl-c1">Inf</span>], [<span class="pl-c1">Inf</span>, <span class="pl-c1">Inf</span>])</pre></div>
<h2 dir="auto"><a id="user-content-integration-with-turing" class="anchor" aria-hidden="true" href="#integration-with-turing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Integration with Turing</h2>
<p dir="auto">The function can also be called with a <code>MCMCChain.Chains</code> object and a <code>DynamicPPL.Model</code> object and will automatically infer the log posterior function and the boundaries :</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="L = bridgesampling(chn, M)"><pre>L <span class="pl-k">=</span> <span class="pl-c1">bridgesampling</span>(chn, M)</pre></div>
<p dir="auto">However this method is numerically less precise than defining the log-posterior explicitly.</p>
<h2 dir="auto"><a id="user-content-error-estimation" class="anchor" aria-hidden="true" href="#error-estimation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Error estimation</h2>
<p dir="auto">The function <code>error_estimate</code> computes an approximation of the relative mean squared error of the estimate and the coefficient of variation.</p>
<h2 dir="auto"><a id="user-content-model-comparison" class="anchor" aria-hidden="true" href="#model-comparison"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model comparison</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Turing
## Hierarchical Normal Model
# Generate data
true_μ = 0.0
true_τ² = 0.5
true_σ² = 1.0
n = 20
true_θ = rand(Normal(true_μ, sqrt(true_τ²)), n)
y = rand.(Normal.(true_θ, sqrt(true_σ²)))

# DynamicPPL models
@model H0(Y) = begin
    τ² ~ InverseGamma(1,1)
    θ ~ filldist(Normal(0.0, sqrt(τ²)), length(Y))
    Y ~ arraydist(Normal.(θ, 1.0))
end

@model H1(Y) = begin
    μ ~ Normal()
    τ² ~ InverseGamma(1,1)
    θ ~ filldist(Normal(μ, sqrt(τ²)), length(Y))
    Y ~ arraydist(Normal.(θ, 1.0))
end

# Sample
M0 = H0(y)
M1 = H1(y)
chn0 = sample(M0, NUTS(), MCMCThreads(), 50_000, 3, burnin=2000)
chn1 = sample(M1, NUTS(), MCMCThreads(), 50_000, 3, burnin=2000)

# Estimate log marginal likelihood : directly with the DynamicPPL model 
L0 = bridgesampling(chn0, M0)
L1 = bridgesampling(chn1, M1)

# Bayes factor
BF10 = bayes_factor(L1, L0)

# Posterior model probabilities
posterior_probabilities([L0, L1])
prior_prob = [0.4, 0.6]
posterior_probabilities([L0, L1], prior_prob)"><pre><span class="pl-k">using</span> Turing
<span class="pl-c"><span class="pl-c">#</span># Hierarchical Normal Model</span>
<span class="pl-c"><span class="pl-c">#</span> Generate data</span>
true_μ <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
true_τ² <span class="pl-k">=</span> <span class="pl-c1">0.5</span>
true_σ² <span class="pl-k">=</span> <span class="pl-c1">1.0</span>
n <span class="pl-k">=</span> <span class="pl-c1">20</span>
true_θ <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">Normal</span>(true_μ, <span class="pl-c1">sqrt</span>(true_τ²)), n)
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>.(<span class="pl-c1">Normal</span>.(true_θ, <span class="pl-c1">sqrt</span>(true_σ²)))

<span class="pl-c"><span class="pl-c">#</span> DynamicPPL models</span>
<span class="pl-c1">@model</span> <span class="pl-en">H0</span>(Y) <span class="pl-k">=</span> <span class="pl-k">begin</span>
    τ² <span class="pl-k">~</span> <span class="pl-c1">InverseGamma</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)
    θ <span class="pl-k">~</span> <span class="pl-c1">filldist</span>(<span class="pl-c1">Normal</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">sqrt</span>(τ²)), <span class="pl-c1">length</span>(Y))
    Y <span class="pl-k">~</span> <span class="pl-c1">arraydist</span>(<span class="pl-c1">Normal</span>.(θ, <span class="pl-c1">1.0</span>))
<span class="pl-k">end</span>

<span class="pl-c1">@model</span> <span class="pl-en">H1</span>(Y) <span class="pl-k">=</span> <span class="pl-k">begin</span>
    μ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>()
    τ² <span class="pl-k">~</span> <span class="pl-c1">InverseGamma</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)
    θ <span class="pl-k">~</span> <span class="pl-c1">filldist</span>(<span class="pl-c1">Normal</span>(μ, <span class="pl-c1">sqrt</span>(τ²)), <span class="pl-c1">length</span>(Y))
    Y <span class="pl-k">~</span> <span class="pl-c1">arraydist</span>(<span class="pl-c1">Normal</span>.(θ, <span class="pl-c1">1.0</span>))
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Sample</span>
M0 <span class="pl-k">=</span> <span class="pl-c1">H0</span>(y)
M1 <span class="pl-k">=</span> <span class="pl-c1">H1</span>(y)
chn0 <span class="pl-k">=</span> <span class="pl-c1">sample</span>(M0, <span class="pl-c1">NUTS</span>(), <span class="pl-c1">MCMCThreads</span>(), <span class="pl-c1">50_000</span>, <span class="pl-c1">3</span>, burnin<span class="pl-k">=</span><span class="pl-c1">2000</span>)
chn1 <span class="pl-k">=</span> <span class="pl-c1">sample</span>(M1, <span class="pl-c1">NUTS</span>(), <span class="pl-c1">MCMCThreads</span>(), <span class="pl-c1">50_000</span>, <span class="pl-c1">3</span>, burnin<span class="pl-k">=</span><span class="pl-c1">2000</span>)

<span class="pl-c"><span class="pl-c">#</span> Estimate log marginal likelihood : directly with the DynamicPPL model </span>
L0 <span class="pl-k">=</span> <span class="pl-c1">bridgesampling</span>(chn0, M0)
L1 <span class="pl-k">=</span> <span class="pl-c1">bridgesampling</span>(chn1, M1)

<span class="pl-c"><span class="pl-c">#</span> Bayes factor</span>
BF10 <span class="pl-k">=</span> <span class="pl-c1">bayes_factor</span>(L1, L0)

<span class="pl-c"><span class="pl-c">#</span> Posterior model probabilities</span>
<span class="pl-c1">posterior_probabilities</span>([L0, L1])
prior_prob <span class="pl-k">=</span> [<span class="pl-c1">0.4</span>, <span class="pl-c1">0.6</span>]
<span class="pl-c1">posterior_probabilities</span>([L0, L1], prior_prob)</pre></div>
</article></div>