<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-tabulartdlearning" class="anchor" aria-hidden="true" href="#tabulartdlearning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TabularTDLearning</h1>
<p dir="auto"><a href="https://github.com/JuliaPOMDP/TabularTDLearning.jl/actions/workflows/CI.yml"><img src="https://github.com/JuliaPOMDP/TabularTDLearning.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaPOMDP/TabularTDLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9bc91a3d8cd94647877748f9bd85ae0b10a12aadfa9e5663defa0eaa49c06c4e/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961504f4d44502f546162756c617254444c6561726e696e672e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d76754a3641783553516a" alt="codecov" data-canonical-src="https://codecov.io/gh/JuliaPOMDP/TabularTDLearning.jl/branch/master/graph/badge.svg?token=vuJ6Ax5SQj" style="max-width: 100%;"></a></p>
<p dir="auto">This repository provides Julia implementations of the following Temporal-Difference reinforcement learning algorithms:</p>
<ul dir="auto">
<li>Q-Learning</li>
<li>SARSA</li>
<li>SARSA lambda</li>
</ul>
<p dir="auto">Note that these solvers are tabular, and will only work with MDPs that have discrete state and action spaces.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="Pkg.add(&quot;TabularTDLearning&quot;)"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>TabularTDLearning<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using POMDPs
using TabularTDLearning
using POMDPModels
using POMDPTools

mdp = SimpleGridWorld()
# use Q-Learning
exppolicy = EpsGreedyPolicy(mdp, 0.01)
solver = QLearningSolver(exploration_policy=exppolicy, learning_rate=0.1, n_episodes=5000, max_episode_length=50, eval_every=50, n_eval_traj=100)
policy = solve(solver, mdp)
# Use SARSA
solver = SARSASolver(exploration_policy=exppolicy, learning_rate=0.1, n_episodes=5000, max_episode_length=50, eval_every=50, n_eval_traj=100)
policy = solve(solver, mdp)
# Use SARSA lambda
solver = SARSALambdaSolver(exploration_policy=exppolicy, learning_rate=0.1, lambda=0.9, n_episodes=5000, max_episode_length=50, eval_every=50, n_eval_traj=100)
policy = solve(solver, mdp)
"><pre><span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> TabularTDLearning
<span class="pl-k">using</span> POMDPModels
<span class="pl-k">using</span> POMDPTools

mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>()
<span class="pl-c"><span class="pl-c">#</span> use Q-Learning</span>
exppolicy <span class="pl-k">=</span> <span class="pl-c1">EpsGreedyPolicy</span>(mdp, <span class="pl-c1">0.01</span>)
solver <span class="pl-k">=</span> <span class="pl-c1">QLearningSolver</span>(exploration_policy<span class="pl-k">=</span>exppolicy, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.1</span>, n_episodes<span class="pl-k">=</span><span class="pl-c1">5000</span>, max_episode_length<span class="pl-k">=</span><span class="pl-c1">50</span>, eval_every<span class="pl-k">=</span><span class="pl-c1">50</span>, n_eval_traj<span class="pl-k">=</span><span class="pl-c1">100</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)
<span class="pl-c"><span class="pl-c">#</span> Use SARSA</span>
solver <span class="pl-k">=</span> <span class="pl-c1">SARSASolver</span>(exploration_policy<span class="pl-k">=</span>exppolicy, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.1</span>, n_episodes<span class="pl-k">=</span><span class="pl-c1">5000</span>, max_episode_length<span class="pl-k">=</span><span class="pl-c1">50</span>, eval_every<span class="pl-k">=</span><span class="pl-c1">50</span>, n_eval_traj<span class="pl-k">=</span><span class="pl-c1">100</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)
<span class="pl-c"><span class="pl-c">#</span> Use SARSA lambda</span>
solver <span class="pl-k">=</span> <span class="pl-c1">SARSALambdaSolver</span>(exploration_policy<span class="pl-k">=</span>exppolicy, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.1</span>, lambda<span class="pl-k">=</span><span class="pl-c1">0.9</span>, n_episodes<span class="pl-k">=</span><span class="pl-c1">5000</span>, max_episode_length<span class="pl-k">=</span><span class="pl-c1">50</span>, eval_every<span class="pl-k">=</span><span class="pl-c1">50</span>, n_eval_traj<span class="pl-k">=</span><span class="pl-c1">100</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)
</pre></div>
</article></div>