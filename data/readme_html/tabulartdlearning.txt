<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-tabulartdlearning" class="anchor" aria-hidden="true" href="#tabulartdlearning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TabularTDLearning</h1>
<p><a href="https://travis-ci.org/JuliaPOMDP/TabularTDLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/543d5dd2ef01417c717989b3924b9a3a0b1a411d45b8fc738999173b9e749878/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961504f4d44502f546162756c617254444c6561726e696e672e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaPOMDP/TabularTDLearning.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/JuliaPOMDP/TabularTDLearning.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/17ae7fd1afecbdfc5473bb316792542072352cb43a15b1190be2a5b5eba7f316/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f4a756c6961504f4d44502f546162756c617254444c6561726e696e672e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/JuliaPOMDP/TabularTDLearning.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a></p>
<p>This repository provides Julia implementations of the following Temporal-Difference reinforcement learning algorithms:</p>
<ul>
<li>Q-Learning</li>
<li>SARSA</li>
<li>SARSA lambda</li>
</ul>
<p>Note that these solvers are tabular, and will only work with MDPs that have discrete state and action spaces.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>This package relies on <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a>. Using POMDPs.jl (should automatically take care of dependencies)</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Pkg.add(&quot;POMDPs&quot;)
import POMDPs
POMDPs.add(&quot;TabularTDLearning&quot;)
"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>POMDPs<span class="pl-pds">"</span></span>)
<span class="pl-k">import</span> POMDPs
POMDPs<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>TabularTDLearning<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using TabularTDLearning
using POMDPPolicies
using POMDPModels

mdp = GridWorld()
# use Q-Learning
exppolicy = EpsGreedyPolicy(mdp, 0.01)
solver = QLearningSolver(exppolicy, learning_rate=0.1, n_episodes=5000, max_episode_length=50, eval_every=50, n_eval_traj=100)
policy = solve(solver, mdp)
# Use SARSA
solver = SARSASolver(exppolicy, learning_rate=0.1, n_episodes=5000, max_episode_length=50, eval_every=50, n_eval_traj=100)
policy = solve(solver, mdp)
# Use SARSA lambda
solver = SARSALambdaSolver(exppolicy, learning_rate=0.1, lambda=0.9, n_episodes=5000, max_episode_length=50, eval_every=50, n_eval_traj=100)
policy = solve(solver, mdp)


"><pre><span class="pl-k">using</span> TabularTDLearning
<span class="pl-k">using</span> POMDPPolicies
<span class="pl-k">using</span> POMDPModels

mdp <span class="pl-k">=</span> <span class="pl-c1">GridWorld</span>()
<span class="pl-c"><span class="pl-c">#</span> use Q-Learning</span>
exppolicy <span class="pl-k">=</span> <span class="pl-c1">EpsGreedyPolicy</span>(mdp, <span class="pl-c1">0.01</span>)
solver <span class="pl-k">=</span> <span class="pl-c1">QLearningSolver</span>(exppolicy, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.1</span>, n_episodes<span class="pl-k">=</span><span class="pl-c1">5000</span>, max_episode_length<span class="pl-k">=</span><span class="pl-c1">50</span>, eval_every<span class="pl-k">=</span><span class="pl-c1">50</span>, n_eval_traj<span class="pl-k">=</span><span class="pl-c1">100</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)
<span class="pl-c"><span class="pl-c">#</span> Use SARSA</span>
solver <span class="pl-k">=</span> <span class="pl-c1">SARSASolver</span>(exppolicy, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.1</span>, n_episodes<span class="pl-k">=</span><span class="pl-c1">5000</span>, max_episode_length<span class="pl-k">=</span><span class="pl-c1">50</span>, eval_every<span class="pl-k">=</span><span class="pl-c1">50</span>, n_eval_traj<span class="pl-k">=</span><span class="pl-c1">100</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)
<span class="pl-c"><span class="pl-c">#</span> Use SARSA lambda</span>
solver <span class="pl-k">=</span> <span class="pl-c1">SARSALambdaSolver</span>(exppolicy, learning_rate<span class="pl-k">=</span><span class="pl-c1">0.1</span>, lambda<span class="pl-k">=</span><span class="pl-c1">0.9</span>, n_episodes<span class="pl-k">=</span><span class="pl-c1">5000</span>, max_episode_length<span class="pl-k">=</span><span class="pl-c1">50</span>, eval_every<span class="pl-k">=</span><span class="pl-c1">50</span>, n_eval_traj<span class="pl-k">=</span><span class="pl-c1">100</span>)
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp)

</pre></div>
</article></div>