<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-tensorcastjl" class="anchor" aria-hidden="true" href="#tensorcastjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TensorCast.jl</h1>
<p><a href="https://pkg.julialang.org/docs/TensorCast/" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable Docs" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://mcabbott.github.io/TensorCast.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3e353c26ddfe819150acbc732248f4f2a37f5175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Latest Docs" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.org/mcabbott/TensorCast.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/3b3d6b1890fed1560ec61e39ad1a286a2e7c628a/68747470733a2f2f7472617669732d63692e6f72672f6d636162626f74742f54656e736f72436173742e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/mcabbott/TensorCast.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/report.html" rel="nofollow"><img src="https://camo.githubusercontent.com/228cae62d0f1417bdc6e7f98606244d2128e4611/68747470733a2f2f6a756c696163692e6769746875622e696f2f4e616e6f736f6c646965725265706f7274732f706b676576616c5f6261646765732f542f54656e736f72436173742e737667" alt="PkgEval" data-canonical-src="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/T/TensorCast.svg" style="max-width:100%;"></a></p>
<p>This package lets you work with many-dimensional arrays in index notation,
by defining a few macros. The first is <code>@cast</code>, which deals both with "casting" into
new shapes (including going to and from an array-of-arrays) and with broadcasting:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@cast</span> A[row][col] <span class="pl-k">:=</span> B[row, col]        <span class="pl-c"><span class="pl-c">#</span> slice a matrix B into its rows, also @cast A[r] := B[r,:]</span>

<span class="pl-c1">@cast</span> C[(i,j), (k,ℓ)] <span class="pl-k">:=</span> D<span class="pl-k">.</span>x[i,j,k,ℓ]   <span class="pl-c"><span class="pl-c">#</span> reshape a 4-tensor D.x to give a matrix</span>

<span class="pl-c1">@cast</span> E[φ,γ] <span class="pl-k">=</span> F[φ]<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> <span class="pl-c1">exp</span>(G[γ])       <span class="pl-c"><span class="pl-c">#</span> broadcast E .= F.^2 .* exp.(G') into existing E</span>

<span class="pl-c1">@cast</span> T[x,y,n] <span class="pl-k">:=</span> <span class="pl-c1">outer</span>(M[:,n])[x,y]    <span class="pl-c"><span class="pl-c">#</span> generalised mapslices, vector -&gt; matrix function</span></pre></div>
<p>Second, <code>@reduce</code> takes sums (or other reductions) over the indicated directions. Among such sums is
matrix multiplication, which can be done more efficiently using <code>@matmul</code> instead:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@reduce</span> K[_,b] <span class="pl-k">:=</span> <span class="pl-c1">prod</span>(a,c) L<span class="pl-k">.</span>field[a,b,c]           <span class="pl-c"><span class="pl-c">#</span> product over dims=(1,3), and drop dims=3</span>

<span class="pl-c1">@reduce</span> S[i] <span class="pl-k">=</span> <span class="pl-c1">sum</span>(n) <span class="pl-k">-</span>P[i,n] <span class="pl-k">*</span> <span class="pl-c1">log</span>(P[i,n]<span class="pl-k">/</span>Q[n])     <span class="pl-c"><span class="pl-c">#</span> sum!(S, @. -P*log(P/Q')) into exising S</span>

<span class="pl-c1">@matmul</span> M[i,j] <span class="pl-k">:=</span> <span class="pl-c1">sum</span>(k,k′) U[i,k,k′] <span class="pl-k">*</span> V[(k,k′),j]  <span class="pl-c"><span class="pl-c">#</span> matrix multiplication, plus reshape</span></pre></div>
<p>All of these are converted into simple Julia array commands like <code>reshape</code> and <code>permutedims</code>
and <code>eachslice</code>, plus a <a href="https://julialang.org/blog/2017/01/moredots" rel="nofollow">broadcasting expression</a> if needed,
and <code>sum</code> /  <code>sum!</code>, or <code>*</code> / <code>mul!</code>. This means that they are very generic, and will (mostly) work well
with <a href="https://github.com/JuliaArrays/StaticArrays.jl">StaticArrays</a>, on the GPU via
<a href="https://github.com/JuliaGPU/CuArrays.jl">CuArrays</a>, and with almost anything else.
For operations with arrays of arrays like <code>mapslices</code>, this package defines gradients for
<a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> (similar to those of <a href="https://github.com/mcabbott/SliceMap.jl">SliceMap.jl</a>).</p>
<p>Similar notation is used by some other packages, although all of them use an implicit sum over
repeated indices. <a href="https://github.com/Jutho/TensorOperations.jl">TensorOperations.jl</a> performs
Einstein-convention contractions and traces:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@tensor</span> A[i] <span class="pl-k">:=</span> B[i,j] <span class="pl-k">*</span> C[j,k] <span class="pl-k">*</span> D[k]      <span class="pl-c"><span class="pl-c">#</span> matrix multiplication, A = B * C * D</span>
<span class="pl-c1">@tensor</span> D[i] <span class="pl-k">:=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> E[i] <span class="pl-k">+</span> F[i,k,k]         <span class="pl-c"><span class="pl-c">#</span> partial trace of F only, Dᵢ = 2Eᵢ + Σⱼ Fᵢⱼⱼ</span></pre></div>
<p>More general contractions are allowed by
<a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a>, but only one term:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@ein</span> W[i,j,k] <span class="pl-k">:=</span> X[i,ξ] <span class="pl-k">*</span> Y[j,ξ] <span class="pl-k">*</span> Z[k,ξ]   <span class="pl-c"><span class="pl-c">#</span> star contraction</span>
W <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds"><span class="pl-c1">ein</span>"</span> iξ,jξ,kξ -&gt; ijk <span class="pl-pds">"</span></span>(X,Y,Z)           <span class="pl-c"><span class="pl-c">#</span> numpy-style notation</span></pre></div>
<p>Instead <a href="https://github.com/ahwillia/Einsum.jl">Einsum.jl</a> and <a href="https://github.com/mcabbott/Tullio.jl">Tullio.jl</a>
sum the entire right hand side, but also allow arbitrary (element-wise) functions:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@einsum</span> S[i] <span class="pl-k">:=</span> <span class="pl-k">-</span>P[i,n] <span class="pl-k">*</span> <span class="pl-c1">log</span>(P[i,n]<span class="pl-k">/</span>Q[n])  <span class="pl-c"><span class="pl-c">#</span> sum over n, for each i (also with @reduce above)</span>
<span class="pl-c1">@einsum</span> G[i] <span class="pl-k">:=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> E[i] <span class="pl-k">+</span> F[i,k,k]         <span class="pl-c"><span class="pl-c">#</span> the sum includes everyting:  Gᵢ = Σⱼ (2Eᵢ + Fᵢⱼⱼ)</span>
<span class="pl-c1">@tullio</span> Z[i,j] <span class="pl-k">:=</span> <span class="pl-c1">abs</span>(A[i<span class="pl-k">+</span>x, j<span class="pl-k">+</span>y] <span class="pl-k">*</span> K[x,y]) <span class="pl-c"><span class="pl-c">#</span> convolution, summing over x and y</span></pre></div>
<p>These produce very different code for actually doing what you request:
The macros <code>@tensor</code> and <code>@ein</code> work out a sequence of basic operations (like contraction and traces),
while <code>@einsum</code> and <code>@tullio</code> simply write the necessary set of nested loops.</p>
<p>For those who speak Python, <code>@cast</code> and <code>@reduce</code> allow similar operations to
<a href="https://github.com/arogozhnikov/einops"><code>einops</code></a> (minus the cool video, but plus broadcasting)
while Einsum / TensorOperations map very roughly to <a href="http://numpy-discussion.10968.n7.nabble.com/einsum-td11810.html" rel="nofollow"><code>einsum</code></a>
/ <a href="https://github.com/dgasmith/opt_einsum"><code>opt_einsum</code></a>.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>You need <a href="https://julialang.org/downloads/" rel="nofollow">Julia</a> 1.0 or later:</p>
<div class="highlight highlight-source-julia"><pre>] add TensorCast</pre></div>
<p>Version 0.2 has substantially re-worked logic, and <a href="https://mcabbott.github.io/TensorCast.jl/dev" rel="nofollow">new docs</a>.
See <a href="https://github.com/mcabbott/TensorCast.jl/releases/tag/v0.2.0">tag page</a> for what's changed.</p>

<h2><a id="user-content-about" class="anchor" aria-hidden="true" href="#about"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>About</h2>
<p>This was a holiday project to learn a bit of metaprogramming, originally <code>TensorSlice.jl</code>.
But it suffered a little scope creep.</p>
</article></div>