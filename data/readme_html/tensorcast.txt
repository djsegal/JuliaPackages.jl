<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-tensorcastjl" class="anchor" aria-hidden="true" href="#tensorcastjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TensorCast.jl</h1>
<p><a href="https://juliahub.com/docs/TensorCast/" rel="nofollow"><img src="https://camo.githubusercontent.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" alt="Stable Docs" data-canonical-src="https://img.shields.io/badge/docs-registered-blue.svg?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzI1cHQiIGhlaWdodD0iMzAwcHQiIHZpZXdCb3g9IjAgMCAzMjUgMzAwIiB2ZXJzaW9uPSIxLjEiPgo8ZyBpZD0ic3VyZmFjZTkxIj4KPHBhdGggc3R5bGU9IiBzdHJva2U6bm9uZTtmaWxsLXJ1bGU6bm9uemVybztmaWxsOnJnYig3OS42JSwyMy41JSwyMCUpO2ZpbGwtb3BhY2l0eToxOyIgZD0iTSAxNTAuODk4NDM4IDIyNSBDIDE1MC44OTg0MzggMjY2LjQyMTg3NSAxMTcuMzIwMzEyIDMwMCA3NS44OTg0MzggMzAwIEMgMzQuNDc2NTYyIDMwMCAwLjg5ODQzOCAyNjYuNDIxODc1IDAuODk4NDM4IDIyNSBDIDAuODk4NDM4IDE4My41NzgxMjUgMzQuNDc2NTYyIDE1MCA3NS44OTg0MzggMTUwIEMgMTE3LjMyMDMxMiAxNTAgMTUwLjg5ODQzOCAxODMuNTc4MTI1IDE1MC44OTg0MzggMjI1ICIvPgo8cGF0aCBzdHlsZT0iIHN0cm9rZTpub25lO2ZpbGwtcnVsZTpub256ZXJvO2ZpbGw6cmdiKDIyJSw1OS42JSwxNC45JSk7ZmlsbC1vcGFjaXR5OjE7IiBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSAiLz4KPHBhdGggc3R5bGU9IiBzdHJva2U6bm9uZTtmaWxsLXJ1bGU6bm9uemVybztmaWxsOnJnYig1OC40JSwzNC41JSw2OS44JSk7ZmlsbC1vcGFjaXR5OjE7IiBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1ICIvPgo8L2c+Cjwvc3ZnPgo=" style="max-width:100%;"></a>
<a href="https://mcabbott.github.io/TensorCast.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3f16986dee4420d08f9440bd9a7847015a99cf8126122bb001438e1f0b12436e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e7376673f6c6f676f3d676974687562" alt="Latest Docs" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg?logo=github" style="max-width:100%;"></a>
<a href="https://github.com/mcabbott/TensorCast.jl/actions?query=workflow%3ACI"><img src="https://github.com/mcabbott/TensorCast.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a></p>
<p>This package lets you work with multi-dimensional arrays in index notation,
by defining a few macros which translate this to broadcasting, permuting, and reducing operations.</p>
<p>The first is <code>@cast</code>, which deals both with "casting" into new shapes (including going to and from an array-of-arrays) and with broadcasting:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@cast A[row][col] := B[row, col]        # slice a matrix B into rows, also @cast A[r] := B[r,:]

@cast C[(i,j), (k,ℓ)] := D.x[i,j,k,ℓ]   # reshape a 4-tensor D.x to give a matrix

@cast E[φ,γ] = F[φ]^2 * exp(G[γ])       # broadcast E .= F.^2 .* exp.(G') into existing E

@cast _[i] := isodd(i) ? log(i) : V[i]  # broadcast a function of the index values

@cast T[x,y,n] := outer(M[:,n])[x,y]    # generalised mapslices, vector -&gt; matrix function
"><pre><span class="pl-c1">@cast</span> A[row][col] <span class="pl-k">:=</span> B[row, col]        <span class="pl-c"><span class="pl-c">#</span> slice a matrix B into rows, also @cast A[r] := B[r,:]</span>

<span class="pl-c1">@cast</span> C[(i,j), (k,ℓ)] <span class="pl-k">:=</span> D<span class="pl-k">.</span>x[i,j,k,ℓ]   <span class="pl-c"><span class="pl-c">#</span> reshape a 4-tensor D.x to give a matrix</span>

<span class="pl-c1">@cast</span> E[φ,γ] <span class="pl-k">=</span> F[φ]<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> <span class="pl-c1">exp</span>(G[γ])       <span class="pl-c"><span class="pl-c">#</span> broadcast E .= F.^2 .* exp.(G') into existing E</span>

<span class="pl-c1">@cast</span> _[i] <span class="pl-k">:=</span> <span class="pl-c1">isodd</span>(i) <span class="pl-k">?</span> <span class="pl-c1">log</span>(i) <span class="pl-k">:</span> V[i]  <span class="pl-c"><span class="pl-c">#</span> broadcast a function of the index values</span>

<span class="pl-c1">@cast</span> T[x,y,n] <span class="pl-k">:=</span> <span class="pl-c1">outer</span>(M[:,n])[x,y]    <span class="pl-c"><span class="pl-c">#</span> generalised mapslices, vector -&gt; matrix function</span></pre></div>
<p>Second, <code>@reduce</code> takes sums (or other reductions) over the indicated directions. Among such sums is
matrix multiplication, which can be done more efficiently using <code>@matmul</code> instead:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@reduce K[_,b] := prod(a,c) L.field[a,b,c]           # product over dims=(1,3), drop dims=3

@reduce S[i] = sum(n) -P[i,n] * log(P[i,n]/Q[n])     # sum!(S, @. -P*log(P/Q')) into exising S

@matmul M[i,j] := sum(k,k′) U[i,k,k′] * V[(k,k′),j]  # matrix multiplication, plus reshape
"><pre><span class="pl-c1">@reduce</span> K[_,b] <span class="pl-k">:=</span> <span class="pl-c1">prod</span>(a,c) L<span class="pl-k">.</span>field[a,b,c]           <span class="pl-c"><span class="pl-c">#</span> product over dims=(1,3), drop dims=3</span>

<span class="pl-c1">@reduce</span> S[i] <span class="pl-k">=</span> <span class="pl-c1">sum</span>(n) <span class="pl-k">-</span>P[i,n] <span class="pl-k">*</span> <span class="pl-c1">log</span>(P[i,n]<span class="pl-k">/</span>Q[n])     <span class="pl-c"><span class="pl-c">#</span> sum!(S, @. -P*log(P/Q')) into exising S</span>

<span class="pl-c1">@matmul</span> M[i,j] <span class="pl-k">:=</span> <span class="pl-c1">sum</span>(k,k′) U[i,k,k′] <span class="pl-k">*</span> V[(k,k′),j]  <span class="pl-c"><span class="pl-c">#</span> matrix multiplication, plus reshape</span></pre></div>
<p>The same notation with <code>@cast</code> applies a function accepting the <code>dims</code> keyword, without reducing:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@cast W[i,j,c,n] := cumsum(c) X[c,i,j,n]^2           # permute, broadcast, cumsum(; dims=3)
"><pre><span class="pl-c1">@cast</span> W[i,j,c,n] <span class="pl-k">:=</span> <span class="pl-c1">cumsum</span>(c) X[c,i,j,n]<span class="pl-k">^</span><span class="pl-c1">2</span>           <span class="pl-c"><span class="pl-c">#</span> permute, broadcast, cumsum(; dims=3)</span></pre></div>
<p>All of these are converted into array commands like <code>reshape</code> and <code>permutedims</code>
and <code>eachslice</code>, plus a <a href="https://julialang.org/blog/2017/01/moredots" rel="nofollow">broadcasting expression</a> if needed,
and <code>sum</code> /  <code>sum!</code>, or <code>*</code> / <code>mul!</code>. This package just provides a convenient notation.</p>
<p>From version 0.4, it relies on <a href="https://github.com/mcabbott/TransmuteDims.jl">TransmuteDims.jl</a>
to handle re-ordering of dimensions, and <a href="https://github.com/mcabbott/LazyStack.jl">LazyStack.jl</a>
to handle slices. It should also now work with <a href="https://github.com/JuliaArrays/OffsetArrays.jl">OffsetArrays.jl</a>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using OffsetArrays
@cast R[n,c] := n^2 + rand(3)[c]  (n in -5:5)        # arbitrary indexing
"><pre><span class="pl-k">using</span> OffsetArrays
<span class="pl-c1">@cast</span> R[n,c] <span class="pl-k">:=</span> n<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">+</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>)[c]  (n <span class="pl-k">in</span> <span class="pl-k">-</span><span class="pl-c1">5</span><span class="pl-k">:</span><span class="pl-c1">5</span>)        <span class="pl-c"><span class="pl-c">#</span> arbitrary indexing</span></pre></div>
<p>And it can be used with some packages which modify broadcasting:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Strided, LoopVectorization, LazyArrays
@cast @strided E[φ,γ] = F[φ]^2 * exp(G[γ])           # multi-threaded
@reduce @avx S[i] := sum(n) -P[i,n] * log(P[i,n])    # SIMD-enhanced
@reduce @lazy M[i,j] := sum(k) U[i,k] * V[j,k]       # non-materialised
"><pre><span class="pl-k">using</span> Strided, LoopVectorization, LazyArrays
<span class="pl-c1">@cast</span> <span class="pl-c1">@strided</span> E[φ,γ] <span class="pl-k">=</span> F[φ]<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> <span class="pl-c1">exp</span>(G[γ])           <span class="pl-c"><span class="pl-c">#</span> multi-threaded</span>
<span class="pl-c1">@reduce</span> <span class="pl-c1">@avx</span> S[i] <span class="pl-k">:=</span> <span class="pl-c1">sum</span>(n) <span class="pl-k">-</span>P[i,n] <span class="pl-k">*</span> <span class="pl-c1">log</span>(P[i,n])    <span class="pl-c"><span class="pl-c">#</span> SIMD-enhanced</span>
<span class="pl-c1">@reduce</span> <span class="pl-c1">@lazy</span> M[i,j] <span class="pl-k">:=</span> <span class="pl-c1">sum</span>(k) U[i,k] <span class="pl-k">*</span> V[j,k]       <span class="pl-c"><span class="pl-c">#</span> non-materialised</span></pre></div>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg; Pkg.add(&quot;TensorCast&quot;)
"><pre><span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>TensorCast<span class="pl-pds">"</span></span>)</pre></div>
<p>The current version requires <a href="https://julialang.org/downloads/" rel="nofollow">Julia 1.4</a> or later.
There are a few pages of <a href="https://mcabbott.github.io/TensorCast.jl/dev" rel="nofollow">documentation</a>.</p>
<h2><a id="user-content-elsewhere" class="anchor" aria-hidden="true" href="#elsewhere"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Elsewhere</h2>
<p>Similar notation is also used by some other packages, although all of them use an implicit sum over
repeated indices. <a href="https://github.com/Jutho/TensorOperations.jl">TensorOperations.jl</a> performs
Einstein-convention contractions and traces:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@tensor A[i] := B[i,j] * C[j,k] * D[k]      # matrix multiplication, A = B * C * D
@tensor D[i] := 2 * E[i] + F[i,k,k]         # partial trace of F only, Dᵢ = 2Eᵢ + Σⱼ Fᵢⱼⱼ
"><pre><span class="pl-c1">@tensor</span> A[i] <span class="pl-k">:=</span> B[i,j] <span class="pl-k">*</span> C[j,k] <span class="pl-k">*</span> D[k]      <span class="pl-c"><span class="pl-c">#</span> matrix multiplication, A = B * C * D</span>
<span class="pl-c1">@tensor</span> D[i] <span class="pl-k">:=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> E[i] <span class="pl-k">+</span> F[i,k,k]         <span class="pl-c"><span class="pl-c">#</span> partial trace of F only, Dᵢ = 2Eᵢ + Σⱼ Fᵢⱼⱼ</span></pre></div>
<p>More general contractions are allowed by
<a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a>, but only one term:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@ein Z[i,j,ξ] := X[i,k,ξ] * Y[j,k,ξ]        # batched matrix multiplication
Z = ein&quot; ikξ,jkξ -&gt; ijξ &quot;(X,Y)              # numpy-style notation
"><pre><span class="pl-c1">@ein</span> Z[i,j,ξ] <span class="pl-k">:=</span> X[i,k,ξ] <span class="pl-k">*</span> Y[j,k,ξ]        <span class="pl-c"><span class="pl-c">#</span> batched matrix multiplication</span>
Z <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds"><span class="pl-c1">ein</span>"</span> ikξ,jkξ -&gt; ijξ <span class="pl-pds">"</span></span>(X,Y)              <span class="pl-c"><span class="pl-c">#</span> numpy-style notation</span></pre></div>
<p>Instead <a href="https://github.com/ahwillia/Einsum.jl">Einsum.jl</a> and <a href="https://github.com/mcabbott/Tullio.jl">Tullio.jl</a>
sum the entire right hand side, but also allow arbitrary (element-wise) functions:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@einsum S[i] := -P[i,n] * log(P[i,n]/Q[n])  # sum over n, for each i (also with @reduce above)
@einsum G[i] := 2 * E[i] + F[i,k,k]         # the sum includes everyting:  Gᵢ = Σⱼ (2Eᵢ + Fᵢⱼⱼ)
@tullio Z[i,j] := abs(A[i+x, j+y] * K[x,y]) # convolution, summing over x and y
"><pre><span class="pl-c1">@einsum</span> S[i] <span class="pl-k">:=</span> <span class="pl-k">-</span>P[i,n] <span class="pl-k">*</span> <span class="pl-c1">log</span>(P[i,n]<span class="pl-k">/</span>Q[n])  <span class="pl-c"><span class="pl-c">#</span> sum over n, for each i (also with @reduce above)</span>
<span class="pl-c1">@einsum</span> G[i] <span class="pl-k">:=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> E[i] <span class="pl-k">+</span> F[i,k,k]         <span class="pl-c"><span class="pl-c">#</span> the sum includes everyting:  Gᵢ = Σⱼ (2Eᵢ + Fᵢⱼⱼ)</span>
<span class="pl-c1">@tullio</span> Z[i,j] <span class="pl-k">:=</span> <span class="pl-c1">abs</span>(A[i<span class="pl-k">+</span>x, j<span class="pl-k">+</span>y] <span class="pl-k">*</span> K[x,y]) <span class="pl-c"><span class="pl-c">#</span> convolution, summing over x and y</span></pre></div>
<p>These produce very different code for actually doing what you request:
The macros <code>@tensor</code> and <code>@ein</code> work out a sequence of basic operations (like contraction and traces),
while <code>@einsum</code> and <code>@tullio</code> write the necessary set of nested loops directly (plus optimisations).</p>
<p>For those who speak Python, <code>@cast</code> and <code>@reduce</code> allow similar operations to
<a href="https://github.com/arogozhnikov/einops"><code>einops</code></a> (minus the cool video, but plus broadcasting)
while <code>@ein</code> &amp; <code>@tensor</code> are closer to <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html" rel="nofollow"><code>einsum</code></a>.</p>
<h2><a id="user-content-about" class="anchor" aria-hidden="true" href="#about"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>About</h2>
<p>This was a holiday project to learn a bit of metaprogramming, originally <code>TensorSlice.jl</code>.
But it suffered a little scope creep.</p>
</article></div>