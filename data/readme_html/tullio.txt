<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><div align="center" dir="auto">
<h1 dir="auto"><a id="user-content-tulliojl" class="anchor" aria-hidden="true" href="#tulliojl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tullio.jl</h1>
<p dir="auto"><a href="https://github.com/mcabbott/Tullio.jl/actions?query=workflow%3ACI"><img src="https://camo.githubusercontent.com/9cc5322483ebb33f47dfb8d1320bd956445e57c432154e202a42d991d472b580/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f6d636162626f74742f54756c6c696f2e6a6c2f63692e796d6c3f6c6f676f3d676974687562" alt="GitHub CI" data-canonical-src="https://img.shields.io/github/actions/workflow/status/mcabbott/Tullio.jl/ci.yml?logo=github" style="max-width: 100%;"></a>
<a href="https://buildkite.com/julialang/tullio-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/35517a281b931e2b464c425e9a6b47dd277d8cbf486c271e63a9199425e05108/68747470733a2f2f696d672e736869656c64732e696f2f6275696c646b6974652f37663766656333356337373431373461353963663631366663366531373131633730653934633038383234383038383735383f636f6c6f723d656565266c6162656c3d677075266c6f676f3d6e7669646961" alt="Buildkite GPU CI" data-canonical-src="https://img.shields.io/buildkite/7f7fec35c774174a59cf616fc6e1711c70e94c088248088758?color=eee&amp;label=gpu&amp;logo=nvidia" style="max-width: 100%;"></a>
<a href="https://github.com/mcabbott/Tullio.jl/releases"><img src="https://camo.githubusercontent.com/0e8b1a02be86fe97b94bf47d055c7420f11065c5c31dae5c52f7a837c751d6df/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f7461672f6d636162626f74742f54756c6c696f2e6a6c3f636f6c6f723d726564266c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c5044393462577767646d567963326c76626a30694d5334774969426c626d4e765a476c755a7a3069565652474c546769507a344b50484e325a79423462577875637a30696148523063446f764c336433647935334d793576636d63764d6a41774d43397a646d6369494868746247357a4f6e68736157357250534a6f644852774f693876643364334c6e637a4c6d39795a7938784f546b354c336873615735724969423361575230614430694d7a4931634851694947686c6157646f644430694d7a41776348516949485a705a58644362336739496a41674d43417a4d6a55674d7a4177496942325a584a7a61573975505349784c6a456950676f385a7942705a443069633356795a6d466a5a546b78496a344b504842686447676763335235624755394969427a64484a7661325536626d39755a54746d615778734c584a3162475536626d3975656d5679627a746d615778734f6e4a6e596967334f5334324a5377794d7934314a5377794d4355704f325a70624777746233426859326c3065546f784f7949675a443069545341784e5441754f446b344e444d34494449794e534244494445314d4334344f5467304d7a67674d6a59324c6a51794d5467334e5341784d5463754d7a49774d7a457949444d774d4341334e5334344f5467304d7a67674d7a417749454d674d7a51754e4463324e54597949444d774d4341774c6a67354f44517a4f4341794e6a59754e4449784f446331494441754f446b344e444d34494449794e534244494441754f446b344e444d34494445344d7934314e7a67784d6a55674d7a51754e4463324e545979494445314d4341334e5334344f5467304d7a67674d54557749454d674d5445334c6a4d794d444d784d6941784e5441674d5455774c6a67354f44517a4f4341784f444d754e5463344d544931494445314d4334344f5467304d7a67674d6a49314943497650676f38634746306143427a64486c735a54306949484e30636d39725a5470756232356c4f325a7062477774636e56735a547075623235365a584a764f325a7062477736636d64694b4449794a5377314f5334324a5377784e4334354a536b375a6d6c73624331766347466a615852354f6a45374969426b50534a4e4944497a4e7934314944633149454d674d6a4d334c6a55674d5445324c6a51794d5467334e5341794d444d754f5449784f446331494445314d4341784e6a49754e5341784e544167517941784d6a45754d4463344d544931494445314d4341344e793431494445784e6934304d6a45344e7a55674f4463754e5341334e534244494467334c6a55674d7a4d754e5463344d544931494445794d5334774e7a67784d6a55674d4341784e6a49754e53417749454d674d6a417a4c6a6b794d5467334e5341774944497a4e79343149444d7a4c6a55334f4445794e5341794d7a63754e5341334e5341694c7a344b504842686447676763335235624755394969427a64484a7661325536626d39755a54746d615778734c584a3162475536626d3975656d5679627a746d615778734f6e4a6e596967314f4334304a53777a4e4334314a5377324f5334344a536b375a6d6c73624331766347466a615852354f6a45374969426b50534a4e49444d794e4334784d4445314e6a49674d6a493149454d674d7a49304c6a45774d5455324d6941794e6a59754e4449784f446331494449354d4334314d6a4d304d7a67674d7a4177494449304f5334784d4445314e6a49674d7a417749454d674d6a41334c6a59334f5459344f43417a4d4441674d5463304c6a45774d5455324d6941794e6a59754e4449784f446331494445334e4334784d4445314e6a49674d6a493149454d674d5463304c6a45774d5455324d6941784f444d754e5463344d544931494449774e7934324e7a6b324f4467674d545577494449304f5334784d4445314e6a49674d54557749454d674d6a6b774c6a55794d7a517a4f4341784e5441674d7a49304c6a45774d5455324d6941784f444d754e5463344d54493149444d794e4334784d4445314e6a49674d6a49314943497650676f384c32632b436a777663335a6e50676f3d" alt="Tag Version" data-canonical-src="https://img.shields.io/github/v/tag/mcabbott/Tullio.jl?color=red&amp;logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzI1cHQiIGhlaWdodD0iMzAwcHQiIHZpZXdCb3g9IjAgMCAzMjUgMzAwIiB2ZXJzaW9uPSIxLjEiPgo8ZyBpZD0ic3VyZmFjZTkxIj4KPHBhdGggc3R5bGU9IiBzdHJva2U6bm9uZTtmaWxsLXJ1bGU6bm9uemVybztmaWxsOnJnYig3OS42JSwyMy41JSwyMCUpO2ZpbGwtb3BhY2l0eToxOyIgZD0iTSAxNTAuODk4NDM4IDIyNSBDIDE1MC44OTg0MzggMjY2LjQyMTg3NSAxMTcuMzIwMzEyIDMwMCA3NS44OTg0MzggMzAwIEMgMzQuNDc2NTYyIDMwMCAwLjg5ODQzOCAyNjYuNDIxODc1IDAuODk4NDM4IDIyNSBDIDAuODk4NDM4IDE4My41NzgxMjUgMzQuNDc2NTYyIDE1MCA3NS44OTg0MzggMTUwIEMgMTE3LjMyMDMxMiAxNTAgMTUwLjg5ODQzOCAxODMuNTc4MTI1IDE1MC44OTg0MzggMjI1ICIvPgo8cGF0aCBzdHlsZT0iIHN0cm9rZTpub25lO2ZpbGwtcnVsZTpub256ZXJvO2ZpbGw6cmdiKDIyJSw1OS42JSwxNC45JSk7ZmlsbC1vcGFjaXR5OjE7IiBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSAiLz4KPHBhdGggc3R5bGU9IiBzdHJva2U6bm9uZTtmaWxsLXJ1bGU6bm9uemVybztmaWxsOnJnYig1OC40JSwzNC41JSw2OS44JSk7ZmlsbC1vcGFjaXR5OjE7IiBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1ICIvPgo8L2c+Cjwvc3ZnPgo=" style="max-width: 100%;"></a></p>
</div>
<p dir="auto">Tullio is a very flexible einsum macro. It understands many array operations written in index notation -- not just matrix multiplication and permutations, but also convolutions, stencils, scatter/gather, and broadcasting. For example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@tullio M[x,y,c] := N[x+i, y+j,c] * K[i,j]     # sum over i,j, and create M

@tullio S[x] = P[x,y] * log(Q[x,y] / R[y])     # sum over y, and write into S

@tullio A[i,j] += B[i,k,l] * C[l,j] * D[k,j]   # sum over k,l, and add to values in A

@tullio (*) Z[j] := X[ind[k],j] * exp(-Y[k])   # product over k"><pre><span class="pl-c1">@tullio</span> M[x,y,c] <span class="pl-k">:=</span> N[x<span class="pl-k">+</span>i, y<span class="pl-k">+</span>j,c] <span class="pl-k">*</span> K[i,j]     <span class="pl-c"><span class="pl-c">#</span> sum over i,j, and create M</span>

<span class="pl-c1">@tullio</span> S[x] <span class="pl-k">=</span> P[x,y] <span class="pl-k">*</span> <span class="pl-c1">log</span>(Q[x,y] <span class="pl-k">/</span> R[y])     <span class="pl-c"><span class="pl-c">#</span> sum over y, and write into S</span>

<span class="pl-c1">@tullio</span> A[i,j] <span class="pl-k">+=</span> B[i,k,l] <span class="pl-k">*</span> C[l,j] <span class="pl-k">*</span> D[k,j]   <span class="pl-c"><span class="pl-c">#</span> sum over k,l, and add to values in A</span>

<span class="pl-c1">@tullio</span> (<span class="pl-k">*</span>) Z[j] <span class="pl-k">:=</span> X[ind[k],j] <span class="pl-k">*</span> <span class="pl-c1">exp</span>(<span class="pl-k">-</span>Y[k])   <span class="pl-c"><span class="pl-c">#</span> product over k</span></pre></div>
<p dir="auto">Used by itself the macro writes ordinary nested loops much like <a href="https://github.com/ahwillia/Einsum.jl"><code>Einsum.@einsum</code></a>.
One difference is that it can parse more expressions, and infer ranges for their indices.
Another is that it will use multi-threading (via <a href="https://julialang.org/blog/2019/07/multithreading/" rel="nofollow"><code>Threads.@spawn</code></a>) and recursive tiling, on large enough arrays.
But it also co-operates with various other packages, provided they are loaded before the macro is called:</p>
<ul dir="auto">
<li>
<p dir="auto">It uses <a href="https://github.com/chriselrod/LoopVectorization.jl"><code>LoopVectorization.@avx</code></a> to speed many things up. (Disable with keyword <code>avx=false</code>.) On a good day this will match the speed of OpenBLAS for matrix multiplication.</p>
</li>
<li>
<p dir="auto">It uses <a href="https://github.com/JuliaGPU/KernelAbstractions.jl"><code>KernelAbstractions.@kernel</code></a> (plus CUDAKernels) to make a GPU version. (Disable with <code>cuda=false</code>.) This is somewhat experimental, and may not be fast.</p>
</li>
</ul>
<p dir="auto">The macro also tries to provide a gradient for use with <a href="https://github.com/FluxML/Tracker.jl">Tracker</a> or (via  <a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules</a>) for <a href="https://github.com/FluxML/Zygote.jl">Zygote</a>, <a href="https://github.com/dfdx/Yota.jl">Yota</a>, etc. 
(Disable with <code>grad=false</code>, or <code>nograd=A</code>.) This is done in one of two ways:</p>
<ul dir="auto">
<li>
<p dir="auto">By default it takes a symbolic derivative of the right hand side expression. This works for reductions over <code>+</code> or <code>min</code>/<code>max</code>. The functions as typed must be known, mostly from <a href="https://github.com/JuliaDiff/DiffRules.jl">DiffRules</a>.</p>
</li>
<li>
<p dir="auto">The option <code>grad=Dual</code> uses instead <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff</a> to differentiate the right hand side (only for reductions over <code>+</code>). This allows for more complicated expressions.</p>
</li>
</ul>
<p dir="auto">The entire right hand side is summed over the full possible range of any indices not appearing on the left.
Pipe operators <code>|&gt;</code> or <code>&lt;|</code> indicate functions to be performed <em>outside</em> the sum, for example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@tullio lse[j] := log &lt;| exp(mat[i,j])   # vec(log.(sum(exp.(mat), dims=1)))"><pre><span class="pl-c1">@tullio</span> lse[j] <span class="pl-k">:=</span> log <span class="pl-k">&lt;</span><span class="pl-k">|</span> <span class="pl-c1">exp</span>(mat[i,j])   <span class="pl-c"><span class="pl-c">#</span> vec(log.(sum(exp.(mat), dims=1)))</span></pre></div>
<p dir="auto">The option <code>@tullio verbose=true</code> will cause it to print index ranges, symbolic derivatives,
and notices when it is unable to use the packages mentioned above.
And <code>verbose=2</code> will print everything.</p>
<p dir="auto">If it's useful in academic work, you can cite it with this DOI: <a href="https://doi.org/10.5281/zenodo.5047409" rel="nofollow"><img src="https://camo.githubusercontent.com/30291bc2d16b5ebea903c515c0fbe1eb3703cc3fcbc3635011e63775fc6499cf/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e353034373430392e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.5047409.svg" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-notation" class="anchor" aria-hidden="true" href="#notation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notation</h2>
<details>
<p dir="auto">Index notation for some simple functions:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg; Pkg.add(&quot;Tullio&quot;)
using Tullio, Test
M = rand(1:20, 3, 7)

@tullio S[1,c] := M[r,c]  # sum over r ∈ 1:3, for each c ∈ 1:7
@test S == sum(M, dims=1) 

@tullio Q[ρ,c] := M[ρ,c] + sqrt(S[1,c])  # loop over ρ &amp; c, no sum -- broadcasting
@test Q ≈ M .+ sqrt.(S)

mult(M,Q) = @tullio P[x,y] := M[x,c] * Q[y,c]  # sum over c ∈ 1:7 -- matrix multiplication
@test mult(M,Q) ≈ M * transpose(Q)

R = [rand(Int8, 3, 4) for δ in 1:5]

@tullio T[j,i,δ] := R[δ][i,j] + 10im  # three nested loops -- concatenation
@test T == permutedims(cat(R...; dims=3), (2,1,3)) .+ 10im

@tullio (max) X[i] := abs2(T[j,i,δ])  # reduce using max, over j and δ
@test X == dropdims(maximum(abs2, T, dims=(1,3)), dims=(1,3))

dbl!(M, S) = @tullio M[r,c] = 2 * S[1,c]  # write into existing matrix, M .= 2 .* S
dbl!(M, S)
@test all(M[r,c] == 2*S[1,c] for r ∈ 1:3, c ∈ 1:7)"><pre><span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Tullio<span class="pl-pds">"</span></span>)
<span class="pl-k">using</span> Tullio, Test
M <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">20</span>, <span class="pl-c1">3</span>, <span class="pl-c1">7</span>)

<span class="pl-c1">@tullio</span> S[<span class="pl-c1">1</span>,c] <span class="pl-k">:=</span> M[r,c]  <span class="pl-c"><span class="pl-c">#</span> sum over r ∈ 1:3, for each c ∈ 1:7</span>
<span class="pl-c1">@test</span> S <span class="pl-k">==</span> <span class="pl-c1">sum</span>(M, dims<span class="pl-k">=</span><span class="pl-c1">1</span>) 

<span class="pl-c1">@tullio</span> Q[ρ,c] <span class="pl-k">:=</span> M[ρ,c] <span class="pl-k">+</span> <span class="pl-c1">sqrt</span>(S[<span class="pl-c1">1</span>,c])  <span class="pl-c"><span class="pl-c">#</span> loop over ρ &amp; c, no sum -- broadcasting</span>
<span class="pl-c1">@test</span> Q <span class="pl-k">≈</span> M <span class="pl-k">.+</span> <span class="pl-c1">sqrt</span>.(S)

<span class="pl-en">mult</span>(M,Q) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> P[x,y] <span class="pl-k">:=</span> M[x,c] <span class="pl-k">*</span> Q[y,c]  <span class="pl-c"><span class="pl-c">#</span> sum over c ∈ 1:7 -- matrix multiplication</span>
<span class="pl-c1">@test</span> <span class="pl-c1">mult</span>(M,Q) <span class="pl-k">≈</span> M <span class="pl-k">*</span> <span class="pl-c1">transpose</span>(Q)

R <span class="pl-k">=</span> [<span class="pl-c1">rand</span>(Int8, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>) <span class="pl-k">for</span> δ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5</span>]

<span class="pl-c1">@tullio</span> T[j,i,δ] <span class="pl-k">:=</span> R[δ][i,j] <span class="pl-k">+</span> <span class="pl-c1">10im</span>  <span class="pl-c"><span class="pl-c">#</span> three nested loops -- concatenation</span>
<span class="pl-c1">@test</span> T <span class="pl-k">==</span> <span class="pl-c1">permutedims</span>(<span class="pl-c1">cat</span>(R<span class="pl-k">...</span>; dims<span class="pl-k">=</span><span class="pl-c1">3</span>), (<span class="pl-c1">2</span>,<span class="pl-c1">1</span>,<span class="pl-c1">3</span>)) <span class="pl-k">.+</span> <span class="pl-c1">10im</span>

<span class="pl-c1">@tullio</span> (max) X[i] <span class="pl-k">:=</span> <span class="pl-c1">abs2</span>(T[j,i,δ])  <span class="pl-c"><span class="pl-c">#</span> reduce using max, over j and δ</span>
<span class="pl-c1">@test</span> X <span class="pl-k">==</span> <span class="pl-c1">dropdims</span>(<span class="pl-c1">maximum</span>(abs2, T, dims<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">3</span>)), dims<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">3</span>))

<span class="pl-en">dbl!</span>(M, S) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> M[r,c] <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> S[<span class="pl-c1">1</span>,c]  <span class="pl-c"><span class="pl-c">#</span> write into existing matrix, M .= 2 .* S</span>
<span class="pl-c1">dbl!</span>(M, S)
<span class="pl-c1">@test</span> <span class="pl-c1">all</span>(M[r,c] <span class="pl-k">==</span> <span class="pl-c1">2</span><span class="pl-k">*</span>S[<span class="pl-c1">1</span>,c] <span class="pl-k">for</span> r <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>, c <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">7</span>)</pre></div>
<p dir="auto">More complicated examples:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Tullio
A = [abs2(i - 11) for i in 1:21]

# Downsample -- range of i is that allowed by both terms:
@tullio B[i] := (A[2i] + A[2i+1])/2  # 1:10 == intersect(1:10, 0:10)

# Shifts -- range of i calculated in terms of that given for j:
@tullio M[i,j] := A[i+j-1]  (j in 1:15)  # i in 1:7
@tullio M[i+_,j] := A[i+j]  (j in 1:15)  # i in 0:6, automatic shift &quot;i+_&quot;

using OffsetArrays # Convolve a filter:
K = OffsetArray([1,-1,2,-1,1], -2:2)
@tullio C[i] := A[i+j] * K[j]    # j ∈ -2:2 implies i ∈ 3:19

# Index by the values in K
@tullio D[i,j] := A[2K[j]+i] ÷ K[j] # extrema(K)==(-1,2) implies i ∈ 3:17

# Wrapped &amp; padded:
@tullio M[i,j] := A[mod(i+j)]  (j in 1:15, i in 1:15)   # wraps around, mod(i+j, axes(A,1))
@tullio M[i,j] := A[clamp(i+j)]  (j in 1:15, i in 1:15) # instead repeats &quot;100&quot;
@tullio M[i+_,j] := A[pad(i+j, 3)]  (j in 1:15)         # fills with zeros

using FFTW # Functions of the indices are OK:
S = [0,1,0,0, 0,0,0,0]
fft(S) ≈ @tullio F[k] := S[x] * exp(-im*pi/8 * (k-1) * x)  (k ∈ axes(S,1))

# Finalisers &lt;| or |&gt; are applied after sum (the two are equivalent):
@tullio N2[j] := sqrt &lt;| M[i,j]^2     # N2 ≈ map(norm, eachcol(M))
@tullio n3[_] := A[i]^3  |&gt; (_)^(1/3) # n3[1] ≈ norm(A,3), with _ anon. func.

# Reduction over any function:
@tullio (*) P[i] := A[i+k]  (k in 0:2) # product
@tullio (max) X[i,_] := D[i,j]         # maximum(D, dims=2), almost

min1(x,y) = ifelse(first(x) &lt; first(y), x, y); # findmin(D, dims=1), almost:
@tullio (min1) Ts[j+_] := (D[i,j], (i,j))  init=(typemax(Int), (0,0))

# Access to fields &amp; arrays -- this uses j ∈ eachindex(first(N).c)
N = [(a=i, b=i^2, c=fill(i^3,3)) for i in 1:10]
@tullio T[i,j] := (N[i].a // 1, N[i].c[j])

# Functions which create arrays are evaluated once:
@tullio R[i,j] := abs.((rand(Int8, 5)[i], rand(Int8, 5)[j]))

using NamedDims, AxisKeys # Dimension names, plus pretty printing:
@tullio M[row=i, col=j, z=k] := A[i+j-1]  (j in 1:15, k in 1:2)
@tullio S[i] := M[col=j-i, z=k, row=i+1] # sum over j,k"><pre><span class="pl-k">using</span> Tullio
A <span class="pl-k">=</span> [<span class="pl-c1">abs2</span>(i <span class="pl-k">-</span> <span class="pl-c1">11</span>) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">21</span>]

<span class="pl-c"><span class="pl-c">#</span> Downsample -- range of i is that allowed by both terms:</span>
<span class="pl-c1">@tullio</span> B[i] <span class="pl-k">:=</span> (A[<span class="pl-c1">2</span>i] <span class="pl-k">+</span> A[<span class="pl-c1">2</span>i<span class="pl-k">+</span><span class="pl-c1">1</span>])<span class="pl-k">/</span><span class="pl-c1">2</span>  <span class="pl-c"><span class="pl-c">#</span> 1:10 == intersect(1:10, 0:10)</span>

<span class="pl-c"><span class="pl-c">#</span> Shifts -- range of i calculated in terms of that given for j:</span>
<span class="pl-c1">@tullio</span> M[i,j] <span class="pl-k">:=</span> A[i<span class="pl-k">+</span>j<span class="pl-k">-</span><span class="pl-c1">1</span>]  (j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>)  <span class="pl-c"><span class="pl-c">#</span> i in 1:7</span>
<span class="pl-c1">@tullio</span> M[i<span class="pl-k">+</span>_,j] <span class="pl-k">:=</span> A[i<span class="pl-k">+</span>j]  (j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>)  <span class="pl-c"><span class="pl-c">#</span> i in 0:6, automatic shift "i+_"</span>

<span class="pl-k">using</span> OffsetArrays <span class="pl-c"><span class="pl-c">#</span> Convolve a filter:</span>
K <span class="pl-k">=</span> <span class="pl-c1">OffsetArray</span>([<span class="pl-c1">1</span>,<span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">1</span>], <span class="pl-k">-</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span>)
<span class="pl-c1">@tullio</span> C[i] <span class="pl-k">:=</span> A[i<span class="pl-k">+</span>j] <span class="pl-k">*</span> K[j]    <span class="pl-c"><span class="pl-c">#</span> j ∈ -2:2 implies i ∈ 3:19</span>

<span class="pl-c"><span class="pl-c">#</span> Index by the values in K</span>
<span class="pl-c1">@tullio</span> D[i,j] <span class="pl-k">:=</span> A[<span class="pl-c1">2</span>K[j]<span class="pl-k">+</span>i] <span class="pl-k">÷</span> K[j] <span class="pl-c"><span class="pl-c">#</span> extrema(K)==(-1,2) implies i ∈ 3:17</span>

<span class="pl-c"><span class="pl-c">#</span> Wrapped &amp; padded:</span>
<span class="pl-c1">@tullio</span> M[i,j] <span class="pl-k">:=</span> A[<span class="pl-c1">mod</span>(i<span class="pl-k">+</span>j)]  (j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>, i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>)   <span class="pl-c"><span class="pl-c">#</span> wraps around, mod(i+j, axes(A,1))</span>
<span class="pl-c1">@tullio</span> M[i,j] <span class="pl-k">:=</span> A[<span class="pl-c1">clamp</span>(i<span class="pl-k">+</span>j)]  (j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>, i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>) <span class="pl-c"><span class="pl-c">#</span> instead repeats "100"</span>
<span class="pl-c1">@tullio</span> M[i<span class="pl-k">+</span>_,j] <span class="pl-k">:=</span> A[<span class="pl-c1">pad</span>(i<span class="pl-k">+</span>j, <span class="pl-c1">3</span>)]  (j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>)         <span class="pl-c"><span class="pl-c">#</span> fills with zeros</span>

<span class="pl-k">using</span> FFTW <span class="pl-c"><span class="pl-c">#</span> Functions of the indices are OK:</span>
S <span class="pl-k">=</span> [<span class="pl-c1">0</span>,<span class="pl-c1">1</span>,<span class="pl-c1">0</span>,<span class="pl-c1">0</span>, <span class="pl-c1">0</span>,<span class="pl-c1">0</span>,<span class="pl-c1">0</span>,<span class="pl-c1">0</span>]
<span class="pl-c1">fft</span>(S) <span class="pl-k">≈</span> <span class="pl-c1">@tullio</span> F[k] <span class="pl-k">:=</span> S[x] <span class="pl-k">*</span> <span class="pl-c1">exp</span>(<span class="pl-k">-</span>im<span class="pl-k">*</span><span class="pl-c1">pi</span><span class="pl-k">/</span><span class="pl-c1">8</span> <span class="pl-k">*</span> (k<span class="pl-k">-</span><span class="pl-c1">1</span>) <span class="pl-k">*</span> x)  (k <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(S,<span class="pl-c1">1</span>))

<span class="pl-c"><span class="pl-c">#</span> Finalisers &lt;| or |&gt; are applied after sum (the two are equivalent):</span>
<span class="pl-c1">@tullio</span> N2[j] <span class="pl-k">:=</span> sqrt <span class="pl-k">&lt;</span><span class="pl-k">|</span> M[i,j]<span class="pl-k">^</span><span class="pl-c1">2</span>     <span class="pl-c"><span class="pl-c">#</span> N2 ≈ map(norm, eachcol(M))</span>
<span class="pl-c1">@tullio</span> n3[_] <span class="pl-k">:=</span> A[i]<span class="pl-k">^</span><span class="pl-c1">3</span>  <span class="pl-k">|&gt;</span> (_)<span class="pl-k">^</span>(<span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">3</span>) <span class="pl-c"><span class="pl-c">#</span> n3[1] ≈ norm(A,3), with _ anon. func.</span>

<span class="pl-c"><span class="pl-c">#</span> Reduction over any function:</span>
<span class="pl-c1">@tullio</span> (<span class="pl-k">*</span>) P[i] <span class="pl-k">:=</span> A[i<span class="pl-k">+</span>k]  (k <span class="pl-k">in</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> product</span>
<span class="pl-c1">@tullio</span> (max) X[i,_] <span class="pl-k">:=</span> D[i,j]         <span class="pl-c"><span class="pl-c">#</span> maximum(D, dims=2), almost</span>

<span class="pl-en">min1</span>(x,y) <span class="pl-k">=</span> <span class="pl-c1">ifelse</span>(<span class="pl-c1">first</span>(x) <span class="pl-k">&lt;</span> <span class="pl-c1">first</span>(y), x, y); <span class="pl-c"><span class="pl-c">#</span> findmin(D, dims=1), almost:</span>
<span class="pl-c1">@tullio</span> (min1) Ts[j<span class="pl-k">+</span>_] <span class="pl-k">:=</span> (D[i,j], (i,j))  init<span class="pl-k">=</span>(<span class="pl-c1">typemax</span>(Int), (<span class="pl-c1">0</span>,<span class="pl-c1">0</span>))

<span class="pl-c"><span class="pl-c">#</span> Access to fields &amp; arrays -- this uses j ∈ eachindex(first(N).c)</span>
N <span class="pl-k">=</span> [(a<span class="pl-k">=</span>i, b<span class="pl-k">=</span>i<span class="pl-k">^</span><span class="pl-c1">2</span>, c<span class="pl-k">=</span><span class="pl-c1">fill</span>(i<span class="pl-k">^</span><span class="pl-c1">3</span>,<span class="pl-c1">3</span>)) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>]
<span class="pl-c1">@tullio</span> T[i,j] <span class="pl-k">:=</span> (N[i]<span class="pl-k">.</span>a <span class="pl-k">//</span> <span class="pl-c1">1</span>, N[i]<span class="pl-k">.</span>c[j])

<span class="pl-c"><span class="pl-c">#</span> Functions which create arrays are evaluated once:</span>
<span class="pl-c1">@tullio</span> R[i,j] <span class="pl-k">:=</span> <span class="pl-c1">abs</span>.((<span class="pl-c1">rand</span>(Int8, <span class="pl-c1">5</span>)[i], <span class="pl-c1">rand</span>(Int8, <span class="pl-c1">5</span>)[j]))

<span class="pl-k">using</span> NamedDims, AxisKeys <span class="pl-c"><span class="pl-c">#</span> Dimension names, plus pretty printing:</span>
<span class="pl-c1">@tullio</span> M[row<span class="pl-k">=</span>i, col<span class="pl-k">=</span>j, z<span class="pl-k">=</span>k] <span class="pl-k">:=</span> A[i<span class="pl-k">+</span>j<span class="pl-k">-</span><span class="pl-c1">1</span>]  (j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">15</span>, k <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span>)
<span class="pl-c1">@tullio</span> S[i] <span class="pl-k">:=</span> M[col<span class="pl-k">=</span>j<span class="pl-k">-</span>i, z<span class="pl-k">=</span>k, row<span class="pl-k">=</span>i<span class="pl-k">+</span><span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span> sum over j,k</span></pre></div>
</details>
<h2 dir="auto"><a id="user-content-fast--slow" class="anchor" aria-hidden="true" href="#fast--slow"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fast &amp; Slow</h2>
<details>
<p dir="auto">When used with LoopVectorization, on straightforward matrix multiplication of real numbers,
<code>@tullio</code> tends to be about as fast as OpenBLAS. Depending on the size, and on your computer.
Here's a speed comparison on mine: <a href="https://github.com/mcabbott/Tullio.jl/blob/master/benchmarks/02/matmul-0.2.5-Float64-1.5.0.png">v2.5</a>.</p>
<p dir="auto">This race is a useful diagnostic, but isn't really the goal. There is little point in avoiding
using BLAS libraries, if you want precisely what they are optimised to give you.
One of the things <code>@tullio</code> is often very fast at is weird tensor contractions,
for which you would otherwise need <code>permutedims</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Tullio, LoopVectorization, NNlib, BenchmarkTools

# Batched matmul, with batch index first in B:
bmm_rev(A, B) = @tullio C[i,k,b] := A[i,j,b] * B[b,k,j]  # (sum over j)

A = randn(20,30,500); B = randn(500,40,30);
bmm_rev(A, B) ≈ NNlib.batched_mul(A, permutedims(B, (3,2,1)))  # true

@btime bmm_rev($A, $B);  # 317.526 μs μs, same speed as un-permuted
@btime NNlib.batched_mul($A, permutedims($B, (3,2,1)));  # 1.478 ms, with MKL"><pre><span class="pl-k">using</span> Tullio, LoopVectorization, NNlib, BenchmarkTools

<span class="pl-c"><span class="pl-c">#</span> Batched matmul, with batch index first in B:</span>
<span class="pl-en">bmm_rev</span>(A, B) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> C[i,k,b] <span class="pl-k">:=</span> A[i,j,b] <span class="pl-k">*</span> B[b,k,j]  <span class="pl-c"><span class="pl-c">#</span> (sum over j)</span>

A <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">20</span>,<span class="pl-c1">30</span>,<span class="pl-c1">500</span>); B <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">500</span>,<span class="pl-c1">40</span>,<span class="pl-c1">30</span>);
<span class="pl-c1">bmm_rev</span>(A, B) <span class="pl-k">≈</span> NNlib<span class="pl-k">.</span><span class="pl-c1">batched_mul</span>(A, <span class="pl-c1">permutedims</span>(B, (<span class="pl-c1">3</span>,<span class="pl-c1">2</span>,<span class="pl-c1">1</span>)))  <span class="pl-c"><span class="pl-c">#</span> true</span>

<span class="pl-c1">@btime</span> <span class="pl-c1">bmm_rev</span>(<span class="pl-k">$</span>A, <span class="pl-k">$</span>B);  <span class="pl-c"><span class="pl-c">#</span> 317.526 μs μs, same speed as un-permuted</span>
<span class="pl-c1">@btime</span> NNlib<span class="pl-k">.</span><span class="pl-c1">batched_mul</span>(<span class="pl-k">$</span>A, <span class="pl-c1">permutedims</span>(<span class="pl-k">$</span>B, (<span class="pl-c1">3</span>,<span class="pl-c1">2</span>,<span class="pl-c1">1</span>)));  <span class="pl-c"><span class="pl-c">#</span> 1.478 ms, with MKL</span></pre></div>
<p dir="auto">Complex numbers aren't handled by LoopVectorization, so will be much slower.</p>
<p dir="auto">Chained multiplication is also very slow, because it doesn't know there's a better
algorithm. Here it just makes 4 loops, instead of multiplying sequentially,
<code>30^4</code> instead of <code>2 * 30^3</code> operations:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="M1, M2, M3 = randn(30,30), randn(30,30), randn(30,30);
@btime $M1 * $M2 * $M3;                                   #  3.525 μs
@btime @tullio M4[i,l] := $M1[i,j] * $M2[j,k] * $M3[k,l]; # 30.401 μs"><pre>M1, M2, M3 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">30</span>,<span class="pl-c1">30</span>), <span class="pl-c1">randn</span>(<span class="pl-c1">30</span>,<span class="pl-c1">30</span>), <span class="pl-c1">randn</span>(<span class="pl-c1">30</span>,<span class="pl-c1">30</span>);
<span class="pl-c1">@btime</span> <span class="pl-k">$</span>M1 <span class="pl-k">*</span> <span class="pl-k">$</span>M2 <span class="pl-k">*</span> <span class="pl-k">$</span>M3;                                   <span class="pl-c"><span class="pl-c">#</span>  3.525 μs</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">@tullio</span> M4[i,l] <span class="pl-k">:=</span> <span class="pl-k">$</span>M1[i,j] <span class="pl-k">*</span> <span class="pl-k">$</span>M2[j,k] <span class="pl-k">*</span> <span class="pl-k">$</span>M3[k,l]; <span class="pl-c"><span class="pl-c">#</span> 30.401 μs</span></pre></div>
<p dir="auto">Or slightly less obviously:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="M, Σ = randn(100,100), randn(100,100);
@tullio R4[i, j] := (M[μ, i] - M[μ,j])' * Σ[μ,ν] * (M[ν, i] - M[ν, j]);
begin
  S = M' * Σ * M  # two N^3 operations, instead of one N^4
  @tullio R3[i,j] := S[i,i] + S[j,j] - S[i,j] - S[j,i]
end;
R3 ≈ R4"><pre>M, Σ <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">100</span>,<span class="pl-c1">100</span>), <span class="pl-c1">randn</span>(<span class="pl-c1">100</span>,<span class="pl-c1">100</span>);
<span class="pl-c1">@tullio</span> R4[i, j] <span class="pl-k">:=</span> (M[μ, i] <span class="pl-k">-</span> M[μ,j])<span class="pl-k">'</span> <span class="pl-k">*</span> Σ[μ,ν] <span class="pl-k">*</span> (M[ν, i] <span class="pl-k">-</span> M[ν, j]);
<span class="pl-k">begin</span>
  S <span class="pl-k">=</span> M<span class="pl-k">'</span> <span class="pl-k">*</span> Σ <span class="pl-k">*</span> M  <span class="pl-c"><span class="pl-c">#</span> two N^3 operations, instead of one N^4</span>
  <span class="pl-c1">@tullio</span> R3[i,j] <span class="pl-k">:=</span> S[i,i] <span class="pl-k">+</span> S[j,j] <span class="pl-k">-</span> S[i,j] <span class="pl-k">-</span> S[j,i]
<span class="pl-k">end</span>;
R3 <span class="pl-k">≈</span> R4</pre></div>
<p dir="auto">Another thing Tullio can be very fast at is broadcast reductions, where it can avoid large allocations. Here LoopVectorization is speeding up <code>log</code>, and Tullio is handling tiled memory access and multi-threading:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="sum_opp(X, Y=X) = @tullio s := X[i,j] * log(Y[j,i])
sum_part(X, Y=X) = @tullio S[i] := X[i,j] * log(Y[j,i])

X = rand(1000,1000);
@btime sum_opp($X)                    #   499.814 μs (93 allocations: 3.97 KiB)
@btime sum($X .* log.(transpose($X))) # 8.759 ms (2 allocations: 7.63 MiB)

@btime sum_part($X)'                           #  1.599 ms (not the same computer!)
@btime sum($X .* log.(transpose($X)), dims=2)  # 13.292 ms"><pre><span class="pl-en">sum_opp</span>(X, Y<span class="pl-k">=</span>X) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> s <span class="pl-k">:=</span> X[i,j] <span class="pl-k">*</span> <span class="pl-c1">log</span>(Y[j,i])
<span class="pl-en">sum_part</span>(X, Y<span class="pl-k">=</span>X) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> S[i] <span class="pl-k">:=</span> X[i,j] <span class="pl-k">*</span> <span class="pl-c1">log</span>(Y[j,i])

X <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">1000</span>,<span class="pl-c1">1000</span>);
<span class="pl-c1">@btime</span> <span class="pl-c1">sum_opp</span>(<span class="pl-k">$</span>X)                    <span class="pl-c"><span class="pl-c">#</span>   499.814 μs (93 allocations: 3.97 KiB)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">sum</span>(<span class="pl-k">$</span>X <span class="pl-k">.*</span> <span class="pl-c1">log</span>.(<span class="pl-c1">transpose</span>(<span class="pl-k">$</span>X))) <span class="pl-c"><span class="pl-c">#</span> 8.759 ms (2 allocations: 7.63 MiB)</span>

<span class="pl-c1">@btime</span> <span class="pl-c1">sum_part</span>(<span class="pl-k">$</span>X)<span class="pl-k">'</span>                           <span class="pl-c"><span class="pl-c">#</span>  1.599 ms (not the same computer!)</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">sum</span>(<span class="pl-k">$</span>X <span class="pl-k">.*</span> <span class="pl-c1">log</span>.(<span class="pl-c1">transpose</span>(<span class="pl-k">$</span>X)), dims<span class="pl-k">=</span><span class="pl-c1">2</span>)  <span class="pl-c"><span class="pl-c">#</span> 13.292 ms</span></pre></div>
<p dir="auto">At present indices using <code>pad</code>, <code>clamp</code> or <code>mod</code> are also slow. These result in extra
checks or operations at every iteration, not just around the edges:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="conv1(x,k) = @tullio y[i+_, j+_] := x[i+a, j+b] * k[a,b]
conv2(x,k) = @tullio y[i+_, j+_] := x[2i-a, 2j-b] * k[a,b]
conv3(x,k) = @tullio y[i+_, j+_] := x[pad(i-a,3), pad(j-b,3)] * k[a,b]

x100 = rand(100,100); k7 = randn(7,7);
@btime conv1($x100, $k7); #  25.574 μs
@btime conv2($x100, $k7); #  44.590 μs
@btime conv3($x100, $k7); #  86.228 μs

using Flux
x104 = reshape(x100,(100,100,1,1)); k74 = reshape(k7,(7,7,1,1)); 
conv1(x100, k7) ≈ @btime CrossCor($k74, false)($x104)       # 586.694 μs
conv2(x100, k7) ≈ @btime Conv($k74, false, stride=2)($x104) # 901.573 μs
conv3(x100, k7) ≈ @btime Conv($k74, false, pad=3)($x104)    # 932.658 μs

using DSP
@btime DSP.conv($x100, $k7); # 198.331 μs"><pre><span class="pl-en">conv1</span>(x,k) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> y[i<span class="pl-k">+</span>_, j<span class="pl-k">+</span>_] <span class="pl-k">:=</span> x[i<span class="pl-k">+</span>a, j<span class="pl-k">+</span>b] <span class="pl-k">*</span> k[a,b]
<span class="pl-en">conv2</span>(x,k) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> y[i<span class="pl-k">+</span>_, j<span class="pl-k">+</span>_] <span class="pl-k">:=</span> x[<span class="pl-c1">2</span>i<span class="pl-k">-</span>a, <span class="pl-c1">2</span>j<span class="pl-k">-</span>b] <span class="pl-k">*</span> k[a,b]
<span class="pl-en">conv3</span>(x,k) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> y[i<span class="pl-k">+</span>_, j<span class="pl-k">+</span>_] <span class="pl-k">:=</span> x[<span class="pl-c1">pad</span>(i<span class="pl-k">-</span>a,<span class="pl-c1">3</span>), <span class="pl-c1">pad</span>(j<span class="pl-k">-</span>b,<span class="pl-c1">3</span>)] <span class="pl-k">*</span> k[a,b]

x100 <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>,<span class="pl-c1">100</span>); k7 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">7</span>,<span class="pl-c1">7</span>);
<span class="pl-c1">@btime</span> <span class="pl-c1">conv1</span>(<span class="pl-k">$</span>x100, <span class="pl-k">$</span>k7); <span class="pl-c"><span class="pl-c">#</span>  25.574 μs</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">conv2</span>(<span class="pl-k">$</span>x100, <span class="pl-k">$</span>k7); <span class="pl-c"><span class="pl-c">#</span>  44.590 μs</span>
<span class="pl-c1">@btime</span> <span class="pl-c1">conv3</span>(<span class="pl-k">$</span>x100, <span class="pl-k">$</span>k7); <span class="pl-c"><span class="pl-c">#</span>  86.228 μs</span>

<span class="pl-k">using</span> Flux
x104 <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(x100,(<span class="pl-c1">100</span>,<span class="pl-c1">100</span>,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)); k74 <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(k7,(<span class="pl-c1">7</span>,<span class="pl-c1">7</span>,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)); 
<span class="pl-c1">conv1</span>(x100, k7) <span class="pl-k">≈</span> <span class="pl-c1">@btime</span> <span class="pl-c1">CrossCor</span>(<span class="pl-k">$</span>k74, <span class="pl-c1">false</span>)(<span class="pl-k">$</span>x104)       <span class="pl-c"><span class="pl-c">#</span> 586.694 μs</span>
<span class="pl-c1">conv2</span>(x100, k7) <span class="pl-k">≈</span> <span class="pl-c1">@btime</span> <span class="pl-c1">Conv</span>(<span class="pl-k">$</span>k74, <span class="pl-c1">false</span>, stride<span class="pl-k">=</span><span class="pl-c1">2</span>)(<span class="pl-k">$</span>x104) <span class="pl-c"><span class="pl-c">#</span> 901.573 μs</span>
<span class="pl-c1">conv3</span>(x100, k7) <span class="pl-k">≈</span> <span class="pl-c1">@btime</span> <span class="pl-c1">Conv</span>(<span class="pl-k">$</span>k74, <span class="pl-c1">false</span>, pad<span class="pl-k">=</span><span class="pl-c1">3</span>)(<span class="pl-k">$</span>x104)    <span class="pl-c"><span class="pl-c">#</span> 932.658 μs</span>

<span class="pl-k">using</span> DSP
<span class="pl-c1">@btime</span> DSP<span class="pl-k">.</span><span class="pl-c1">conv</span>(<span class="pl-k">$</span>x100, <span class="pl-k">$</span>k7); <span class="pl-c"><span class="pl-c">#</span> 198.331 μs</span></pre></div>
</details>
<h2 dir="auto"><a id="user-content-gradients--gpu" class="anchor" aria-hidden="true" href="#gradients--gpu"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Gradients &amp; GPU</h2>
<details><summary><b>Derivatives &amp; GPU</b></summary>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Tullio
mul(A, B) = @tullio C[i,k] := A[i,j] * B[j,k]

A = rand(3,40); B = rand(40,500);
A * B ≈ mul(A, B) # true

using Tracker # or Zygote
ΔA = Tracker.gradient((A,B) -&gt; sum(mul(A, B)), A, B)[1]
ΔA ≈ ones(3,500) * B' # true

using CUDA, CUDAKernels, KernelAbstractions # Now defined with a GPU version:
mul(A, B) = @tullio C[i,k] := A[i,j] * B[j,k]

cu(A * B) ≈ mul(cu(A), cu(B)) # true

cu(ΔA) ≈ Tracker.gradient((A,B) -&gt; sum(mul(A, B)), cu(A), cu(B))[1] # true

# Reduction over min/max:
Tracker.gradient(x -&gt; (@tullio (max) res := x[i]^3), [1,2,3,-2,-1,3])[1]"><pre><span class="pl-k">using</span> Tullio
<span class="pl-en">mul</span>(A, B) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> C[i,k] <span class="pl-k">:=</span> A[i,j] <span class="pl-k">*</span> B[j,k]

A <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">40</span>); B <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">40</span>,<span class="pl-c1">500</span>);
A <span class="pl-k">*</span> B <span class="pl-k">≈</span> <span class="pl-c1">mul</span>(A, B) <span class="pl-c"><span class="pl-c">#</span> true</span>

<span class="pl-k">using</span> Tracker <span class="pl-c"><span class="pl-c">#</span> or Zygote</span>
ΔA <span class="pl-k">=</span> Tracker<span class="pl-k">.</span><span class="pl-c1">gradient</span>((A,B) <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(<span class="pl-c1">mul</span>(A, B)), A, B)[<span class="pl-c1">1</span>]
ΔA <span class="pl-k">≈</span> <span class="pl-c1">ones</span>(<span class="pl-c1">3</span>,<span class="pl-c1">500</span>) <span class="pl-k">*</span> B<span class="pl-k">'</span> <span class="pl-c"><span class="pl-c">#</span> true</span>

<span class="pl-k">using</span> CUDA, CUDAKernels, KernelAbstractions <span class="pl-c"><span class="pl-c">#</span> Now defined with a GPU version:</span>
<span class="pl-en">mul</span>(A, B) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> C[i,k] <span class="pl-k">:=</span> A[i,j] <span class="pl-k">*</span> B[j,k]

<span class="pl-c1">cu</span>(A <span class="pl-k">*</span> B) <span class="pl-k">≈</span> <span class="pl-c1">mul</span>(<span class="pl-c1">cu</span>(A), <span class="pl-c1">cu</span>(B)) <span class="pl-c"><span class="pl-c">#</span> true</span>

<span class="pl-c1">cu</span>(ΔA) <span class="pl-k">≈</span> Tracker<span class="pl-k">.</span><span class="pl-c1">gradient</span>((A,B) <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(<span class="pl-c1">mul</span>(A, B)), <span class="pl-c1">cu</span>(A), <span class="pl-c1">cu</span>(B))[<span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span> true</span>

<span class="pl-c"><span class="pl-c">#</span> Reduction over min/max:</span>
Tracker<span class="pl-k">.</span><span class="pl-c1">gradient</span>(x <span class="pl-k">-&gt;</span> (<span class="pl-c1">@tullio</span> (max) res <span class="pl-k">:=</span> x[i]<span class="pl-k">^</span><span class="pl-c1">3</span>), [<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>,<span class="pl-k">-</span><span class="pl-c1">2</span>,<span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">3</span>])[<span class="pl-c1">1</span>]</pre></div>
<p dir="auto">Some warnings are in order:</p>
<ul dir="auto">
<li>Complete reductions to a number will not work on the GPU at present.
They were extremely slow, and a re-organisation of multi-threading for the CPU case killed them, sorry.</li>
<li>Gradients are not calculated for scalars, only arrays.
Thus for example <code>gradient(a -&gt; (@tullio _ := $a * A[i]), 3.14)</code> will be zero.</li>
<li>When using <code>grad=Dual</code>, the right hand side is evaluated a second time during the backward pass.
This avoids needing memory to store partials, but if the function is expensive, it may be slow.</li>
</ul>
</details>
<h2 dir="auto"><a id="user-content-larger-expressions" class="anchor" aria-hidden="true" href="#larger-expressions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Larger Expressions</h2>
<details>
<p dir="auto">The expression need not be just one line, for example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@tullio out[x, y] := @inbounds(begin  # sum over k
        a,b = off[k]
        mat[mod(x+a), mod(y+b)]
    end) (x in axes(mat,1), y in axes(mat,2)) grad=Dual nograd=off"><pre><span class="pl-c1">@tullio</span> out[x, y] <span class="pl-k">:=</span> <span class="pl-c1">@inbounds</span>(<span class="pl-k">begin</span>  <span class="pl-c"><span class="pl-c">#</span> sum over k</span>
        a,b <span class="pl-k">=</span> off[k]
        mat[<span class="pl-c1">mod</span>(x<span class="pl-k">+</span>a), <span class="pl-c1">mod</span>(y<span class="pl-k">+</span>b)]
    <span class="pl-k">end</span>) (x <span class="pl-k">in</span> <span class="pl-c1">axes</span>(mat,<span class="pl-c1">1</span>), y <span class="pl-k">in</span> <span class="pl-c1">axes</span>(mat,<span class="pl-c1">2</span>)) grad<span class="pl-k">=</span>Dual nograd<span class="pl-k">=</span>off</pre></div>
<p dir="auto">Here the macro cannot infer the range of the output's indices <code>x,y</code>, so they must be provided explicitly.
(If writing into an existing array, with <code>out[x,y] = begin ...</code> or <code>+=</code>, then ranges would be taken from there.)
Because it sees assignment being made, it does not attempt to sum over <code>a,b</code>, and it assumes that indices could go out of bounds so does not add <code>@inbounds</code> for you.
(Although in fact <code>mod(x+a) == mod(x+a, axes(mat,1))</code> is safe.)
It will also not be able to take a symbolic derivative, but dual numbers will work fine.</p>
<p dir="auto">More examples:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Tullio, OffsetArrays

# A convolution with cyclic indices
mat = zeros(10,10,1); mat[2,2] = 101; mat[10,10] = 1;
@tullio kern[i,j] := 1/(1+i^2+j^2)  (i in -3:3, j in -3:3)

@tullio out[x,y,c] := begin
    xi = mod(x+i, axes(mat,1)) # xi = ... means that it won't be summed,
    # yj = mod(y+j, axes(mat,2))
    @inbounds trunc(Int, mat[xi, mod(y+j), c] * kern[i,j]) # and disables automatic @inbounds,
end (x in 1:10, y in 1:10) # and prevents range of x from being inferred.

# A stencil?
offsets = [(a,b) for a in -2:2 for b in -2:2 if a&gt;=b] # vector of tuples

@tullio out[x,y,1] = begin
        a,b = offsets[k]
        i = clamp(x+a, extrema(axes(mat,1))...)
        # j = clamp(y+b, extrema(axes(mat,2))...) # can be written clamp(y+b)
        @inbounds mat[i, clamp(y+b), 1] * 10
    end # ranges of x,y read from out[x,y,1]

# Applying a vector of functions
fs = [sin, cos, tan]
xs = randn(3,100)
@tullio ys[r,c] := (fs[r])(xs[r,c])

using Zygote, ForwardDiff
rowmap(fs, xs) = @tullio ys[r,c] := (fs[r])(xs[r,c]) grad=Dual nograd=fs
Zygote.gradient(sum∘rowmap, fs, ones(3,2))
[f'(1) for f in fs] # agrees"><pre><span class="pl-k">using</span> Tullio, OffsetArrays

<span class="pl-c"><span class="pl-c">#</span> A convolution with cyclic indices</span>
mat <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(<span class="pl-c1">10</span>,<span class="pl-c1">10</span>,<span class="pl-c1">1</span>); mat[<span class="pl-c1">2</span>,<span class="pl-c1">2</span>] <span class="pl-k">=</span> <span class="pl-c1">101</span>; mat[<span class="pl-c1">10</span>,<span class="pl-c1">10</span>] <span class="pl-k">=</span> <span class="pl-c1">1</span>;
<span class="pl-c1">@tullio</span> kern[i,j] <span class="pl-k">:=</span> <span class="pl-c1">1</span><span class="pl-k">/</span>(<span class="pl-c1">1</span><span class="pl-k">+</span>i<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">+</span>j<span class="pl-k">^</span><span class="pl-c1">2</span>)  (i <span class="pl-k">in</span> <span class="pl-k">-</span><span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">3</span>, j <span class="pl-k">in</span> <span class="pl-k">-</span><span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">3</span>)

<span class="pl-c1">@tullio</span> out[x,y,c] <span class="pl-k">:=</span> <span class="pl-k">begin</span>
    xi <span class="pl-k">=</span> <span class="pl-c1">mod</span>(x<span class="pl-k">+</span>i, <span class="pl-c1">axes</span>(mat,<span class="pl-c1">1</span>)) <span class="pl-c"><span class="pl-c">#</span> xi = ... means that it won't be summed,</span>
    <span class="pl-c"><span class="pl-c">#</span> yj = mod(y+j, axes(mat,2))</span>
    <span class="pl-c1">@inbounds</span> <span class="pl-c1">trunc</span>(Int, mat[xi, <span class="pl-c1">mod</span>(y<span class="pl-k">+</span>j), c] <span class="pl-k">*</span> kern[i,j]) <span class="pl-c"><span class="pl-c">#</span> and disables automatic @inbounds,</span>
<span class="pl-k">end</span> (x <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>, y <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> and prevents range of x from being inferred.</span>

<span class="pl-c"><span class="pl-c">#</span> A stencil?</span>
offsets <span class="pl-k">=</span> [(a,b) <span class="pl-k">for</span> a <span class="pl-k">in</span> <span class="pl-k">-</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span> <span class="pl-k">for</span> b <span class="pl-k">in</span> <span class="pl-k">-</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span> <span class="pl-k">if</span> a<span class="pl-k">&gt;=</span>b] <span class="pl-c"><span class="pl-c">#</span> vector of tuples</span>

<span class="pl-c1">@tullio</span> out[x,y,<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-k">begin</span>
        a,b <span class="pl-k">=</span> offsets[k]
        i <span class="pl-k">=</span> <span class="pl-c1">clamp</span>(x<span class="pl-k">+</span>a, <span class="pl-c1">extrema</span>(<span class="pl-c1">axes</span>(mat,<span class="pl-c1">1</span>))<span class="pl-k">...</span>)
        <span class="pl-c"><span class="pl-c">#</span> j = clamp(y+b, extrema(axes(mat,2))...) # can be written clamp(y+b)</span>
        <span class="pl-c1">@inbounds</span> mat[i, <span class="pl-c1">clamp</span>(y<span class="pl-k">+</span>b), <span class="pl-c1">1</span>] <span class="pl-k">*</span> <span class="pl-c1">10</span>
    <span class="pl-k">end</span> <span class="pl-c"><span class="pl-c">#</span> ranges of x,y read from out[x,y,1]</span>

<span class="pl-c"><span class="pl-c">#</span> Applying a vector of functions</span>
fs <span class="pl-k">=</span> [sin, cos, tan]
xs <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>,<span class="pl-c1">100</span>)
<span class="pl-c1">@tullio</span> ys[r,c] <span class="pl-k">:=</span> (fs[r])(xs[r,c])

<span class="pl-k">using</span> Zygote, ForwardDiff
<span class="pl-en">rowmap</span>(fs, xs) <span class="pl-k">=</span> <span class="pl-c1">@tullio</span> ys[r,c] <span class="pl-k">:=</span> (fs[r])(xs[r,c]) grad<span class="pl-k">=</span>Dual nograd<span class="pl-k">=</span>fs
Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(sum<span class="pl-k">∘</span>rowmap, fs, <span class="pl-c1">ones</span>(<span class="pl-c1">3</span>,<span class="pl-c1">2</span>))
[f<span class="pl-k">'</span>(<span class="pl-c1">1</span>) <span class="pl-k">for</span> f <span class="pl-k">in</span> fs] <span class="pl-c"><span class="pl-c">#</span> agrees</span></pre></div>
</details>
<h2 dir="auto"><a id="user-content-keyword-options" class="anchor" aria-hidden="true" href="#keyword-options"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Keyword Options</h2>
<details>
<p dir="auto">The default setting is:
<code>@tullio threads=true fastmath=true avx=true tensor=true cuda=256 grad=Base verbose=false A[i,j] := ...</code></p>
<ul dir="auto">
<li><code>threads=false</code> turns off threading, while <code>threads=64^3</code> sets a threshold size at which to divide the work (replacing the macro's best guess).</li>
<li><code>avx=false</code> turns off the use of <code>LoopVectorization</code>, while <code>avx=4</code> inserts <code>@avx unroll=4 for i in ...</code>.</li>
<li><code>grad=false</code> turns off gradient calculation, and <code>grad=Dual</code> switches it to use <code>ForwardDiff</code> (which must be loaded).</li>
<li><code>nograd=A</code> turns of the gradient calculation just for <code>A</code>, and <code>nograd=(A,B,C)</code> does this for several arrays.</li>
<li><code>tensor=false</code> turns off the use of <code>TensorOperations</code>.</li>
<li>Assignment <code>xi = ...</code> removes <code>xi</code> from the list of indices: its range is note calculated, and it will not be summed over. It also disables <code>@inbounds</code> since this is now up to you.</li>
<li><code>verbose=true</code> prints things like the index ranges inferred, and gradient calculations. <code>verbose=2</code> prints absolutely everything.</li>
<li><code>A[i,j] := ...</code> makes a new array, while <code>A[i,j] = ...</code> and <code>A[i,j] += ...</code> write into an existing one. <code>A[row=i, col=j] := ...</code> makes a new <code>NamedDimsArray</code>.</li>
<li><code>@tullio (*) A[i,j] := ...</code> is a product, as is <code>@tullio A[i,j] *= ...</code>. For other reductions, <code>@tullio (f) A[i,j] ^= ...</code> is an in-place update.</li>
<li><code>init=0.0</code> gives the initial value for reductions. For <code>+</code>, <code>*</code>, <code>min</code>, <code>min</code>, <code>&amp;</code>, <code>|</code> it has sensible defaults, for other reductions uses zero.</li>
</ul>
<p dir="auto">Implicit:</p>
<ul dir="auto">
<li>Indices without shifts must have the same range everywhere they appear, but those with shifts (even <code>A[i+0]</code>) run over the intersection of possible ranges.</li>
<li>Shifted output indices must start at 1, unless <code>OffsetArrays</code> is visible in the calling module.</li>
<li>The use of <code>@avx</code>, and the calculation of gradients, are switched off by sufficiently complex syntax (such as arrays of arrays).</li>
<li>Gradient hooks are attached for any or all of <code>ReverseDiff</code>, <code>Tracker</code> &amp; <code>Zygote</code>. These packages need not be loaded when the macro is run.</li>
<li>Gradients are only defined for reductions over <code>(+)</code> (default) and <code>min</code>, <code>max</code>.</li>
<li>GPU kernels are only constructed when both <code>KernelAbstractions</code> and <code>CUDA</code> are visible. The default <code>cuda=256</code> is passed to <code>kernel(CUDA(), 256)</code>.</li>
<li>The CPU kernels from <code>KernelAbstractions</code> are called only when <code>threads=false</code>; they are not at present very fast, but perhaps useful for testing.</li>
</ul>
<p dir="auto">Extras:</p>
<ul dir="auto">
<li><code>A[i] := i^2  (i in 1:10)</code> is how you specify a range for indices when this can't be inferred.</li>
<li><code>A[i] := B[i, $col] - C[i, 2]</code> is how you fix one index to a constant (to prevent <code>col</code> being summed over).</li>
<li><code>A[i] := $d * B[i]</code> is the preferred way to include other constants. Note that no gradient is calculated for <code>d</code>.</li>
<li>Within indexing, <code>A[mod(i), clamp(j)]</code> both maps <code>i</code> &amp; <code>j</code> to lie within <code>axes(A)</code>, and disables inference of their ranges from <code>A</code>.</li>
<li>Similarly, <code>A[pad(i,3)]</code> extends the range of <code>i</code>, inserting zeros outside of <code>A</code>. Instead of zero, <code>pad=NaN</code> uses this value as padding. The implementation of this (and <code>mod</code>, <code>clamp</code>) is not very fast at present.</li>
<li>On the left, when making a new array, an underscore like <code>A[i+_] :=</code> inserts whatever shift is needed to make <code>A</code> one-based.</li>
<li><code>Tullio.@printgrad (x+y)*log(x/z)   x y z</code> prints out how symbolic derivatives will be done.</li>
</ul>
<p dir="auto">Macros:</p>
<ul dir="auto">
<li><code>Tullio.@tensor</code> is a macro which uses TensorOperations to evaluate expressions, but provides gradient definitions. (Previously this was automatic behaviour, when TensorOperations.jl was loaded &amp; the expression was suitable.)</li>
<li><code>Tullio.@einsum</code> is a variant with a few changes, to allow the running of Einsum.jl's tests.</li>
</ul>
</details>
<h2 dir="auto"><a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it Works</h2>
<details>
<p dir="auto">The following three macros all end up calling the same functions as does <code>C = A * B</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@tensor C[i,j] := A[i,k] * B[k,j]         # TensorOperations.jl
@ein C[i,j] := A[i,k] * B[k,j]            # OMEinsum.jl
@matmul C[i,j] := sum(k) A[i,k] * B[k,j]  # TensorCast.jl"><pre><span class="pl-c1">@tensor</span> C[i,j] <span class="pl-k">:=</span> A[i,k] <span class="pl-k">*</span> B[k,j]         <span class="pl-c"><span class="pl-c">#</span> TensorOperations.jl</span>
<span class="pl-c1">@ein</span> C[i,j] <span class="pl-k">:=</span> A[i,k] <span class="pl-k">*</span> B[k,j]            <span class="pl-c"><span class="pl-c">#</span> OMEinsum.jl</span>
<span class="pl-c1">@matmul</span> C[i,j] <span class="pl-k">:=</span> <span class="pl-c1">sum</span>(k) A[i,k] <span class="pl-k">*</span> B[k,j]  <span class="pl-c"><span class="pl-c">#</span> TensorCast.jl</span></pre></div>
<p dir="auto">But this one writes its own for-loops:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@einsum C[i,j] := A[i,k] * B[k,j]         # Einsum.jl"><pre><span class="pl-c1">@einsum</span> C[i,j] <span class="pl-k">:=</span> A[i,k] <span class="pl-k">*</span> B[k,j]         <span class="pl-c"><span class="pl-c">#</span> Einsum.jl</span></pre></div>
<p dir="auto">expanding out to roughly this:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="T = promote_type(eltype(A), eltype(B))
C = Array{T}(undef, size(A,1), size(B,2))
@inbounds for j in 1:size(B,2)
    for i in 1:size(A,1)
        acc = zero(T)
        for k in 1:size(A,2)
            acc += A[i,k] * B[k,j]
        end
        C[i,j] = acc
    end
end"><pre>T <span class="pl-k">=</span> <span class="pl-c1">promote_type</span>(<span class="pl-c1">eltype</span>(A), <span class="pl-c1">eltype</span>(B))
C <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{T}</span>(undef, <span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), <span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>))
<span class="pl-c1">@inbounds</span> <span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
    <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>)
        acc <span class="pl-k">=</span> <span class="pl-c1">zero</span>(T)
        <span class="pl-k">for</span> k <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
            acc <span class="pl-k">+=</span> A[i,k] <span class="pl-k">*</span> B[k,j]
        <span class="pl-k">end</span>
        C[i,j] <span class="pl-k">=</span> acc
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p dir="auto">Tullio does something similar, but working through a few functions. Taking a slightly more complicated example, this:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@tullio C[i,j] := tanh &lt;| A[i,k] * B[k,j]"><pre><span class="pl-c1">@tullio</span> C[i,j] <span class="pl-k">:=</span> tanh <span class="pl-k">&lt;</span><span class="pl-k">|</span> A[i,k] <span class="pl-k">*</span> B[k,j]</pre></div>
<p dir="auto">expands to roughly this:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="function act!(::Type, C::AbstractArray{T}, A, B, ax_i, ax_j, ax_k, keep=nothing, final=true) where T
    @inbounds @fastmath for i in ax_i
        for j in ax_j
            acc = isnothing(keep) ? zero(T) : C[i,j]
            for k in ax_k
                acc += A[i,k] * B[k,j]
            end
            C[i,j] = isnothing(final) ? acc : tanh(acc)
        end
    end
end

function make(A, B)
    ax_i = axes(A,1)
    ax_j = axes(B,2)
    ax_k = axes(A,2) # and check this is == axes(B,1)
    rhs(A,B,i,j,k) = tanh(A[i,k] * B[k,j])
    T = Core.Compiler.return_type(rhs, eltype.((A,B,1,1,1))) # plus a fallback
    C = similar(A, T, (ax_i, ax_j))
    Tullio.threader(act!, Array{T}, C, (A,B), (ax_i,ax_j), (ax_k,), +, 64^3)
    return C
end

C = Tullio.Eval(make, ∇make)(A, B)"><pre><span class="pl-k">function</span> <span class="pl-en">act!</span>(<span class="pl-k">::</span><span class="pl-c1">Type</span>, C<span class="pl-k">::</span><span class="pl-c1">AbstractArray{T}</span>, A, B, ax_i, ax_j, ax_k, keep<span class="pl-k">=</span><span class="pl-c1">nothing</span>, final<span class="pl-k">=</span><span class="pl-c1">true</span>) <span class="pl-k">where</span> T
    <span class="pl-c1">@inbounds</span> <span class="pl-c1">@fastmath</span> <span class="pl-k">for</span> i <span class="pl-k">in</span> ax_i
        <span class="pl-k">for</span> j <span class="pl-k">in</span> ax_j
            acc <span class="pl-k">=</span> <span class="pl-c1">isnothing</span>(keep) <span class="pl-k">?</span> <span class="pl-c1">zero</span>(T) <span class="pl-k">:</span> C[i,j]
            <span class="pl-k">for</span> k <span class="pl-k">in</span> ax_k
                acc <span class="pl-k">+=</span> A[i,k] <span class="pl-k">*</span> B[k,j]
            <span class="pl-k">end</span>
            C[i,j] <span class="pl-k">=</span> <span class="pl-c1">isnothing</span>(final) <span class="pl-k">?</span> acc <span class="pl-k">:</span> <span class="pl-c1">tanh</span>(acc)
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">make</span>(A, B)
    ax_i <span class="pl-k">=</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">1</span>)
    ax_j <span class="pl-k">=</span> <span class="pl-c1">axes</span>(B,<span class="pl-c1">2</span>)
    ax_k <span class="pl-k">=</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> and check this is == axes(B,1)</span>
    <span class="pl-en">rhs</span>(A,B,i,j,k) <span class="pl-k">=</span> <span class="pl-c1">tanh</span>(A[i,k] <span class="pl-k">*</span> B[k,j])
    T <span class="pl-k">=</span> Core<span class="pl-k">.</span>Compiler<span class="pl-k">.</span><span class="pl-c1">return_type</span>(rhs, <span class="pl-c1">eltype</span>.((A,B,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>))) <span class="pl-c"><span class="pl-c">#</span> plus a fallback</span>
    C <span class="pl-k">=</span> <span class="pl-c1">similar</span>(A, T, (ax_i, ax_j))
    Tullio<span class="pl-k">.</span><span class="pl-c1">threader</span>(act!, Array{T}, C, (A,B), (ax_i,ax_j), (ax_k,), <span class="pl-k">+</span>, <span class="pl-c1">64</span><span class="pl-k">^</span><span class="pl-c1">3</span>)
    <span class="pl-k">return</span> C
<span class="pl-k">end</span>

C <span class="pl-k">=</span> Tullio<span class="pl-k">.</span><span class="pl-c1">Eval</span>(make, ∇make)(A, B)</pre></div>
<p dir="auto">This division allows it to dispatch to other methods of <code>act!</code>: one generated with <code>@avx</code> if LoopVectorization is loaded, and one for <code>::CuArray</code> if KernelAbstractions is loaded.</p>
<p dir="auto">It also allows <code>threader</code> to divide the work, calling <code>act!</code> many times, from different threads, on small tiles made by dividing the longest axis (say <code>ax_i</code>) in half, repeatedly. If it divides up <code>ax_k</code>, these are done sequentially, with <code>keep=true</code> on all ranges except the first, and <code>final=nothing</code> on all except the last. But <code>ax_i</code> and <code>ax_j</code> are safe to do in parallel.</p>
<p dir="auto">Finally, <code>Eval</code> exists to give Zygote and friends somewhere to attach themselves. The gradient calculation looks roughly like this:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@adjoint function (e::Eval)(AB...)
    C = e.fwd(AB...)
    C, ΔC -&gt; e.rev(ΔC, C, AB...)
end

function ∇act!(::Type, ΔC, ΔA, ΔB, C, A, B, ax_i, ax_j, ax_k, keep)
    for k in ax_k, i in ax_i, j in ax_j
        ex = ΔC[i,j] * (1-C[i,j])^2
        ΔA[i,k] += ex * B[k,j]
        ΔB[k,j] += A[i,k] * ex
    end
end

function ∇make(ΔC, C, A, B)
    ΔA = similar(A) .= 0
    ΔB = similar(B) .= 0
    ax_i, ax_k = axes(A); ax_j = axes(B,2)
    Tullio.∇threader(∇act!, Array{T}, (ax_k,), (ax_i, ax_j), nothing)
    return (ΔA, ΔB)
end"><pre><span class="pl-c1">@adjoint</span> <span class="pl-k">function</span> (e<span class="pl-k">::</span><span class="pl-c1">Eval</span>)(AB<span class="pl-k">...</span>)
    C <span class="pl-k">=</span> e<span class="pl-k">.</span><span class="pl-c1">fwd</span>(AB<span class="pl-k">...</span>)
    C, ΔC <span class="pl-k">-&gt;</span> e<span class="pl-k">.</span><span class="pl-c1">rev</span>(ΔC, C, AB<span class="pl-k">...</span>)
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">∇act!</span>(<span class="pl-k">::</span><span class="pl-c1">Type</span>, ΔC, ΔA, ΔB, C, A, B, ax_i, ax_j, ax_k, keep)
    <span class="pl-k">for</span> k <span class="pl-k">in</span> ax_k, i <span class="pl-k">in</span> ax_i, j <span class="pl-k">in</span> ax_j
        ex <span class="pl-k">=</span> ΔC[i,j] <span class="pl-k">*</span> (<span class="pl-c1">1</span><span class="pl-k">-</span>C[i,j])<span class="pl-k">^</span><span class="pl-c1">2</span>
        ΔA[i,k] <span class="pl-k">+=</span> ex <span class="pl-k">*</span> B[k,j]
        ΔB[k,j] <span class="pl-k">+=</span> A[i,k] <span class="pl-k">*</span> ex
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">∇make</span>(ΔC, C, A, B)
    ΔA <span class="pl-k">=</span> <span class="pl-c1">similar</span>(A) <span class="pl-k">.=</span> <span class="pl-c1">0</span>
    ΔB <span class="pl-k">=</span> <span class="pl-c1">similar</span>(B) <span class="pl-k">.=</span> <span class="pl-c1">0</span>
    ax_i, ax_k <span class="pl-k">=</span> <span class="pl-c1">axes</span>(A); ax_j <span class="pl-k">=</span> <span class="pl-c1">axes</span>(B,<span class="pl-c1">2</span>)
    Tullio.<span class="pl-c1">∇threader</span>(∇act!, Array{T}, (ax_k,), (ax_i, ax_j), <span class="pl-c1">nothing</span>)
    <span class="pl-k">return</span> (ΔA, ΔB)
<span class="pl-k">end</span></pre></div>
<p dir="auto">In this case, it is the loop over <code>k</code> which can be safely broken among different threads, since both <code>ΔA</code> and <code>ΔB</code> have this index. Both <code>ΔA</code> and <code>ΔB</code> are filled in at once.</p>
<p dir="auto">Notice that the derivative of <code>y = tanh(z)</code> has been written in terms of <code>y</code> (the final result of the forward pass) but free of <code>z</code> (the result of the sum, which was not saved). If this is not possible, it will fail.</p>
<p dir="auto">If using <code>grad=Dual</code>, the gradient kernel looks different. This method cannot handle finalisers like <code>tanh</code> above, but for plain <code>@tullio C[i,j] := A[i,k] * B[k,j]</code> it would read:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="function ∇act!(::Type, ΔC, ΔA, ΔB, C, A, B, ax_i, ax_j, ax_k, keep)
    eps1 = ForwardDiff.Dual(0, (1,0))
    eps2 = ForwardDiff.Dual(0, (0,1))
    for k in ax_k, i in ax_i, j in ax_j
        res = (A[i,k] + eps1) * (B[k,j] + eps2)
        ΔA[i,k] += ForwardDiff.partials(res, 1) * ΔC[i,j]
        ΔB[k,j] += ForwardDiff.partials(res, 2) * ΔC[i,j]
    end
end"><pre><span class="pl-k">function</span> <span class="pl-en">∇act!</span>(<span class="pl-k">::</span><span class="pl-c1">Type</span>, ΔC, ΔA, ΔB, C, A, B, ax_i, ax_j, ax_k, keep)
    eps1 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span>(<span class="pl-c1">0</span>, (<span class="pl-c1">1</span>,<span class="pl-c1">0</span>))
    eps2 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span>(<span class="pl-c1">0</span>, (<span class="pl-c1">0</span>,<span class="pl-c1">1</span>))
    <span class="pl-k">for</span> k <span class="pl-k">in</span> ax_k, i <span class="pl-k">in</span> ax_i, j <span class="pl-k">in</span> ax_j
        res <span class="pl-k">=</span> (A[i,k] <span class="pl-k">+</span> eps1) <span class="pl-k">*</span> (B[k,j] <span class="pl-k">+</span> eps2)
        ΔA[i,k] <span class="pl-k">+=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">partials</span>(res, <span class="pl-c1">1</span>) <span class="pl-k">*</span> ΔC[i,j]
        ΔB[k,j] <span class="pl-k">+=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">partials</span>(res, <span class="pl-c1">2</span>) <span class="pl-k">*</span> ΔC[i,j]
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p dir="auto">Writing <code>@tullio verbose=2</code> will print all of these functions out.</p>
<p dir="auto">Scalar reductions, such as <code>@tullio s := A[i,j] * log(B[j,i])</code>, are slightly different in that the <code>act!</code> function simply returns the sum, i.e. the variable <code>acc</code> above.</p>
</details>
<h2 dir="auto"><a id="user-content-elsewhere" class="anchor" aria-hidden="true" href="#elsewhere"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Elsewhere</h2>
<p dir="auto">Back-end friends &amp; relatives:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/chriselrod/LoopVectorization.jl">LoopVectorization.jl</a> is used here, if available.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/MasonProtter/Gaius.jl">Gaius.jl</a> and <a href="https://github.com/chriselrod/PaddedMatrices.jl">PaddedMatrices.jl</a> build on that.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/vchuravy/GPUifyLoops.jl">GPUifyLoops.jl</a> and <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a> generate GPU-compatible kernels.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/tkf/ThreadsX.jl">ThreadsX.jl</a> does threaded reductions, and much else.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/Jutho/Strided.jl">Strided.jl</a> does multi-threaded broadcasting.</p>
</li>
</ul>
<p dir="auto">Front-end near-lookalikes:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/ahwillia/Einsum.jl">Einsum.jl</a> makes simple loops. See <a href="https://github.com/mcabbott/Tullio.jl/blob/master/test/einsum.jl">tests/einsum.jl</a> where <code>using Tullio: @einsum</code> is an almost-seamless replacement.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/Jutho/TensorOperations.jl">TensorOperations.jl</a> and <a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a> identify patterns on which they can call various basic operations. <a href="https://github.com/ho-oto/TensorRules.jl">TensorRules.jl</a> makes <code>@tensor</code> differentiable; see also <a href="https://github.com/mcabbott/TensorGrad.jl">TensorGrad.jl</a> and <a href="https://github.com/mcabbott/TensorTrack.jl">TensorTrack.jl</a> for earlier attempts.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/mcabbott/TensorCast.jl">TensorCast.jl</a> expresses everything as Julia array operations, broadcasting and reduction. (OMEinsum.jl also treats some cases as a special lazy broadcast-reduction.)</p>
</li>
</ul>
<p dir="auto">Things you can't run:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://www.youtube.com/watch?v=Rp7sTl9oPNI" rel="nofollow">Tortilla.jl</a> seems to exist, publicly, only in this very nice talk.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/shashi/ArrayMeta.jl">ArrayMeta.jl</a> was a Julia 0.5 take on some of this.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/MikeInnes/Tokamak">Tokamak.jl</a> was another, see <a href="https://github.com/tkelman/Tokamak.jl">readme here</a>.</p>
</li>
</ul>
</article></div>