<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-stheno" class="anchor" aria-hidden="true" href="#stheno"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Stheno</h1>
<p><a href="https://travis-ci.org/willtebbutt/Stheno.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/de1b8757989c0936ad515d686a9a7b6b65abba35/68747470733a2f2f7472617669732d63692e6f72672f77696c6c746562627574742f537468656e6f2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/willtebbutt/Stheno.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="http://codecov.io/github/willtebbutt/Stheno.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/e6a36c037470b6970fa734ff42aa5e15485dc161/687474703a2f2f636f6465636f762e696f2f6769746875622f77696c6c746562627574742f537468656e6f2e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/willtebbutt/Stheno.jl/coverage.svg?branch=master" style="max-width:100%;"></a>
<a href="https://willtebbutt.github.io/Stheno.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://willtebbutt.github.io/Stheno.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3e353c26ddfe819150acbc732248f4f2a37f5175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></p>
<p>Stheno is designed to make doing non-standard things with Gaussian processes straightforward. It has an intuitive modeling syntax, is inherently able to handle both multi-input and multi-output problems, and trivially supports interdomain pseudo-point approximations. We call this Gaussian process Probabilistic Programming (GPPP).</p>
<p><a href="https://github.com/wesselb/stheno">We also have a Python version of the package</a></p>
<p>Please open issues liberally -- if there's anything that's unclear or doesn't work, we would very much like to know about it.</p>
<p><strong>Installation</strong> - <code>] add Stheno</code>.</p>
<h2><a id="user-content-a-couple-of-examples" class="anchor" aria-hidden="true" href="#a-couple-of-examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A Couple of Examples</h2>
<p>The primary sources of information regarding this package are the <a href="https://willtebbutt.github.io/Stheno.jl/stable" rel="nofollow">documentation</a> and the examples folder, but here are a couple of flashy examples to get started with.</p>
<p>Please raise an issue immediately if either of these examples don't work -- they're not currently included in CI, so there's always a higher chance that they'll be outdated than the internals of the package.</p>
<p>In this first example we define a simple Gaussian process, make observations of different bits of it, and visualise the posterior. We are trivially able to condition on both observations of both <code>f₁</code> <em>and</em> <code>f₃</code>, which is a very non-standard capability.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span></span>
<span class="pl-c"><span class="pl-c">#</span> We'll get going by setting up our model, generating some toy observations, and</span>
<span class="pl-c"><span class="pl-c">#</span> constructing the posterior processes produced by conditioning on these observations.</span>
<span class="pl-c"><span class="pl-c">#</span></span>

<span class="pl-k">using</span> Stheno, Random, Plots
<span class="pl-k">using</span> Stheno<span class="pl-k">:</span> <span class="pl-c1">@model</span>

<span class="pl-c"><span class="pl-c">#</span> Create a pseudo random number generator for reproducibility.</span>
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>);

<span class="pl-c"><span class="pl-c">#</span> Define a distribution over f₁, f₂, and f₃, where f₃(x) = f₁(x) + f₂(x).</span>
<span class="pl-c1">@model</span> <span class="pl-k">function</span> <span class="pl-en">model</span>()
    f₁ <span class="pl-k">=</span> <span class="pl-c1">GP</span>(<span class="pl-c1">randn</span>(rng), <span class="pl-c1">EQ</span>())
    f₂ <span class="pl-k">=</span> <span class="pl-c1">GP</span>(<span class="pl-c1">EQ</span>())
    f₃ <span class="pl-k">=</span> f₁ <span class="pl-k">+</span> f₂
    <span class="pl-k">return</span> f₁, f₂, f₃
<span class="pl-k">end</span>
f₁, f₂, f₃ <span class="pl-k">=</span> <span class="pl-c1">model</span>();

<span class="pl-c"><span class="pl-c">#</span> Sample `N₁` / `N₂` locations at which to measure `f₁` / `f₃`.</span>
N₁, N₃ <span class="pl-k">=</span> <span class="pl-c1">10</span>, <span class="pl-c1">11</span>;
X₁, X₃ <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, N₁) <span class="pl-k">*</span> <span class="pl-c1">10</span>, <span class="pl-c1">rand</span>(rng, N₃) <span class="pl-k">*</span> <span class="pl-c1">10</span>;

<span class="pl-c"><span class="pl-c">#</span> Sample toy observations of `f₁` / `f₃` at `X₁` / `X₃`.</span>
σ² <span class="pl-k">=</span> <span class="pl-c1">1e-2</span>
ŷ₁, ŷ₃ <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, [<span class="pl-c1">f₁</span>(X₁, σ²), <span class="pl-c1">f₃</span>(X₃, σ²)]);

<span class="pl-c"><span class="pl-c">#</span> Compute the posterior processes. `f₁′`, `f₂′`, `f₃′` are just new processes.</span>
(f₁′, f₂′, f₃′) <span class="pl-k">=</span> (f₁, f₂, f₃) <span class="pl-k">|</span> (<span class="pl-c1">f₁</span>(X₁, σ²)←ŷ₁, <span class="pl-c1">f₃</span>(X₃, σ²)←ŷ₃);



<span class="pl-c"><span class="pl-c">#</span></span>
<span class="pl-c"><span class="pl-c">#</span> The are various things that we can do with a Stheno model.</span>
<span class="pl-c"><span class="pl-c">#</span></span>

<span class="pl-c"><span class="pl-c">#</span> Sample jointly from the posterior over each process.</span>
Np, S <span class="pl-k">=</span> <span class="pl-c1">500</span>, <span class="pl-c1">25</span>;
Xp <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">2.5</span>, stop<span class="pl-k">=</span><span class="pl-c1">12.5</span>, length<span class="pl-k">=</span>Np);
f₁′Xp, f₂′Xp, f₃′Xp <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, [<span class="pl-c1">f₁′</span>(Xp, <span class="pl-c1">1e-9</span>), <span class="pl-c1">f₂′</span>(Xp, <span class="pl-c1">1e-9</span>), <span class="pl-c1">f₃′</span>(Xp, <span class="pl-c1">1e-9</span>)], S);

<span class="pl-c"><span class="pl-c">#</span> Compute posterior marginals.</span>
ms1 <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f₁′</span>(Xp));
ms2 <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f₂′</span>(Xp));
ms3 <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f₃′</span>(Xp));

<span class="pl-c"><span class="pl-c">#</span> Pull and mean and std of each posterior marginal.</span>
μf₁′, σf₁′ <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(ms1), <span class="pl-c1">std</span>.(ms1);
μf₂′, σf₂′ <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(ms2), <span class="pl-c1">std</span>.(ms2);
μf₃′, σf₃′ <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(ms3), <span class="pl-c1">std</span>.(ms3);

<span class="pl-c"><span class="pl-c">#</span> Compute the logpdf of the observations.</span>
l <span class="pl-k">=</span> <span class="pl-c1">logpdf</span>([<span class="pl-c1">f₁</span>(X₁, σ²), <span class="pl-c1">f₃</span>(X₃, σ²)], [ŷ₁, ŷ₃])

<span class="pl-c"><span class="pl-c">#</span> Compute the ELBO of the observations, with pseudo-points at the same locations as the</span>
<span class="pl-c"><span class="pl-c">#</span> observations. Could have placed them anywhere we fancy, even in f₂.</span>
l <span class="pl-k">≈</span> <span class="pl-c1">elbo</span>([<span class="pl-c1">f₁</span>(X₁, σ²), <span class="pl-c1">f₃</span>(X₃, σ²)], [ŷ₁, ŷ₃], [<span class="pl-c1">f₁</span>(X₁), <span class="pl-c1">f₃</span>(X₃)])



<span class="pl-c"><span class="pl-c">#</span></span>
<span class="pl-c"><span class="pl-c">#</span> Stheno has some convenience plotting functionality for GPs with 1D inputs:</span>
<span class="pl-c"><span class="pl-c">#</span></span>

<span class="pl-c"><span class="pl-c">#</span> Instantiate plot and chose backend.</span>
<span class="pl-c1">plotly</span>();
posterior_plot <span class="pl-k">=</span> <span class="pl-c1">plot</span>();

<span class="pl-c"><span class="pl-c">#</span> Plot posteriors.</span>
<span class="pl-c1">plot!</span>(posterior_plot, <span class="pl-c1">f₁′</span>(Xp); samples<span class="pl-k">=</span>S, color<span class="pl-k">=</span><span class="pl-c1">:red</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>f1<span class="pl-pds">"</span></span>);
<span class="pl-c1">plot!</span>(posterior_plot, <span class="pl-c1">f₂′</span>(Xp); samples<span class="pl-k">=</span>S, color<span class="pl-k">=</span><span class="pl-c1">:green</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>f2<span class="pl-pds">"</span></span>);
<span class="pl-c1">plot!</span>(posterior_plot, <span class="pl-c1">f₃′</span>(Xp); samples<span class="pl-k">=</span>S, color<span class="pl-k">=</span><span class="pl-c1">:blue</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>f3<span class="pl-pds">"</span></span>);

<span class="pl-c"><span class="pl-c">#</span> Plot observations.</span>
<span class="pl-c1">scatter!</span>(posterior_plot, X₁, ŷ₁;
    markercolor<span class="pl-k">=</span><span class="pl-c1">:red</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);
<span class="pl-c1">scatter!</span>(posterior_plot, X₃, ŷ₃;
    markercolor<span class="pl-k">=</span><span class="pl-c1">:blue</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);

<span class="pl-c1">display</span>(posterior_plot);</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/willtebbutt/stheno_models/blob/master/exact/process_decomposition.png"><img src="https://github.com/willtebbutt/stheno_models/raw/master/exact/process_decomposition.png" alt="" style="max-width:100%;"></a></p>
<p>In the above figure, we have visualised the posterior distribution of all of the processes. Bold lines are posterior means, and shaded areas are three posterior standard deviations from these means. Thin lines are samples from the posterior processes.</p>
<p>This example can also be found in <code>examples/basic_gppp/process_decomposition.jl</code>, which also contains other toy examples of GPPP in action.</p>
<p>In this next example we make observations of two different noisy versions of the same latent process. Again, this is just about doable in existing GP packages if you know what you're doing, but isn't straightforward.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Stheno, Random, Plots
<span class="pl-k">using</span> Stheno<span class="pl-k">:</span> <span class="pl-c1">@model</span>, Noise

<span class="pl-c"><span class="pl-c">#</span> Create a pseudo random number generator for reproducibility.</span>
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>);

<span class="pl-c1">@model</span> <span class="pl-k">function</span> <span class="pl-en">model</span>()

    <span class="pl-c"><span class="pl-c">#</span> Define a smooth latent process that we wish to infer.</span>
    f <span class="pl-k">=</span> <span class="pl-c1">GP</span>(<span class="pl-c1">EQ</span>())

    <span class="pl-c"><span class="pl-c">#</span> Define the two noise processes described.</span>
    noise1 <span class="pl-k">=</span> <span class="pl-c1">sqrt</span>(<span class="pl-c1">1e-2</span>) <span class="pl-k">*</span> <span class="pl-c1">GP</span>(<span class="pl-c1">Noise</span>()) <span class="pl-k">+</span> (x<span class="pl-k">-&gt;</span><span class="pl-c1">sin</span>.(x) <span class="pl-k">.-</span> <span class="pl-c1">5.0</span> <span class="pl-k">.+</span> <span class="pl-c1">sqrt</span>.(<span class="pl-c1">abs</span>.(x)))
    noise2 <span class="pl-k">=</span> <span class="pl-c1">sqrt</span>(<span class="pl-c1">1e-1</span>) <span class="pl-k">*</span> <span class="pl-c1">GP</span>(<span class="pl-c1">3.5</span>, <span class="pl-c1">Noise</span>())

    <span class="pl-c"><span class="pl-c">#</span> Define the processes that we get to observe.</span>
    y1 <span class="pl-k">=</span> f <span class="pl-k">+</span> noise1
    y2 <span class="pl-k">=</span> f <span class="pl-k">+</span> noise2

    <span class="pl-k">return</span> f, noise1, noise2, y1, y2
<span class="pl-k">end</span>
f, noise₁, noise₂, y₁, y₂ <span class="pl-k">=</span> <span class="pl-c1">model</span>();

<span class="pl-c"><span class="pl-c">#</span> Generate some toy observations of `y1` and `y2`.</span>
X₁, X₂ <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, <span class="pl-c1">3</span>) <span class="pl-k">*</span> <span class="pl-c1">10</span>, <span class="pl-c1">rand</span>(rng, <span class="pl-c1">10</span>) <span class="pl-k">*</span> <span class="pl-c1">10</span>;
ŷ₁, ŷ₂ <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, [<span class="pl-c1">y₁</span>(X₁), <span class="pl-c1">y₂</span>(X₂)]);

<span class="pl-c"><span class="pl-c">#</span> Compute the posterior processes.</span>
(f′, y₁′, y₂′) <span class="pl-k">=</span> (f, y₁, y₂) <span class="pl-k">|</span> (<span class="pl-c1">y₁</span>(X₁)←ŷ₁, <span class="pl-c1">y₂</span>(X₂)←ŷ₂);

<span class="pl-c"><span class="pl-c">#</span> Sample jointly from the posterior processes and compute posterior marginals.</span>
Xp <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">2.5</span>, stop<span class="pl-k">=</span><span class="pl-c1">12.5</span>, length<span class="pl-k">=</span><span class="pl-c1">500</span>);
f′Xp, y₁′Xp, y₂′Xp <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, [<span class="pl-c1">f′</span>(Xp, <span class="pl-c1">1e-9</span>), <span class="pl-c1">y₁′</span>(Xp, <span class="pl-c1">1e-9</span>), <span class="pl-c1">y₂′</span>(Xp, <span class="pl-c1">1e-9</span>)], <span class="pl-c1">100</span>);

ms1 <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f′</span>(Xp));
ms2 <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">y₁′</span>(Xp));
ms3 <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">y₂′</span>(Xp));

μf′, σf′ <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(ms1), <span class="pl-c1">std</span>.(ms1);
μy₁′, σy₁′ <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(ms2), <span class="pl-c1">std</span>.(ms2);
μy₂′, σy₂′ <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(ms3), <span class="pl-c1">std</span>.(ms3);

<span class="pl-c"><span class="pl-c">#</span> Instantiate plot and chose backend</span>
<span class="pl-c1">plotly</span>();
posterior_plot <span class="pl-k">=</span> <span class="pl-c1">plot</span>();

<span class="pl-c"><span class="pl-c">#</span> Plot posteriors</span>
<span class="pl-c1">plot!</span>(posterior_plot, <span class="pl-c1">y₁′</span>(Xp); samples<span class="pl-k">=</span>S, sample_seriestype<span class="pl-k">=</span><span class="pl-c1">:scatter</span>, color<span class="pl-k">=</span><span class="pl-c1">:red</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);
<span class="pl-c1">plot!</span>(posterior_plot, <span class="pl-c1">y₂′</span>(Xp); samples<span class="pl-k">=</span>S, sample_seriestype<span class="pl-k">=</span><span class="pl-c1">:scatter</span>, color<span class="pl-k">=</span><span class="pl-c1">:green</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);
<span class="pl-c1">plot!</span>(posterior_plot, <span class="pl-c1">f′</span>(Xp); samples<span class="pl-k">=</span>S, color<span class="pl-k">=</span><span class="pl-c1">:blue</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Latent Function<span class="pl-pds">"</span></span>);

<span class="pl-c"><span class="pl-c">#</span> Plot observations</span>
<span class="pl-c1">scatter!</span>(posterior_plot, X₁, ŷ₁;
    markercolor<span class="pl-k">=</span><span class="pl-c1">:red</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.8</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Sensor 1<span class="pl-pds">"</span></span>);
<span class="pl-c1">scatter!</span>(posterior_plot, X₂, ŷ₂;
    markercolor<span class="pl-k">=</span><span class="pl-c1">:green</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.8</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Sensor 2<span class="pl-pds">"</span></span>);

<span class="pl-c1">display</span>(posterior_plot);</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/willtebbutt/stheno_models/blob/master/exact/simple_sensor_fusion.png"><img src="https://github.com/willtebbutt/stheno_models/raw/master/exact/simple_sensor_fusion.png" alt="" style="max-width:100%;"></a></p>
<p>As before, we visualise the posterior distribution through its marginal statistics and joint samples. Note that the posterior samples over the unobserved process are (unsurprisingly) smooth, whereas the posterior samples over the noisy processes still look uncorrelated and noise-like.</p>
<p>As before, this example can also be found in <code>examples/basic_gppp/process_decomposition.jl</code>.</p>
<h2><a id="user-content-hyperparameter-learning-and-inference" class="anchor" aria-hidden="true" href="#hyperparameter-learning-and-inference"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hyperparameter learning and inference</h2>
<p>Fortunately, there is really no need for this package to explicitly provide support for hyperparameter optimisation as the functionality is already available elsewhere -- it's sufficient that it plays nicely with other fantastic packages in the ecosystem such as <a href="https://github.com/FluxML/Zygote.jl/">Zygote.jl</a> (reverse-mode algorithmic differentiation), <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> (non-linear optimisation), <a href="https://github.com/TuringLang/AdvancedHMC.jl/">AdvancedHMC.jl</a> (Hamiltonian Monte Carlo / NUTS), and <a href="https://github.com/cscherrer/Soss.jl/">Soss.jl</a> (a probabilistic programming framework that provides some very helpful glue). For concrete examples of the use of each of these packages in conjunction with Stheno, see the <code>Getting Started</code> section of the <a href="https://willtebbutt.github.io/Stheno.jl/dev" rel="nofollow">(dev) docs</a>.</p>
<h2><a id="user-content-non-gaussian-problems" class="anchor" aria-hidden="true" href="#non-gaussian-problems"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Non-Gaussian problems</h2>
<p>Stheno doesn't currently have support for non-Gaussian likelihoods, and as such they're on the up-for-grabs list below. If you would like to see these in this package, please do get in touch (open an issue so that we can discuss where to get started, or open a PR if you're feeling ambitious).</p>
<h2><a id="user-content-gps--deep-learning" class="anchor" aria-hidden="true" href="#gps--deep-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GPs + Deep Learning</h2>
<p>The plan is not to support the combination of GPs and Deep Learning explicitly, but rather to ensure that Stheno and <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> play nicely with one another. Both packages now work with <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>, so you can use that to sort out gradient information.</p>
<h2><a id="user-content-things-that-are-up-for-grabs" class="anchor" aria-hidden="true" href="#things-that-are-up-for-grabs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Things that are up for grabs</h2>
<p>Obviously, improvements to code documentation are always welcome, and if you want to write some more unit / integration tests, please feel free. In terms of larger items that require some attention, here are some thoughts:</p>
<ul>
<li>An implementation of SVI from <a href="https://arxiv.org/abs/1309.6835" rel="nofollow">Gaussian Processes for Big Data</a>.</li>
<li>Kronecker-factored matrices: this is quite a general issue which might be best be addressed by the creation of a separate package. It would be very helpful to have an implementation of the <code>AbstractMatrix</code> interface which implements multiplication, inversion, eigenfactorisation etc, which can then be utilised in Stheno.</li>
<li>Primitives for multi-output GPs: although Stheno does fundamentally have support for multi-output GPs, in the same way that it's helpful to implement so-called "fat" nodes in Automatic Differentiation systems, it may well be helpful to implement specialised multi-output processes in Stheno for performance's sake.</li>
<li>Some decent benchmarks: development has not focused on performance so far, but it would be extremely helpful to have a wide range of benchmarks so that we can begin to ensure that time is spent optimally. This would involve comparing against <a href="https://github.com/STOR-i/GaussianProcesses.jl">GaussianProcesses.jl</a>, but also some other non-Julia packages.</li>
<li>Non-Gaussian likelihoods: there are a <em>lot</em> of approximate inference schemes that have been developed for GPs in particular contexts. <a href="https://gitlab.com/hnickisch/gpml-matlab" rel="nofollow">GPML</a> probably has the most mature set of these, and would be a good place to start the transfer from. There's also <a href="https://arxiv.org/abs/1803.09151" rel="nofollow">Natural Gradients in Practice</a> that might be a good startin point for a Monte Carlo approximation to natural gradient varitional inference. A good place to start with these would be to just make them for <code>GP</code>s, as opposed to any <code>AbstractGP</code>, as this is the simplest case.</li>
</ul>
<p>If you are interested in any of the above, please either open an issue or PR. Better still, if there's something not listed here that you think would be good to see, please open an issue to start a discussion regarding it.</p>
</article></div>