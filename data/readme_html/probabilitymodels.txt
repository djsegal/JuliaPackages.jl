<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-probabilitymodels" class="anchor" aria-hidden="true" href="#probabilitymodels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ProbabilityModels</h1>
<p><a href="https://github.com/chriselrod/ProbabilityModels.jl/actions?query=workflow%3ACI"><img src="https://github.com/chriselrod/ProbabilityModels.jl/workflows/CI/badge.svg" alt="CI" style="max-width:100%;"></a>
<a href="https://github.com/chriselrod/ProbabilityModels.jl/actions?query=workflow%3A%22CI+%28Julia+nightly%29%22"><img src="https://github.com/chriselrod/ProbabilityModels.jl/workflows/CI%20(Julia%20nightly)/badge.svg" alt="CI (Julia nightly)" style="max-width:100%;"></a></p>
<p>This is alpha-quality software. It is under active development. Optimistically, I hope to have it and its dependencies reasonably well documented and tested, and all the libraries registered, by the end of the year. There is a roadmap issue <a href="https://github.com/chriselrod/ProbabilityModels.jl/issues/5">here</a>.</p>
<p>The primary goal of this library is to make it as easy as possible to specify models that run as quickly as possible $-$ providing both log densities and the associated gradients. This allows the library to be a front end to Hamiltonian Monte Carlo backends, such as <a href="https://github.com/tpapp/DynamicHMC.jl">DynamicHMC.jl</a>. <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a>/<a href="https://github.com/TuringLang/Turing.jl">Turing's NUTS</a> is also probably worth looking into, as it <a href="https://discourse.julialang.org/t/mcmc-landscape/25654/11?u=elrod" rel="nofollow">reportedly converges the most reliably</a>, at least within DiffEqBayes.</p>
<p><em>A brief introduction.</em></p>
<p>First, you specify a model using a DSL:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using PaddedMatrices, StructuredMatrices, DistributionParameters, LoopVectorization
using InplaceDHMC, VectorizedRNG
using Random, SpecialFunctions, MCMCChainSummaries, LinearAlgebra
using ProbabilityModels, LoopVectorization, SLEEFPirates, SIMDPirates, ProbabilityDistributions, PaddedMatrices
using ProbabilityModels: HierarchicalCentering, ∂HierarchicalCentering, ITPExpectedValue, ∂ITPExpectedValue
using DistributionParameters: CovarianceMatrix, MissingDataVector#, add
using PaddedMatrices: vexp
BLAS.set_num_threads(1)

@model ITPModel begin
    # Non-hierarchical Priors
    ρ ~ Beta(3, 1)
    lκ ~ lsgg(8.5, 1.5, 3.0, 1.5271796258079011) # μ = 1, σ² = 10
    σ ~ Gamma(1.5, 0.25) # μ = 6, σ² = 2.4
    θ ~ Normal(10)
    L ~ LKJ(2.0)
    # Hierarchical Priors.
    # h subscript, for highest in the hierarhcy.
    μₕ₁ ~ Normal(10) # μ = 0
    μₕ₂ ~ Normal(10) # μ = 0
    σₕ ~ Normal(10) # μ = 0
    # Raw μs; non-cenetered parameterization
    μᵣ₁ ~ Normal() # μ = 0, σ = 1
    μᵣ₂ ~ Normal() # μ = 0, σ = 1
    # Center the μs
    μᵦ₁ = HierarchicalCentering(μᵣ₁, μₕ₁, σₕ)
    μᵦ₂ = HierarchicalCentering(μᵣ₂, μₕ₂, σₕ)
    σᵦ ~ Normal(10) # μ = 0
    # Raw βs; non-cenetered parameterization
    βᵣ₁ ~ Normal()
    βᵣ₂ ~ Normal()
    # Center the βs.
    β₁ = HierarchicalCentering(βᵣ₁, μᵦ₁, σᵦ, domains)
    β₂ = HierarchicalCentering(βᵣ₂, μᵦ₂, σᵦ, domains)
    # Likelihood
    κ = vexp(lκ)
    μ₁ = vec(ITPExpectedValue(time, β₁, κ, θ))
    μ₂ = vec(ITPExpectedValue(time, β₂, κ, θ))
    Σ = CovarianceMatrix(ρ, Diagonal(σ) * L, time)
    # Tuple (Y₁, Y₂)
    (Y₁, Y₂) ~ Normal((μ₁, μ₂)[AvailableData], Σ[AvailableData])
end
# Defined model: ITPModel.
# Unknowns: Y₂, domains, μₕ₂, μᵣ₁, time, σ, AvailableData, σᵦ, θ, μᵣ₂, ρ, σₕ, lκ, μₕ₁, L, Y₁, βᵣ₂, βᵣ₁.
"><pre><span class="pl-k">using</span> PaddedMatrices, StructuredMatrices, DistributionParameters, LoopVectorization
<span class="pl-k">using</span> InplaceDHMC, VectorizedRNG
<span class="pl-k">using</span> Random, SpecialFunctions, MCMCChainSummaries, LinearAlgebra
<span class="pl-k">using</span> ProbabilityModels, LoopVectorization, SLEEFPirates, SIMDPirates, ProbabilityDistributions, PaddedMatrices
<span class="pl-k">using</span> ProbabilityModels<span class="pl-k">:</span> HierarchicalCentering, ∂HierarchicalCentering, ITPExpectedValue, ∂ITPExpectedValue
<span class="pl-k">using</span> DistributionParameters<span class="pl-k">:</span> CovarianceMatrix, MissingDataVector<span class="pl-c"><span class="pl-c">#</span>, add</span>
<span class="pl-k">using</span> PaddedMatrices<span class="pl-k">:</span> vexp
BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>)

<span class="pl-c1">@model</span> ITPModel <span class="pl-k">begin</span>
    <span class="pl-c"><span class="pl-c">#</span> Non-hierarchical Priors</span>
    ρ <span class="pl-k">~</span> <span class="pl-c1">Beta</span>(<span class="pl-c1">3</span>, <span class="pl-c1">1</span>)
    lκ <span class="pl-k">~</span> <span class="pl-c1">lsgg</span>(<span class="pl-c1">8.5</span>, <span class="pl-c1">1.5</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">1.5271796258079011</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 1, σ² = 10</span>
    σ <span class="pl-k">~</span> <span class="pl-c1">Gamma</span>(<span class="pl-c1">1.5</span>, <span class="pl-c1">0.25</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 6, σ² = 2.4</span>
    θ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>)
    L <span class="pl-k">~</span> <span class="pl-c1">LKJ</span>(<span class="pl-c1">2.0</span>)
    <span class="pl-c"><span class="pl-c">#</span> Hierarchical Priors.</span>
    <span class="pl-c"><span class="pl-c">#</span> h subscript, for highest in the hierarhcy.</span>
    μₕ₁ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    μₕ₂ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    σₕ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    <span class="pl-c"><span class="pl-c">#</span> Raw μs; non-cenetered parameterization</span>
    μᵣ₁ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>() <span class="pl-c"><span class="pl-c">#</span> μ = 0, σ = 1</span>
    μᵣ₂ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>() <span class="pl-c"><span class="pl-c">#</span> μ = 0, σ = 1</span>
    <span class="pl-c"><span class="pl-c">#</span> Center the μs</span>
    μᵦ₁ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(μᵣ₁, μₕ₁, σₕ)
    μᵦ₂ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(μᵣ₂, μₕ₂, σₕ)
    σᵦ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    <span class="pl-c"><span class="pl-c">#</span> Raw βs; non-cenetered parameterization</span>
    βᵣ₁ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>()
    βᵣ₂ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>()
    <span class="pl-c"><span class="pl-c">#</span> Center the βs.</span>
    β₁ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(βᵣ₁, μᵦ₁, σᵦ, domains)
    β₂ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(βᵣ₂, μᵦ₂, σᵦ, domains)
    <span class="pl-c"><span class="pl-c">#</span> Likelihood</span>
    κ <span class="pl-k">=</span> <span class="pl-c1">vexp</span>(lκ)
    μ₁ <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">ITPExpectedValue</span>(time, β₁, κ, θ))
    μ₂ <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">ITPExpectedValue</span>(time, β₂, κ, θ))
    Σ <span class="pl-k">=</span> <span class="pl-c1">CovarianceMatrix</span>(ρ, <span class="pl-c1">Diagonal</span>(σ) <span class="pl-k">*</span> L, time)
    <span class="pl-c"><span class="pl-c">#</span> Tuple (Y₁, Y₂)</span>
    (Y₁, Y₂) <span class="pl-k">~</span> <span class="pl-c1">Normal</span>((μ₁, μ₂)[AvailableData], Σ[AvailableData])
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> Defined model: ITPModel.</span>
<span class="pl-c"><span class="pl-c">#</span> Unknowns: Y₂, domains, μₕ₂, μᵣ₁, time, σ, AvailableData, σᵦ, θ, μᵣ₂, ρ, σₕ, lκ, μₕ₁, L, Y₁, βᵣ₂, βᵣ₁.</span></pre></div>
<p>The <code>@model</code> macro uses the following expression to define a struct and several functions.
The struct has one field for each unknown.</p>
<p>We can then define an instance of the struct, where we specify each of these unknowns either with an instance (e.g. assign a piece of data), or with a type.
Those specified with a type are unknown parameters (of that type); those specified with an instance treat that instance as known priors or data. For example,
(if we had the appropriate functions defined in scope), we could create an instance with:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="K = 7; D = 3
data = ITPModel(
    domains = domains,
    AvailableData = missingness,
    Y₁ = Y₁,
    Y₂ = Y₂,
    time = time_vector,
    L = LKJCorrCholesky{K},
    ρ = RealVector{K,Bounds(0,1)},
    lκ = RealVector{K},
    θ = RealVector{K},
    μₕ₁ = RealFloat,
    μₕ₂ = RealFloat,
    μᵣ₁ = RealVector{D},
    μᵣ₂ = RealVector{D},
    βᵣ₁ = RealVector{K},
    βᵣ₂ = RealVector{K},
    σᵦ = RealFloat{Bounds(0,Inf)},
    σₕ = RealFloat{Bounds(0,Inf)},
    σ = RealVector{K,Bounds(0,Inf)}
)
"><pre>K <span class="pl-k">=</span> <span class="pl-c1">7</span>; D <span class="pl-k">=</span> <span class="pl-c1">3</span>
data <span class="pl-k">=</span> <span class="pl-c1">ITPModel</span>(
    domains <span class="pl-k">=</span> domains,
    AvailableData <span class="pl-k">=</span> missingness,
    Y₁ <span class="pl-k">=</span> Y₁,
    Y₂ <span class="pl-k">=</span> Y₂,
    time <span class="pl-k">=</span> time_vector,
    L <span class="pl-k">=</span> LKJCorrCholesky{K},
    ρ <span class="pl-k">=</span> RealVector{K,<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">1</span>)},
    lκ <span class="pl-k">=</span> RealVector{K},
    θ <span class="pl-k">=</span> RealVector{K},
    μₕ₁ <span class="pl-k">=</span> RealFloat,
    μₕ₂ <span class="pl-k">=</span> RealFloat,
    μᵣ₁ <span class="pl-k">=</span> RealVector{D},
    μᵣ₂ <span class="pl-k">=</span> RealVector{D},
    βᵣ₁ <span class="pl-k">=</span> RealVector{K},
    βᵣ₂ <span class="pl-k">=</span> RealVector{K},
    σᵦ <span class="pl-k">=</span> RealFloat{<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">Inf</span>)},
    σₕ <span class="pl-k">=</span> RealFloat{<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">Inf</span>)},
    σ <span class="pl-k">=</span> RealVector{K,<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">Inf</span>)}
)</pre></div>
<p>This would let <code>Y₁</code> equal the variable <code>Y₁</code>, while <code>ρ</code> is a vector (with length <code>K=7</code>) of unknown parameters bounded between 0 and 1. The default bounds are <code>(-Inf,Inf)</code>.</p>
<p>Before spewing boilerplate to generate random true values and fake data, a brief summary of the model:
We have longitudinal multivariate observations for some number of subjects. However, not all observations are measured at all times. That is, while subjects may be measured at multiple times (I use $T=36$ times below), only some measurements are taken at any given time, yielding missing data.
Therefore, we subset the full covariance matrix (produced from a vector of autocorrelations, and the Cholesky factor of a covariance matrix across measurements) to find the marginal.</p>
<p>The expected value is function of time (<code>ITPExpectedValue</code>). This function returns a matrix  (<code>time x measurement</code>), so we <code>vec</code> it and subset it.</p>
<p>We also expect some measurements to bare more in common than others, so we group them into "domains", and provide hierarchical priors. We use a non-cenetered parameterization, and the function <code>HierarchicalCentering</code> then centers our parameters for us. There are two methods we use above: one takes scalars, to transform a vector. The other accepts different domain means and standard deviations, and uses these to transform a vector, taking the indices from the <code>Domains</code> argument. That is, if the first element of <code>Domains</code> is 2, indicating that the first 2 measurements belong to the first domain, it will transform the first two elements of <code>βᵣ₁</code> with the first element of <code>μᵦ₁</code> and <code>σᵦ</code> (if either <code>μᵦ₁</code> or <code>σᵦ</code> are scalars, they will be broadcasted across each domain).</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function rinvscaledgamma(::Val{N},a::T,b::T,c::T) where {N,T}
    rg = MutableFixedSizeVector{N,T}(undef)
    log100 = log(100)
    @inbounds for n ∈ 1:N
        rg[n] = log100 / exp(log(VectorizedRNG.randgamma(a/c)) / c + b)
    end
    rg
end

const domains = ProbabilityModels.Domains(2,2,3)

const n_endpoints = sum(domains)

const times = MutableFixedSizeVector{36,Float64,36}(undef); times .= 0:35;

structured_missing_pattern = push!(vcat(([1,0,0,0,0] for i ∈ 1:7)...), 1);
missing_pattern = vcat(
    structured_missing_pattern, fill(1, 4length(times)), structured_missing_pattern, structured_missing_pattern
);

const availabledata = MissingDataVector{Float64}(missing_pattern);

const κ₀ = (8.5, 1.5, 3.0)

AR1(ρ, t) = @. ρ ^ abs(t - t')


function generate_true_parameters(domains, times, κ₀)
    K = sum(domains)
    D = length(domains)
    T = length(times)
    μ = MutableFixedSizeVector{K,Float64}(undef)
    offset = 0
    for i ∈ domains
        domain_mean = 5randn()
        for j ∈ 1+offset:i+offset
            μ[j] = domain_mean + 5randn()
        end
        offset += i
    end
    σ = VectorizedRNG.randgamma( 6.0, 1/6.0)
    ρ = VectorizedRNG.randbeta(4.0,4.0)
    κ = rinvscaledgamma(Val(K), κ₀...)
    lt = last(times)
    β = MutableFixedSizeVector{7,Float64,7}(( 0.0625,  0.0575,  0.0525,  0.0475,  0.0425,  0.04,  0.0375))
    θ₁ = @. μ' - β' * ( 1.0 - exp( - κ' * times) ) / (1.0 - exp( - κ' * lt) ) 
    θ₂ = @. μ' + β' * ( 1.0 - exp( - κ' * times) ) / (1.0 - exp( - κ' * lt) ) 
    
    L_T, info = LAPACK.potrf!('L', AR1(ρ, times))
    @inbounds for tc ∈ 2:T, tr ∈ 1:tc-1
        L_T[tr,tc] = 0.0
    end
    X = PaddedMatrices.MutableFixedSizeMatrix{K,K+3,Float64,K}(undef); randn!(X)
    U_K, info = LAPACK.potrf!('U', BLAS.syrk!('U', 'N', σ, X, 0.0, zero(MutableFixedSizeMatrix{K,K,Float64,K})))
    (
        U_K = U_K, L_T = L_T, μ = μ, θ₁ = θ₁, θ₂ = θ₂, domains = domains, time = times
    )
end

@generated function randomize!(
    sp::PaddedMatrices.StackPointer,
    A::AbstractArray{T,P},
    B::PaddedMatrices.AbstractMutableFixedSizeMatrix{M,M,T},
    C::PaddedMatrices.AbstractMutableFixedSizeMatrix{N,N,T},
    D::PaddedMatrices.AbstractMutableFixedSizeMatrix{M,N,T}
) where {M,N,T,P}
    quote
        @boundscheck begin
            d = size(A,1)
            for p ∈ 2:$(P-1)
                d *= size(A,p)
            end
            d == M*N || PaddedMatrices.ThrowBoundsError(&quot;Earlier dims size(A) == $(size(A)) does not match size(D) == ($M,$N)&quot;)
        end
        ptr = pointer(sp, $T)
        E = PtrMatrix{$M,$N,$T,$M}( ptr )
        F = PtrMatrix{$M,$N,$T,$M}( ptr + $(sizeof(T) * M * N) )
        ptr_A = pointer(A)
        GC.@preserve A begin
            for n ∈ 0:size(A,$P)-1
                Aₙ = PtrMatrix{$M,$N,$T,$M}( ptr_A + n*$(sizeof(T)*M*N) )
                randn!(ProbabilityModels.GLOBAL_PCGs[1], E)
                mul!(F, B, E)
                Aₙ .= D
                PaddedMatrices.gemm!(Aₙ, F, C)
            end
        end
        sp
    end
end

sample_data( N, truth, missingness, missingvals = (Val{0}(),Val{0}()) ) = sample_data( N, truth, missingness, missingvals, truth.domains )
@generated function sample_data(
    N::Tuple{Int,Int},
	truth, missingness,
	::Tuple{Val{M1},Val{M2}},
	::ProbabilityModels.Domains{S}
) where {S,M1,M2}
    K = sum(S)
    D = length(S)
    quote
        N₁, N₂ = N
        L_T = truth.L_T
        U_K = truth.U_K
        T = size(L_T,1)

        sp = ProbabilityModels.STACK_POINTER_REF[]
        (sp,Y₁) = PaddedMatrices.DynamicPtrArray{Float64,3}(sp, (T, $K, N₁), T)
        (sp,Y₂) = PaddedMatrices.DynamicPtrArray{Float64,3}(sp, (T, $K, N₂), T)
        randomize!(sp, Y₁, L_T, U_K, truth.θ₁)
        randomize!(sp, Y₂, L_T, U_K, truth.θ₂)
        
        c = length(missingness.indices)
        inds = missingness.indices
        
        Y₁sub = reshape(Y₁, (T * $K, N₁))[inds, :]
        $(M1 &gt; 0 ? quote
            Y₁union = Array{Union{Missing,Float64}}(Y₁sub)
            perm = randperm(length(Y₁sub))
            @inbounds for m in 1:$M1
                Y₁union[perm[m]] = Base.missing
	    	end
    		Y₁ = convert(MissingDataArray{$M1,Bounds(-Inf,Inf)}, Y₁union)
		end : quote
			Y₁ = Y₁sub
		end)

        Y₂sub = reshape(Y₂, (T * $K, N₂))[inds, :]
		$(M2 &gt; 0 ? quote
            Y₂union = Array{Union{Missing,Float64}}(Y₂sub)
            perm = randperm(length(Y₂sub))
			@inbounds for m in 1:$M2
                Y₂union[perm[m]] = Base.missing
            end
            Y₂ = convert(MissingDataArray{$M2,Bounds(-Inf,Inf)},Y₂union)
		end : quote
		    Y₂ = Y₂sub
		end)
		
        ITPModel(
            domains = truth.domains,
	        AvailableData = missingness,
            Y₁ = Y₁,
            Y₂ = Y₂,
            time = truth.time,
            L = LKJCorrCholesky{$K},
            ρ = RealVector{$K,Bounds(0,1)},
            lκ = RealVector{$K},
            θ = RealVector{$K},
            μₕ₁ = RealFloat,
            μₕ₂ = RealFloat,
            μᵣ₁ = RealVector{$D},
            μᵣ₂ = RealVector{$D},
            βᵣ₁ = RealVector{$K},
            βᵣ₂ = RealVector{$K},
            σᵦ = RealFloat{Bounds(0,Inf)},
            σₕ = RealFloat{Bounds(0,Inf)},
            σ = RealVector{$K,Bounds(0,Inf)}
        )
    end
end

"><pre><span class="pl-k">function</span> <span class="pl-en">rinvscaledgamma</span>(<span class="pl-k">::</span><span class="pl-c1">Val{N}</span>,a<span class="pl-k">::</span><span class="pl-c1">T</span>,b<span class="pl-k">::</span><span class="pl-c1">T</span>,c<span class="pl-k">::</span><span class="pl-c1">T</span>) <span class="pl-k">where</span> {N,T}
    rg <span class="pl-k">=</span> <span class="pl-c1">MutableFixedSizeVector</span><span class="pl-c1">{N,T}</span>(undef)
    log100 <span class="pl-k">=</span> <span class="pl-c1">log</span>(<span class="pl-c1">100</span>)
    <span class="pl-c1">@inbounds</span> <span class="pl-k">for</span> n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span>N
        rg[n] <span class="pl-k">=</span> log100 <span class="pl-k">/</span> <span class="pl-c1">exp</span>(<span class="pl-c1">log</span>(VectorizedRNG<span class="pl-k">.</span><span class="pl-c1">randgamma</span>(a<span class="pl-k">/</span>c)) <span class="pl-k">/</span> c <span class="pl-k">+</span> b)
    <span class="pl-k">end</span>
    rg
<span class="pl-k">end</span>

<span class="pl-k">const</span> domains <span class="pl-k">=</span> ProbabilityModels<span class="pl-k">.</span><span class="pl-c1">Domains</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)

<span class="pl-k">const</span> n_endpoints <span class="pl-k">=</span> <span class="pl-c1">sum</span>(domains)

<span class="pl-k">const</span> times <span class="pl-k">=</span> <span class="pl-c1">MutableFixedSizeVector</span><span class="pl-c1">{36,Float64,36}</span>(undef); times <span class="pl-k">.=</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">35</span>;

structured_missing_pattern <span class="pl-k">=</span> <span class="pl-c1">push!</span>(<span class="pl-c1">vcat</span>(([<span class="pl-c1">1</span>,<span class="pl-c1">0</span>,<span class="pl-c1">0</span>,<span class="pl-c1">0</span>,<span class="pl-c1">0</span>] <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">7</span>)<span class="pl-k">...</span>), <span class="pl-c1">1</span>);
missing_pattern <span class="pl-k">=</span> <span class="pl-c1">vcat</span>(
    structured_missing_pattern, <span class="pl-c1">fill</span>(<span class="pl-c1">1</span>, <span class="pl-c1">4</span><span class="pl-c1">length</span>(times)), structured_missing_pattern, structured_missing_pattern
);

<span class="pl-k">const</span> availabledata <span class="pl-k">=</span> <span class="pl-c1">MissingDataVector</span><span class="pl-c1">{Float64}</span>(missing_pattern);

<span class="pl-k">const</span> κ₀ <span class="pl-k">=</span> (<span class="pl-c1">8.5</span>, <span class="pl-c1">1.5</span>, <span class="pl-c1">3.0</span>)

<span class="pl-en">AR1</span>(ρ, t) <span class="pl-k">=</span> <span class="pl-c1">@.</span> ρ <span class="pl-k">^</span> <span class="pl-c1">abs</span>(t <span class="pl-k">-</span> t<span class="pl-k">'</span>)


<span class="pl-k">function</span> <span class="pl-en">generate_true_parameters</span>(domains, times, κ₀)
    K <span class="pl-k">=</span> <span class="pl-c1">sum</span>(domains)
    D <span class="pl-k">=</span> <span class="pl-c1">length</span>(domains)
    T <span class="pl-k">=</span> <span class="pl-c1">length</span>(times)
    μ <span class="pl-k">=</span> <span class="pl-c1">MutableFixedSizeVector</span><span class="pl-c1">{K,Float64}</span>(undef)
    offset <span class="pl-k">=</span> <span class="pl-c1">0</span>
    <span class="pl-k">for</span> i <span class="pl-k">∈</span> domains
        domain_mean <span class="pl-k">=</span> <span class="pl-c1">5</span><span class="pl-c1">randn</span>()
        <span class="pl-k">for</span> j <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">+</span>offset<span class="pl-k">:</span>i<span class="pl-k">+</span>offset
            μ[j] <span class="pl-k">=</span> domain_mean <span class="pl-k">+</span> <span class="pl-c1">5</span><span class="pl-c1">randn</span>()
        <span class="pl-k">end</span>
        offset <span class="pl-k">+=</span> i
    <span class="pl-k">end</span>
    σ <span class="pl-k">=</span> VectorizedRNG<span class="pl-k">.</span><span class="pl-c1">randgamma</span>( <span class="pl-c1">6.0</span>, <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">6.0</span>)
    ρ <span class="pl-k">=</span> VectorizedRNG<span class="pl-k">.</span><span class="pl-c1">randbeta</span>(<span class="pl-c1">4.0</span>,<span class="pl-c1">4.0</span>)
    κ <span class="pl-k">=</span> <span class="pl-c1">rinvscaledgamma</span>(<span class="pl-c1">Val</span>(K), κ₀<span class="pl-k">...</span>)
    lt <span class="pl-k">=</span> <span class="pl-c1">last</span>(times)
    β <span class="pl-k">=</span> <span class="pl-c1">MutableFixedSizeVector</span><span class="pl-c1">{7,Float64,7}</span>(( <span class="pl-c1">0.0625</span>,  <span class="pl-c1">0.0575</span>,  <span class="pl-c1">0.0525</span>,  <span class="pl-c1">0.0475</span>,  <span class="pl-c1">0.0425</span>,  <span class="pl-c1">0.04</span>,  <span class="pl-c1">0.0375</span>))
    θ₁ <span class="pl-k">=</span> <span class="pl-c1">@.</span> μ<span class="pl-k">'</span> <span class="pl-k">-</span> β<span class="pl-k">'</span> <span class="pl-k">*</span> ( <span class="pl-c1">1.0</span> <span class="pl-k">-</span> <span class="pl-c1">exp</span>( <span class="pl-k">-</span> κ<span class="pl-k">'</span> <span class="pl-k">*</span> times) ) <span class="pl-k">/</span> (<span class="pl-c1">1.0</span> <span class="pl-k">-</span> <span class="pl-c1">exp</span>( <span class="pl-k">-</span> κ<span class="pl-k">'</span> <span class="pl-k">*</span> lt) ) 
    θ₂ <span class="pl-k">=</span> <span class="pl-c1">@.</span> μ<span class="pl-k">'</span> <span class="pl-k">+</span> β<span class="pl-k">'</span> <span class="pl-k">*</span> ( <span class="pl-c1">1.0</span> <span class="pl-k">-</span> <span class="pl-c1">exp</span>( <span class="pl-k">-</span> κ<span class="pl-k">'</span> <span class="pl-k">*</span> times) ) <span class="pl-k">/</span> (<span class="pl-c1">1.0</span> <span class="pl-k">-</span> <span class="pl-c1">exp</span>( <span class="pl-k">-</span> κ<span class="pl-k">'</span> <span class="pl-k">*</span> lt) ) 
    
    L_T, info <span class="pl-k">=</span> LAPACK<span class="pl-k">.</span><span class="pl-c1">potrf!</span>(<span class="pl-s"><span class="pl-pds">'</span>L<span class="pl-pds">'</span></span>, <span class="pl-c1">AR1</span>(ρ, times))
    <span class="pl-c1">@inbounds</span> <span class="pl-k">for</span> tc <span class="pl-k">∈</span> <span class="pl-c1">2</span><span class="pl-k">:</span>T, tr <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span>tc<span class="pl-k">-</span><span class="pl-c1">1</span>
        L_T[tr,tc] <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
    <span class="pl-k">end</span>
    X <span class="pl-k">=</span> PaddedMatrices<span class="pl-k">.</span><span class="pl-c1">MutableFixedSizeMatrix</span><span class="pl-c1">{K,K+3,Float64,K}</span>(undef); <span class="pl-c1">randn!</span>(X)
    U_K, info <span class="pl-k">=</span> LAPACK<span class="pl-k">.</span><span class="pl-c1">potrf!</span>(<span class="pl-s"><span class="pl-pds">'</span>U<span class="pl-pds">'</span></span>, BLAS<span class="pl-k">.</span><span class="pl-c1">syrk!</span>(<span class="pl-s"><span class="pl-pds">'</span>U<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>N<span class="pl-pds">'</span></span>, σ, X, <span class="pl-c1">0.0</span>, <span class="pl-c1">zero</span>(MutableFixedSizeMatrix{K,K,Float64,K})))
    (
        U_K <span class="pl-k">=</span> U_K, L_T <span class="pl-k">=</span> L_T, μ <span class="pl-k">=</span> μ, θ₁ <span class="pl-k">=</span> θ₁, θ₂ <span class="pl-k">=</span> θ₂, domains <span class="pl-k">=</span> domains, time <span class="pl-k">=</span> times
    )
<span class="pl-k">end</span>

<span class="pl-c1">@generated</span> <span class="pl-k">function</span> <span class="pl-en">randomize!</span>(
    sp<span class="pl-k">::</span><span class="pl-c1">PaddedMatrices.StackPointer</span>,
    A<span class="pl-k">::</span><span class="pl-c1">AbstractArray{T,P}</span>,
    B<span class="pl-k">::</span><span class="pl-c1">PaddedMatrices.AbstractMutableFixedSizeMatrix{M,M,T}</span>,
    C<span class="pl-k">::</span><span class="pl-c1">PaddedMatrices.AbstractMutableFixedSizeMatrix{N,N,T}</span>,
    D<span class="pl-k">::</span><span class="pl-c1">PaddedMatrices.AbstractMutableFixedSizeMatrix{M,N,T}</span>
) <span class="pl-k">where</span> {M,N,T,P}
    <span class="pl-k">quote</span>
        <span class="pl-c1">@boundscheck</span> <span class="pl-k">begin</span>
            d <span class="pl-k">=</span> <span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>)
            <span class="pl-k">for</span> p <span class="pl-k">∈</span> <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-k">$</span>(P<span class="pl-k">-</span><span class="pl-c1">1</span>)
                d <span class="pl-k">*=</span> <span class="pl-c1">size</span>(A,p)
            <span class="pl-k">end</span>
            d <span class="pl-k">==</span> M<span class="pl-k">*</span>N <span class="pl-k">||</span> PaddedMatrices<span class="pl-k">.</span><span class="pl-c1">ThrowBoundsError</span>(<span class="pl-s"><span class="pl-pds">"</span>Earlier dims size(A) == <span class="pl-v">$(<span class="pl-c1">size</span>(A))</span> does not match size(D) == (<span class="pl-v">$M</span>,<span class="pl-v">$N</span>)<span class="pl-pds">"</span></span>)
        <span class="pl-k">end</span>
        ptr <span class="pl-k">=</span> <span class="pl-c1">pointer</span>(sp, <span class="pl-k">$</span>T)
        E <span class="pl-k">=</span> <span class="pl-c1">PtrMatrix</span><span class="pl-c1">{$M,$N,$T,$M}</span>( ptr )
        F <span class="pl-k">=</span> <span class="pl-c1">PtrMatrix</span><span class="pl-c1">{$M,$N,$T,$M}</span>( ptr <span class="pl-k">+</span> <span class="pl-k">$</span>(<span class="pl-c1">sizeof</span>(T) <span class="pl-k">*</span> M <span class="pl-k">*</span> N) )
        ptr_A <span class="pl-k">=</span> <span class="pl-c1">pointer</span>(A)
        GC<span class="pl-k">.</span><span class="pl-c1">@preserve</span> A <span class="pl-k">begin</span>
            <span class="pl-k">for</span> n <span class="pl-k">∈</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-k">$</span>P)<span class="pl-k">-</span><span class="pl-c1">1</span>
                Aₙ <span class="pl-k">=</span> <span class="pl-c1">PtrMatrix</span><span class="pl-c1">{$M,$N,$T,$M}</span>( ptr_A <span class="pl-k">+</span> n<span class="pl-k">*</span><span class="pl-k">$</span>(<span class="pl-c1">sizeof</span>(T)<span class="pl-k">*</span>M<span class="pl-k">*</span>N) )
                <span class="pl-c1">randn!</span>(ProbabilityModels<span class="pl-k">.</span>GLOBAL_PCGs[<span class="pl-c1">1</span>], E)
                <span class="pl-c1">mul!</span>(F, B, E)
                Aₙ <span class="pl-k">.=</span> D
                PaddedMatrices<span class="pl-k">.</span><span class="pl-c1">gemm!</span>(Aₙ, F, C)
            <span class="pl-k">end</span>
        <span class="pl-k">end</span>
        sp
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-en">sample_data</span>( N, truth, missingness, missingvals <span class="pl-k">=</span> (<span class="pl-en">Val</span><span class="pl-c1">{0}</span>(),<span class="pl-en">Val</span><span class="pl-c1">{0}</span>()) ) <span class="pl-k">=</span> <span class="pl-c1">sample_data</span>( N, truth, missingness, missingvals, truth<span class="pl-k">.</span>domains )
<span class="pl-c1">@generated</span> <span class="pl-k">function</span> <span class="pl-en">sample_data</span>(
    N<span class="pl-k">::</span><span class="pl-c1">Tuple{Int,Int}</span>,
	truth, missingness,
	<span class="pl-k">::</span><span class="pl-c1">Tuple{Val{M1},Val{M2}}</span>,
	<span class="pl-k">::</span><span class="pl-c1">ProbabilityModels.Domains{S}</span>
) <span class="pl-k">where</span> {S,M1,M2}
    K <span class="pl-k">=</span> <span class="pl-c1">sum</span>(S)
    D <span class="pl-k">=</span> <span class="pl-c1">length</span>(S)
    <span class="pl-k">quote</span>
        N₁, N₂ <span class="pl-k">=</span> N
        L_T <span class="pl-k">=</span> truth<span class="pl-k">.</span>L_T
        U_K <span class="pl-k">=</span> truth<span class="pl-k">.</span>U_K
        T <span class="pl-k">=</span> <span class="pl-c1">size</span>(L_T,<span class="pl-c1">1</span>)

        sp <span class="pl-k">=</span> ProbabilityModels<span class="pl-k">.</span>STACK_POINTER_REF[]
        (sp,Y₁) <span class="pl-k">=</span> PaddedMatrices<span class="pl-k">.</span><span class="pl-c1">DynamicPtrArray</span><span class="pl-c1">{Float64,3}</span>(sp, (T, <span class="pl-k">$</span>K, N₁), T)
        (sp,Y₂) <span class="pl-k">=</span> PaddedMatrices<span class="pl-k">.</span><span class="pl-c1">DynamicPtrArray</span><span class="pl-c1">{Float64,3}</span>(sp, (T, <span class="pl-k">$</span>K, N₂), T)
        <span class="pl-c1">randomize!</span>(sp, Y₁, L_T, U_K, truth<span class="pl-k">.</span>θ₁)
        <span class="pl-c1">randomize!</span>(sp, Y₂, L_T, U_K, truth<span class="pl-k">.</span>θ₂)
        
        c <span class="pl-k">=</span> <span class="pl-c1">length</span>(missingness<span class="pl-k">.</span>indices)
        inds <span class="pl-k">=</span> missingness<span class="pl-k">.</span>indices
        
        Y₁sub <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(Y₁, (T <span class="pl-k">*</span> <span class="pl-k">$</span>K, N₁))[inds, :]
        <span class="pl-k">$</span>(M1 <span class="pl-k">&gt;</span> <span class="pl-c1">0</span> <span class="pl-k">?</span> <span class="pl-k">quote</span>
            Y₁union <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{Union{Missing,Float64}}</span>(Y₁sub)
            perm <span class="pl-k">=</span> <span class="pl-c1">randperm</span>(<span class="pl-c1">length</span>(Y₁sub))
            <span class="pl-c1">@inbounds</span> <span class="pl-k">for</span> m <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-k">$</span>M1
                Y₁union[perm[m]] <span class="pl-k">=</span> Base<span class="pl-k">.</span><span class="pl-c1">missing</span>
	    	<span class="pl-k">end</span>
    		Y₁ <span class="pl-k">=</span> <span class="pl-c1">convert</span>(MissingDataArray{<span class="pl-k">$</span>M1,<span class="pl-c1">Bounds</span>(<span class="pl-k">-</span><span class="pl-c1">Inf</span>,<span class="pl-c1">Inf</span>)}, Y₁union)
		<span class="pl-k">end</span> <span class="pl-k">:</span> <span class="pl-k">quote</span>
			Y₁ <span class="pl-k">=</span> Y₁sub
		<span class="pl-k">end</span>)

        Y₂sub <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(Y₂, (T <span class="pl-k">*</span> <span class="pl-k">$</span>K, N₂))[inds, :]
		<span class="pl-k">$</span>(M2 <span class="pl-k">&gt;</span> <span class="pl-c1">0</span> <span class="pl-k">?</span> <span class="pl-k">quote</span>
            Y₂union <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{Union{Missing,Float64}}</span>(Y₂sub)
            perm <span class="pl-k">=</span> <span class="pl-c1">randperm</span>(<span class="pl-c1">length</span>(Y₂sub))
			<span class="pl-c1">@inbounds</span> <span class="pl-k">for</span> m <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-k">$</span>M2
                Y₂union[perm[m]] <span class="pl-k">=</span> Base<span class="pl-k">.</span><span class="pl-c1">missing</span>
            <span class="pl-k">end</span>
            Y₂ <span class="pl-k">=</span> <span class="pl-c1">convert</span>(MissingDataArray{<span class="pl-k">$</span>M2,<span class="pl-c1">Bounds</span>(<span class="pl-k">-</span><span class="pl-c1">Inf</span>,<span class="pl-c1">Inf</span>)},Y₂union)
		<span class="pl-k">end</span> <span class="pl-k">:</span> <span class="pl-k">quote</span>
		    Y₂ <span class="pl-k">=</span> Y₂sub
		<span class="pl-k">end</span>)
		
        <span class="pl-c1">ITPModel</span>(
            domains <span class="pl-k">=</span> truth<span class="pl-k">.</span>domains,
	        AvailableData <span class="pl-k">=</span> missingness,
            Y₁ <span class="pl-k">=</span> Y₁,
            Y₂ <span class="pl-k">=</span> Y₂,
            time <span class="pl-k">=</span> truth<span class="pl-k">.</span>time,
            L <span class="pl-k">=</span> LKJCorrCholesky{<span class="pl-k">$</span>K},
            ρ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>K,<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">1</span>)},
            lκ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>K},
            θ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>K},
            μₕ₁ <span class="pl-k">=</span> RealFloat,
            μₕ₂ <span class="pl-k">=</span> RealFloat,
            μᵣ₁ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>D},
            μᵣ₂ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>D},
            βᵣ₁ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>K},
            βᵣ₂ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>K},
            σᵦ <span class="pl-k">=</span> RealFloat{<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">Inf</span>)},
            σₕ <span class="pl-k">=</span> RealFloat{<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">Inf</span>)},
            σ <span class="pl-k">=</span> RealVector{<span class="pl-k">$</span>K,<span class="pl-c1">Bounds</span>(<span class="pl-c1">0</span>,<span class="pl-c1">Inf</span>)}
        )
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
</pre></div>
<p>The library <a href="https://github.com/chriselrod/DistributionParameters.jl">DistributionParameters.jl</a> provides a variety of parameter types.
These types define constrianing transformations, to transform an unconstrained parameter vector and add the appropriate jacobians.</p>
<p>All parameters are typed by size. The library currently provides a DynamicHMC interface, defining <code>logdensity(::Value,::ITPModel)</code> and <code>logdensity(::ValueGradient,::ITPModel)</code> methods.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="

"><pre></pre></div>
<p>For comparison, a Stan implementation of this model takes about to 13ms to evaluate the gradient.</p>
<p>The <code>@model</code> macro also defines a helper function for <a href="https://mc-stan.org/docs/2_19/reference-manual/variable-transforms-chapter.html" rel="nofollow">constraining</a> unconstrained parameter vectors:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="

"><pre></pre></div>
<p>So you can use this to constrain the unconstrained parameter vectors <code>DynamicHMC</code> sampled and proceed with your convergence assessments and posterior analysis as normal.</p>
<p>Alternatively, it also supports <a href="https://github.com/chriselrod/MCMCChainSummaries.jl">MCMCChainSummaries.jl</a>, constraining the parameters for you and providing posterior summaries as well as plotting methods:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="
"><pre></pre></div>
<p>The difference in time between the slowest chain (748 s) and the overall time (775 s) roughly yields the compilation time. If we refit the model, the total time would roughly equal the length of time to sample from the slowest chain.</p>
<p>If you don't pass the sampler a tuner object, it'll create one by default with <code>M=5</code> and <code>term=50</code>. The section on <a href="https://mc-stan.org/docs/2_19/reference-manual/hmc-algorithm-parameters.html" rel="nofollow">Automatic Parameter Tuning</a> in the Stan reference manual explains what these mean. I added an extra slow adaptation step (with twice the length of the previous step) and doubled the length of the terminal adaptation window (a final window calculating step size), as this makes adaptation more consistent.
The idea is to try and decrease the probability of one or two chains being much slower than the others. We can look at NUTS statistics of the chains:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; NUTS_statistics.(chains)
18-element Array{DynamicHMC.NUTS_Statistics{Float64,DataStructures.Accumulator{DynamicHMC.Termination,Int64},DataStructures.Accumulator{Int64,Int64}},1}:
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.94, min/25%/median/75%/max: 0.0 0.94 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 6% DoubledTurn =&gt; 93%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 13% 6 =&gt; 87%
                         
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.96 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 0% DoubledTurn =&gt; 100%
  depth: 4 =&gt; 0% 5 =&gt; 1% 6 =&gt; 99% 7 =&gt; 0%
                                 
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.96 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 0% DoubledTurn =&gt; 99%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 1% 6 =&gt; 99% 7 =&gt; 0%
                  
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.97 0.99 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 0% DoubledTurn =&gt; 99%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 0% 6 =&gt; 100% 7 =&gt; 0%
                 
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.98, min/25%/median/75%/max: 0.0 0.98 0.99 1.0 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 17% DoubledTurn =&gt; 83%
  depth: 4 =&gt; 0% 5 =&gt; 0% 6 =&gt; 93% 7 =&gt; 7% 8 =&gt; 0%
                          
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.97 0.99 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 0% DoubledTurn =&gt; 100%
  depth: 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 0% 6 =&gt; 100% 7 =&gt; 0%
                        
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.94, min/25%/median/75%/max: 0.0 0.95 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 4% DoubledTurn =&gt; 96%
  depth: 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 8% 6 =&gt; 92% 7 =&gt; 0%
                          
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.95, min/25%/median/75%/max: 0.0 0.96 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 1% DoubledTurn =&gt; 99%
  depth: 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 1% 6 =&gt; 99% 7 =&gt; 0%
                          
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.94, min/25%/median/75%/max: 0.0 0.95 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 3% DoubledTurn =&gt; 97%
  depth: 1 =&gt; 0% 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 6% 6 =&gt; 94% 7 =&gt; 0%
          
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.94, min/25%/median/75%/max: 0.0 0.94 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 2% DoubledTurn =&gt; 98%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 4% 6 =&gt; 96% 8 =&gt; 0%
                  
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.91, min/25%/median/75%/max: 0.0 0.92 0.97 0.99 1.0
  termination: AdjacentDivergent =&gt; 1% AdjacentTurn =&gt; 12% DoubledTurn =&gt; 87%
  depth: 1 =&gt; 0% 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 28% 6 =&gt; 71% 7 =&gt; 0%
        
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.96 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 0% DoubledTurn =&gt; 100%
  depth: 1 =&gt; 0% 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 1% 6 =&gt; 99% 7 =&gt; 0% 8 =&gt; 0%
 
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.96 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 0% DoubledTurn =&gt; 100%
  depth: 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 1% 6 =&gt; 99% 7 =&gt; 0% 8 =&gt; 0%
                 
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.94, min/25%/median/75%/max: 0.0 0.94 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 4% DoubledTurn =&gt; 96%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 8% 6 =&gt; 92% 7 =&gt; 0%
                  
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.95, min/25%/median/75%/max: 0.0 0.95 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 1% DoubledTurn =&gt; 99%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 3% 6 =&gt; 97% 7 =&gt; 0%
                  
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.96, min/25%/median/75%/max: 0.0 0.96 0.98 0.99 1.0
  termination: AdjacentDivergent =&gt; 0% AdjacentTurn =&gt; 1% DoubledTurn =&gt; 99%
  depth: 2 =&gt; 0% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 1% 6 =&gt; 99% 7 =&gt; 0% 8 =&gt; 0%
          
 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.91, min/25%/median/75%/max: 0.0 0.92 0.97 0.99 1.0
  termination: AdjacentDivergent =&gt; 1% AdjacentTurn =&gt; 16% DoubledTurn =&gt; 83%
  depth: 1 =&gt; 0% 2 =&gt; 1% 3 =&gt; 0% 4 =&gt; 1% 5 =&gt; 39% 6 =&gt; 60% 7 =&gt; 0% 8 =&gt; 0%

 Hamiltonian Monte Carlo sample of length 10000
  acceptance rate mean: 0.91, min/25%/median/75%/max: 0.0 0.92 0.97 0.99 1.0
  termination: AdjacentDivergent =&gt; 2% AdjacentTurn =&gt; 9% DoubledTurn =&gt; 89%
  depth: 1 =&gt; 0% 2 =&gt; 1% 3 =&gt; 0% 4 =&gt; 0% 5 =&gt; 20% 6 =&gt; 78% 7 =&gt; 0%
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">NUTS_statistics</span>.(chains)
<span class="pl-c1">18</span><span class="pl-k">-</span>element Array{DynamicHMC<span class="pl-k">.</span>NUTS_Statistics{Float64,DataStructures<span class="pl-k">.</span>Accumulator{DynamicHMC<span class="pl-k">.</span>Termination,Int64},DataStructures<span class="pl-k">.</span>Accumulator{Int64,Int64}},<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.94</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.94</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">6</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">93</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">13</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">87</span><span class="pl-k">%</span>
                         
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.96</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">100</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                                 
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.96</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                  
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.97</span> <span class="pl-c1">0.99</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">100</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                 
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.98</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">17</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">83</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">93</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">7</span><span class="pl-k">%</span> <span class="pl-c1">8</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                          
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.97</span> <span class="pl-c1">0.99</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">100</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">100</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                        
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.94</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.95</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">4</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">96</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">8</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">92</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                          
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.95</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.96</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                          
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.94</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.95</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">3</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">97</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">6</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">94</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
          
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.94</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.94</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">98</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">4</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">96</span><span class="pl-k">%</span> <span class="pl-c1">8</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                  
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.91</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.92</span> <span class="pl-c1">0.97</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">12</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">87</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">28</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">71</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
        
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.96</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">100</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">8</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
 
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.96</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">100</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">8</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                 
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.94</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.94</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">4</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">96</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">8</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">92</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                  
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.95</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.95</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">3</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">97</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
                  
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.96</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.96</span> <span class="pl-c1">0.98</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">99</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">8</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>
          
 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.91</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.92</span> <span class="pl-c1">0.97</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">16</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">83</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">39</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">60</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">8</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span>

 Hamiltonian Monte Carlo sample of length <span class="pl-c1">10000</span>
  acceptance rate mean<span class="pl-k">:</span> <span class="pl-c1">0.91</span>, min<span class="pl-k">/</span><span class="pl-c1">25</span><span class="pl-k">%/</span>median<span class="pl-k">/</span><span class="pl-c1">75</span><span class="pl-k">%/</span>max<span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-c1">0.92</span> <span class="pl-c1">0.97</span> <span class="pl-c1">0.99</span> <span class="pl-c1">1.0</span>
  termination<span class="pl-k">:</span> AdjacentDivergent <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span><span class="pl-k">%</span> AdjacentTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">9</span><span class="pl-k">%</span> DoubledTurn <span class="pl-k">=&gt;</span> <span class="pl-c1">89</span><span class="pl-k">%</span>
  depth<span class="pl-k">:</span> <span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">2</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span><span class="pl-k">%</span> <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">4</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">20</span><span class="pl-k">%</span> <span class="pl-c1">6</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">78</span><span class="pl-k">%</span> <span class="pl-c1">7</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span><span class="pl-k">%</span></pre></div>
<p>and see that the fifth chain had an acceptance rate of 0.98, vs the target acceptance rate of <code>δ = 0.95</code>. 7% of samples hit a treedepth of 7, meaning this was probably the slowest chain (worker 15). Maybe increasing term further would help.</p>
<p><em>Overview of how the library works.</em></p>
<p>The <code>ITPModel</code> struct (or whatever you've named your model, with the first argument to the macro) is defined as a struct with a field for each unknown. Each field is parametrically typed.</p>
<p><code>logdensity</code> and <code>constrain</code> are defined as generated functions, so that they can compile appropriate code given these parameteric types. The <code>@model</code> macro does a little preprocessing of the expression; most of the code generation occurs within these generated functions.</p>
<p>The macro's preprocessing consists of simply translating the sampling statements into log probability increments (<code>target</code>, terminology taken from the <a href="https://mc-stan.org/users/documentation/" rel="nofollow">Stan</a> language), and lowering the expression.</p>
<p>We can manually perform these passes on the expression (note that variables with <code>#</code> in their names are NOT comments -- they're hygienic names, guaranteeing all the variables I add don't clash with any of the model's variables):</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="itp_q = quote
    # Non-hierarchical Priors
    ρ ~ Beta(3, 1)
    κ ~ Gamma(0.1, 0.1) # μ = 1, σ² = 10
    σ ~ Gamma(1.5, 0.25) # μ = 6, σ² = 2.4
    θ ~ Normal(10)
    L ~ LKJ(2.0)

    # Hierarchical Priors.
    # h subscript, for highest in the hierarhcy.
    μₕ₁ ~ Normal(10) # μ = 0
    μₕ₂ ~ Normal(10) # μ = 0
    σₕ ~ Normal(10) # μ = 0
    # Raw μs; non-cenetered parameterization
    μᵣ₁ ~ Normal() # μ = 0, σ = 1
    μᵣ₂ ~ Normal() # μ = 0, σ = 1
    # Center the μs
    μᵦ₁ = HierarchicalCentering(μᵣ₁, μₕ₁, σₕ)
    μᵦ₂ = HierarchicalCentering(μᵣ₂, μₕ₂, σₕ)
    σᵦ ~ Normal(10) # μ = 0
    # Raw βs; non-cenetered parameterization
    βᵣ₁ ~ Normal()
    βᵣ₂ ~ Normal()
    # Center the βs.
    β₁ = HierarchicalCentering(βᵣ₁, μᵦ₁, σᵦ, domains)
    β₂ = HierarchicalCentering(βᵣ₂, μᵦ₂, σᵦ, domains)

    # Likelihood
    μ₁ = vec(ITPExpectedValue(time, β₁, κ, θ))
    μ₂ = vec(ITPExpectedValue(time, β₂, κ, θ))
    Σ = CovarianceMatrix(ρ, Diagonal(σ) * L, time)

    (Y₁, Y₂) ~ Normal((μ₁, μ₂)[AvailableData], Σ[AvailableData])

end

itp_preprocessed = itp_q |&gt; ProbabilityModels.translate_sampling_statements |&gt; ProbabilityModels.flatten_expression;

using MacroTools: striplines

striplines(itp_preprocessed)
"><pre>itp_q <span class="pl-k">=</span> <span class="pl-k">quote</span>
    <span class="pl-c"><span class="pl-c">#</span> Non-hierarchical Priors</span>
    ρ <span class="pl-k">~</span> <span class="pl-c1">Beta</span>(<span class="pl-c1">3</span>, <span class="pl-c1">1</span>)
    κ <span class="pl-k">~</span> <span class="pl-c1">Gamma</span>(<span class="pl-c1">0.1</span>, <span class="pl-c1">0.1</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 1, σ² = 10</span>
    σ <span class="pl-k">~</span> <span class="pl-c1">Gamma</span>(<span class="pl-c1">1.5</span>, <span class="pl-c1">0.25</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 6, σ² = 2.4</span>
    θ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>)
    L <span class="pl-k">~</span> <span class="pl-c1">LKJ</span>(<span class="pl-c1">2.0</span>)

    <span class="pl-c"><span class="pl-c">#</span> Hierarchical Priors.</span>
    <span class="pl-c"><span class="pl-c">#</span> h subscript, for highest in the hierarhcy.</span>
    μₕ₁ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    μₕ₂ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    σₕ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    <span class="pl-c"><span class="pl-c">#</span> Raw μs; non-cenetered parameterization</span>
    μᵣ₁ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>() <span class="pl-c"><span class="pl-c">#</span> μ = 0, σ = 1</span>
    μᵣ₂ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>() <span class="pl-c"><span class="pl-c">#</span> μ = 0, σ = 1</span>
    <span class="pl-c"><span class="pl-c">#</span> Center the μs</span>
    μᵦ₁ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(μᵣ₁, μₕ₁, σₕ)
    μᵦ₂ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(μᵣ₂, μₕ₂, σₕ)
    σᵦ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">10</span>) <span class="pl-c"><span class="pl-c">#</span> μ = 0</span>
    <span class="pl-c"><span class="pl-c">#</span> Raw βs; non-cenetered parameterization</span>
    βᵣ₁ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>()
    βᵣ₂ <span class="pl-k">~</span> <span class="pl-c1">Normal</span>()
    <span class="pl-c"><span class="pl-c">#</span> Center the βs.</span>
    β₁ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(βᵣ₁, μᵦ₁, σᵦ, domains)
    β₂ <span class="pl-k">=</span> <span class="pl-c1">HierarchicalCentering</span>(βᵣ₂, μᵦ₂, σᵦ, domains)

    <span class="pl-c"><span class="pl-c">#</span> Likelihood</span>
    μ₁ <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">ITPExpectedValue</span>(time, β₁, κ, θ))
    μ₂ <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">ITPExpectedValue</span>(time, β₂, κ, θ))
    Σ <span class="pl-k">=</span> <span class="pl-c1">CovarianceMatrix</span>(ρ, <span class="pl-c1">Diagonal</span>(σ) <span class="pl-k">*</span> L, time)

    (Y₁, Y₂) <span class="pl-k">~</span> <span class="pl-c1">Normal</span>((μ₁, μ₂)[AvailableData], Σ[AvailableData])

<span class="pl-k">end</span>

itp_preprocessed <span class="pl-k">=</span> itp_q <span class="pl-k">|&gt;</span> ProbabilityModels<span class="pl-k">.</span>translate_sampling_statements <span class="pl-k">|&gt;</span> ProbabilityModels<span class="pl-k">.</span>flatten_expression;

<span class="pl-k">using</span> MacroTools<span class="pl-k">:</span> striplines

<span class="pl-c1">striplines</span>(itp_preprocessed)</pre></div>
<p>This yields the following expression:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="quote
    ##SSAValue##1## = Beta(ρ, 3, 1)
    ##SSAValue##2## = vadd(target, ##SSAValue##1##)
    target = ##SSAValue##2##
    ##SSAValue##4## = Gamma(κ, 0.1, 0.1)
    ##SSAValue##5## = vadd(target, ##SSAValue##4##)
    target = ##SSAValue##5##
    ##SSAValue##7## = Gamma(σ, 1.5, 0.25)
    ##SSAValue##8## = vadd(target, ##SSAValue##7##)
    target = ##SSAValue##8##
    ##SSAValue##10## = Normal(θ, 10)
    ##SSAValue##11## = vadd(target, ##SSAValue##10##)
    target = ##SSAValue##11##
    ##SSAValue##13## = LKJ(L, 2.0)
    ##SSAValue##14## = vadd(target, ##SSAValue##13##)
    target = ##SSAValue##14##
    ##SSAValue##16## = Normal(μₕ₁, 10)
    ##SSAValue##17## = vadd(target, ##SSAValue##16##)
    target = ##SSAValue##17##
    ##SSAValue##19## = Normal(μₕ₂, 10)
    ##SSAValue##20## = vadd(target, ##SSAValue##19##)
    target = ##SSAValue##20##
    ##SSAValue##22## = Normal(σₕ, 10)
    ##SSAValue##23## = vadd(target, ##SSAValue##22##)
    target = ##SSAValue##23##
    ##SSAValue##25## = Normal(μᵣ₁)
    ##SSAValue##26## = vadd(target, ##SSAValue##25##)
    target = ##SSAValue##26##
    ##SSAValue##28## = Normal(μᵣ₂)
    ##SSAValue##29## = vadd(target, ##SSAValue##28##)
    target = ##SSAValue##29##
    ##SSAValue##31## = HierarchicalCentering(μᵣ₁, μₕ₁, σₕ)
    μᵦ₁ = ##SSAValue##31##
    ##SSAValue##33## = HierarchicalCentering(μᵣ₂, μₕ₂, σₕ)
    μᵦ₂ = ##SSAValue##33##
    ##SSAValue##35## = Normal(σᵦ, 10)
    ##SSAValue##36## = vadd(target, ##SSAValue##35##)
    target = ##SSAValue##36##
    ##SSAValue##38## = Normal(βᵣ₁)
    ##SSAValue##39## = vadd(target, ##SSAValue##38##)
    target = ##SSAValue##39##
    ##SSAValue##41## = Normal(βᵣ₂)
    ##SSAValue##42## = vadd(target, ##SSAValue##41##)
    target = ##SSAValue##42##
    ##SSAValue##44## = HierarchicalCentering(βᵣ₁, μᵦ₁, σᵦ, domains)
    β₁ = ##SSAValue##44##
    ##SSAValue##46## = HierarchicalCentering(βᵣ₂, μᵦ₂, σᵦ, domains)
    β₂ = ##SSAValue##46##
    ##SSAValue##48## = ITPExpectedValue(time, β₁, κ, θ)
    ##SSAValue##49## = vec(##SSAValue##48##)
    μ₁ = ##SSAValue##49##
    ##SSAValue##51## = ITPExpectedValue(time, β₂, κ, θ)
    ##SSAValue##52## = vec(##SSAValue##51##)
    μ₂ = ##SSAValue##52##
    ##SSAValue##54## = Diagonal(σ)
    ##SSAValue##55## = ##SSAValue##54## * L
    ##SSAValue##56## = CovarianceMatrix(ρ, ##SSAValue##55##, time)
    Σ = ##SSAValue##56##
    ##SSAValue##58## = Core.tuple(Y₁, Y₂)
    ##SSAValue##59## = Core.tuple(μ₁, μ₂)
    ##SSAValue##60## = Base.getindex(##SSAValue##59##, AvailableData)
    ##SSAValue##61## = Base.getindex(Σ, AvailableData)
    ##SSAValue##62## = Normal(##SSAValue##58##, ##SSAValue##60##, ##SSAValue##61##)
    ##SSAValue##63## = vadd(target, ##SSAValue##62##)
    target = ##SSAValue##63##
end
"><pre><span class="pl-k">quote</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##1## = Beta(ρ, 3, 1)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##2## = vadd(target, ##SSAValue##1##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##2##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##4## = Gamma(κ, 0.1, 0.1)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##5## = vadd(target, ##SSAValue##4##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##5##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##7## = Gamma(σ, 1.5, 0.25)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##8## = vadd(target, ##SSAValue##7##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##8##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##10## = Normal(θ, 10)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##11## = vadd(target, ##SSAValue##10##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##11##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##13## = LKJ(L, 2.0)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##14## = vadd(target, ##SSAValue##13##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##14##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##16## = Normal(μₕ₁, 10)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##17## = vadd(target, ##SSAValue##16##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##17##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##19## = Normal(μₕ₂, 10)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##20## = vadd(target, ##SSAValue##19##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##20##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##22## = Normal(σₕ, 10)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##23## = vadd(target, ##SSAValue##22##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##23##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##25## = Normal(μᵣ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##26## = vadd(target, ##SSAValue##25##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##26##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##28## = Normal(μᵣ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##29## = vadd(target, ##SSAValue##28##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##29##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##31## = HierarchicalCentering(μᵣ₁, μₕ₁, σₕ)</span>
    μᵦ₁ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##31##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##33## = HierarchicalCentering(μᵣ₂, μₕ₂, σₕ)</span>
    μᵦ₂ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##33##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##35## = Normal(σᵦ, 10)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##36## = vadd(target, ##SSAValue##35##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##36##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##38## = Normal(βᵣ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##39## = vadd(target, ##SSAValue##38##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##39##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##41## = Normal(βᵣ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##42## = vadd(target, ##SSAValue##41##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##42##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##44## = HierarchicalCentering(βᵣ₁, μᵦ₁, σᵦ, domains)</span>
    β₁ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##44##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##46## = HierarchicalCentering(βᵣ₂, μᵦ₂, σᵦ, domains)</span>
    β₂ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##46##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##48## = ITPExpectedValue(time, β₁, κ, θ)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##49## = vec(##SSAValue##48##)</span>
    μ₁ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##49##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##51## = ITPExpectedValue(time, β₂, κ, θ)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##52## = vec(##SSAValue##51##)</span>
    μ₂ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##52##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##54## = Diagonal(σ)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##55## = ##SSAValue##54## * L</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##56## = CovarianceMatrix(ρ, ##SSAValue##55##, time)</span>
    Σ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##56##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##58## = Core.tuple(Y₁, Y₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##59## = Core.tuple(μ₁, μ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##60## = Base.getindex(##SSAValue##59##, AvailableData)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##61## = Base.getindex(Σ, AvailableData)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##62## = Normal(##SSAValue##58##, ##SSAValue##60##, ##SSAValue##61##)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##63## = vadd(target, ##SSAValue##62##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##63##</span>
<span class="pl-k">end</span></pre></div>
<p>After translating the sampling statements into function calls, the code is transformed with <code>Meta.lower</code>. The resulting <code>Code.Info</code> is then transformed back into a Julia expression, as you see above. This has the advantage of flattening the expression, so that there is at most only a single function call per line.</p>
<p>The <code>logdensity</code> generated functions are defined with the above expressions assigned to a variable, so that they can apply additional transformations.</p>
<p>I'll focus on the <code>ValueGradient</code> method, as this one performs the more interesting transformations. Aside from loading and constraining all the parameters from an input vector, it performs a <code>reverse_diff!</code> pass on the expression.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="forward_pass = quote end
reverse_pass = quote end
tracked_vars = Set([:ρ, :κ, :σ, :θ, :L, :μₕ₁, :μₕ₂, :σₕ, :μᵣ₁, :μᵣ₂, :σᵦ, :βᵣ₁, :βᵣ₂, ]);
ProbabilityModels.reverse_diff_pass!(forward_pass, reverse_pass, itp_preprocessed, tracked_vars);
striplines(forward_pass)
"><pre>forward_pass <span class="pl-k">=</span> <span class="pl-k">quote</span> <span class="pl-k">end</span>
reverse_pass <span class="pl-k">=</span> <span class="pl-k">quote</span> <span class="pl-k">end</span>
tracked_vars <span class="pl-k">=</span> <span class="pl-c1">Set</span>([<span class="pl-c1">:ρ</span>, <span class="pl-c1">:κ</span>, <span class="pl-c1">:σ</span>, <span class="pl-c1">:θ</span>, <span class="pl-c1">:L</span>, <span class="pl-c1">:μₕ₁</span>, <span class="pl-c1">:μₕ₂</span>, <span class="pl-c1">:σₕ</span>, <span class="pl-c1">:μᵣ₁</span>, <span class="pl-c1">:μᵣ₂</span>, <span class="pl-c1">:σᵦ</span>, <span class="pl-c1">:βᵣ₁</span>, <span class="pl-c1">:βᵣ₂</span>, ]);
ProbabilityModels<span class="pl-k">.</span><span class="pl-c1">reverse_diff_pass!</span>(forward_pass, reverse_pass, itp_preprocessed, tracked_vars);
<span class="pl-c1">striplines</span>(forward_pass)</pre></div>
<p>It walks the expression, replacing each function with a function that also returns the adjoint; the forward pass becomes:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="quote
    (##SSAValue##1##, ###adjoint###_##∂##SSAValue##1####∂ρ##) = ProbabilityDistributions.∂Beta(ρ, 3, 1, Val{(true, false, false)}())
    ##SSAValue##2## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##1##)
    target = ##SSAValue##2##
    (##SSAValue##4##, ###adjoint###_##∂##SSAValue##4####∂κ##) = ProbabilityDistributions.∂Gamma(κ, 0.1, 0.1, Val{(true, false, false)}())
    ##SSAValue##5## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##4##)
    target = ##SSAValue##5##
    (##SSAValue##7##, ###adjoint###_##∂##SSAValue##7####∂σ##) = ProbabilityDistributions.∂Gamma(σ, 1.5, 0.25, Val{(true, false, false)}())
    ##SSAValue##8## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##7##)
    target = ##SSAValue##8##
    (##SSAValue##10##, ###adjoint###_##∂##SSAValue##10####∂θ##) = ProbabilityDistributions.∂Normal(θ, 10, Val{(true, false)}())
    ##SSAValue##11## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##10##)
    target = ##SSAValue##11##
    (##SSAValue##13##, ###adjoint###_##∂##SSAValue##13####∂L##) = ProbabilityDistributions.∂LKJ(L, 2.0, Val{(true, false)}())
    ##SSAValue##14## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##13##)
    target = ##SSAValue##14##
    (##SSAValue##16##, ###adjoint###_##∂##SSAValue##16####∂μₕ₁##) = ProbabilityDistributions.∂Normal(μₕ₁, 10, Val{(true, false)}())
    ##SSAValue##17## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##16##)
    target = ##SSAValue##17##
    (##SSAValue##19##, ###adjoint###_##∂##SSAValue##19####∂μₕ₂##) = ProbabilityDistributions.∂Normal(μₕ₂, 10, Val{(true, false)}())
    ##SSAValue##20## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##19##)
    target = ##SSAValue##20##
    (##SSAValue##22##, ###adjoint###_##∂##SSAValue##22####∂σₕ##) = ProbabilityDistributions.∂Normal(σₕ, 10, Val{(true, false)}())
    ##SSAValue##23## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##22##)
    target = ##SSAValue##23##
    (##SSAValue##25##, ###adjoint###_##∂##SSAValue##25####∂μᵣ₁##) = ProbabilityDistributions.∂Normal(μᵣ₁, Val{(true,)}())
    ##SSAValue##26## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##25##)
    target = ##SSAValue##26##
    (##SSAValue##28##, ###adjoint###_##∂##SSAValue##28####∂μᵣ₂##) = ProbabilityDistributions.∂Normal(μᵣ₂, Val{(true,)}())
    ##SSAValue##29## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##28##)
    target = ##SSAValue##29##
    (##SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂μᵣ₁##, ###adjoint###_##∂##SSAValue##31####∂μₕ₁##, ###adjoint###_##∂##SSAValue##31####∂σₕ##) = ∂HierarchicalCentering(μᵣ₁, μₕ₁, σₕ, Val{(true, true, true)}())
    μᵦ₁ = ##SSAValue##31##
    (##SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂μᵣ₂##, ###adjoint###_##∂##SSAValue##33####∂μₕ₂##, ###adjoint###_##∂##SSAValue##33####∂σₕ##) = ∂HierarchicalCentering(μᵣ₂, μₕ₂, σₕ, Val{(true, true, true)}())
    μᵦ₂ = ##SSAValue##33##
    (##SSAValue##35##, ###adjoint###_##∂##SSAValue##35####∂σᵦ##) = ProbabilityDistributions.∂Normal(σᵦ, 10, Val{(true, false)}())
    ##SSAValue##36## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##35##)
    target = ##SSAValue##36##
    (##SSAValue##38##, ###adjoint###_##∂##SSAValue##38####∂βᵣ₁##) = ProbabilityDistributions.∂Normal(βᵣ₁, Val{(true,)}())
    ##SSAValue##39## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##38##)
    target = ##SSAValue##39##
    (##SSAValue##41##, ###adjoint###_##∂##SSAValue##41####∂βᵣ₂##) = ProbabilityDistributions.∂Normal(βᵣ₂, Val{(true,)}())
    ##SSAValue##42## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##41##)
    target = ##SSAValue##42##
    (##SSAValue##44##, ###adjoint###_##∂##SSAValue##44####βᵣ₁##, ###adjoint###_##∂##SSAValue##44####∂μᵦ₁##, ###adjoint###_##∂##SSAValue##44####∂σᵦ##) = ∂HierarchicalCentering(βᵣ₁, μᵦ₁, σᵦ, domains, Val{(true, true, true)}())
    β₁ = ##SSAValue##44##
    (##SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂βᵣ₂##, ###adjoint###_##∂##SSAValue##46####∂μᵦ₂##, ###adjoint###_##∂##SSAValue##46####∂σᵦ##) = ∂HierarchicalCentering(βᵣ₂, μᵦ₂, σᵦ, domains, Val{(true, true, true)}())
    β₂ = ##SSAValue##46##
    (##SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂β₁##, ###adjoint###_##∂##SSAValue##48####∂κ##, ###adjoint###_##∂##SSAValue##48####∂θ##) = ProbabilityModels.∂ITPExpectedValue(time, β₁, κ, θ, Val{(true, true, true)}())
    (##SSAValue##49##, ###adjoint###_##∂##SSAValue##49####∂##SSAValue##48####) = ProbabilityModels.∂vec(##SSAValue##48##)
    μ₁ = ##SSAValue##49##
    (##SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂β₂##, ###adjoint###_##∂##SSAValue##51####∂κ##, ###adjoint###_##∂##SSAValue##51####∂θ##) = ProbabilityModels.∂ITPExpectedValue(time, β₂, κ, θ, Val{(true, true, true)}())
    (##SSAValue##52##, ###adjoint###_##∂##SSAValue##52####∂##SSAValue##51####) = ProbabilityModels.∂vec(##SSAValue##51##)
    μ₂ = ##SSAValue##52##
    ##SSAValue##54## = LinearAlgebra.Diagonal(σ)
    ##SSAValue##55## = ##SSAValue##54## * L
    (##SSAValue##56##, ###adjoint###_##∂##SSAValue##56####∂ρ##, ###adjoint###_##∂##SSAValue##56####∂##SSAValue##55####) = ProbabilityModels.DistributionParameters.∂CovarianceMatrix(ρ, ##SSAValue##55##, time, Val{(true, true)}())
    Σ = ##SSAValue##56##
    ##SSAValue##58## = Core.tuple(Y₁, Y₂)
    ##SSAValue##59## = Core.tuple(μ₁, μ₂)
    (##SSAValue##60##, ###adjoint###_##∂##SSAValue##60####∂##SSAValue##59####) = PaddedMatrices.∂getindex(##SSAValue##59##, AvailableData)
    (##SSAValue##61##, ###adjoint###_##∂##SSAValue##61####∂Σ##) = PaddedMatrices.∂getindex(Σ, AvailableData)
    (##SSAValue##62##, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##60####, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##61####) = ProbabilityDistributions.∂Normal(##SSAValue##58##, ##SSAValue##60##, ##SSAValue##61##, Val{(false, true, true)}())
    ##SSAValue##63## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##62##)
    target = ##SSAValue##63##
end
"><pre><span class="pl-k">quote</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##1##, ###adjoint###_##∂##SSAValue##1####∂ρ##) = ProbabilityDistributions.∂Beta(ρ, 3, 1, Val{(true, false, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##2## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##1##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##2##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##4##, ###adjoint###_##∂##SSAValue##4####∂κ##) = ProbabilityDistributions.∂Gamma(κ, 0.1, 0.1, Val{(true, false, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##5## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##4##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##5##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##7##, ###adjoint###_##∂##SSAValue##7####∂σ##) = ProbabilityDistributions.∂Gamma(σ, 1.5, 0.25, Val{(true, false, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##8## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##7##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##8##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##10##, ###adjoint###_##∂##SSAValue##10####∂θ##) = ProbabilityDistributions.∂Normal(θ, 10, Val{(true, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##11## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##10##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##11##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##13##, ###adjoint###_##∂##SSAValue##13####∂L##) = ProbabilityDistributions.∂LKJ(L, 2.0, Val{(true, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##14## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##13##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##14##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##16##, ###adjoint###_##∂##SSAValue##16####∂μₕ₁##) = ProbabilityDistributions.∂Normal(μₕ₁, 10, Val{(true, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##17## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##16##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##17##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##19##, ###adjoint###_##∂##SSAValue##19####∂μₕ₂##) = ProbabilityDistributions.∂Normal(μₕ₂, 10, Val{(true, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##20## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##19##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##20##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##22##, ###adjoint###_##∂##SSAValue##22####∂σₕ##) = ProbabilityDistributions.∂Normal(σₕ, 10, Val{(true, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##23## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##22##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##23##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##25##, ###adjoint###_##∂##SSAValue##25####∂μᵣ₁##) = ProbabilityDistributions.∂Normal(μᵣ₁, Val{(true,)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##26## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##25##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##26##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##28##, ###adjoint###_##∂##SSAValue##28####∂μᵣ₂##) = ProbabilityDistributions.∂Normal(μᵣ₂, Val{(true,)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##29## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##28##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##29##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂μᵣ₁##, ###adjoint###_##∂##SSAValue##31####∂μₕ₁##, ###adjoint###_##∂##SSAValue##31####∂σₕ##) = ∂HierarchicalCentering(μᵣ₁, μₕ₁, σₕ, Val{(true, true, true)}())</span>
    μᵦ₁ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##31##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂μᵣ₂##, ###adjoint###_##∂##SSAValue##33####∂μₕ₂##, ###adjoint###_##∂##SSAValue##33####∂σₕ##) = ∂HierarchicalCentering(μᵣ₂, μₕ₂, σₕ, Val{(true, true, true)}())</span>
    μᵦ₂ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##33##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##35##, ###adjoint###_##∂##SSAValue##35####∂σᵦ##) = ProbabilityDistributions.∂Normal(σᵦ, 10, Val{(true, false)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##36## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##35##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##36##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##38##, ###adjoint###_##∂##SSAValue##38####∂βᵣ₁##) = ProbabilityDistributions.∂Normal(βᵣ₁, Val{(true,)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##39## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##38##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##39##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##41##, ###adjoint###_##∂##SSAValue##41####∂βᵣ₂##) = ProbabilityDistributions.∂Normal(βᵣ₂, Val{(true,)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##42## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##41##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##42##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##44##, ###adjoint###_##∂##SSAValue##44####βᵣ₁##, ###adjoint###_##∂##SSAValue##44####∂μᵦ₁##, ###adjoint###_##∂##SSAValue##44####∂σᵦ##) = ∂HierarchicalCentering(βᵣ₁, μᵦ₁, σᵦ, domains, Val{(true, true, true)}())</span>
    β₁ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##44##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂βᵣ₂##, ###adjoint###_##∂##SSAValue##46####∂μᵦ₂##, ###adjoint###_##∂##SSAValue##46####∂σᵦ##) = ∂HierarchicalCentering(βᵣ₂, μᵦ₂, σᵦ, domains, Val{(true, true, true)}())</span>
    β₂ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##46##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂β₁##, ###adjoint###_##∂##SSAValue##48####∂κ##, ###adjoint###_##∂##SSAValue##48####∂θ##) = ProbabilityModels.∂ITPExpectedValue(time, β₁, κ, θ, Val{(true, true, true)}())</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##49##, ###adjoint###_##∂##SSAValue##49####∂##SSAValue##48####) = ProbabilityModels.∂vec(##SSAValue##48##)</span>
    μ₁ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##49##</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂β₂##, ###adjoint###_##∂##SSAValue##51####∂κ##, ###adjoint###_##∂##SSAValue##51####∂θ##) = ProbabilityModels.∂ITPExpectedValue(time, β₂, κ, θ, Val{(true, true, true)}())</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##52##, ###adjoint###_##∂##SSAValue##52####∂##SSAValue##51####) = ProbabilityModels.∂vec(##SSAValue##51##)</span>
    μ₂ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##52##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##54## = LinearAlgebra.Diagonal(σ)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##55## = ##SSAValue##54## * L</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##56##, ###adjoint###_##∂##SSAValue##56####∂ρ##, ###adjoint###_##∂##SSAValue##56####∂##SSAValue##55####) = ProbabilityModels.DistributionParameters.∂CovarianceMatrix(ρ, ##SSAValue##55##, time, Val{(true, true)}())</span>
    Σ <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##56##</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##58## = Core.tuple(Y₁, Y₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##59## = Core.tuple(μ₁, μ₂)</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##60##, ###adjoint###_##∂##SSAValue##60####∂##SSAValue##59####) = PaddedMatrices.∂getindex(##SSAValue##59##, AvailableData)</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##61##, ###adjoint###_##∂##SSAValue##61####∂Σ##) = PaddedMatrices.∂getindex(Σ, AvailableData)</span>
    (<span class="pl-c"><span class="pl-c">#</span>#SSAValue##62##, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##60####, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##61####) = ProbabilityDistributions.∂Normal(##SSAValue##58##, ##SSAValue##60##, ##SSAValue##61##, Val{(false, true, true)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>#SSAValue##63## = ProbabilityModels.SIMDPirates.vadd(target, ##SSAValue##62##)</span>
    target <span class="pl-k">=</span> <span class="pl-c"><span class="pl-c">#</span>#SSAValue##63##</span>
<span class="pl-k">end</span></pre></div>
<p>It also prepends the corresponding operations to a second expression for the reverse diff pass:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="striplines(reverse_pass)
"><pre><span class="pl-c1">striplines</span>(reverse_pass)</pre></div>
<p>yielding:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="quote
    ###seed#####SSAValue##63## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##63##)
    ###seed#####SSAValue##62## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##63##, ###seed#####SSAValue##62##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##63##, ###seed###target)
    ###seed#####SSAValue##61## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##62##, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##61####, ###seed#####SSAValue##61##)
    ###seed#####SSAValue##60## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##62##, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##60####, ###seed#####SSAValue##60##)
    ###seed###Σ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##61##, ###adjoint###_##∂##SSAValue##61####∂Σ##, ###seed###Σ)
    ###seed#####SSAValue##59## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##60##, ###adjoint###_##∂##SSAValue##60####∂##SSAValue##59####, ###seed#####SSAValue##59##)
    ###seed###μ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##59##[2], ###seed###μ₂)
    ###seed###μ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##59##[1], ###seed###μ₁)
    ###seed#####SSAValue##56## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###Σ, ###seed#####SSAValue##56##)
    ###seed#####SSAValue##55## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##56##, ###adjoint###_##∂##SSAValue##56####∂##SSAValue##55####, ###seed#####SSAValue##55##)
    ###seed###ρ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##56##, ###adjoint###_##∂##SSAValue##56####∂ρ##, ###seed###ρ)
    (###adjoint###_##∂##SSAValue##55####∂##SSAValue##54####, ###adjoint###_##∂##SSAValue##55####∂L##) = ProbabilityModels.∂mul(##SSAValue##54##, L, Val{(true, true)}())
    ###seed###L = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##55##, ###adjoint###_##∂##SSAValue##55####∂L##, ###seed###L)
    ###seed#####SSAValue##54## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##55##, ###adjoint###_##∂##SSAValue##55####∂##SSAValue##54####, ###seed#####SSAValue##54##)
    ###seed###σ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##54##, ###seed###σ)
    ###seed#####SSAValue##52## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μ₂, ###seed#####SSAValue##52##)
    ###seed#####SSAValue##51## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##52##, ###adjoint###_##∂##SSAValue##52####∂##SSAValue##51####, ###seed#####SSAValue##51##)
    ###seed###θ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂θ##, ###seed###θ)
    ###seed###κ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂κ##, ###seed###κ)
    ###seed###β₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂β₂##, ###seed###β₂)
    ###seed#####SSAValue##49## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μ₁, ###seed#####SSAValue##49##)
    ###seed#####SSAValue##48## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##49##, ###adjoint###_##∂##SSAValue##49####∂##SSAValue##48####, ###seed#####SSAValue##48##)
    ###seed###θ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂θ##, ###seed###θ)
    ###seed###κ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂κ##, ###seed###κ)
    ###seed###β₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂β₁##, ###seed###β₁)
    ###seed#####SSAValue##46## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###β₂, ###seed#####SSAValue##46##)
    ###seed###σᵦ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂σᵦ##, ###seed###σᵦ)
    ###seed###μᵦ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂μᵦ₂##, ###seed###μᵦ₂)
    ###seed###βᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂βᵣ₂##, ###seed###βᵣ₂)
    ###seed#####SSAValue##44## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###β₁, ###seed#####SSAValue##44##)
    ###seed###σᵦ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##44##, ###adjoint###_##∂##SSAValue##44####∂σᵦ##, ###seed###σᵦ)
    ###seed###μᵦ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##44##, ###adjoint###_##∂##SSAValue##44####∂μᵦ₁##, ###seed###μᵦ₁)
    ###seed###βᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##44##, ###adjoint###_##∂##SSAValue##44####∂βᵣ₁##, ###seed###βᵣ₁)
    ###seed#####SSAValue##42## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##42##)
    ###seed#####SSAValue##41## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##42##, ###seed#####SSAValue##41##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##42##, ###seed###target)
    ###seed###βᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##41##, ###adjoint###_##∂##SSAValue##41####∂βᵣ₂##, ###seed###βᵣ₂)
    ###seed#####SSAValue##39## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##39##)
    ###seed#####SSAValue##38## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##39##, ###seed#####SSAValue##38##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##39##, ###seed###target)
    ###seed###βᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##38##, ###adjoint###_##∂##SSAValue##38####∂βᵣ₁##, ###seed###βᵣ₁)
    ###seed#####SSAValue##36## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##36##)
    ###seed#####SSAValue##35## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##36##, ###seed#####SSAValue##35##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##36##, ###seed###target)
    ###seed###σᵦ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##35##, ###adjoint###_##∂##SSAValue##35####∂σᵦ##, ###seed###σᵦ)
    ###seed#####SSAValue##33## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μᵦ₂, ###seed#####SSAValue##33##)
    ###seed###σₕ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂σₕ##, ###seed###σₕ)
    ###seed###μₕ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂μₕ₂##, ###seed###μₕ)
    ###seed###μᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂μᵣ₂##, ###seed###μᵣ₂)
    ###seed#####SSAValue##31## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μᵦ₁, ###seed#####SSAValue##31##)
    ###seed###σₕ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂σₕ##, ###seed###σₕ)
    ###seed###μₕ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂μₕ₁##, ###seed###μₕ₁)
    ###seed###μᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂μᵣ₁##, ###seed###μᵣ₁)
    ###seed#####SSAValue##29## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##29##)
    ###seed#####SSAValue##28## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##29##, ###seed#####SSAValue##28##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##29##, ###seed###target)
    ###seed###μᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##28##, ###adjoint###_##∂##SSAValue##28####∂μᵣ₂##, ###seed###μᵣ₂)
    ###seed#####SSAValue##26## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##26##)
    ###seed#####SSAValue##25## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##26##, ###seed#####SSAValue##25##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##26##, ###seed###target)
    ###seed###μᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##25##, ###adjoint###_##∂##SSAValue##25####∂μᵣ₁##, ###seed###μᵣ₁)
    ###seed#####SSAValue##23## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##23##)
    ###seed#####SSAValue##22## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##23##, ###seed#####SSAValue##22##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##23##, ###seed###target)
    ###seed###σₕ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##22##, ###adjoint###_##∂##SSAValue##22####∂σₕ##, ###seed###σₕ)
    ###seed#####SSAValue##20## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##20##)
    ###seed#####SSAValue##19## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##20##, ###seed#####SSAValue##19##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##20##, ###seed###target)
    ###seed###μₕ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##19##, ###adjoint###_##∂##SSAValue##19####∂μₕ₂##, ###seed###μₕ₂)
    ###seed#####SSAValue##17## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##17##)
    ###seed#####SSAValue##16## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##17##, ###seed#####SSAValue##16##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##17##, ###seed###target)
    ###seed###μₕ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##16##, ###adjoint###_##∂##SSAValue##16####∂μₕ₁##, ###seed###μₕ₁)
    ###seed#####SSAValue##14## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##14##)
    ###seed#####SSAValue##13## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##14##, ###seed#####SSAValue##13##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##14##, ###seed###target)
    ###seed###L = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##13##, ###adjoint###_##∂##SSAValue##13####∂L##, ###seed###L)
    ###seed#####SSAValue##11## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##11##)
    ###seed#####SSAValue##10## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##11##, ###seed#####SSAValue##10##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##11##, ###seed###target)
    ###seed###θ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##10##, ###adjoint###_##∂##SSAValue##10####∂θ##, ###seed###θ)
    ###seed#####SSAValue##8## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##8##)
    ###seed#####SSAValue##7## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##8##, ###seed#####SSAValue##7##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##8##, ###seed###target)
    ###seed###σ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##7##, ###adjoint###_##∂##SSAValue##7####∂σ##, ###seed###σ)
    ###seed#####SSAValue##5## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##5##)
    ###seed#####SSAValue##4## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##5##, ###seed#####SSAValue##4##)
    ###seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##5##, ###seed###target)
    ###seed###κ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##4##, ###adjoint###_##∂##SSAValue##4####∂κ##, ###seed###κ)
    ###seed#####SSAValue##2## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##2##)
    ###seed#####SSAValue##1## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##2##, ###seed#####SSAValue##1##)
    ###seed###ρ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##1##, ###adjoint###_##∂##SSAValue##1####∂ρ##, ###seed###ρ)
end
"><pre><span class="pl-k">quote</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##63## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##63##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##62## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##63##, ###seed#####SSAValue##62##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##63##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##61## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##62##, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##61####, ###seed#####SSAValue##61##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##60## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##62##, ###adjoint###_##∂##SSAValue##62####∂##SSAValue##60####, ###seed#####SSAValue##60##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###Σ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##61##, ###adjoint###_##∂##SSAValue##61####∂Σ##, ###seed###Σ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##59## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##60##, ###adjoint###_##∂##SSAValue##60####∂##SSAValue##59####, ###seed#####SSAValue##59##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##59##[2], ###seed###μ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##59##[1], ###seed###μ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##56## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###Σ, ###seed#####SSAValue##56##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##55## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##56##, ###adjoint###_##∂##SSAValue##56####∂##SSAValue##55####, ###seed#####SSAValue##55##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###ρ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##56##, ###adjoint###_##∂##SSAValue##56####∂ρ##, ###seed###ρ)</span>
    (<span class="pl-c"><span class="pl-c">#</span>##adjoint###_##∂##SSAValue##55####∂##SSAValue##54####, ###adjoint###_##∂##SSAValue##55####∂L##) = ProbabilityModels.∂mul(##SSAValue##54##, L, Val{(true, true)}())</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###L = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##55##, ###adjoint###_##∂##SSAValue##55####∂L##, ###seed###L)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##54## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##55##, ###adjoint###_##∂##SSAValue##55####∂##SSAValue##54####, ###seed#####SSAValue##54##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##54##, ###seed###σ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##52## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μ₂, ###seed#####SSAValue##52##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##51## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##52##, ###adjoint###_##∂##SSAValue##52####∂##SSAValue##51####, ###seed#####SSAValue##51##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###θ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂θ##, ###seed###θ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###κ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂κ##, ###seed###κ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###β₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##51##, ###adjoint###_##∂##SSAValue##51####∂β₂##, ###seed###β₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##49## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μ₁, ###seed#####SSAValue##49##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##48## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##49##, ###adjoint###_##∂##SSAValue##49####∂##SSAValue##48####, ###seed#####SSAValue##48##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###θ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂θ##, ###seed###θ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###κ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂κ##, ###seed###κ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###β₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##48##, ###adjoint###_##∂##SSAValue##48####∂β₁##, ###seed###β₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##46## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###β₂, ###seed#####SSAValue##46##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σᵦ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂σᵦ##, ###seed###σᵦ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μᵦ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂μᵦ₂##, ###seed###μᵦ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###βᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##46##, ###adjoint###_##∂##SSAValue##46####∂βᵣ₂##, ###seed###βᵣ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##44## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###β₁, ###seed#####SSAValue##44##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σᵦ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##44##, ###adjoint###_##∂##SSAValue##44####∂σᵦ##, ###seed###σᵦ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μᵦ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##44##, ###adjoint###_##∂##SSAValue##44####∂μᵦ₁##, ###seed###μᵦ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###βᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##44##, ###adjoint###_##∂##SSAValue##44####∂βᵣ₁##, ###seed###βᵣ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##42## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##42##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##41## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##42##, ###seed#####SSAValue##41##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##42##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###βᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##41##, ###adjoint###_##∂##SSAValue##41####∂βᵣ₂##, ###seed###βᵣ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##39## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##39##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##38## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##39##, ###seed#####SSAValue##38##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##39##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###βᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##38##, ###adjoint###_##∂##SSAValue##38####∂βᵣ₁##, ###seed###βᵣ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##36## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##36##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##35## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##36##, ###seed#####SSAValue##35##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##36##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σᵦ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##35##, ###adjoint###_##∂##SSAValue##35####∂σᵦ##, ###seed###σᵦ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##33## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μᵦ₂, ###seed#####SSAValue##33##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σₕ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂σₕ##, ###seed###σₕ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μₕ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂μₕ₂##, ###seed###μₕ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##33##, ###adjoint###_##∂##SSAValue##33####∂μᵣ₂##, ###seed###μᵣ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##31## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###μᵦ₁, ###seed#####SSAValue##31##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σₕ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂σₕ##, ###seed###σₕ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μₕ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂μₕ₁##, ###seed###μₕ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##31##, ###adjoint###_##∂##SSAValue##31####∂μᵣ₁##, ###seed###μᵣ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##29## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##29##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##28## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##29##, ###seed#####SSAValue##28##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##29##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μᵣ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##28##, ###adjoint###_##∂##SSAValue##28####∂μᵣ₂##, ###seed###μᵣ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##26## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##26##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##25## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##26##, ###seed#####SSAValue##25##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##26##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μᵣ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##25##, ###adjoint###_##∂##SSAValue##25####∂μᵣ₁##, ###seed###μᵣ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##23## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##23##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##22## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##23##, ###seed#####SSAValue##22##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##23##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σₕ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##22##, ###adjoint###_##∂##SSAValue##22####∂σₕ##, ###seed###σₕ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##20## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##20##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##19## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##20##, ###seed#####SSAValue##19##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##20##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μₕ₂ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##19##, ###adjoint###_##∂##SSAValue##19####∂μₕ₂##, ###seed###μₕ₂)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##17## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##17##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##16## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##17##, ###seed#####SSAValue##16##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##17##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###μₕ₁ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##16##, ###adjoint###_##∂##SSAValue##16####∂μₕ₁##, ###seed###μₕ₁)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##14## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##14##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##13## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##14##, ###seed#####SSAValue##13##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##14##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###L = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##13##, ###adjoint###_##∂##SSAValue##13####∂L##, ###seed###L)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##11## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##11##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##10## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##11##, ###seed#####SSAValue##10##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##11##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###θ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##10##, ###adjoint###_##∂##SSAValue##10####∂θ##, ###seed###θ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##8## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##8##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##7## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##8##, ###seed#####SSAValue##7##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##8##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###σ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##7##, ###adjoint###_##∂##SSAValue##7####∂σ##, ###seed###σ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##5## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##5##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##4## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##5##, ###seed#####SSAValue##4##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###target = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##5##, ###seed###target)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###κ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##4##, ###adjoint###_##∂##SSAValue##4####∂κ##, ###seed###κ)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##2## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed###target, ###seed#####SSAValue##2##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed#####SSAValue##1## = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##2##, ###seed#####SSAValue##1##)</span>
    <span class="pl-c"><span class="pl-c">#</span>##seed###ρ = ProbabilityModels.PaddedMatrices.RESERVED_INCREMENT_SEED_RESERVED(###seed#####SSAValue##1##, ###adjoint###_##∂##SSAValue##1####∂ρ##, ###seed###ρ)</span>
<span class="pl-k">end</span></pre></div>
<p>The <code>RESERVED_INCREMENT_SEED_RESERVED</code> and <code>RESERVED_MULTIPLY_SEED_RESERVED</code> fall back to <code>mulladd</code> and <code>*</code> methods, respectively. They however allow for different overloads.</p>
<p>The keys to performance are that this source-to-source reverse mode differentation avoids unnecessary overhead, and does not interfere with vectorization, which typical dual-number based approaches, like Stan's var type or ForwardDiff's duals, do.
The reverse diff pass uses <a href="https://github.com/JuliaDiff/DiffRules.jl">DiffRules.jl</a> as well as custom derivatives defined for probability distributions and a few other special functions.</p>
<p>By defining high level derivatives for commonly used functions we can optimize them much further than if we relied on autodiff for them. Additionally, we abuse multiple dispatch here: whenevever there is some structure we can exploit in the adjoint, we return a type with <code>INCREMENT/MULTIPLY</code> methods defined to exploit it. Examples include <a href="https://github.com/chriselrod/StructuredMatrices.jl/blob/master/src/block_diagonal.jl#L4">BlockDiagonalColumnView</a>s or various <a href="https://github.com/chriselrod/ProbabilityModels.jl/blob/master/src/adjoints.jl#L55">Reducer</a> types.</p>
<p>Another optimization is that it performs a <a href="https://github.com/chriselrod/PaddedMatrices.jl/blob/master/src/stack_pointer.jl#L55">stack pointer pass</a>. Supported functions are passed a pointer to a preallocated block of memory. This memory reuse can both give us considerable savings, and allow the memory to stay hot in the cache. <a href="https://github.com/chriselrod/PaddedMatrices.jl">PaddedMatrices.jl</a>, aside from providing <a href="https://bayeswatch.org/2019/06/06/small-matrix-multiplication-performance-shootout/" rel="nofollow">optimized matrix operations</a> (contrary to the name, it's performance advantage over other libraries is actually greatest when none of them use padding), it also provides <code>PtrArray</code> (parameterized by size) and <code>DynamicPtrArray</code> (dynamically sized) types for taking advantage of this preallocated memory.</p>
</article></div>