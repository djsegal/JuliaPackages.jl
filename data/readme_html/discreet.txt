<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-discreet" class="anchor" aria-hidden="true" href="#discreet"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Discreet</h1>
<p dir="auto"><a href="https://github.com/cynddl/Discreet.jl/actions/workflows/CI.yml"><img src="https://github.com/cynddl/Discreet.jl/actions/workflows/CI.yml/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="http://codecov.io/github/cynddl/Discreet.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/1aa2e8436d473556b0e4d047ffb29a7c5509422753e2d4efd30c359fb7897ffd/687474703a2f2f636f6465636f762e696f2f6769746875622f63796e64646c2f44697363726565742e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="Coverage status" data-canonical-src="http://codecov.io/github/cynddl/Discreet.jl/coverage.svg?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">Discreet is a small opinionated toolbox to estimate entropy and mutual information from discrete samples. It contains methods to adjust results and correct over- or under-estimations.</p>
<h2 dir="auto"><a id="user-content-estimating-entropy" class="anchor" aria-hidden="true" href="#estimating-entropy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Estimating entropy</h2>
<p dir="auto">Discreet uses StatsBase's FrequencyWeights and ProbabilityWeights types.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using StatsBase: FrequencyWeights, ProbabilityWeights
using Discreet

dist = FrequencyWeights([1, 1, 1, 1, 1, 1])
entropy(dist)  # Naive method: log(6) ≈ 1.792

entropy(dist; method=:CS)  # Chao-Shen correction: ≈ 3.840

entropy(dist; method=:Shrink)  # Shrinkage correction: ≈ 1.792

dist = ProbabilityWeights([.5, .5])
entropy(dist)  # log(2) ≈ 0.693"><pre><span class="pl-k">using</span> StatsBase<span class="pl-k">:</span> FrequencyWeights, ProbabilityWeights
<span class="pl-k">using</span> Discreet

dist <span class="pl-k">=</span> <span class="pl-c1">FrequencyWeights</span>([<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>])
<span class="pl-c1">entropy</span>(dist)  <span class="pl-c"><span class="pl-c">#</span> Naive method: log(6) ≈ 1.792</span>

<span class="pl-c1">entropy</span>(dist; method<span class="pl-k">=</span><span class="pl-c1">:CS</span>)  <span class="pl-c"><span class="pl-c">#</span> Chao-Shen correction: ≈ 3.840</span>

<span class="pl-c1">entropy</span>(dist; method<span class="pl-k">=</span><span class="pl-c1">:Shrink</span>)  <span class="pl-c"><span class="pl-c">#</span> Shrinkage correction: ≈ 1.792</span>

dist <span class="pl-k">=</span> <span class="pl-c1">ProbabilityWeights</span>([.<span class="pl-c1">5</span>, .<span class="pl-c1">5</span>])
<span class="pl-c1">entropy</span>(dist)  <span class="pl-c"><span class="pl-c">#</span> log(2) ≈ 0.693</span></pre></div>
<p dir="auto">Discreet can also estimate the entropy of a sample:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Discreet

data = [&quot;tomato&quot;, &quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;, &quot;tomato&quot;]
estimate_entropy(data)  # == entropy(FrequencyWeights([2, 2, 1]))"><pre><span class="pl-k">using</span> Discreet

data <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>tomato<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>apple<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>apple<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>banana<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>tomato<span class="pl-pds">"</span></span>]
<span class="pl-c1">estimate_entropy</span>(data)  <span class="pl-c"><span class="pl-c">#</span> == entropy(FrequencyWeights([2, 2, 1]))</span></pre></div>
<h2 dir="auto"><a id="user-content-estimate-mutual-information" class="anchor" aria-hidden="true" href="#estimate-mutual-information"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Estimate mutual information</h2>
<p dir="auto">Discrete provides similar routines to estimate mutual information.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Discreet

labels_a = [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]
labels_b = [1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2]
mutual_information(labels_a, labels_b)  # Naive method: ≈ 0.410

mutual_information(labels_a, labels_b; method=:CS)  # Chao-Shen correction: ≈ 0.148

mutual_information(labels_a, labels_b; normalize=true)  # Normalized score (between 0 and 1): ≈ 0.382"><pre><span class="pl-k">using</span> Discreet

labels_a <span class="pl-k">=</span> [<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>]
labels_b <span class="pl-k">=</span> [<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>]
<span class="pl-c1">mutual_information</span>(labels_a, labels_b)  <span class="pl-c"><span class="pl-c">#</span> Naive method: ≈ 0.410</span>

<span class="pl-c1">mutual_information</span>(labels_a, labels_b; method<span class="pl-k">=</span><span class="pl-c1">:CS</span>)  <span class="pl-c"><span class="pl-c">#</span> Chao-Shen correction: ≈ 0.148</span>

<span class="pl-c1">mutual_information</span>(labels_a, labels_b; normalize<span class="pl-k">=</span><span class="pl-c1">true</span>)  <span class="pl-c"><span class="pl-c">#</span> Normalized score (between 0 and 1): ≈ 0.382</span></pre></div>
</article></div>