<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h2><a id="user-content-juml" class="anchor" aria-hidden="true" href="#juml"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>JuML</h2>
<p>JuML is a machine learning package written in pure Julia. This is still very much work in progress so the package is not registered yet and you will need to clone this repo to try it.</p>
<p>At the moment JuML contains a custom built <em>DataFrame</em> with associated types (<em>Factor</em>, <em>Covariate</em> etc) and an independent XGBoost implementation (logistic only). The XGBoost part around 600 lines of Julia code and has speed similar to the original C++ implementation with smaller memory footprint.</p>
<h3><a id="user-content-example-usage-airline-dataset-with-1m-obs" class="anchor" aria-hidden="true" href="#example-usage-airline-dataset-with-1m-obs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example usage: Airline dataset with 1M obs</h3>
<p>The datasets can be downloaded from here:</p>
<p><a href="https://s3.amazonaws.com/benchm-ml--main/train-1m.csv" rel="nofollow">https://s3.amazonaws.com/benchm-ml--main/train-1m.csv</a></p>
<p><a href="https://s3.amazonaws.com/benchm-ml--main/test.csv" rel="nofollow">https://s3.amazonaws.com/benchm-ml--main/test.csv</a></p>
<p>Let's rename the datasets into <em>airlinetrain</em> and <em>airlinetest</em>.
First we have to import the csv datasets into a special binary format.
We will import both datasets into 1 <em>airlinetraintest</em> dataframe with test data stacked under train data:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using JuML
importcsv(&quot;your-path\\airlinetrain.csv&quot;; path2 = &quot;your-path\\airlinetest.csv&quot;, outname = &quot;airlinetraintest&quot;)
"><pre><code>using JuML
importcsv("your-path\\airlinetrain.csv"; path2 = "your-path\\airlinetest.csv", outname = "airlinetraintest")
</code></pre></div>
<p>The data will be converted into a special binary format and saved in a new folder named <em>airlinetraintest</em>. Each data column is stored in a separate binary file.</p>
<p>We can now load the dataset into JuML DataFrame:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="traintest_df = DataFrame(&quot;your-path\\airlinetraintest&quot;) # note we are passing a path to a folder
"><pre><code>traintest_df = DataFrame("your-path\\airlinetraintest") # note we are passing a path to a folder
</code></pre></div>
<p>You should see a summary of the dataframe in your REPL. JuML DataFrame is just a collection of Factors and Covariates. Categorical data is stored in Factors and numeric data in Covariates.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="factors = traintest_df.factors
covariates = traintest_df.covariates
"><pre><code>factors = traintest_df.factors
covariates = traintest_df.covariates
</code></pre></div>
<p>We can access each Factor or Covariate by name:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="distance = traintest_df[&quot;Distance&quot;]
deptime = traintest_df[&quot;DepTime&quot;]
dep_delayed_15min = traintest_df[&quot;dep_delayed_15min&quot;]
"><pre><code>distance = traintest_df["Distance"]
deptime = traintest_df["DepTime"]
dep_delayed_15min = traintest_df["dep_delayed_15min"]
</code></pre></div>
<p>We can see a quick stat summary:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="summary(distance)
summary(deptime)
summary(dep_delayed_15min)
"><pre><code>summary(distance)
summary(deptime)
summary(dep_delayed_15min)
</code></pre></div>
<p>JuML XGBoost expects label to be a Covariate and all features to be Factors. Our label is <em>dep_delayed_15min</em>, which is a Factor, and there are 2 Covariates in the data: <em>Distance</em> and <em>DepTime</em>. Fortunately we can easily convert between factors and covariates in JuML.</p>
<p>Let's create a Covariate which is equal to 1 when <em>dep_delayed_15min</em> is Y and 0 otherwise:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="label = covariate(traintest_df[&quot;dep_delayed_15min&quot;], level -&gt; level == &quot;Y&quot; ? 1.0 : 0.0)
"><pre><code>label = covariate(traintest_df["dep_delayed_15min"], level -&gt; level == "Y" ? 1.0 : 0.0)
</code></pre></div>
<p>Covariates can be converted into factors by binning. We can bin on every possible value with function <em>factor</em>:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="deptime = factor(traintest_df[&quot;DepTime&quot;])
distance = factor(traintest_df[&quot;Distance&quot;])
"><pre><code>deptime = factor(traintest_df["DepTime"])
distance = factor(traintest_df["Distance"])
</code></pre></div>
<p>We have stacked train and test data in 1 dataframe. We will need to define selectors for each part:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="trainsel = BoolVariate(&quot;trainsel&quot;, (1:1100000) .&lt;= 1000000)
validsel = BoolVariate(&quot;validsel&quot;, (1:1100000) .&gt; 1000000)
"><pre><code>trainsel = BoolVariate("trainsel", (1:1100000) .&lt;= 1000000)
validsel = BoolVariate("validsel", (1:1100000) .&gt; 1000000)
</code></pre></div>
<p>The last thing to do before we can run XGBoost is to create a vector of factors as features. We need to add <em>deptime</em> and <em>distance</em> factors to train dataframe factors (<em>dep_delayed_15min</em> will be excluded from the model features automatically):</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="factors = [traintest_df.factors; [deptime, distance]]
"><pre><code>factors = [traintest_df.factors; [deptime, distance]]
</code></pre></div>
<p>We are now ready to run XGBoost:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="model = xgblogit(label, factors; trainselector = trainsel, validselector = validsel, η = 0.3, λ = 1.0, γ = 0.0, minchildweight = 1.0, nrounds = 2, maxdepth = 5, ordstumps = false, caching = true, usefloat64 = true, singlethread = true);
"><pre><code>model = xgblogit(label, factors; trainselector = trainsel, validselector = validsel, η = 0.3, λ = 1.0, γ = 0.0, minchildweight = 1.0, nrounds = 2, maxdepth = 5, ordstumps = false, caching = true, usefloat64 = true, singlethread = true);
</code></pre></div>
<p>We can now calculate auc and logloss for both train and validation:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="trainauc, testauc = getauc(model.pred, label, trainsel, validsel)
trainlogloss, testlogloss = getlogloss(model.pred, label, trainsel, validsel)
"><pre><code>trainauc, testauc = getauc(model.pred, label, trainsel, validsel)
trainlogloss, testlogloss = getlogloss(model.pred, label, trainsel, validsel)
</code></pre></div>
</article></div>