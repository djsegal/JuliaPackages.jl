<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h2 dir="auto"><a id="user-content-juml" class="anchor" aria-hidden="true" href="#juml"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>JuML</h2>
<p dir="auto">JuML is a machine learning package written in pure Julia. This is still very much work in progress so the package is not registered yet and you will need to clone this repo to try it.</p>
<p dir="auto">At the moment JuML contains a custom built <em>DataFrame</em> with associated types (<em>Factor</em>, <em>Covariate</em> etc) and an independent XGBoost implementation (logistic only). The XGBoost part around 600 lines of Julia code and has speed similar to the original C++ implementation with smaller memory footprint.</p>
<h3 dir="auto"><a id="user-content-example-usage-airline-dataset-with-1m-obs" class="anchor" aria-hidden="true" href="#example-usage-airline-dataset-with-1m-obs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example usage: Airline dataset with 1M obs</h3>
<p dir="auto">The datasets can be downloaded from here:</p>
<p dir="auto"><a href="https://s3.amazonaws.com/benchm-ml--main/train-1m.csv" rel="nofollow">https://s3.amazonaws.com/benchm-ml--main/train-1m.csv</a></p>
<p dir="auto"><a href="https://s3.amazonaws.com/benchm-ml--main/test.csv" rel="nofollow">https://s3.amazonaws.com/benchm-ml--main/test.csv</a></p>
<p dir="auto">Let's rename the datasets into <em>airlinetrain</em> and <em>airlinetest</em>.
First we have to import the csv datasets into a special binary format.
We will import both datasets into 1 <em>airlinetraintest</em> dataframe with test data stacked under train data:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using JuML
importcsv(&quot;your-path\\airlinetrain.csv&quot;; path2 = &quot;your-path\\airlinetest.csv&quot;, outname = &quot;airlinetraintest&quot;)"><pre class="notranslate"><code>using JuML
importcsv("your-path\\airlinetrain.csv"; path2 = "your-path\\airlinetest.csv", outname = "airlinetraintest")
</code></pre></div>
<p dir="auto">The data will be converted into a special binary format and saved in a new folder named <em>airlinetraintest</em>. Each data column is stored in a separate binary file.</p>
<p dir="auto">We can now load the dataset into JuML DataFrame:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="traintest_df = DataFrame(&quot;your-path\\airlinetraintest&quot;) # note we are passing a path to a folder"><pre class="notranslate"><code>traintest_df = DataFrame("your-path\\airlinetraintest") # note we are passing a path to a folder
</code></pre></div>
<p dir="auto">You should see a summary of the dataframe in your REPL. JuML DataFrame is just a collection of Factors and Covariates. Categorical data is stored in Factors and numeric data in Covariates.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="factors = traintest_df.factors
covariates = traintest_df.covariates"><pre class="notranslate"><code>factors = traintest_df.factors
covariates = traintest_df.covariates
</code></pre></div>
<p dir="auto">We can access each Factor or Covariate by name:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="distance = traintest_df[&quot;Distance&quot;]
deptime = traintest_df[&quot;DepTime&quot;]
dep_delayed_15min = traintest_df[&quot;dep_delayed_15min&quot;]"><pre class="notranslate"><code>distance = traintest_df["Distance"]
deptime = traintest_df["DepTime"]
dep_delayed_15min = traintest_df["dep_delayed_15min"]
</code></pre></div>
<p dir="auto">We can see a quick stat summary:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="summary(distance)
summary(deptime)
summary(dep_delayed_15min)"><pre class="notranslate"><code>summary(distance)
summary(deptime)
summary(dep_delayed_15min)
</code></pre></div>
<p dir="auto">JuML XGBoost expects label to be a Covariate and all features to be Factors. Our label is <em>dep_delayed_15min</em>, which is a Factor, and there are 2 Covariates in the data: <em>Distance</em> and <em>DepTime</em>. Fortunately we can easily convert between factors and covariates in JuML.</p>
<p dir="auto">Let's create a Covariate which is equal to 1 when <em>dep_delayed_15min</em> is Y and 0 otherwise:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="label = covariate(traintest_df[&quot;dep_delayed_15min&quot;], level -&gt; level == &quot;Y&quot; ? 1.0 : 0.0)"><pre class="notranslate"><code>label = covariate(traintest_df["dep_delayed_15min"], level -&gt; level == "Y" ? 1.0 : 0.0)
</code></pre></div>
<p dir="auto">Covariates can be converted into factors by binning. We can bin on every possible value with function <em>factor</em>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="deptime = factor(traintest_df[&quot;DepTime&quot;])
distance = factor(traintest_df[&quot;Distance&quot;])"><pre class="notranslate"><code>deptime = factor(traintest_df["DepTime"])
distance = factor(traintest_df["Distance"])
</code></pre></div>
<p dir="auto">We have stacked train and test data in 1 dataframe. We will need to define selectors for each part:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="trainsel = BoolVariate(&quot;trainsel&quot;, (1:1100000) .&lt;= 1000000)
validsel = BoolVariate(&quot;validsel&quot;, (1:1100000) .&gt; 1000000)"><pre class="notranslate"><code>trainsel = BoolVariate("trainsel", (1:1100000) .&lt;= 1000000)
validsel = BoolVariate("validsel", (1:1100000) .&gt; 1000000)
</code></pre></div>
<p dir="auto">The last thing to do before we can run XGBoost is to create a vector of factors as features. We need to add <em>deptime</em> and <em>distance</em> factors to train dataframe factors (<em>dep_delayed_15min</em> will be excluded from the model features automatically):</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="factors = [traintest_df.factors; [deptime, distance]]"><pre class="notranslate"><code>factors = [traintest_df.factors; [deptime, distance]]
</code></pre></div>
<p dir="auto">We are now ready to run XGBoost:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="model = xgblogit(label, factors; trainselector = trainsel, validselector = validsel, η = 0.3, λ = 1.0, γ = 0.0, minchildweight = 1.0, nrounds = 2, maxdepth = 5, ordstumps = false, caching = true, usefloat64 = true, singlethread = true);"><pre class="notranslate"><code>model = xgblogit(label, factors; trainselector = trainsel, validselector = validsel, η = 0.3, λ = 1.0, γ = 0.0, minchildweight = 1.0, nrounds = 2, maxdepth = 5, ordstumps = false, caching = true, usefloat64 = true, singlethread = true);
</code></pre></div>
<p dir="auto">We can now calculate auc and logloss for both train and validation:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="trainauc, testauc = getauc(model.pred, label, trainsel, validsel)
trainlogloss, testlogloss = getlogloss(model.pred, label, trainsel, validsel)"><pre class="notranslate"><code>trainauc, testauc = getauc(model.pred, label, trainsel, validsel)
trainlogloss, testlogloss = getlogloss(model.pred, label, trainsel, validsel)
</code></pre></div>
</article></div>