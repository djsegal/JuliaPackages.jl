<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a href="https://travis-ci.org/sbos/DirichletProcessMixtures.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ef5b833f127c800699eb42f0a9d9dfdaa115bb96/68747470733a2f2f7472617669732d63692e6f72672f73626f732f4469726963686c657450726f636573734d697874757265732e6a6c2e706e67" alt="Build Status" data-canonical-src="https://travis-ci.org/sbos/DirichletProcessMixtures.jl.png" style="max-width:100%;"></a>
<a href="http://pkg.julialang.org/?pkg=DirichletProcessMixtures&amp;ver=release" rel="nofollow"><img src="https://camo.githubusercontent.com/8bc8e283c74e084cf3e62b5de7c0aa49225e4c5e/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f4469726963686c657450726f636573734d697874757265735f72656c656173652e737667" alt="DirichletProcessMixtures" data-canonical-src="http://pkg.julialang.org/badges/DirichletProcessMixtures_release.svg" style="max-width:100%;"></a></p>
<h1><a id="user-content-dirichletprocessmixturesjl" class="anchor" aria-hidden="true" href="#dirichletprocessmixturesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DirichletProcessMixtures.jl</h1>
<p>This package implements Dirichlet Process Mixture Models in Julia using variational inference for truncated stick-breaking representation of Dirichlet Process.</p>
<h2><a id="user-content-almost-infinite-mixture-of-gaussians" class="anchor" aria-hidden="true" href="#almost-infinite-mixture-of-gaussians"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>(almost) infinite mixture of Gaussians</h2>
<p>Most likely you need this package especially for this purpose, this is how to do Gaussian clustering. You may check <a href="demo.jl">demo code</a> which contains almost all functionality you may need.</p>
<p>First off, you define your prior over parameters of mixture component (i.e. mean and precision matrix) using <code>NormalWishart</code> distribution:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> DirichletProcessMixtures
<span class="pl-k">using</span> Distributions

prior <span class="pl-k">=</span> <span class="pl-c1">NormalWishart</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">1e-7</span>, <span class="pl-c1">eye</span>(<span class="pl-c1">2</span>) <span class="pl-k">/</span> <span class="pl-c1">4</span>, <span class="pl-c1">4.0001</span>)</pre></div>
<p>Then you generate your mixture</p>
<div class="highlight highlight-source-julia"><pre>x <span class="pl-k">=</span> <span class="pl-k">...</span> <span class="pl-c"><span class="pl-c">#</span> your data, x[:, i] - is i-th data point</span>
T <span class="pl-k">=</span> <span class="pl-c1">20</span> <span class="pl-c"><span class="pl-c">#</span> truncation level</span>
alpha <span class="pl-k">=</span> <span class="pl-c1">0.1</span> <span class="pl-c"><span class="pl-c">#</span> Dirichlet process parameter, controls how many clusters you need a priori</span>
gm, theta, predictive_likelihood <span class="pl-k">=</span> <span class="pl-c1">gaussian_mixture</span>(prior, T, alpha, x)</pre></div>
<p><code>gm</code> is an internal representation of mixture model. <code>theta</code> is array of size <code>T</code> whose elements refer to parameters of posterior <code>NormalWishart</code>'s. Finally, <code>predictive_likelihood</code> is a function which takes a matrix containing test data and returns per-point test loglikelihood. Now we can perform inference in our model</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">iter_callback</span>(mix<span class="pl-k">::</span><span class="pl-c1">TSBPMM</span>, iter<span class="pl-k">::</span><span class="pl-c1">Int64</span>, lower_bound<span class="pl-k">::</span><span class="pl-c1">Float64</span>)
    pl <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">predictive_likelihood</span>(xtest)) <span class="pl-k">/</span> M
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>iteration <span class="pl-v">$iter</span> test likelihood=<span class="pl-v">$pl</span>, lower_bound=<span class="pl-v">$lower_bound</span><span class="pl-pds">"</span></span>)
<span class="pl-k">end</span>

maxiter <span class="pl-k">=</span> <span class="pl-c1">200</span>
ltol <span class="pl-k">=</span> <span class="pl-c1">1e-5</span>
niter <span class="pl-k">=</span> <span class="pl-c1">infer</span>(gm, maxiter, ltol; iter_callback<span class="pl-k">=</span>iter_callback)</pre></div>
<p>You may see that <code>infer</code> method performs not more than <code>maxiter</code> iterations until lower bound tolerance reaches <code>ltol</code> value, calling <code>iter_callback</code> at each iteration if provided.</p>
<p>Another useful quantities you may need from mixture model:</p>
<ul>
<li><code>gm.z</code> - TxN array with expected mixture component assignments</li>
<li><code>gm.qv</code> - posterior <code>Beta</code> distributions for stick-breaking proportions</li>
</ul>
<h2><a id="user-content-general-interface" class="anchor" aria-hidden="true" href="#general-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>General interface</h2>
<p>It is also possible to implement custom mixture models with conjugate priors for mixture components, but this remains to be documented yet. For a reference implementation of custom mixture model use <a href="src/gaussian_mixture.jl">mixture of Gaussians</a>.</p>
</article></div>