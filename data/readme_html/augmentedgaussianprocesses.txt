<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="docs/src/assets/banner.png"><img src="docs/src/assets/banner.png" alt="AugmentedGaussianProcesses.jl" style="max-width:100%;"></a></p>
<p><a href="https://theogf.github.io/AugmentedGaussianProcesses.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Docs Latest" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://theogf.github.io/AugmentedGaussianProcesses.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Docs Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/workflows/CI/badge.svg"><img src="https://github.com/theogf/AugmentedGaussianProcesses.jl/workflows/CI/badge.svg" alt="BuildStatus" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/theogf/AugmentedGaussianProcesses.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/af74aeb858e86ed9bb820e3c7bbb67d224f0df1f84f43a92265187d483062c46/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7468656f67662f4175676d656e746564476175737369616e50726f6365737365732e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/theogf/AugmentedGaussianProcesses.jl/badge.svg?branch=master" style="max-width:100%;"></a>
<a href="https://zenodo.org/badge/latestdoi/118922202" rel="nofollow"><img src="https://camo.githubusercontent.com/45e4d88d7a8fd78534fbe618f799ff6460867931f93548a2a13e59a1d754e439/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3131383932323230322e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/118922202.svg" style="max-width:100%;"></a></p>
<p>AugmentedGaussianProcesses.jl is a Julia package in development for <strong>Data Augmented Sparse Gaussian Processes</strong>. It contains a collection of models for different <strong>gaussian and non-gaussian likelihoods</strong>, which are transformed via data augmentation into <strong>conditionally conjugate likelihood</strong> allowing for <strong>extremely fast inference</strong> via block coordinate updates. There are also more options to use more traditional <strong>variational inference</strong> via quadrature or Monte Carlo integration.</p>
<p>The theory for the augmentation is given in the following paper : <a href="https://arxiv.org/abs/2002.11451" rel="nofollow">Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models</a></p>
<h3><a id="user-content-you-can-also-use-the-package-in-python-via-pyjulia" class="anchor" aria-hidden="true" href="#you-can-also-use-the-package-in-python-via-pyjulia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>You can also use the package in Python via <a href="https://github.com/JuliaPy/pyjulia">PyJulia</a>!</h3>
<h1><a id="user-content-packages-models-" class="anchor" aria-hidden="true" href="#packages-models-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Packages models :</h1>
<h2><a id="user-content-two-gp-classification-likelihood" class="anchor" aria-hidden="true" href="#two-gp-classification-likelihood"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Two GP classification likelihood</h2>
<ul>
<li><strong>BayesianSVM</strong> : A Classifier with a likelihood equivalent to the classic SVM <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/blob/master/examples/Classification%20-%20BayesianSVM.ipynb" rel="nofollow">IJulia example</a>/<a href="https://arxiv.org/abs/1707.05532" rel="nofollow">Reference</a></li>
<li><strong>Logistic</strong> : A Classifier with a Bernoulli likelihood with the logistic link <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/blob/master/examples/Classification%20-%20Logistic.ipynb" rel="nofollow">IJulia example</a>/<a href="https://arxiv.org/abs/1802.06383" rel="nofollow">Reference</a></li>
</ul>
<p align="center">
  <a target="_blank" rel="noopener noreferrer" href="docs/src/assets/Classification.png"><img src="docs/src/assets/Classification.png" style="max-width:100%;"></a>
</p>
<hr>
<h2><a id="user-content-four-gp-regression-likelihood" class="anchor" aria-hidden="true" href="#four-gp-regression-likelihood"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Four GP Regression likelihood</h2>
<ul>
<li><strong>Gaussian</strong> : The standard Gaussian Process regression model with a Gaussian Likelihood (no data augmentation was needed here) <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/blob/master/examples/Regression%20-%20Gaussian.ipynb" rel="nofollow">IJulia example</a>/<a href="https://arxiv.org/abs/1309.6835" rel="nofollow">Reference</a></li>
<li><strong>StudentT</strong> : The standard Gaussian Process regression with a Student-t likelihood (the degree of freedom ν is not optimizable for the moment) <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/blob/master/examples/Regression%20-%20StudentT.ipynb" rel="nofollow">IJulia example</a>/<a href="http://www.jmlr.org/papers/volume12/jylanki11a/jylanki11a.pdf" rel="nofollow">Reference</a></li>
<li><strong>Laplace</strong> : Gaussian Process regression with a Laplace likelihood <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/blob/master/examples/Regression%20-%20Laplace.ipynb" rel="nofollow">IJulia example</a>/(No reference at the moment)</li>
<li><strong>Heteroscedastic</strong> : Regression with non-stationary noise, given by an additional GP. (no reference at the moment)</li>
</ul>
<p align="center">
   <a target="_blank" rel="noopener noreferrer" href="docs/src/assets/Regression.png"><img src="docs/src/assets/Regression.png" style="max-width:100%;"></a>
 </p>
<hr>
<h2><a id="user-content-two-gp-event-counting-likelihoods" class="anchor" aria-hidden="true" href="#two-gp-event-counting-likelihoods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Two GP event counting likelihoods</h2>
<ul>
<li><strong>Discrete Poisson Process</strong> : Estimating a the Poisson parameter λ at every point (as λ₀σ(f)). (no reference at the moment)</li>
<li><strong>Negative Binomial</strong> : Estimating the success probability at every point for a negative binomial distribution (no reference at the miment)</li>
</ul>
 <p align="center">
    <a target="_blank" rel="noopener noreferrer" href="docs/src/assets/Events.png"><img src="docs/src/assets/Events.png" style="max-width:100%;"></a>
  </p>
<hr>
<h2><a id="user-content-one-multi-class-classification-likelihood" class="anchor" aria-hidden="true" href="#one-multi-class-classification-likelihood"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>One Multi-Class Classification Likelihood</h2>
<ul>
<li><strong>Logistic-SoftMax</strong> : A modified version of the softmax where the exponential is replaced by the logistic function <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/blob/master/examples/MultiClass%20-%20LogisticSoftMax.ipynb" rel="nofollow">IJulia example</a>/<a href="https://arxiv.org/abs/1905.09670" rel="nofollow">Reference</a></li>
</ul>
 <p align="center">
   <a target="_blank" rel="noopener noreferrer" href="docs/src/assets/final3D.png"><img src="docs/src/assets/final3D.png" width="400px" style="max-width:100%;"></a>
 </p>
<hr>
<h2><a id="user-content-multi-ouput-models" class="anchor" aria-hidden="true" href="#multi-ouput-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multi-Ouput models</h2>
<ul>
<li>It is also possible to create a multi-ouput model where the outputs are a linear combination of inducing variables see  <a href="">IJulia example in preparation</a>/[Reference][neuripsmultiouput]</li>
</ul>
<h2><a id="user-content-more-models-in-development" class="anchor" aria-hidden="true" href="#more-models-in-development"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>More models in development</h2>
<ul>
<li><strong>Probit</strong> : A Classifier with a Bernoulli likelihood with the probit link</li>
<li><strong>Online</strong> : Allowing for all algorithms to work online as well</li>
</ul>
<h2><a id="user-content-install-the-package" class="anchor" aria-hidden="true" href="#install-the-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install the package</h2>
<p>The package requires at least <a href="https://julialang.org/" rel="nofollow">Julia 1.3</a>
Run <code>julia</code>, press <code>]</code> and type <code>add AugmentedGaussianProcesses</code>, it will install the package and all its dependencies.</p>
<h2><a id="user-content-use-the-package" class="anchor" aria-hidden="true" href="#use-the-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Use the package</h2>
<p>A complete documentation is available <a href="https://theogf.github.io/AugmentedGaussianProcesses.jl/stable" rel="nofollow">in the docs</a>. For a short start now you can use this very basic example where <code>X_train</code> is a matrix <code>N x D</code> where <code>N</code> is the number of training points and <code>D</code> is the number of dimensions and <code>Y_train</code> is a vector of outputs (or matrix of independent outputs).</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using AugmentedGaussianProcesses;
using KernelFunctions
model = SVGP(X_train, Y_train, SqExponentialKernel(), LogisticLikelihood(),AnalyticSVI(100), 64)
train!(model, 100)
Y_predic = predict_y(model, X_test) #For getting the label directly
Y_predic_prob, Y_predic_prob_var = proba_y(model,X_test) #For getting the likelihood (and likelihood uncertainty) of predicting class 1
"><pre><span class="pl-k">using</span> AugmentedGaussianProcesses;
<span class="pl-k">using</span> KernelFunctions
model <span class="pl-k">=</span> <span class="pl-c1">SVGP</span>(X_train, Y_train, <span class="pl-c1">SqExponentialKernel</span>(), <span class="pl-c1">LogisticLikelihood</span>(),<span class="pl-c1">AnalyticSVI</span>(<span class="pl-c1">100</span>), <span class="pl-c1">64</span>)
<span class="pl-c1">train!</span>(model, <span class="pl-c1">100</span>)
Y_predic <span class="pl-k">=</span> <span class="pl-c1">predict_y</span>(model, X_test) <span class="pl-c"><span class="pl-c">#</span>For getting the label directly</span>
Y_predic_prob, Y_predic_prob_var <span class="pl-k">=</span> <span class="pl-c1">proba_y</span>(model,X_test) <span class="pl-c"><span class="pl-c">#</span>For getting the likelihood (and likelihood uncertainty) of predicting class 1</span></pre></div>
<p>Both <a href="https://theogf.github.io/AugmentedGaussianProcesses.jl/stable/" rel="nofollow">documentation</a> and <a href="https://nbviewer.jupyter.org/github/theogf/AugmentedGaussianProcesses.jl/tree/master/examples/" rel="nofollow">examples/tutorials</a> are available.</p>
<h2><a id="user-content-references-" class="anchor" aria-hidden="true" href="#references-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References :</h2>
<p>Check out <a href="https://theogf.github.io" rel="nofollow">my website for more news</a></p>
<p><a href="http://www.gaussianprocess.org/gpml/" rel="nofollow">"Gaussian Processes for Machine Learning"</a> by Carl Edward Rasmussen and Christopher K.I. Williams</p>
<p>AISTATS 20' <a href="https://arxiv.org/abs/2002.11451" rel="nofollow">"Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models"</a> by  Théo Galy-Fajou, Florian Wenzel and Manfred Opper [<a href="https://arxiv.org/abs/2002.11451][autoconj" rel="nofollow">https://arxiv.org/abs/2002.11451][autoconj</a>]</p>
<p>UAI 19' "Multi-Class Gaussian Process Classification Made Conjugate: Efficient Inference via Data Augmentation" by Théo Galy-Fajou, Florian Wenzel, Christian Donner and Manfred Opper <a href="https://arxiv.org/abs/1905.09670" rel="nofollow">https://arxiv.org/abs/1905.09670</a></p>
<p>ECML 17' "Bayesian Nonlinear Support Vector Machines for Big Data" by Florian Wenzel, Théo Galy-Fajou, Matthäus Deutsch and Marius Kloft. <a href="https://arxiv.org/abs/1707.05532" rel="nofollow">https://arxiv.org/abs/1707.05532</a></p>
<p>AAAI 19' "Efficient Gaussian Process Classification using Polya-Gamma Variables" by Florian Wenzel, Théo Galy-Fajou, Christian Donner, Marius Kloft and Manfred Opper. <a href="https://arxiv.org/abs/1802.06383" rel="nofollow">https://arxiv.org/abs/1802.06383</a></p>
<p>NeurIPS 18' "Moreno-Muñoz, Pablo, Antonio Artés, and Mauricio Álvarez. "Heterogeneous multi-output Gaussian process prediction." Advances in Neural Information Processing Systems. 2018." [<a href="https://papers.nips.cc/paper/7905-heterogeneous-multi-output-gaussian-process-prediction][neuripsmultiouput" rel="nofollow">https://papers.nips.cc/paper/7905-heterogeneous-multi-output-gaussian-process-prediction][neuripsmultiouput</a>]</p>
<p>UAI 13' "Gaussian Process for Big Data" by James Hensman, Nicolo Fusi and Neil D. Lawrence <a href="https://arxiv.org/abs/1309.6835" rel="nofollow">https://arxiv.org/abs/1309.6835</a></p>
<p>JMLR 11' "Robust Gaussian process regression with a Student-t likelihood." by Jylänki Pasi, Jarno Vanhatalo, and Aki Vehtari.  <a href="http://www.jmlr.org/papers/volume12/jylanki11a/jylanki11a.pdf" rel="nofollow">http://www.jmlr.org/papers/v12/jylanki11a.html</a></p>
</article></div>