<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-scientific-computing-benchmarks" class="anchor" aria-hidden="true" href="#scientific-computing-benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scientific Computing Benchmarks</h1>
<p><a href="https://travis-ci.org/jonathanBieler/ScientificComputingBenchmarks.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e69563fc893bb6521ead9915ae6e60eb715f5a46/68747470733a2f2f7472617669732d63692e6f72672f6a6f6e617468616e4269656c65722f536369656e7469666963436f6d707574696e6742656e63686d61726b732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/jonathanBieler/ScientificComputingBenchmarks.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/jonathanBieler/ScientificComputingBenchmarks.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/d0b5d678ec506e4034c8938f59b09aea895f2f91/68747470733a2f2f636f6465636f762e696f2f67682f6a6f6e617468616e4269656c65722f536369656e7469666963436f6d707574696e6742656e63686d61726b732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/jonathanBieler/ScientificComputingBenchmarks.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>Benchmark common scientific programming operations in Julia, R and Python.</p>
<p>All timings are show relative to Julia (with absolute value in millisecond shown
for Julia). Each benchmark is run ten times and the best timing is kept.</p>
<h2><a id="user-content-results" class="anchor" aria-hidden="true" href="#results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Results</h2>
<p><strong>Bioinformatics</strong></p>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Julia</th>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../master/src/Benchmarks/Bioinformatics/Genomics/">Computing mean quality of reads</a></td>
<td>1.0 (1618.41ms)</td>
<td>1.71</td>
<td>5.83</td>
</tr>
</tbody>
</table>
<blockquote>
</blockquote>
<p><strong>DataFrames</strong></p>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Julia</th>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../master/src/Benchmarks/DataFrames/DataFrames/">Loading a 10k row dataframe</a></td>
<td>1.0 (16.7ms)</td>
<td>1.89</td>
<td>0.59</td>
</tr>
<tr>
<td><a href="../master/src/Benchmarks/DataFrames/DataFrames/">join</a></td>
<td>1.0 (9.71ms)</td>
<td>1.53</td>
<td>2.11</td>
</tr>
<tr>
<td><a href="../master/src/Benchmarks/DataFrames/DataFrames/">apply function on a column</a></td>
<td>1.0 (0.06ms)</td>
<td>13.16</td>
<td>31.46</td>
</tr>
<tr>
<td><a href="../master/src/Benchmarks/DataFrames/DataFrames/">sort</a></td>
<td>1.0 (2.54ms)</td>
<td>0.22</td>
<td>0.89</td>
</tr>
</tbody>
</table>
<blockquote>
</blockquote>
<p><strong>Optimization</strong></p>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Julia</th>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../master/src/Benchmarks/Optimization/Function_Minimization/">Optimize a function with Nelder-Mead</a></td>
<td>1.0 (1.0ms)</td>
<td>1423.36</td>
<td>61.46</td>
</tr>
</tbody>
</table>
<blockquote>
</blockquote>
<p><strong>RosettaCode</strong></p>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Julia</th>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../master/src/Benchmarks/RosettaCode/RosettaCode/">Runge-Kutta method</a></td>
<td>1.0 (0.06ms)</td>
<td>55.17</td>
<td>34.31</td>
</tr>
</tbody>
</table>
<blockquote>
</blockquote>
<p><strong>Statistics</strong></p>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Julia</th>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../master/src/Benchmarks/Statistics/Evaluating_pdf/">Evaluate pdf</a></td>
<td>1.0 (0.18ms)</td>
<td>13.33</td>
<td>457.52</td>
</tr>
<tr>
<td><a href="../master/src/Benchmarks/Statistics/Evaluating_pdf/">Evaluate pdf (vectorized)</a></td>
<td>1.0 (0.2ms)</td>
<td>6.97</td>
<td>5.56</td>
</tr>
<tr>
<td><a href="../master/src/Benchmarks/Statistics/Evaluating_pdf/">Generate random numbers</a></td>
<td>1.0 (0.87ms)</td>
<td>0.96</td>
<td>3.87</td>
</tr>
</tbody>
</table>
<blockquote>
</blockquote>
<p><strong>Misc</strong></p>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Julia</th>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../master/src/Benchmarks/Misc/Sleep/">Sleep for 10ms</a></td>
<td>1.0 (11.09ms)</td>
<td>0.91</td>
<td>0.91</td>
</tr>
</tbody>
</table>
<blockquote>
</blockquote>
<h2><a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contribute</h2>
<h3><a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Setup</h3>
<ul>
<li>Clone the package on Julia &gt;v0.7.</li>
<li>Install <a href="https://conda.io/miniconda.html" rel="nofollow">Python 3.6</a> and <a href="https://www.r-project.org" rel="nofollow">R</a>.</li>
<li>Install dependencies for <a href="https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl/blob/master/.travis.yml#L24">Python</a>  and <a href="https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl/blob/master/.travis.yml#L31">R</a>.</li>
<li>Test the package to run the benchmarks.</li>
</ul>
<p>Note that the commands <code>julia</code>, <code>python</code> and <code>R</code> need to refer to the correct versions.</p>
<h3><a id="user-content-adding-benchmarks" class="anchor" aria-hidden="true" href="#adding-benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding benchmarks</h3>
<p><a href="https://github.com/jonathanBieler/ScientificComputingBenchmarks.jl/tree/master/src/Benchmarks">Benchmarks</a> are organized in categories. Each subfolder in a category contains
a file for each language and a file can contain several benchmarks that are
run together. You can simply add a benchmark by editing these files.</p>
<p>To add a new category or a new subfolder use:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ScientificComputingBenchmarks
ScientificComputingBenchmarks<span class="pl-k">.</span><span class="pl-c1">add_benchmark</span>(<span class="pl-s"><span class="pl-pds">"</span>Category<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>Subfolder<span class="pl-pds">"</span></span>)</pre></div>
<p>This will create the folders and templates for each language.</p>
<p>Packages for R and Python are installed in the <code>.travis.yml</code> file, and dependencies
for Julia need to be added to this package.</p>
</article></div>