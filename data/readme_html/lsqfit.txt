<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-lsqfitjl" class="anchor" aria-hidden="true" href="#lsqfitjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>LsqFit.jl</h1>
<p>The LsqFit package is a small library that provides basic least-squares fitting in pure Julia under an MIT license. The basic functionality was originaly in <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a>, before being separated into this library.  At this time, <code>LsqFit</code> only utilizes the Levenberg-Marquardt algorithm for non-linear fitting.</p>
<p><a href="https://travis-ci.org/JuliaNLSolvers/LsqFit.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/fbc6955fb34a6542279b4df50d59f5bf6d1ac92e/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614e4c536f6c766572732f4c73714669742e6a6c2e737667" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaNLSolvers/LsqFit.jl.svg" style="max-width:100%;"></a>
<a href="https://julianlsolvers.github.io/LsqFit.jl/latest/" rel="nofollow"><img src="https://camo.githubusercontent.com/57bae07ecd50a99519ad0516d91f4ec8f0f48e12/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="latest" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic Usage</h2>
<p>There are top-level methods <code>curve_fit()</code> and <code>estimate_errors()</code> that are useful for fitting data to non-linear models. See the following example. Let's first define the model function:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> LsqFit

<span class="pl-c"><span class="pl-c">#</span> a two-parameter exponential model</span>
<span class="pl-c"><span class="pl-c">#</span> x: array of independent variables</span>
<span class="pl-c"><span class="pl-c">#</span> p: array of model parameters</span>
<span class="pl-c"><span class="pl-c">#</span> model(x, p) will accept the full data set as the first argument `x`.</span>
<span class="pl-c"><span class="pl-c">#</span> This means that we need to write our model function so it applies</span>
<span class="pl-c"><span class="pl-c">#</span> the model to the full dataset. We use `@.` to apply the calculations</span>
<span class="pl-c"><span class="pl-c">#</span> across all rows.</span>
<span class="pl-c1">@.</span> <span class="pl-en">model</span>(x, p) <span class="pl-k">=</span> p[<span class="pl-c1">1</span>]<span class="pl-k">*</span><span class="pl-c1">exp</span>(<span class="pl-k">-</span>x<span class="pl-k">*</span>p[<span class="pl-c1">2</span>])</pre></div>
<p>The function applies the per observation function <code>p[1]*exp(-x[i]*p[2])</code> to the full dataset in <code>x</code>, with <code>i</code> denoting an observation row. We simulate some data and chose our "true" parameters.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> some example data</span>
<span class="pl-c"><span class="pl-c">#</span> xdata: independent variables</span>
<span class="pl-c"><span class="pl-c">#</span> ydata: dependent variable</span>
xdata <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">0</span>, stop<span class="pl-k">=</span><span class="pl-c1">10</span>, length<span class="pl-k">=</span><span class="pl-c1">20</span>)
ydata <span class="pl-k">=</span> <span class="pl-c1">model</span>(xdata, [<span class="pl-c1">1.0</span> <span class="pl-c1">2.0</span>]) <span class="pl-k">+</span> <span class="pl-c1">0.01</span><span class="pl-k">*</span><span class="pl-c1">randn</span>(<span class="pl-c1">length</span>(xdata))
p0 <span class="pl-k">=</span> [<span class="pl-c1">0.5</span>, <span class="pl-c1">0.5</span>]</pre></div>
<p>Now, we're ready to fit the model.</p>
<div class="highlight highlight-source-julia"><pre>fit <span class="pl-k">=</span> <span class="pl-c1">curve_fit</span>(model, xdata, ydata, p0)
<span class="pl-c"><span class="pl-c">#</span> fit is a composite type (LsqFitResult), with some interesting values:</span>
<span class="pl-c"><span class="pl-c">#</span>	dof(fit): degrees of freedom</span>
<span class="pl-c"><span class="pl-c">#</span>	coef(fit): best fit parameters</span>
<span class="pl-c"><span class="pl-c">#</span>	fit.resid: residuals = vector of residuals</span>
<span class="pl-c"><span class="pl-c">#</span>	fit.jacobian: estimated Jacobian at solution</span>
lb <span class="pl-k">=</span> [<span class="pl-c1">1.1</span>, <span class="pl-k">-</span><span class="pl-c1">0.5</span>]
ub <span class="pl-k">=</span> [<span class="pl-c1">1.9</span>, <span class="pl-c1">Inf</span>]
p0_bounds <span class="pl-k">=</span> [<span class="pl-c1">1.2</span>, <span class="pl-c1">1.2</span>] <span class="pl-c"><span class="pl-c">#</span> we have to start inside the bounds </span>
<span class="pl-c"><span class="pl-c">#</span> Optional upper and/or lower bounds on the free parameters can be passed as an argument.</span>
<span class="pl-c"><span class="pl-c">#</span> Bounded and unbouded variables can be mixed by setting `-Inf` if no lower bounds</span>
<span class="pl-c"><span class="pl-c">#</span> is to be enforced for that variable and similarly for `+Inf`</span>
fit_bounds <span class="pl-k">=</span> <span class="pl-c1">curve_fit</span>(model, xdata, ydata, p0_bounds, lower<span class="pl-k">=</span>lb, upper<span class="pl-k">=</span>ub)

<span class="pl-c"><span class="pl-c">#</span> We can estimate errors on the fit parameters,</span>
<span class="pl-c"><span class="pl-c">#</span> to get standard error of each parameter:</span>
sigma <span class="pl-k">=</span> <span class="pl-c1">stderror</span>(fit)
<span class="pl-c"><span class="pl-c">#</span> to get margin of error and confidence interval of each parameter at 5% significance level:</span>
margin_of_error <span class="pl-k">=</span> <span class="pl-c1">margin_error</span>(fit, <span class="pl-c1">0.05</span>)
confidence_inter <span class="pl-k">=</span> <span class="pl-c1">confidence_interval</span>(fit, <span class="pl-c1">0.05</span>)

<span class="pl-c"><span class="pl-c">#</span> The finite difference method is used above to approximate the Jacobian.</span>
<span class="pl-c"><span class="pl-c">#</span> Alternatively, a function which calculates it exactly can be supplied instead.</span>
<span class="pl-k">function</span> <span class="pl-en">jacobian_model</span>(x,p)
    J <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{Float64}</span>(undef, <span class="pl-c1">length</span>(x), <span class="pl-c1">length</span>(p))
    <span class="pl-c1">@.</span> J[:,<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-c1">exp</span>(<span class="pl-k">-</span>x<span class="pl-k">*</span>p[<span class="pl-c1">2</span>])     <span class="pl-c"><span class="pl-c">#</span>dmodel/dp[1]</span>
    <span class="pl-c1">@.</span> <span class="pl-c1">@views</span> J[:,<span class="pl-c1">2</span>] <span class="pl-k">=</span> <span class="pl-k">-</span>x<span class="pl-k">*</span>p[<span class="pl-c1">1</span>]<span class="pl-k">*</span>J[:,<span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span>dmodel/dp[2], thanks to @views we don't allocate memory for the J[:,1] slice</span>
    J
<span class="pl-k">end</span>
fit <span class="pl-k">=</span> <span class="pl-c1">curve_fit</span>(model, jacobian_model, xdata, ydata, p0)</pre></div>
<h2><a id="user-content-multivariate-regression" class="anchor" aria-hidden="true" href="#multivariate-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multivariate regression</h2>
<p>There's nothing inherently different if there are more than one variable entering the problem. We just need to specify the columns appropriately in our model specification:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">@.</span> <span class="pl-en">multimodel</span>(x, p) <span class="pl-k">=</span> p[<span class="pl-c1">1</span>]<span class="pl-k">*</span><span class="pl-c1">exp</span>(<span class="pl-k">-</span>x[:, <span class="pl-c1">1</span>]<span class="pl-k">*</span>p[<span class="pl-c1">2</span>]<span class="pl-k">+</span>x[:, <span class="pl-c1">2</span>]<span class="pl-k">*</span>p[<span class="pl-c1">3</span>])</pre></div>
<h2><a id="user-content-evaluating-the-jacobian-and-using-automatic-differentiation" class="anchor" aria-hidden="true" href="#evaluating-the-jacobian-and-using-automatic-differentiation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluating the Jacobian and using automatic differentiation</h2>
<p>The default is to calculate the Jacobian using a central finite differences scheme if no Jacobian function is provided. The defaul is to use central differences because it can be more accurate than forward finite differences, but at the expense of computational cost. It is possible to switch to forward finite differences, like MINPACK uses for example, by specifying <code>autodiff=:finiteforward</code>:</p>
<pre><code>fit = curve_fit(model, xdata, ydata, p0; autodiff=:finiteforward)
</code></pre>
<p>It is also possible to use forward mode automatic differentiation as implemented in ForwardDiff.jl by using the <code>autodiff=:forwarddiff</code> keyword.</p>
<pre><code>fit = curve_fit(model, xdata, ydata, p0; autodiff=:forwarddiff)
</code></pre>
<p>Here, you have to be careful not to manually restrict any types in your code to, say, <code>Float64</code>, because ForwardDiff.jl works by passing a special number type through your functions, to auto<em>magically</em> calculate the value and gradient with one evaluation.</p>
<h2><a id="user-content-inplace-model-and-jacobian" class="anchor" aria-hidden="true" href="#inplace-model-and-jacobian"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Inplace model and jacobian</h2>
<p>It is possible to either use an inplace model, or an inplace model <em>and</em> an inplace jacobian. It might be pertinent to use this feature when <code>curve_fit</code> is slow, or consumes a lot of memory</p>
<pre><code>model_inplace(F, x, p) = (@. F = p[1] * exp(-x * p[2]))

function jacobian_inplace(J::Array{Float64,2},x,p)
        @. J[:,1] = exp(-x*p[2])     
        @. @views J[:,2] = -x*p[1]*J[:,1] 
    end
fit = curve_fit(model_inplace, jacobian_inplace, xdata, ydata, p0; inplace = true)
</code></pre>
<h2><a id="user-content-geodesic-acceleration" class="anchor" aria-hidden="true" href="#geodesic-acceleration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Geodesic acceleration</h2>
<p>This package implements optional geodesic acceleration, as outlined by <a href="https://arxiv.org/pdf/1010.1449.pdf" rel="nofollow">this paper</a>. To enable it, one needs to specify the function computing the <em><a href="https://math.stackexchange.com/questions/2342410/why-is-mathbfdt-h-mathbfd-the-second-directional-derivative" rel="nofollow">directional second derivative</a></em> of the function that is fitted, as the <code>avv!</code> parameter. It is also preferable to set <code>lambda</code> and <code>min_step_quality</code>to <code>0</code>:</p>
<pre><code>curve_fit(model, xdata, ydata, p0; avv! = Avv!,lambda=0, min_step_quality = 0)
</code></pre>
<p><code>Avv!</code> must have the following form:</p>
<ul>
<li><code>p</code> is the array of parameters</li>
<li><code>v</code>is the direction in which the direction is taken</li>
<li><code>dir_deriv</code> is the output vector (the function is necessarily inplace)</li>
</ul>
<pre><code>function Avv!(dir_deriv,p,v)
        v1 = v[1]
        v2 = v[2]
        for i=1:length(xdata)
            #compute all the elements of the Hessian matrix
            h11 = 0
            h12 = (-xdata[i] * exp(-xdata[i] * p[2]))
            #h21 = h12
            h22 = (xdata[i]^2 * p[1] * exp(-xdata[i] * p[2]))

            # manually compute v'Hv. This whole process might seem cumbersome, but 
            # allocating temporary matrices quickly becomes REALLY expensive and might even 
            # render the use of geodesic acceleration terribly inefficient  
            dir_deriv[i] = h11*v1^2 + 2*h12*v1*v2 + h22*v2^2

        end
end 
</code></pre>
<p>Typically, if the model to fit outputs <code>[y_1(x),y_2(x),...,y_m(x)]</code>, and that the input data is <code>xdata</code> then <code>Avv!</code>should output an array of size <code>m</code>, where each element is <code>v'*H_i(xdata,p)*v</code>, where <code>H_i</code>is the Hessian matrix of the output <code>y_i</code>with respect to the parameter vector <code>p</code>.</p>
<p>Depending on the size of the dataset, the complexity of the model and the desired tolerance in the fit result, it may be worthwhile to use automatic differentiation (e.g. via <code>Zygote.jl</code> or <code>ForwardDiff.jl</code>) to determine the directional derivative. Although this is potentially less efficient than calculating the directional derivative manually, this additional information will generally lead to more accurate results.</p>
<p>An example of such an implementation is given by:</p>
<pre><code>using LinearAlgebra, Zygote

function Avv!(dir_deriv,p,v)
    for i=1:length(xdata)
        dir_deriv[i] = transpose(v) * Zygote.hessian(z-&gt;model(xdata[i],z),p) * v
    end
end
</code></pre>
<h2><a id="user-content-existing-functionality" class="anchor" aria-hidden="true" href="#existing-functionality"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Existing Functionality</h2>
<p><code>fit = curve_fit(model, [jacobian], x, y, [w,] p0; kwargs...)</code>:</p>
<ul>
<li><code>model</code>: function that takes two arguments (x, params)</li>
<li><code>jacobian</code>: (optional) function that returns the Jacobian matrix of <code>model</code></li>
<li><code>x</code>: the independent variable</li>
<li><code>y</code>: the dependent variable that constrains <code>model</code></li>
<li><code>w</code>: (optional) weight applied to the residual; can be a vector (of <code>length(x)</code> size or empty) or matrix (inverse covariance matrix)</li>
<li><code>p0</code>: initial guess of the model parameters</li>
<li><code>kwargs</code>: tuning parameters for fitting, passed to <code>levenberg_marquardt</code>, such as <code>maxIter</code>, <code>show_trace</code> or <code>lower</code> and <code>upper</code> bounds</li>
<li><code>fit</code>: composite type of results (<code>LsqFitResult</code>)</li>
</ul>
<p>This performs a fit using a non-linear iteration to minimize the (weighted) residual between the model and the dependent variable data (<code>y</code>). The weight (<code>w</code>) can be neglected (as per the example) to perform an unweighted fit. An unweighted fit is the numerical equivalent of <code>w=1</code> for each point  (although unweighted error estimates are handled differently from weighted error estimates even when the weights are uniform).</p>
<hr>
<p><code>sigma = stderror(fit; atol, rtol)</code>:</p>
<ul>
<li><code>fit</code>: result of curve_fit (a <code>LsqFitResult</code> type)</li>
<li><code>atol</code>: absolute tolerance for negativity check</li>
<li><code>rtol</code>: relative tolerance for negativity check</li>
</ul>
<p>This returns the error or uncertainty of each parameter fit to the model and already scaled by the associated degrees of freedom.  Please note, this is a LOCAL quantity calculated from the jacobian of the model evaluated at the best fit point and NOT the result of a parameter exploration.</p>
<p>If no weights are provided for the fits, the variance is estimated from the mean squared error of the fits. If weights are provided, the weights are assumed to be the inverse of the variances or of the covariance matrix, and errors are estimated based on these and the jacobian, assuming a linearization of the model around the minimum squared error point.</p>
<p><code>margin_of_error = margin_error(fit, alpha=0.05; atol, rtol)</code>:</p>
<ul>
<li><code>fit</code>: result of curve_fit (a <code>LsqFitResult</code> type)</li>
<li><code>alpha</code>: significance level</li>
<li><code>atol</code>: absolute tolerance for negativity check</li>
<li><code>rtol</code>: relative tolerance for negativity check</li>
</ul>
<p>This returns the product of standard error and critical value of each parameter at <code>alpha</code> significance level.</p>
<p><code>confidence_interval = confidence_interval(fit, alpha=0.05; atol, rtol)</code>:</p>
<ul>
<li><code>fit</code>: result of curve_fit (a <code>LsqFitResult</code> type)</li>
<li><code>alpha</code>: significance level</li>
<li><code>atol</code>: absolute tolerance for negativity check</li>
<li><code>rtol</code>: relative tolerance for negativity check</li>
</ul>
<p>This returns confidence interval of each parameter at <code>alpha</code> significance level.</p>
<hr>
<p><code>covar = estimate_covar(fit)</code>:</p>
<ul>
<li><code>fit</code>: result of curve_fit (a <code>LsqFitResult</code> type)</li>
<li><code>covar</code>: parameter covariance matrix calculated from the jacobian of the model at the fit point, using the weights (if specified) as the inverse covariance of observations</li>
</ul>
<p>This returns the parameter covariance matrix evaluted at the best fit point.</p>
</article></div>