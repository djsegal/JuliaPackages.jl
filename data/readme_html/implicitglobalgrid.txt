<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content---implicitglobalgridjl-" class="anchor" aria-hidden="true" href="#--implicitglobalgridjl-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a> <a target="_blank" rel="noopener noreferrer" href="docs/logo/logo_ImplicitGlobalGrid.png"><img src="docs/logo/logo_ImplicitGlobalGrid.png" alt="ImplicitGlobalGrid.jl" width="50" style="max-width: 100%;"></a> ImplicitGlobalGrid.jl </h1>
<p dir="auto"><a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl/actions"><img src="https://github.com/eth-cscs/ImplicitGlobalGrid.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a></p>
<p dir="auto">ImplicitGlobalGrid is an outcome of a collaboration of the Swiss National Supercomputing Centre, ETH Zurich (Dr. Samuel Omlin) with Stanford University (Dr. Ludovic Räss) and the Swiss Geocomputing Centre (Prof. Yuri Podladchikov). It renders the distributed parallelization of stencil-based GPU and CPU applications on a regular staggered grid almost trivial and enables close to ideal weak scaling of real-world applications on thousands of GPUs [<a href="https://pretalx.com/juliacon2019/talk/LGHLC3/" rel="nofollow">1</a>, <a href="https://pasc19.pasc-conference.org/program/schedule/presentation/?id=msa218&amp;sess=sess144" rel="nofollow">2</a>, <a href="https://www.youtube.com/watch?v=vPsfZUqI4_0" rel="nofollow">3</a>]:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="docs/images/fig_parEff_HM3D_Julia_CUDA_all_Daint_extrapol.png"><img src="docs/images/fig_parEff_HM3D_Julia_CUDA_all_Daint_extrapol.png" alt="Weak scaling Piz Daint" style="max-width: 100%;"></a></p>
<p dir="auto">ImplicitGlobalGrid relies on the Julia MPI wrapper (<a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a>) to perform halo updates close to hardware limit and leverages CUDA-aware or ROCm-aware MPI for GPU-applications. The communication can straightforwardly be hidden behind computation [<a href="https://pretalx.com/juliacon2019/talk/LGHLC3/" rel="nofollow">1</a>, <a href="https://www.youtube.com/watch?v=vPsfZUqI4_0" rel="nofollow">3</a>] (how this can be done automatically when using ParallelStencil.jl is shown in [<a href="https://www.youtube.com/watch?v=vPsfZUqI4_0" rel="nofollow">3</a>]; a general approach particularly suited for CUDA C applications is explained in [<a href="https://on-demand.gputechconf.com/gtc/2019/video/_/S9368/" rel="nofollow">4</a>]).</p>
<p dir="auto">A particularity of ImplicitGlobalGrid is the automatic <em>implicit creation of the global computational grid</em> based on the number of processes the application is run with (and based on the process topology, which can be explicitly chosen by the user or automatically defined). As a consequence, the user only needs to write a code to solve his problem on one GPU/CPU (<em>local grid</em>); then, <strong>as little as three functions can be enough to transform a single GPU/CPU application into a massively scaling Multi-GPU/CPU application</strong>. See the <a href="#multi-gpu-with-three-functions">example</a> below. 1-D, 2-D and 3-D grids are supported. Here is a sketch of the global grid that results from running a 2-D solver with 4 processes (P1-P4) (a 2x2 process topology is created by default in this case):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="docs/images/implicit_global_grid.png"><img src="docs/images/implicit_global_grid.png" alt="Implicit global grid" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contents</h2>
<ul dir="auto">
<li><a href="#multi-gpu-with-three-functions">Multi-GPU with three functions</a></li>
<li><a href="#50-lines-multi-gpu-example">50-lines Multi-GPU example</a></li>
<li><a href="#straightforward-in-situ-visualization--monitoring">Straightforward in-situ visualization / monitoring</a></li>
<li><a href="#seamless-interoperability-with-mpijl">Seamless interoperability with MPI.jl</a></li>
<li><a href="#cuda-awarerocm-aware-mpi-support">CUDA-aware/ROCm-aware MPI support</a></li>
<li><a href="#module-documentation-callable-from-the-julia-repl--ijulia">Module documentation callable from the Julia REPL / IJulia</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#references">References</a></li>
</ul>
<h2 dir="auto"><a id="user-content-multi-gpu-with-three-functions" class="anchor" aria-hidden="true" href="#multi-gpu-with-three-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Multi-GPU with three functions</h2>
<p dir="auto">Only three functions are required to perform halo updates close to hardware limit:</p>
<ul dir="auto">
<li><code>init_global_grid</code></li>
<li><code>update_halo!</code></li>
<li><code>finalize_global_grid</code></li>
</ul>
<p dir="auto">Three additional functions are provided to query Cartesian coordinates with respect to the global computational grid if required:</p>
<ul dir="auto">
<li><code>x_g</code></li>
<li><code>y_g</code></li>
<li><code>z_g</code></li>
</ul>
<p dir="auto">Moreover, the following three functions allow to query the size of the global grid:</p>
<ul dir="auto">
<li><code>nx_g</code></li>
<li><code>ny_g</code></li>
<li><code>nz_g</code></li>
</ul>
<p dir="auto">The following Multi-GPU 3-D heat diffusion solver illustrates how these functions enable the creation of massively parallel applications.</p>
<h2 dir="auto"><a id="user-content-50-lines-multi-gpu-example" class="anchor" aria-hidden="true" href="#50-lines-multi-gpu-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>50-lines Multi-GPU example</h2>
<p dir="auto">This simple Multi-GPU 3-D heat diffusion solver uses ImplicitGlobalGrid. It relies fully on the broadcasting capabilities of <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>'s <code>CuArray</code> type to perform the stencil-computations with maximal simplicity (<a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> enables also writing explicit GPU kernels which can lead to significantly better performance for these computations).</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ImplicitGlobalGrid, CUDA

@views d_xa(A) = A[2:end  , :     , :     ] .- A[1:end-1, :     , :     ];
@views d_xi(A) = A[2:end  ,2:end-1,2:end-1] .- A[1:end-1,2:end-1,2:end-1];
@views d_ya(A) = A[ :     ,2:end  , :     ] .- A[ :     ,1:end-1, :     ];
@views d_yi(A) = A[2:end-1,2:end  ,2:end-1] .- A[2:end-1,1:end-1,2:end-1];
@views d_za(A) = A[ :     , :     ,2:end  ] .- A[ :     , :     ,1:end-1];
@views d_zi(A) = A[2:end-1,2:end-1,2:end  ] .- A[2:end-1,2:end-1,1:end-1];
@views  inn(A) = A[2:end-1,2:end-1,2:end-1]

@views function diffusion3D()
    # Physics
    lam        = 1.0;                                       # Thermal conductivity
    cp_min     = 1.0;                                       # Minimal heat capacity
    lx, ly, lz = 10.0, 10.0, 10.0;                          # Length of domain in dimensions x, y and z

    # Numerics
    nx, ny, nz = 256, 256, 256;                             # Number of gridpoints dimensions x, y and z
    nt         = 100000;                                    # Number of time steps
    init_global_grid(nx, ny, nz);                           # Initialize the implicit global grid
    dx         = lx/(nx_g()-1);                             # Space step in dimension x
    dy         = ly/(ny_g()-1);                             # ...        in dimension y
    dz         = lz/(nz_g()-1);                             # ...        in dimension z

    # Array initializations
    T     = CUDA.zeros(Float64, nx,   ny,   nz  );
    Cp    = CUDA.zeros(Float64, nx,   ny,   nz  );
    dTedt = CUDA.zeros(Float64, nx-2, ny-2, nz-2);
    qx    = CUDA.zeros(Float64, nx-1, ny-2, nz-2);
    qy    = CUDA.zeros(Float64, nx-2, ny-1, nz-2);
    qz    = CUDA.zeros(Float64, nx-2, ny-2, nz-1);

    # Initial conditions (heat capacity and temperature with two Gaussian anomalies each)
    Cp .= cp_min .+ CuArray([5*exp(-((x_g(ix,dx,Cp)-lx/1.5))^2-((y_g(iy,dy,Cp)-ly/2))^2-((z_g(iz,dz,Cp)-lz/1.5))^2) +
                             5*exp(-((x_g(ix,dx,Cp)-lx/3.0))^2-((y_g(iy,dy,Cp)-ly/2))^2-((z_g(iz,dz,Cp)-lz/1.5))^2) for ix=1:size(T,1), iy=1:size(T,2), iz=1:size(T,3)])
    T  .= CuArray([100*exp(-((x_g(ix,dx,T)-lx/2)/2)^2-((y_g(iy,dy,T)-ly/2)/2)^2-((z_g(iz,dz,T)-lz/3.0)/2)^2) +
                    50*exp(-((x_g(ix,dx,T)-lx/2)/2)^2-((y_g(iy,dy,T)-ly/2)/2)^2-((z_g(iz,dz,T)-lz/1.5)/2)^2) for ix=1:size(T,1), iy=1:size(T,2), iz=1:size(T,3)])

    # Time loop
    dt = min(dx*dx,dy*dy,dz*dz)*cp_min/lam/8.1;                                               # Time step for the 3D Heat diffusion
    for it = 1:nt
        qx    .= -lam.*d_xi(T)./dx;                                                           # Fourier's law of heat conduction: q_x   = -λ δT/δx
        qy    .= -lam.*d_yi(T)./dy;                                                           # ...                               q_y   = -λ δT/δy
        qz    .= -lam.*d_zi(T)./dz;                                                           # ...                               q_z   = -λ δT/δz
        dTedt .= 1.0./inn(Cp).*(-d_xa(qx)./dx .- d_ya(qy)./dy .- d_za(qz)./dz);               # Conservation of energy:           δT/δt = 1/cₚ (-δq_x/δx - δq_y/dy - δq_z/dz)
        T[2:end-1,2:end-1,2:end-1] .= inn(T) .+ dt.*dTedt;                                    # Update of temperature             T_new = T_old + δT/δt
        update_halo!(T);                                                                      # Update the halo of T
    end

    finalize_global_grid();                                                                   # Finalize the implicit global grid
end

diffusion3D()"><pre><span class="pl-k">using</span> ImplicitGlobalGrid, CUDA

<span class="pl-c1">@views</span> <span class="pl-en">d_xa</span>(A) <span class="pl-k">=</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>  , <span class="pl-k">:</span>     , <span class="pl-k">:</span>     ] <span class="pl-k">.-</span> A[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>, <span class="pl-k">:</span>     , <span class="pl-k">:</span>     ];
<span class="pl-c1">@views</span> <span class="pl-en">d_xi</span>(A) <span class="pl-k">=</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>  ,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">.-</span> A[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>];
<span class="pl-c1">@views</span> <span class="pl-en">d_ya</span>(A) <span class="pl-k">=</span> A[ <span class="pl-k">:</span>     ,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>  , <span class="pl-k">:</span>     ] <span class="pl-k">.-</span> A[ <span class="pl-k">:</span>     ,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>, <span class="pl-k">:</span>     ];
<span class="pl-c1">@views</span> <span class="pl-en">d_yi</span>(A) <span class="pl-k">=</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>  ,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">.-</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>];
<span class="pl-c1">@views</span> <span class="pl-en">d_za</span>(A) <span class="pl-k">=</span> A[ <span class="pl-k">:</span>     , <span class="pl-k">:</span>     ,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>  ] <span class="pl-k">.-</span> A[ <span class="pl-k">:</span>     , <span class="pl-k">:</span>     ,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>];
<span class="pl-c1">@views</span> <span class="pl-en">d_zi</span>(A) <span class="pl-k">=</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>  ] <span class="pl-k">.-</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>];
<span class="pl-c1">@views</span>  <span class="pl-en">inn</span>(A) <span class="pl-k">=</span> A[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>]

<span class="pl-c1">@views</span> <span class="pl-k">function</span> <span class="pl-en">diffusion3D</span>()
    <span class="pl-c"><span class="pl-c">#</span> Physics</span>
    lam        <span class="pl-k">=</span> <span class="pl-c1">1.0</span>;                                       <span class="pl-c"><span class="pl-c">#</span> Thermal conductivity</span>
    cp_min     <span class="pl-k">=</span> <span class="pl-c1">1.0</span>;                                       <span class="pl-c"><span class="pl-c">#</span> Minimal heat capacity</span>
    lx, ly, lz <span class="pl-k">=</span> <span class="pl-c1">10.0</span>, <span class="pl-c1">10.0</span>, <span class="pl-c1">10.0</span>;                          <span class="pl-c"><span class="pl-c">#</span> Length of domain in dimensions x, y and z</span>

    <span class="pl-c"><span class="pl-c">#</span> Numerics</span>
    nx, ny, nz <span class="pl-k">=</span> <span class="pl-c1">256</span>, <span class="pl-c1">256</span>, <span class="pl-c1">256</span>;                             <span class="pl-c"><span class="pl-c">#</span> Number of gridpoints dimensions x, y and z</span>
    nt         <span class="pl-k">=</span> <span class="pl-c1">100000</span>;                                    <span class="pl-c"><span class="pl-c">#</span> Number of time steps</span>
    <span class="pl-c1">init_global_grid</span>(nx, ny, nz);                           <span class="pl-c"><span class="pl-c">#</span> Initialize the implicit global grid</span>
    dx         <span class="pl-k">=</span> lx<span class="pl-k">/</span>(<span class="pl-c1">nx_g</span>()<span class="pl-k">-</span><span class="pl-c1">1</span>);                             <span class="pl-c"><span class="pl-c">#</span> Space step in dimension x</span>
    dy         <span class="pl-k">=</span> ly<span class="pl-k">/</span>(<span class="pl-c1">ny_g</span>()<span class="pl-k">-</span><span class="pl-c1">1</span>);                             <span class="pl-c"><span class="pl-c">#</span> ...        in dimension y</span>
    dz         <span class="pl-k">=</span> lz<span class="pl-k">/</span>(<span class="pl-c1">nz_g</span>()<span class="pl-k">-</span><span class="pl-c1">1</span>);                             <span class="pl-c"><span class="pl-c">#</span> ...        in dimension z</span>

    <span class="pl-c"><span class="pl-c">#</span> Array initializations</span>
    T     <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">zeros</span>(Float64, nx,   ny,   nz  );
    Cp    <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">zeros</span>(Float64, nx,   ny,   nz  );
    dTedt <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">zeros</span>(Float64, nx<span class="pl-k">-</span><span class="pl-c1">2</span>, ny<span class="pl-k">-</span><span class="pl-c1">2</span>, nz<span class="pl-k">-</span><span class="pl-c1">2</span>);
    qx    <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">zeros</span>(Float64, nx<span class="pl-k">-</span><span class="pl-c1">1</span>, ny<span class="pl-k">-</span><span class="pl-c1">2</span>, nz<span class="pl-k">-</span><span class="pl-c1">2</span>);
    qy    <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">zeros</span>(Float64, nx<span class="pl-k">-</span><span class="pl-c1">2</span>, ny<span class="pl-k">-</span><span class="pl-c1">1</span>, nz<span class="pl-k">-</span><span class="pl-c1">2</span>);
    qz    <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">zeros</span>(Float64, nx<span class="pl-k">-</span><span class="pl-c1">2</span>, ny<span class="pl-k">-</span><span class="pl-c1">2</span>, nz<span class="pl-k">-</span><span class="pl-c1">1</span>);

    <span class="pl-c"><span class="pl-c">#</span> Initial conditions (heat capacity and temperature with two Gaussian anomalies each)</span>
    Cp <span class="pl-k">.=</span> cp_min <span class="pl-k">.+</span> <span class="pl-c1">CuArray</span>([<span class="pl-c1">5</span><span class="pl-k">*</span><span class="pl-c1">exp</span>(<span class="pl-k">-</span>((<span class="pl-c1">x_g</span>(ix,dx,Cp)<span class="pl-k">-</span>lx<span class="pl-k">/</span><span class="pl-c1">1.5</span>))<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">y_g</span>(iy,dy,Cp)<span class="pl-k">-</span>ly<span class="pl-k">/</span><span class="pl-c1">2</span>))<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">z_g</span>(iz,dz,Cp)<span class="pl-k">-</span>lz<span class="pl-k">/</span><span class="pl-c1">1.5</span>))<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">+</span>
                             <span class="pl-c1">5</span><span class="pl-k">*</span><span class="pl-c1">exp</span>(<span class="pl-k">-</span>((<span class="pl-c1">x_g</span>(ix,dx,Cp)<span class="pl-k">-</span>lx<span class="pl-k">/</span><span class="pl-c1">3.0</span>))<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">y_g</span>(iy,dy,Cp)<span class="pl-k">-</span>ly<span class="pl-k">/</span><span class="pl-c1">2</span>))<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">z_g</span>(iz,dz,Cp)<span class="pl-k">-</span>lz<span class="pl-k">/</span><span class="pl-c1">1.5</span>))<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">for</span> ix<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(T,<span class="pl-c1">1</span>), iy<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(T,<span class="pl-c1">2</span>), iz<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(T,<span class="pl-c1">3</span>)])
    T  <span class="pl-k">.=</span> <span class="pl-c1">CuArray</span>([<span class="pl-c1">100</span><span class="pl-k">*</span><span class="pl-c1">exp</span>(<span class="pl-k">-</span>((<span class="pl-c1">x_g</span>(ix,dx,T)<span class="pl-k">-</span>lx<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">y_g</span>(iy,dy,T)<span class="pl-k">-</span>ly<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">z_g</span>(iz,dz,T)<span class="pl-k">-</span>lz<span class="pl-k">/</span><span class="pl-c1">3.0</span>)<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">+</span>
                    <span class="pl-c1">50</span><span class="pl-k">*</span><span class="pl-c1">exp</span>(<span class="pl-k">-</span>((<span class="pl-c1">x_g</span>(ix,dx,T)<span class="pl-k">-</span>lx<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">y_g</span>(iy,dy,T)<span class="pl-k">-</span>ly<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">-</span>((<span class="pl-c1">z_g</span>(iz,dz,T)<span class="pl-k">-</span>lz<span class="pl-k">/</span><span class="pl-c1">1.5</span>)<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">for</span> ix<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(T,<span class="pl-c1">1</span>), iy<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(T,<span class="pl-c1">2</span>), iz<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(T,<span class="pl-c1">3</span>)])

    <span class="pl-c"><span class="pl-c">#</span> Time loop</span>
    dt <span class="pl-k">=</span> <span class="pl-c1">min</span>(dx<span class="pl-k">*</span>dx,dy<span class="pl-k">*</span>dy,dz<span class="pl-k">*</span>dz)<span class="pl-k">*</span>cp_min<span class="pl-k">/</span>lam<span class="pl-k">/</span><span class="pl-c1">8.1</span>;                                               <span class="pl-c"><span class="pl-c">#</span> Time step for the 3D Heat diffusion</span>
    <span class="pl-k">for</span> it <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span>nt
        qx    <span class="pl-k">.=</span> <span class="pl-k">-</span>lam<span class="pl-k">.*</span><span class="pl-c1">d_xi</span>(T)<span class="pl-k">./</span>dx;                                                           <span class="pl-c"><span class="pl-c">#</span> Fourier's law of heat conduction: q_x   = -λ δT/δx</span>
        qy    <span class="pl-k">.=</span> <span class="pl-k">-</span>lam<span class="pl-k">.*</span><span class="pl-c1">d_yi</span>(T)<span class="pl-k">./</span>dy;                                                           <span class="pl-c"><span class="pl-c">#</span> ...                               q_y   = -λ δT/δy</span>
        qz    <span class="pl-k">.=</span> <span class="pl-k">-</span>lam<span class="pl-k">.*</span><span class="pl-c1">d_zi</span>(T)<span class="pl-k">./</span>dz;                                                           <span class="pl-c"><span class="pl-c">#</span> ...                               q_z   = -λ δT/δz</span>
        dTedt <span class="pl-k">.=</span> <span class="pl-c1">1.0</span><span class="pl-k">./</span><span class="pl-c1">inn</span>(Cp)<span class="pl-k">.*</span>(<span class="pl-k">-</span><span class="pl-c1">d_xa</span>(qx)<span class="pl-k">./</span>dx <span class="pl-k">.-</span> <span class="pl-c1">d_ya</span>(qy)<span class="pl-k">./</span>dy <span class="pl-k">.-</span> <span class="pl-c1">d_za</span>(qz)<span class="pl-k">./</span>dz);               <span class="pl-c"><span class="pl-c">#</span> Conservation of energy:           δT/δt = 1/cₚ (-δq_x/δx - δq_y/dy - δq_z/dz)</span>
        T[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">.=</span> <span class="pl-c1">inn</span>(T) <span class="pl-k">.+</span> dt<span class="pl-k">.*</span>dTedt;                                    <span class="pl-c"><span class="pl-c">#</span> Update of temperature             T_new = T_old + δT/δt</span>
        <span class="pl-c1">update_halo!</span>(T);                                                                      <span class="pl-c"><span class="pl-c">#</span> Update the halo of T</span>
    <span class="pl-k">end</span>

    <span class="pl-c1">finalize_global_grid</span>();                                                                   <span class="pl-c"><span class="pl-c">#</span> Finalize the implicit global grid</span>
<span class="pl-k">end</span>

<span class="pl-c1">diffusion3D</span>()</pre></div>
<p dir="auto">The corresponding file can be found <a href="docs/examples/diffusion3D_multigpu_CuArrays_novis.jl">here</a>. A basic cpu-only example is available <a href="docs/examples/diffusion3D_multicpu_novis.jl">here</a> (no usage of multi-threading).</p>
<h2 dir="auto"><a id="user-content-straightforward-in-situ-visualization--monitoring" class="anchor" aria-hidden="true" href="#straightforward-in-situ-visualization--monitoring"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Straightforward in-situ visualization / monitoring</h2>
<p dir="auto">ImplicitGlobalGrid provides a function to gather an array from each process into a one large array on a single process, assembled according to the global grid:</p>
<ul dir="auto">
<li><code>gather!</code></li>
</ul>
<p dir="auto">This enables straightforward in-situ visualization or monitoring of Multi-GPU/CPU applications using e.g. the <a href="https://github.com/JuliaPlots/Plots.jl">Julia Plots package</a> as shown in the following (the GR backend is used as it is particularly fast according to the <a href="http://docs.juliaplots.org/latest/backends/" rel="nofollow">Julia Plots documentation</a>). It is enough to add a couple of lines to the previous example (omitted unmodified lines are represented with <code>#(...)</code>):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ImplicitGlobalGrid, CUDA, Plots
#(...)

@views function diffusion3D()
    # Physics
    #(...)

    # Numerics
    #(...)
    me, dims   = init_global_grid(nx, ny, nz);              # Initialize the implicit global grid
    #(...)

    # Array initializations
    #(...)

    # Initial conditions (heat capacity and temperature with two Gaussian anomalies each)
    #(...)

    # Preparation of visualisation
    gr()
    ENV[&quot;GKSwstype&quot;]=&quot;nul&quot;
    anim = Animation();
    nx_v = (nx-2)*dims[1];
    ny_v = (ny-2)*dims[2];
    nz_v = (nz-2)*dims[3];
    T_v  = zeros(nx_v, ny_v, nz_v);
    T_nohalo = zeros(nx-2, ny-2, nz-2);

    # Time loop
    #(...)
    for it = 1:nt
        if mod(it, 1000) == 1                                                                 # Visualize only every 1000th time step
            T_nohalo .= Array(T[2:end-1,2:end-1,2:end-1]);                                    # Copy data to CPU removing the halo
            gather!(T_nohalo, T_v)                                                            # Gather data on process 0 (could be interpolated/sampled first)
            if (me==0) heatmap(transpose(T_v[:,ny_v÷2,:]), aspect_ratio=1); frame(anim); end  # Visualize it on process 0
        end
        #(...)
    end

    # Postprocessing
    if (me==0) gif(anim, &quot;diffusion3D.gif&quot;, fps = 15) end                                     # Create a gif movie on process 0
    if (me==0) mp4(anim, &quot;diffusion3D.mp4&quot;, fps = 15) end                                     # Create a mp4 movie on process 0
    finalize_global_grid();                                                                   # Finalize the implicit global grid
end

diffusion3D()"><pre><span class="pl-k">using</span> ImplicitGlobalGrid, CUDA, Plots
<span class="pl-c"><span class="pl-c">#</span>(...)</span>

<span class="pl-c1">@views</span> <span class="pl-k">function</span> <span class="pl-en">diffusion3D</span>()
    <span class="pl-c"><span class="pl-c">#</span> Physics</span>
    <span class="pl-c"><span class="pl-c">#</span>(...)</span>

    <span class="pl-c"><span class="pl-c">#</span> Numerics</span>
    <span class="pl-c"><span class="pl-c">#</span>(...)</span>
    me, dims   <span class="pl-k">=</span> <span class="pl-c1">init_global_grid</span>(nx, ny, nz);              <span class="pl-c"><span class="pl-c">#</span> Initialize the implicit global grid</span>
    <span class="pl-c"><span class="pl-c">#</span>(...)</span>

    <span class="pl-c"><span class="pl-c">#</span> Array initializations</span>
    <span class="pl-c"><span class="pl-c">#</span>(...)</span>

    <span class="pl-c"><span class="pl-c">#</span> Initial conditions (heat capacity and temperature with two Gaussian anomalies each)</span>
    <span class="pl-c"><span class="pl-c">#</span>(...)</span>

    <span class="pl-c"><span class="pl-c">#</span> Preparation of visualisation</span>
    <span class="pl-c1">gr</span>()
    <span class="pl-c1">ENV</span>[<span class="pl-s"><span class="pl-pds">"</span>GKSwstype<span class="pl-pds">"</span></span>]<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>nul<span class="pl-pds">"</span></span>
    anim <span class="pl-k">=</span> <span class="pl-c1">Animation</span>();
    nx_v <span class="pl-k">=</span> (nx<span class="pl-k">-</span><span class="pl-c1">2</span>)<span class="pl-k">*</span>dims[<span class="pl-c1">1</span>];
    ny_v <span class="pl-k">=</span> (ny<span class="pl-k">-</span><span class="pl-c1">2</span>)<span class="pl-k">*</span>dims[<span class="pl-c1">2</span>];
    nz_v <span class="pl-k">=</span> (nz<span class="pl-k">-</span><span class="pl-c1">2</span>)<span class="pl-k">*</span>dims[<span class="pl-c1">3</span>];
    T_v  <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(nx_v, ny_v, nz_v);
    T_nohalo <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(nx<span class="pl-k">-</span><span class="pl-c1">2</span>, ny<span class="pl-k">-</span><span class="pl-c1">2</span>, nz<span class="pl-k">-</span><span class="pl-c1">2</span>);

    <span class="pl-c"><span class="pl-c">#</span> Time loop</span>
    <span class="pl-c"><span class="pl-c">#</span>(...)</span>
    <span class="pl-k">for</span> it <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span>nt
        <span class="pl-k">if</span> <span class="pl-c1">mod</span>(it, <span class="pl-c1">1000</span>) <span class="pl-k">==</span> <span class="pl-c1">1</span>                                                                 <span class="pl-c"><span class="pl-c">#</span> Visualize only every 1000th time step</span>
            T_nohalo <span class="pl-k">.=</span> <span class="pl-c1">Array</span>(T[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>,<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>]);                                    <span class="pl-c"><span class="pl-c">#</span> Copy data to CPU removing the halo</span>
            <span class="pl-c1">gather!</span>(T_nohalo, T_v)                                                            <span class="pl-c"><span class="pl-c">#</span> Gather data on process 0 (could be interpolated/sampled first)</span>
            <span class="pl-k">if</span> (me<span class="pl-k">==</span><span class="pl-c1">0</span>) <span class="pl-c1">heatmap</span>(<span class="pl-c1">transpose</span>(T_v[:,ny_v<span class="pl-k">÷</span><span class="pl-c1">2</span>,:]), aspect_ratio<span class="pl-k">=</span><span class="pl-c1">1</span>); <span class="pl-c1">frame</span>(anim); <span class="pl-k">end</span>  <span class="pl-c"><span class="pl-c">#</span> Visualize it on process 0</span>
        <span class="pl-k">end</span>
        <span class="pl-c"><span class="pl-c">#</span>(...)</span>
    <span class="pl-k">end</span>

    <span class="pl-c"><span class="pl-c">#</span> Postprocessing</span>
    <span class="pl-k">if</span> (me<span class="pl-k">==</span><span class="pl-c1">0</span>) <span class="pl-c1">gif</span>(anim, <span class="pl-s"><span class="pl-pds">"</span>diffusion3D.gif<span class="pl-pds">"</span></span>, fps <span class="pl-k">=</span> <span class="pl-c1">15</span>) <span class="pl-k">end</span>                                     <span class="pl-c"><span class="pl-c">#</span> Create a gif movie on process 0</span>
    <span class="pl-k">if</span> (me<span class="pl-k">==</span><span class="pl-c1">0</span>) <span class="pl-c1">mp4</span>(anim, <span class="pl-s"><span class="pl-pds">"</span>diffusion3D.mp4<span class="pl-pds">"</span></span>, fps <span class="pl-k">=</span> <span class="pl-c1">15</span>) <span class="pl-k">end</span>                                     <span class="pl-c"><span class="pl-c">#</span> Create a mp4 movie on process 0</span>
    <span class="pl-c1">finalize_global_grid</span>();                                                                   <span class="pl-c"><span class="pl-c">#</span> Finalize the implicit global grid</span>
<span class="pl-k">end</span>

<span class="pl-c1">diffusion3D</span>()</pre></div>
<p dir="auto">Here is the resulting movie when running the application on 8 GPUs, solving 3-D heat diffusion with heterogeneous heat capacity (two Gaussian anomalies) on a global computational grid of size 510x510x510 grid points. It shows the x-z-dimension plane in the middle of the dimension y:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="docs/movies/diffusion3D_8gpus.gif"><img src="docs/movies/diffusion3D_8gpus.gif" alt="Implicit global grid" data-animated-image="" style="max-width: 100%;"></a></p>
<p dir="auto">The simulation producing this movie - <em>including the in-situ visualization</em> - took 29 minutes on 8 NVIDIA® Tesla® P100 GPUs on Piz Daint (an optimized solution using <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>'s native kernel programming capabilities can be more than 10 times faster).
The complete example can be found <a href="docs/examples/diffusion3D_multigpu_CuArrays.jl">here</a>. A corresponding basic cpu-only example is available <a href="docs/examples/diffusion3D_multicpu.jl">here</a> (no usage of multi-threading) and a movie of a simulation with 254x254x254 grid points which it produced within 34 minutes using 8 Intel® Xeon® E5-2690 v3 is found <a href="docs/movies/diffusion3D_8cpus.gif">here</a> (with 8 processes, no multi-threading).</p>
<h2 dir="auto"><a id="user-content-seamless-interoperability-with-mpijl" class="anchor" aria-hidden="true" href="#seamless-interoperability-with-mpijl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Seamless interoperability with MPI.jl</h2>
<p dir="auto">ImplicitGlobalGrid is seamlessly interoperable with <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a>. The Cartesian MPI communicator it uses is created by default when calling <code>init_global_grid</code> and can then be obtained as follows (variable <code>comm_cart</code>):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="me, dims, nprocs, coords, comm_cart = init_global_grid(nx, ny, nz);"><pre>me, dims, nprocs, coords, comm_cart <span class="pl-k">=</span> <span class="pl-c1">init_global_grid</span>(nx, ny, nz);</pre></div>
<p dir="auto">Moreover, the automatic initialization and finalization of MPI can be deactivated in order to replace them with direct calls to <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="init_global_grid(nx, ny, nz; init_MPI=false);"><pre><span class="pl-c1">init_global_grid</span>(nx, ny, nz; init_MPI<span class="pl-k">=</span><span class="pl-c1">false</span>);</pre></div>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="finalize_global_grid(;finalize_MPI=false)"><pre><span class="pl-c1">finalize_global_grid</span>(;finalize_MPI<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p dir="auto">Besides, <code>init_global_grid</code> makes every argument it passes to an <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a> function customizable via its keyword arguments.</p>
<h2 dir="auto"><a id="user-content-cuda-awarerocm-aware-mpi-support" class="anchor" aria-hidden="true" href="#cuda-awarerocm-aware-mpi-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CUDA-aware/ROCm-aware MPI support</h2>
<p dir="auto">If the system supports CUDA-aware/ROCm-aware MPI, it may be activated for ImplicitGlobalGrid by setting an environment variable as specified in the module documentation callable from the <a href="https://docs.julialang.org/en/v1/stdlib/REPL/" rel="nofollow">Julia REPL</a> or in <a href="https://github.com/JuliaLang/IJulia.jl">IJulia</a> (see next section).</p>
<h2 dir="auto"><a id="user-content-module-documentation-callable-from-the-julia-repl--ijulia" class="anchor" aria-hidden="true" href="#module-documentation-callable-from-the-julia-repl--ijulia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Module documentation callable from the Julia REPL / IJulia</h2>
<p dir="auto">The module documentation can be called from the <a href="https://docs.julialang.org/en/v1/stdlib/REPL/" rel="nofollow">Julia REPL</a> or in <a href="https://github.com/JuliaLang/IJulia.jl">IJulia</a>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; using ImplicitGlobalGrid
julia&gt;?
help?&gt; ImplicitGlobalGrid
search: ImplicitGlobalGrid

  Module ImplicitGlobalGrid

  Renders the distributed parallelization of stencil-based GPU and CPU applications on a
  regular staggered grid almost trivial and enables close to ideal weak scaling of
  real-world applications on thousands of GPUs.

  General overview and examples
  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡

  https://github.com/eth-cscs/ImplicitGlobalGrid.jl

  Functions
  ≡≡≡≡≡≡≡≡≡≡≡

    •    init_global_grid

    •    finalize_global_grid

    •    update_halo!

    •    gather!

    •    select_device

    •    nx_g

    •    ny_g

    •    nz_g

    •    x_g

    •    y_g

    •    z_g

    •    tic

    •    toc

  To see a description of a function type ?&lt;functionname&gt;.

  │ Performance note
  │
  │  If the system supports CUDA-aware MPI (for Nvidia GPUs) or
  │  ROCm-aware MPI (for AMD GPUs), it may be activated for
  │  ImplicitGlobalGrid by setting one of the following environment
  │  variables (at latest before the call to init_global_grid):
  │
  │  shell&gt; export IGG_CUDAAWARE_MPI=1
  │
  │  shell&gt; export IGG_ROCMAWARE_MPI=1

julia&gt;"><pre lang="julia-repl" class="notranslate"><code>julia&gt; using ImplicitGlobalGrid
julia&gt;?
help?&gt; ImplicitGlobalGrid
search: ImplicitGlobalGrid

  Module ImplicitGlobalGrid

  Renders the distributed parallelization of stencil-based GPU and CPU applications on a
  regular staggered grid almost trivial and enables close to ideal weak scaling of
  real-world applications on thousands of GPUs.

  General overview and examples
  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡

  https://github.com/eth-cscs/ImplicitGlobalGrid.jl

  Functions
  ≡≡≡≡≡≡≡≡≡≡≡

    •    init_global_grid

    •    finalize_global_grid

    •    update_halo!

    •    gather!

    •    select_device

    •    nx_g

    •    ny_g

    •    nz_g

    •    x_g

    •    y_g

    •    z_g

    •    tic

    •    toc

  To see a description of a function type ?&lt;functionname&gt;.

  │ Performance note
  │
  │  If the system supports CUDA-aware MPI (for Nvidia GPUs) or
  │  ROCm-aware MPI (for AMD GPUs), it may be activated for
  │  ImplicitGlobalGrid by setting one of the following environment
  │  variables (at latest before the call to init_global_grid):
  │
  │  shell&gt; export IGG_CUDAAWARE_MPI=1
  │
  │  shell&gt; export IGG_ROCMAWARE_MPI=1

julia&gt;
</code></pre></div>
<h2 dir="auto"><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dependencies</h2>
<p dir="auto">ImplicitGlobalGrid relies on the Julia MPI wrapper (<a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a>), the Julia CUDA package (<a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> [<a href="https://doi.org/10.1109/TPDS.2018.2872064" rel="nofollow">5</a>, <a href="https://doi.org/10.1016/j.advengsoft.2019.02.002" rel="nofollow">6</a>]) and the Julia AMDGPU package (<a href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a>).</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">ImplicitGlobalGrid may be installed directly with the <a href="https://docs.julialang.org/en/v1/stdlib/Pkg/index.html" rel="nofollow">Julia package manager</a> from the REPL:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt;]
  pkg&gt; add ImplicitGlobalGrid
  pkg&gt; test ImplicitGlobalGrid"><pre lang="julia-repl" class="notranslate"><code>julia&gt;]
  pkg&gt; add ImplicitGlobalGrid
  pkg&gt; test ImplicitGlobalGrid
</code></pre></div>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<p dir="auto">[1] <a href="https://pretalx.com/juliacon2019/talk/LGHLC3/" rel="nofollow">Räss, L., Omlin, S., &amp; Podladchikov, Y. Y. (2019). Porting a Massively Parallel Multi-GPU Application to Julia: a 3-D Nonlinear Multi-Physics Flow Solver. JuliaCon Conference, Baltimore, USA.</a></p>
<p dir="auto">[2] <a href="https://pasc19.pasc-conference.org/program/schedule/presentation/?id=msa218&amp;sess=sess144" rel="nofollow">Räss, L., Omlin, S., &amp; Podladchikov, Y. Y. (2019). A Nonlinear Multi-Physics 3-D Solver: From CUDA C + MPI to Julia. PASC19 Conference, Zurich, Switzerland.</a></p>
<p dir="auto">[3] <a href="https://www.youtube.com/watch?v=vPsfZUqI4_0" rel="nofollow">Omlin, S., Räss, L., Kwasniewski, G., Malvoisin, B., &amp; Podladchikov, Y. Y. (2020). Solving Nonlinear Multi-Physics on GPU Supercomputers with Julia. JuliaCon Conference, virtual.</a></p>
<p dir="auto">[4] <a href="https://on-demand.gputechconf.com/gtc/2019/video/_/S9368/" rel="nofollow">Räss, L., Omlin, S., &amp; Podladchikov, Y. Y. (2019). Resolving Spontaneous Nonlinear Multi-Physics Flow Localisation in 3-D: Tackling Hardware Limit. GPU Technology Conference 2019, San Jose, Silicon Valley, CA, USA.</a></p>
<p dir="auto">[5] <a href="https://doi.org/10.1109/TPDS.2018.2872064" rel="nofollow">Besard, T., Foket, C., &amp; De Sutter, B. (2018). Effective Extensible Programming: Unleashing Julia on GPUs. IEEE Transactions on Parallel and Distributed Systems, 30(4), 827-841. doi: 10.1109/TPDS.2018.2872064</a></p>
<p dir="auto">[6] <a href="https://doi.org/10.1016/j.advengsoft.2019.02.002" rel="nofollow">Besard, T., Churavy, V., Edelman, A., &amp; De Sutter B. (2019). Rapid software prototyping for heterogeneous and distributed platforms. Advances in Engineering Software, 132, 29-46. doi: 10.1016/j.advengsoft.2019.02.002</a></p>
</article></div>