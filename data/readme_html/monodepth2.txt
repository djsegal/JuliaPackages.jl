<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-monodepth2jl" class="anchor" aria-hidden="true" href="#monodepth2jl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Monodepth2.jl</h1>
<p dir="auto">Implementation of the <a href="https://arxiv.org/abs/1806.01260" rel="nofollow">"Digging Into Self-Supervised Monocular Depth Estimation"</a> paper.</p>
<h2 dir="auto"><a id="user-content-monodepth" class="anchor" aria-hidden="true" href="#monodepth"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Monodepth</h2>
<p dir="auto">Monocular depth estimation. Using single image to predict disparity map.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="./res/depth-kitti.gif"><img src="./res/depth-kitti.gif" alt="Depth" data-animated-image="" style="max-width: 100%;"></a></p>
<p dir="auto">Training parameters:</p>
<ul dir="auto">
<li>resolution <code>416x128</code>;</li>
<li>ResNet 18 model;</li>
<li>no automasking &amp; using pose prediction network.</li>
</ul>
<h3 dir="auto"><a id="user-content-supported-datasets" class="anchor" aria-hidden="true" href="#supported-datasets"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Supported datasets</h3>
<ul dir="auto">
<li><a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow">KITTI odometry</a></li>
<li><a href="https://github.com/commaai/depth10k">CommaAI Depth10k</a></li>
</ul>
<h3 dir="auto"><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install</h3>
<p dir="auto">Install model, image-augmentation library and the package itself:</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="]add https://github.com/pxl-th/ResNet.jl.git
]add https://github.com/pxl-th/Augmentations.jl.git
]add https://github.com/pxl-th/Monodepth2.jl.git"><pre>]add https://github.com/pxl-th/ResNet.jl.git
]add https://github.com/pxl-th/Augmentations.jl.git
]add https://github.com/pxl-th/Monodepth2.jl.git</pre></div>
<h2 dir="auto"><a id="user-content-simple-disparity-estimation" class="anchor" aria-hidden="true" href="#simple-disparity-estimation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Simple disparity estimation</h2>
<p dir="auto">Simple disparity estimation using gradient descent with parameters:</p>
<ul dir="auto">
<li>disparity map;</li>
<li>rotation vector (so3);</li>
<li>translation vector.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="./res/image.png"><img src="./res/image.png" alt="Triplet" style="max-width: 100%;"></a></p>
<p dir="auto">Visualization of the disparity map learning dynamics for the triplet above.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="./res/simple-depth.gif"><img src="./res/simple-depth.gif" alt="Depth" data-animated-image="" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-important" class="anchor" aria-hidden="true" href="#important"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Important</h2>
<ul dir="auto">
<li>
<p dir="auto">Norm function is computed using <code>sqrt.(sum(abs2, ...))</code>.
However, <code>sqrt</code> function has <code>NaN</code> gradient at <code>0</code>.
This can be mitigated by defining subgradient or even better,
<code>norm</code> function that can act on the given axis,
<a href="https://github.com/pytorch/pytorch/issues/37354" data-hovercard-type="issue" data-hovercard-url="/pytorch/pytorch/issues/37354/hovercard">similar to PyTorch</a>.</p>
</li>
<li>
<p dir="auto">For poses, struct <code>Pose</code> is used instead of arrays or tuple because
of <a href="https://github.com/FluxML/Zygote.jl/issues/522" data-hovercard-type="issue" data-hovercard-url="/FluxML/Zygote.jl/issues/522/hovercard">this issue</a>.</p>
</li>
</ul>
</article></div>