<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-protogradjl" class="anchor" aria-hidden="true" href="#protogradjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ProtoGrad.jl</h1>
<p dir="auto"><a href="https://github.com/lostella/ProtoGrad.jl/actions?query=workflow%3ATest"><img src="https://github.com/lostella/ProtoGrad.jl/workflows/Test/badge.svg" alt="Build status" style="max-width: 100%;"></a></p>
<p dir="auto">ProtoGrad is an experimental Julia package to work with gradient-based model optimization, including (of course!) deep learning.
It aims at being simple, composable, and flexible.
This said, it's very much of a work-in-progress playground for ideas, so don't expect feature completeness or stability just yet.</p>
<p dir="auto">The package builds on top of much more mature and popular libraries, above all <a href="https://github.com/FluxML/Zygote.jl">Zygote</a> (for automatic differentiation) and <a href="https://github.com/FluxML/NNlib.jl">NNLib</a> (providing common operators in deep learning).</p>
<p dir="auto">Check out the <a href="./examples/">examples folder</a> on how to use ProtoGrad to construct and train models, or keep following the present README to get a feeling for the package philosophy.</p>
<p dir="auto">It all begins, naturally, with</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ProtoGrad"><pre><span class="pl-k">using</span> ProtoGrad</pre></div>
<h2 dir="auto"><a id="user-content-models" class="anchor" aria-hidden="true" href="#models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Models</h2>
<p dir="auto">Models are just callable objects, whose type extends the <code>ProtoGrad.Model</code> abstract type.
The following (overly) simple example defines some type of linear model (a better version of this is <code>ProtoGrad.Linear</code>):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="struct LinearModel &lt;: ProtoGrad.Model
    A
    b
end

(m::LinearModel)(x) = m.A * x .+ m.b"><pre><span class="pl-k">struct</span> LinearModel <span class="pl-k">&lt;:</span> <span class="pl-c1">ProtoGrad.Model</span>
    A
    b
<span class="pl-k">end</span>

(m<span class="pl-k">::</span><span class="pl-c1">LinearModel</span>)(x) <span class="pl-k">=</span> m<span class="pl-k">.</span>A <span class="pl-k">*</span> x <span class="pl-k">.+</span> m<span class="pl-k">.</span>b</pre></div>
<p dir="auto">All attributes of a model are interpreted as parameters to be optimized, and so gradients will be taken with respect to them. It is therefore assumed that all attributes are</p>
<ol dir="auto">
<li>Numerical arrays, i.e. of type <code>&lt;:AbstractArray{&lt;:AbstractFloat}</code>;</li>
<li>Functions;</li>
<li>Other <code>Model</code> objects;</li>
<li><code>Tuple</code>s of objects of the above types.</li>
</ol>
<blockquote>
<p dir="auto"><strong>Note:</strong> This means, for example, that hyper-paramenters cannot be stored as attributes.
Some hyperparameters are implicit in the model structure (e.g. number of layers or units);
otherwise, they can be stored as type parameters (as <a href="https://docs.julialang.org/en/v1/manual/types/#%22Value-types%22" rel="nofollow">"value types"</a>).</p>
</blockquote>
<p dir="auto">Models defined this way get the structure of a vector space, for free:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="m = LinearModel(randn(3, 5), randn(3))
m_scaled = 2 * m # this is also of type LinearModel
m_sum = m + m_scaled # this too"><pre>m <span class="pl-k">=</span> <span class="pl-c1">LinearModel</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">3</span>, <span class="pl-c1">5</span>), <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>))
m_scaled <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> m <span class="pl-c"><span class="pl-c">#</span> this is also of type LinearModel</span>
m_sum <span class="pl-k">=</span> m <span class="pl-k">+</span> m_scaled <span class="pl-c"><span class="pl-c">#</span> this too</span></pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Main.##310.LinearModel"><pre class="notranslate"><code>Main.##310.LinearModel
</code></pre></div>
<p dir="auto">The dot-syntax for in-place assignment and loop fusion can also be used:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="m_scaled .= 2 .* m
m_sum .= m .+ m_scaled"><pre>m_scaled <span class="pl-k">.=</span> <span class="pl-c1">2</span> <span class="pl-k">.*</span> m
m_sum <span class="pl-k">.=</span> m <span class="pl-k">.+</span> m_scaled</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Main.##310.LinearModel"><pre class="notranslate"><code>Main.##310.LinearModel
</code></pre></div>
<p dir="auto">And you can take dot products too!</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using LinearAlgebra
dot(m, m_sum)"><pre><span class="pl-k">using</span> LinearAlgebra
<span class="pl-c1">dot</span>(m, m_sum)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="60.98490613000809"><pre class="notranslate"><code>60.98490613000809
</code></pre></div>
<h2 dir="auto"><a id="user-content-objective-functions" class="anchor" aria-hidden="true" href="#objective-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Objective functions</h2>
<p dir="auto">Training a model usually amounts to optimizing some objective function.
In principle, any custom function of the model will do.
For example, we can use the mean squared error</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="mean_squared_error(yhat, y) = sum((yhat .- y).^2) / size(y)[end]"><pre><span class="pl-en">mean_squared_error</span>(yhat, y) <span class="pl-k">=</span> <span class="pl-c1">sum</span>((yhat <span class="pl-k">.-</span> y)<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">/</span> <span class="pl-c1">size</span>(y)[<span class="pl-c1">end</span>]</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="mean_squared_error (generic function with 1 method)"><pre class="notranslate"><code>mean_squared_error (generic function with 1 method)
</code></pre></div>
<p dir="auto">together with some data (here artificially generated, according to a random, noisy linear model)</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="A_original = randn(3, 5)
b_original = randn(3)
x = randn(5, 300)
y = A_original * x .+ b_original .+ randn(3, 300)"><pre>A_original <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>, <span class="pl-c1">5</span>)
b_original <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>)
x <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">5</span>, <span class="pl-c1">300</span>)
y <span class="pl-k">=</span> A_original <span class="pl-k">*</span> x <span class="pl-k">.+</span> b_original <span class="pl-k">.+</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>, <span class="pl-c1">300</span>)</pre></div>
<p dir="auto">to define the objective:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="objective = model -&gt; mean_squared_error(model(x), y)

objective(m) # returns some &quot;large&quot; loss value"><pre>objective <span class="pl-k">=</span> model <span class="pl-k">-&gt;</span> <span class="pl-c1">mean_squared_error</span>(<span class="pl-c1">model</span>(x), y)

<span class="pl-c1">objective</span>(m) <span class="pl-c"><span class="pl-c">#</span> returns some "large" loss value</span></pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="16.988103019562196"><pre class="notranslate"><code>16.988103019562196
</code></pre></div>
<p dir="auto">Stochastic approximations to the full-data objective above can be implemented by iterating the data in batches, and coupling it with the loss, as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using StatsBase

batch_size = 64
batches = ProtoGrad.forever() do
    idx = sample(1:size(x)[end], batch_size, replace = false)
    return (x[:, idx], y[:, idx])
end

stochastic_objective = ProtoGrad.SupervisedObjective(mean_squared_error, batches)"><pre><span class="pl-k">using</span> StatsBase

batch_size <span class="pl-k">=</span> <span class="pl-c1">64</span>
batches <span class="pl-k">=</span> ProtoGrad<span class="pl-k">.</span><span class="pl-c1">forever</span>() <span class="pl-k">do</span>
    idx <span class="pl-k">=</span> <span class="pl-c1">sample</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(x)[<span class="pl-c1">end</span>], batch_size, replace <span class="pl-k">=</span> <span class="pl-c1">false</span>)
    <span class="pl-k">return</span> (x[:, idx], y[:, idx])
<span class="pl-k">end</span>

stochastic_objective <span class="pl-k">=</span> ProtoGrad<span class="pl-k">.</span><span class="pl-c1">SupervisedObjective</span>(mean_squared_error, batches)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="ProtoGrad.SupervisedObjective{typeof(Main.##310.mean_squared_error), Base.Generator{Base.Iterators.Repeated{Nothing}, ProtoGrad.var&quot;#1#2&quot;{Main.##310.var&quot;#3#4&quot;}}, Tuple{Matrix{Float64}, Matrix{Float64}}, Nothing}(mean_squared_error, Base.Generator{Base.Iterators.Repeated{Nothing}, ProtoGrad.var&quot;#1#2&quot;{Main.##310.var&quot;#3#4&quot;}}(ProtoGrad.var&quot;#1#2&quot;{Main.##310.var&quot;#3#4&quot;}(Main.##310.var&quot;#3#4&quot;()), Base.Iterators.Repeated{Nothing}(nothing)))"><pre class="notranslate"><code>ProtoGrad.SupervisedObjective{typeof(Main.##310.mean_squared_error), Base.Generator{Base.Iterators.Repeated{Nothing}, ProtoGrad.var"#1#2"{Main.##310.var"#3#4"}}, Tuple{Matrix{Float64}, Matrix{Float64}}, Nothing}(mean_squared_error, Base.Generator{Base.Iterators.Repeated{Nothing}, ProtoGrad.var"#1#2"{Main.##310.var"#3#4"}}(ProtoGrad.var"#1#2"{Main.##310.var"#3#4"}(Main.##310.var"#3#4"()), Base.Iterators.Repeated{Nothing}(nothing)))
</code></pre></div>
<p dir="auto">Evaluating this new objective on <code>m</code> will yield</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="stochastic_objective(m) |&gt; println # a different
stochastic_objective(m) |&gt; println # value
stochastic_objective(m) |&gt; println # every
stochastic_objective(m) |&gt; println # time"><pre><span class="pl-c1">stochastic_objective</span>(m) <span class="pl-k">|&gt;</span> println <span class="pl-c"><span class="pl-c">#</span> a different</span>
<span class="pl-c1">stochastic_objective</span>(m) <span class="pl-k">|&gt;</span> println <span class="pl-c"><span class="pl-c">#</span> value</span>
<span class="pl-c1">stochastic_objective</span>(m) <span class="pl-k">|&gt;</span> println <span class="pl-c"><span class="pl-c">#</span> every</span>
<span class="pl-c1">stochastic_objective</span>(m) <span class="pl-k">|&gt;</span> println <span class="pl-c"><span class="pl-c">#</span> time</span></pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="13.799104481641637
16.473127356129876
18.438358936788205
16.652131475339687
"><pre class="notranslate"><code>13.799104481641637
16.473127356129876
18.438358936788205
16.652131475339687

</code></pre></div>
<h2 dir="auto"><a id="user-content-gradient-computation" class="anchor" aria-hidden="true" href="#gradient-computation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Gradient computation</h2>
<p dir="auto">Computing the gradient of our objective with respect to the model is easy:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="grad, val = ProtoGrad.gradient(objective, m)"><pre>grad, val <span class="pl-k">=</span> ProtoGrad<span class="pl-k">.</span><span class="pl-c1">gradient</span>(objective, m)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="(Main.##310.LinearModel, 16.988103019562196)"><pre class="notranslate"><code>(Main.##310.LinearModel, 16.988103019562196)
</code></pre></div>
<p dir="auto">Here <code>val</code> is the value of the objective evaluated at <code>m</code>, while <code>grad</code> contains its gradient with respect to <strong>all</strong> attributes of <code>m</code>. Most importantly <strong><code>grad</code> is itself a <code>LinearModel</code> object</strong>. Therefore, <code>grad</code> can be added or subtracted from <code>m</code>, used in dot products and so on.</p>
<h2 dir="auto"><a id="user-content-fitting-models-to-the-objective" class="anchor" aria-hidden="true" href="#fitting-models-to-the-objective"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fitting models to the objective</h2>
<p dir="auto">Fitting models using gradient-based algorithms is now relatively simple.
The following loop is plain gradient descent with constant stepsize:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="m_fit = copy(m)
for it in 1:100
    grad, _ = ProtoGrad.gradient(objective, m_fit)
    m_fit .= m_fit .- 0.1 .* grad
end"><pre>m_fit <span class="pl-k">=</span> <span class="pl-c1">copy</span>(m)
<span class="pl-k">for</span> it <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
    grad, _ <span class="pl-k">=</span> ProtoGrad<span class="pl-k">.</span><span class="pl-c1">gradient</span>(objective, m_fit)
    m_fit <span class="pl-k">.=</span> m_fit <span class="pl-k">.-</span> <span class="pl-c1">0.1</span> <span class="pl-k">.*</span> grad
<span class="pl-k">end</span></pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="┌ Warning: Assignment to `grad` in soft scope is ambiguous because a global variable by the same name exists: `grad` will be treated as a new local. Disambiguate by using `local grad` to suppress this warning or `global grad` to assign to the existing global variable.
└ @ string:3
"><pre class="notranslate"><code>┌ Warning: Assignment to `grad` in soft scope is ambiguous because a global variable by the same name exists: `grad` will be treated as a new local. Disambiguate by using `local grad` to suppress this warning or `global grad` to assign to the existing global variable.
└ @ string:3

</code></pre></div>
<p dir="auto">To verify that this worked, we can check that the objective value is much smaller for <code>m_fit</code> than it was for <code>m</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="objective(m_fit) # returns a small loss value compared to `m`"><pre><span class="pl-c1">objective</span>(m_fit) <span class="pl-c"><span class="pl-c">#</span> returns a small loss value compared to `m`</span></pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="2.949699249534306"><pre class="notranslate"><code>2.949699249534306
</code></pre></div>
<p dir="auto">ProtoGrad implements gradient descent and other optimization algorithms in the form of iterators. The following will yield the same iterations as we just did:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="optimizer = ProtoGrad.GradientDescent(stepsize=1e-1)
iterations = Iterators.take(optimizer(m, objective), 100)"><pre>optimizer <span class="pl-k">=</span> ProtoGrad<span class="pl-k">.</span><span class="pl-c1">GradientDescent</span>(stepsize<span class="pl-k">=</span><span class="pl-c1">1e-1</span>)
iterations <span class="pl-k">=</span> Iterators<span class="pl-k">.</span><span class="pl-c1">take</span>(<span class="pl-c1">optimizer</span>(m, objective), <span class="pl-c1">100</span>)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Base.Iterators.Take{ProtoGrad.GradientDescentIterable{Main.##310.LinearModel, Main.##310.var&quot;#1#2&quot;, Float64}}(ProtoGrad.GradientDescentIterable{Main.##310.LinearModel, Main.##310.var&quot;#1#2&quot;, Float64}(Main.##310.LinearModel, Main.##310.var&quot;#1#2&quot;(), 0.1), 100)"><pre class="notranslate"><code>Base.Iterators.Take{ProtoGrad.GradientDescentIterable{Main.##310.LinearModel, Main.##310.var"#1#2", Float64}}(ProtoGrad.GradientDescentIterable{Main.##310.LinearModel, Main.##310.var"#1#2", Float64}(Main.##310.LinearModel, Main.##310.var"#1#2"(), 0.1), 100)
</code></pre></div>
<p dir="auto">The <code>iterations</code> object is an iterator that can be looped over, and its elements be inspected (for example to decide when to stop training). For the sake of compactness, here we will just take the output of the last iteration as solution:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="m_fit = ProtoGrad.last(iterations).solution"><pre>m_fit <span class="pl-k">=</span> ProtoGrad<span class="pl-k">.</span><span class="pl-c1">last</span>(iterations)<span class="pl-k">.</span>solution</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Main.##310.LinearModel"><pre class="notranslate"><code>Main.##310.LinearModel
</code></pre></div>
<hr>
<p dir="auto"><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p>
</article></div>