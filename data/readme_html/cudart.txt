<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-cudart" class="anchor" aria-hidden="true" href="#cudart"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>CUDArt</h1>
<p><strong>IMPORTANT NOTE</strong>: this package is not actively developed, please use <a href="https://github.com/JuliaGPU/CUDAdrv.jl">CUDAdrv</a> instead!</p>
<p><strong>Build status</strong>: <a href="http://ci.maleadt.net/shields/url.php?builder=CUDArt-julia05-x86-64bit" rel="nofollow"><img src="https://camo.githubusercontent.com/0809bdff694d4b3b79868ef15759da183ba5646f/687474703a2f2f63692e6d616c656164742e6e65742f736869656c64732f6275696c642e7068703f6275696c6465723d4355444172742d6a756c696130352d7838362d3634626974266e616d653d6a756c6961253230302e35" alt="" data-canonical-src="http://ci.maleadt.net/shields/build.php?builder=CUDArt-julia05-x86-64bit&amp;name=julia%200.5" style="max-width:100%;"></a> <a href="http://ci.maleadt.net/shields/url.php?builder=CUDArt-julia06-x86-64bit" rel="nofollow"><img src="https://camo.githubusercontent.com/6bb9865288c92798bda46b51159ad55ed2d8a5e4/687474703a2f2f63692e6d616c656164742e6e65742f736869656c64732f6275696c642e7068703f6275696c6465723d4355444172742d6a756c696130362d7838362d3634626974266e616d653d6a756c6961253230302e36" alt="" data-canonical-src="http://ci.maleadt.net/shields/build.php?builder=CUDArt-julia06-x86-64bit&amp;name=julia%200.6" style="max-width:100%;"></a></p>
<p><strong>Code coverage</strong>: <a href="https://codecov.io/gh/JuliaGPU/CUDArt.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ad2f0fc7c87c0c217c585f2053c3ce41658203f4/68747470733a2f2f636f6465636f762e696f2f67682f4a756c69614750552f4355444172742e6a6c2f636f7665726167652e737667" alt="" data-canonical-src="https://codecov.io/gh/JuliaGPU/CUDArt.jl/coverage.svg" style="max-width:100%;"></a></p>
<p>This package wraps the <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/" rel="nofollow">CUDA runtime API</a>.
For a wrapper of the <a href="http://docs.nvidia.com/cuda/cuda-driver-api/" rel="nofollow">driver API</a>, see
<a href="https://github.com/JuliaGPU/CUDAdrv.jl">CUDAdrv</a>.</p>
<p>CUDAdrv.jl is the preferred way to program a GPU from Julia; Only use CUDArt.jl if you <em>really</em> require the runtime API.</p>
<h2><a id="user-content-platform-support" class="anchor" aria-hidden="true" href="#platform-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Platform support</h2>
<p>This has been tested on Linux, OSX, and Windows. With Windows, at least Visual Studio
2010/2012/2013/2015 are supported.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>First, you need to have an NVIDIA GPU device in your computer (one that is available for
computation, i.e., most likely not your graphics card), and the CUDA library installed. You
have to perform these steps manually. <strong>Choose either 32-bit or 64-bit versions to match
your Julia installation.</strong></p>
<p>Install the Julia package using:</p>
<div class="highlight highlight-source-julia"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>CUDArt<span class="pl-pds">"</span></span>)</pre></div>
<p>During installation, it should compile a couple of files in the <code>deps/</code> directory. These
files provide utility functions necessary for certain functionality in this package. If the
build step fails, try fixing the problems and running <code>Pkg.build("CUDArt")</code> manually.</p>
<p>After installation, it's probably a good idea to run the <code>test/runtests.jl</code> script to find
out whether everything is working on your system, or just say <code>Pkg.test("CUDArt")</code>.</p>
<p>In case of errors, one thing to check is your CUDA installation itself. For example, examine
whether the <code>*.ptx</code> files are present in <code>deps/</code> and <code>test/</code>; look at those files and make
sure they seem appropriate. (E.g., if your computer is 64-bit, are they compiled for
64-bit?)</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<p>Start by saying <code>using CUDArt</code>, or <code>import CUDArt</code> if you prefer to qualify everything with
the module name. For most use cases, you'll also need to install and import the <code>CUDAdrv</code>
package, which among other things provides functionality to launch kernels.</p>
<h3><a id="user-content-gpu-initialization" class="anchor" aria-hidden="true" href="#gpu-initialization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GPU initialization</h3>
<p>One or more GPUs can be initialized, used for computations, and freed for other uses. There
are some complexities in this process due to the interaction with Julia's garbage
collection---a CUDA array object allocated in one "session" should not be usable if you
close the device and then open a new "session." Fortunately, CUDArt should make the process
transparent, and as a user you shouldn't have to think about this at all.</p>
<p>The easiest way to ensure that you get full functionality, with proper cleanup of resources,
is by using the <code>do</code> block syntax:</p>
<div class="highlight highlight-source-julia"><pre>result <span class="pl-k">=</span> <span class="pl-c1">devices</span>(dev<span class="pl-k">-&gt;</span><span class="pl-c1">true</span>) <span class="pl-k">do</span> devlist
    <span class="pl-c"><span class="pl-c">#</span> Code that does GPU computations</span>
<span class="pl-k">end</span></pre></div>
<p>The argument to <code>devices</code> is a function that accepts an integer input (the integer
representing the CUDA device, starting with 1) and returns <code>true</code> or <code>false</code>, indicating
whether the device should or should not be used, respectively. <code>dev-&gt;true</code> means that very
device will be used. The <code>devlist</code> variable will be defined inside the block, and is a
<code>Vector{Int}</code> of the available devices.</p>
<p>If you need to make sure that only devices with sufficient capabilities are used, then use a
construct like this:</p>
<div class="highlight highlight-source-julia"><pre>result <span class="pl-k">=</span> <span class="pl-c1">devices</span>(dev<span class="pl-k">-&gt;</span><span class="pl-c1">capability</span>(dev)[<span class="pl-c1">1</span>]<span class="pl-k">&gt;=</span><span class="pl-c1">2</span>) <span class="pl-k">do</span> devlist
    <span class="pl-c"><span class="pl-c">#</span> Code that does GPU computations</span>
<span class="pl-k">end</span></pre></div>
<p>This will select all devices that have a major capability of 2 or higher. You can query any
of the properties of your device; see the <code>device_properties</code> and <code>attribute</code> functions and
the list of
<a href="http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp" rel="nofollow">fields</a>.
If you want to restrict your computations to just one device (perhaps leaving other devices
for other users), use the <code>nmax</code> keyword:</p>
<div class="highlight highlight-source-julia"><pre>result <span class="pl-k">=</span> <span class="pl-c1">devices</span>(func, nmax<span class="pl-k">=</span><span class="pl-c1">1</span>) <span class="pl-k">do</span> devlist
    <span class="pl-c"><span class="pl-c">#</span> Code that does GPU computations</span>
<span class="pl-k">end</span></pre></div>
<p>Finally, you can request only those devices that are not busy with other tasks using:</p>
<div class="highlight highlight-source-julia"><pre>result <span class="pl-k">=</span> <span class="pl-c1">devices</span>(func, status<span class="pl-k">=</span><span class="pl-c1">:free</span>) <span class="pl-k">do</span> devlist
    <span class="pl-c"><span class="pl-c">#</span> Code that does GPU computations</span>
<span class="pl-k">end</span></pre></div>
<p>You can wait for specific devices to become available with <code>wait_free(devlist)</code>.</p>
<p>The <code>do</code> block syntax initializes the devices and loads some utility functions (defined in
<code>deps/utils.cu</code>) onto each GPU; it also ensures proper freeing of memory and unloading of
code when the <code>do</code> block finishes. Should you want to initialize the utilities manually, you
can do so by calling <code>CUDArt.init(devlist)</code> and <code>CUDArt.close(devlist)</code> where <code>devlist</code> is
an integer device number or a list of them, e.g. <code>0</code> or <code>[0,1]</code>. This can be handy in case
of trouble, because unfortunately the <code>do</code> syntax does not usually result in ideal
backtraces.</p>
<p>If your work doesn't require any of the utility functions, you can manually manage the
device:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">device</span>(dev)
<span class="pl-c"><span class="pl-c">#</span> Code that does GPU computations</span>
<span class="pl-c1">device_reset</span>(dev)</pre></div>
<p>where <code>dev</code> is the integer device number.</p>
<h4><a id="user-content-choosingquerying-the-active-device" class="anchor" aria-hidden="true" href="#choosingquerying-the-active-device"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Choosing/querying the active device</h4>
<p>At any point in your code, the command <code>device(dev)</code> makes <code>dev</code> the active device. For
example, commands that allocate device memory will be executed on whichever device is
currently active.</p>
<p>Calling <code>dev = device()</code> will return the currently-active device</p>
<h3><a id="user-content-arrays" class="anchor" aria-hidden="true" href="#arrays"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Arrays</h3>
<h4><a id="user-content-device-arrays" class="anchor" aria-hidden="true" href="#device-arrays"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Device arrays</h4>
<p>CUDArt supports two main types of device arrays: <code>CudaArray</code>s and <code>CudaPitchedArray</code>s.
These correspond to contiguous memory blocks and "pitched pointers", respectively.</p>
<p>To declare an uninitialized array on the device, use:</p>
<div class="highlight highlight-source-julia"><pre>d_A <span class="pl-k">=</span> <span class="pl-c1">CudaArray</span>(Float64, (<span class="pl-c1">200</span>,<span class="pl-c1">300</span>))
d_B <span class="pl-k">=</span> <span class="pl-c1">CudaPitchedArray</span>(Int32, (<span class="pl-c1">15</span>, <span class="pl-c1">40</span>, <span class="pl-c1">27</span>))</pre></div>
<p>The <code>d_</code> is a conventional way of reminding yourself that the array is allocated on the device.
To copy a host array to the device, use any of</p>
<div class="highlight highlight-source-julia"><pre>d_A <span class="pl-k">=</span> <span class="pl-c1">CudaArray</span>(A)
d_AP <span class="pl-k">=</span> <span class="pl-c1">CudaPitchedArray</span>(A)
<span class="pl-c1">copy!</span>(d_A, A)
<span class="pl-c1">copy!</span>(d_AP, A)</pre></div>
<p>To copy a device array back to the host, use either of</p>
<div class="highlight highlight-source-julia"><pre>A <span class="pl-k">=</span> <span class="pl-c1">to_host</span>(d_A)
<span class="pl-c1">copy!</span>(A, d_A)</pre></div>
<p>Most of the typical Julia functions, like <code>size</code>, <code>ndims</code>, <code>reinterpret</code>, <code>eltype</code>, <code>fill!</code>,
etc.,  work on CUDA array types. One noteworthy omission is that you can't directly index a
CUDA array: <code>d_A[2,4]</code> will fail. This is not supported because host/device memory transfers
are relatively slow, and you don't want to write code that (on the host side) makes use of
individual elements in a device array. If you want to inspect the values in a device array,
first use <code>to_host</code> to copy it to host memory.</p>
<p>You can find out which device is storing an array using:</p>
<div class="highlight highlight-source-julia"><pre>dev <span class="pl-k">=</span> <span class="pl-c1">device</span>(d_A)</pre></div>
<h4><a id="user-content-host-arrays" class="anchor" aria-hidden="true" href="#host-arrays"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Host arrays</h4>
<p>Another important array type is the <code>HostArray</code>, which is allocated by the CUDA library
using pinned memory:</p>
<div class="highlight highlight-source-julia"><pre>h_A <span class="pl-k">=</span> <span class="pl-c1">HostArray</span>(Float32, (<span class="pl-c1">1000</span>,<span class="pl-c1">1200</span>))</pre></div>
<p>There are circumstances where using a <code>HostArray</code> may improve the speed of memory transfers,
or allow asynchronous operations using <code>Stream</code>s.</p>
<p><strong>Warning: using a <code>HostArray</code> in conjunction with a large memory-mapped file has been
observed to cause segfaults; at the present time there is no known workaround.</strong></p>
<h3><a id="user-content-modules-and-custom-kernels" class="anchor" aria-hidden="true" href="#modules-and-custom-kernels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Modules and custom kernels</h3>
<p>This will not teach you about CUDA programming; for that, please refer to the CUDA
documentation and other online sources. You can find an example file in <code>deps/utils.cu</code>.</p>
<h4><a id="user-content-compiling-your-own-modules" class="anchor" aria-hidden="true" href="#compiling-your-own-modules"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Compiling your own modules</h4>
<p>You can write and use your own custom kernels, first writing a <code>.cu</code> file and compiling it
as a <code>ptx</code> module. On Linux, compilation would look something like this:</p>
<pre><code>nvcc -ptx mycudamodule.cu
</code></pre>
<p>You can specify that the code should be compiled for compute capability 2.0 devices or
higher using:</p>
<pre><code>nvcc -ptx -gencode=arch=compute_20,code=sm_20 mycudamodule.cu
</code></pre>
<p>If you want to write code that will support multiple datatypes (e.g., <code>Float32</code> and
<code>Float64</code>), it's recommended that you use C++ and write your code using templates. Then use
<code>extern C</code> to instantiate bindings for each datatype. For example:</p>
<pre><code>template &lt;typename T&gt;
__device__ void kernel_function1(T *data) {
    // Code goes here
}
template &lt;typename T1, typename T2&gt;
__device__ void kernel_function2(T1 *data1, T2 *data2) {
    // Code goes here
}

extern "C"
{
    void __global__ kernel_function1_float(float *data) {kernel_function1(data);}
    void __global__ kernel_function1_double(double *data) {kernel_function1(data);}
    void __global__ kernel_function2_int_float(int *data1, float *data2) {kernel_function2(data1,data2);}
}
</code></pre>
<h4><a id="user-content-initializing-and-freeing-ptx-modules" class="anchor" aria-hidden="true" href="#initializing-and-freeing-ptx-modules"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Initializing and freeing PTX modules</h4>
<p>To easily make your kernels available, the recommended approach is to define something
analogous to the following for each <code>ptx</code> module (this example uses the kernels described in
the previous section):</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">module</span> MyCudaModule

<span class="pl-k">import</span> CUDAdrv<span class="pl-k">:</span> CuModule, CuModuleFile, CuFunction, cudacall
<span class="pl-k">using</span> CUDArt

<span class="pl-k">export</span> function1

<span class="pl-k">const</span> ptxdict <span class="pl-k">=</span> <span class="pl-c1">Dict</span>()
<span class="pl-k">const</span> mdlist <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{CuModule}</span>(<span class="pl-c1">0</span>)

<span class="pl-k">function</span> <span class="pl-en">mdinit</span>(devlist)
    <span class="pl-k">global</span> ptxdict
    <span class="pl-k">global</span> mdlist
    <span class="pl-c1">isempty</span>(mdlist) <span class="pl-k">||</span> <span class="pl-c1">error</span>(<span class="pl-s"><span class="pl-pds">"</span>mdlist is not empty<span class="pl-pds">"</span></span>)
    <span class="pl-k">for</span> dev <span class="pl-k">in</span> devlist
        <span class="pl-c1">device</span>(dev)
        md <span class="pl-k">=</span> <span class="pl-c1">CuModuleFile</span>(<span class="pl-s"><span class="pl-pds">"</span>mycudamodule.ptx<span class="pl-pds">"</span></span>)
        ptxdict[(dev, <span class="pl-s"><span class="pl-pds">"</span>function1<span class="pl-pds">"</span></span>, Float32)] <span class="pl-k">=</span> <span class="pl-c1">CuFunction</span>(md, <span class="pl-s"><span class="pl-pds">"</span>kernel_function1_float<span class="pl-pds">"</span></span>)
        ptxdict[(dev, <span class="pl-s"><span class="pl-pds">"</span>function1<span class="pl-pds">"</span></span>, Float64)] <span class="pl-k">=</span> <span class="pl-c1">CuFunction</span>(md, <span class="pl-s"><span class="pl-pds">"</span>kernel_function1_double<span class="pl-pds">"</span></span>)
        ptxdict[(dev, <span class="pl-s"><span class="pl-pds">"</span>function2<span class="pl-pds">"</span></span>, Int32, Float32)] <span class="pl-k">=</span> <span class="pl-c1">CuFunction</span>(md, <span class="pl-s"><span class="pl-pds">"</span>kernel_function2_int_float<span class="pl-pds">"</span></span>)

        <span class="pl-c1">push!</span>(mdlist, md)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-en">mdclose</span>() <span class="pl-k">=</span> (<span class="pl-c1">empty!</span>(mdlist); <span class="pl-c1">empty!</span>(ptxdict))

<span class="pl-k">function</span> <span class="pl-en">init</span>(f<span class="pl-k">::</span><span class="pl-c1">Function</span>, devlist)
    <span class="pl-k">local</span> ret
    <span class="pl-c1">mdinit</span>(devlist)
    <span class="pl-k">try</span>
        ret <span class="pl-k">=</span> <span class="pl-c1">f</span>(devlist)
    <span class="pl-k">finally</span>
        <span class="pl-c1">mdclose</span>()
    <span class="pl-k">end</span>
    ret
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">function1</span><span class="pl-c1">{T}</span>(data<span class="pl-k">::</span><span class="pl-c1">CudaArray{T}</span>)
    dev <span class="pl-k">=</span> <span class="pl-c1">device</span>(data)
    cufunction1 <span class="pl-k">=</span> ptxdict[(dev, <span class="pl-s"><span class="pl-pds">"</span>function1<span class="pl-pds">"</span></span>, T)]
    <span class="pl-c"><span class="pl-c">#</span> Set up grid and block, see below</span>
    <span class="pl-c1">cudacall</span>(cufunction1, grid, block, (Ptr{T},), data)
<span class="pl-k">end</span>

<span class="pl-k">...</span>

<span class="pl-k">end</span>  <span class="pl-c"><span class="pl-c">#</span> MyCudaModule</span></pre></div>
<p>Usage will look something like the following:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> CUDArt, MyCudaModule

A <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">10</span>,<span class="pl-c1">5</span>)

result <span class="pl-k">=</span> <span class="pl-c1">devices</span>(dev<span class="pl-k">-&gt;</span><span class="pl-c1">capability</span>(dev)[<span class="pl-c1">1</span>]<span class="pl-k">&gt;=</span><span class="pl-c1">2</span>) <span class="pl-k">do</span> devlist
    MyCudaModule<span class="pl-k">.</span><span class="pl-c1">init</span>(devlist) <span class="pl-k">do</span> dev
        <span class="pl-c1">device</span>(dev)
        <span class="pl-c1">function1</span>(<span class="pl-c1">CudaArray</span>(A))
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<h4><a id="user-content-grid-and-block-dimensions" class="anchor" aria-hidden="true" href="#grid-and-block-dimensions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Grid and block dimensions</h4>
<p>To be written.</p>
<h3><a id="user-content-streams" class="anchor" aria-hidden="true" href="#streams"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Streams</h3>
<p>One can use streams to manage or synchronize computations between the CPU &amp; GPU, or using
multiple CUDA devices.  Using <a href="http://docs.julialang.org/en/latest/manual/parallel-computing/" rel="nofollow">Julia's <code>@sync</code> and <code>@async</code>
macros</a>, here is a short
demonstration that activates processing on multiple devices:</p>
<div class="highlight highlight-source-julia"><pre>measured_sleep_time <span class="pl-k">=</span> CUDArt<span class="pl-k">.</span><span class="pl-c1">devices</span>(dev<span class="pl-k">-&gt;</span><span class="pl-c1">true</span>, nmax<span class="pl-k">=</span><span class="pl-c1">2</span>) <span class="pl-k">do</span> devlist
    sleeptime <span class="pl-k">=</span> <span class="pl-c1">0.5</span>
    results <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">3</span><span class="pl-k">*</span><span class="pl-c1">length</span>(devlist))
    streams <span class="pl-k">=</span> [(<span class="pl-c1">device</span>(dev); <span class="pl-c1">Stream</span>()) <span class="pl-k">for</span> dev <span class="pl-k">in</span> devlist]
    <span class="pl-c"><span class="pl-c">#</span> Force one run to precompile</span>
    <span class="pl-c1">cudasleep</span>(sleeptime; dev<span class="pl-k">=</span>devlist[<span class="pl-c1">1</span>], stream<span class="pl-k">=</span>streams[<span class="pl-c1">1</span>])
    <span class="pl-c1">wait</span>(streams[<span class="pl-c1">1</span>])
    i <span class="pl-k">=</span> <span class="pl-c1">1</span>
    <span class="pl-en">nextidx</span>() <span class="pl-k">=</span> (idx<span class="pl-k">=</span>i; i<span class="pl-k">+=</span><span class="pl-c1">1</span>; idx)
    <span class="pl-c1">@sync</span> <span class="pl-k">begin</span>
        <span class="pl-k">for</span> idev <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(devlist)
            <span class="pl-c1">@async</span> <span class="pl-k">begin</span>
                <span class="pl-k">while</span> <span class="pl-c1">true</span>
                    idx <span class="pl-k">=</span> <span class="pl-c1">nextidx</span>()
                    <span class="pl-k">if</span> idx <span class="pl-k">&gt;</span> <span class="pl-c1">length</span>(results)
                        <span class="pl-k">break</span>
                    <span class="pl-k">end</span>
                    tstart <span class="pl-k">=</span> <span class="pl-c1">time</span>()
                    dev <span class="pl-k">=</span> devlist[idev]
                    stream <span class="pl-k">=</span> streams[idev]
                    <span class="pl-c1">cudasleep</span>(sleeptime; dev<span class="pl-k">=</span>dev, stream<span class="pl-k">=</span>stream)
                    <span class="pl-c1">wait</span>(stream)
                    tstop <span class="pl-k">=</span> <span class="pl-c1">time</span>()
                    results[idx] <span class="pl-k">=</span> tstop<span class="pl-k">-</span>tstart
                <span class="pl-k">end</span>
            <span class="pl-k">end</span>
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
    results
<span class="pl-k">end</span></pre></div>
<p>In a more realistic version of this demonstration, you would "feed" work and collect the
results from all of your CUDA devices using a single Julia process to organize the efforts.</p>
<h2><a id="user-content-random-notes" class="anchor" aria-hidden="true" href="#random-notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Random notes</h2>
<h3><a id="user-content-notes-on-memory" class="anchor" aria-hidden="true" href="#notes-on-memory"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Notes on memory</h3>
<p>Julia convention is that matrices are stored in column-major order, whereas C (and CUDA) use
row-major. For efficiency this wrapper avoids reordering memory, so that the linear sequence
of addresses is the same between main memory and the GPU. For most usages, this is probably
what you want.</p>
<p>However, for the purposes of linear algebra, this effectively means that one is storing the
transpose of matrices on the GPU. (TODO: create <code>CudaMatrix</code> and <code>CudaPitchedMatrix</code> types
that will automatically take the transpose when copying between main and GPU memory. This
will be useful for cuBLAS.)</p>
<p>Note that the size of a CudaArray/CudaPitchedArray is represented as the size <em>of the
corresponding main-memory object</em>; thus, an array's dimensions (as reported by Julia) will
not change when you copy it between main and GPU memory.</p>
</article></div>