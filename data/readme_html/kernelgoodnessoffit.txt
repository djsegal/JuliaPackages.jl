<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-kernelgoodnessoffitjl" class="anchor" aria-hidden="true" href="#kernelgoodnessoffitjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>KernelGoodnessOfFit.jl</h1>
<p>This package provides implementations goodness-of-fit tests based on Stein discrepancy and Reproducing Kernel Hilbert Spaces (RKHSs). At the moment, the goodness-of-fit tests supported are</p>
<ul>
<li><code>KSD</code>: goodness-of-fit test based on Kernelized Stein Discrepancy (KSD) with bootstrapping [1]</li>
<li><code>FSSDrand</code> and <code>FSSDopt</code>: goodness-of-fit test based on Finite-Set Stein Discrepancy (FSSD), both with randomized and optimized test-parameters [2]</li>
</ul>
<p>Due to the nature of the tests and implementation, we allow both user-specified distributions and kernels.</p>
<h1><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h1>
<h2><a id="user-content-univariate-gaussian" class="anchor" aria-hidden="true" href="#univariate-gaussian"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Univariate Gaussian</h2>
<h3><a id="user-content-fssdopt" class="anchor" aria-hidden="true" href="#fssdopt"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>FSSDopt</code></h3>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributions <span class="pl-c"><span class="pl-c">#</span> provides a large number of standard distributions</span>
<span class="pl-k">using</span> KernelGoodnessOfFit

p <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>) <span class="pl-c"><span class="pl-c">#</span> true distribution</span>
xs <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">rand</span>(p, <span class="pl-c1">100</span>), <span class="pl-c1">1</span>, <span class="pl-c1">100</span>);  <span class="pl-c"><span class="pl-c">#</span> expects samples in shape (d, n) for d-dimensional data</span>

q <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>) <span class="pl-c"><span class="pl-c">#</span> proposal distribution</span>

<span class="pl-c1">FSSDopt</span>(xs, q)</pre></div>
<p>This returns something similar to:</p>
<pre><code>Finite-Set Stein Discrepancy optimized (FSSD-opt)
-------------------------------------------------
Population details:
    parameter of interest:   Finite-Set Stein Discrepancy (FSSD)
    value under h_0:         0
    point estimate:          3.0167943827814967

Test summary:
    outcome with 95% confidence: reject h_0
    p-value:                     0.0077

Details:
    kernel:           GaussianRBF(0.43228083254028116)
    test locations:   [2.10232 -0.323255 … -1.39591 0.716078]
    num. simulate H₀: 3000
</code></pre>
<h3><a id="user-content-fssdrand" class="anchor" aria-hidden="true" href="#fssdrand"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>FSSDrand</code></h3>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributions <span class="pl-c"><span class="pl-c">#</span> provides a large number of standard distributions</span>
<span class="pl-k">using</span> KernelGoodnessOfFit

p <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>) <span class="pl-c"><span class="pl-c">#</span> true distribution</span>
xs <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">rand</span>(p, <span class="pl-c1">100</span>), <span class="pl-c1">1</span>, <span class="pl-c1">100</span>);  <span class="pl-c"><span class="pl-c">#</span> expects samples in shape (d, n) for d-dimensional data</span>

q <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>) <span class="pl-c"><span class="pl-c">#</span> proposal distribution</span>

<span class="pl-c1">FSSDrand</span>(xs, q)</pre></div>
<p>This returns something similar to:</p>
<pre><code>Finite-Set Stein Discrepancy randomized (FSSD-rand)
---------------------------------------------------
Population details:
    parameter of interest:   Finite-Set Stein Discrepancy (FSSD)
    value under h_0:         0
    point estimate:          45.570034393743555

Test summary:
    outcome with 95% confidence: reject h_0
    p-value:                     &lt;1e-99

Details:
    kernel:           GaussianRBF(1.1219378550323837)
    test locations:   [0.277374 -0.704376 … 0.329766 -0.739727]
    num. simulate H₀: 3000
</code></pre>
<h2><a id="user-content-custom-distribution" class="anchor" aria-hidden="true" href="#custom-distribution"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Custom distribution</h2>
<p>The <code>KSD</code> and <code>FSSD</code> both only require the <code>∇log(p(x))</code>. Therefore, to perform the tests on some arbitrary distribution, we only need to provide an implementation of <code>gradlogpdf</code> from <code>Distributions.jl</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributions <span class="pl-c"><span class="pl-c">#</span> provide implementations of `gradlogpdf` for most standard distributions</span>
<span class="pl-k">using</span> KernelGoodnessOfFit

<span class="pl-k">import</span> Distributions<span class="pl-k">:</span> gradlogpdf <span class="pl-c"><span class="pl-c">#</span> allow extensions</span>

μ <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
σ² <span class="pl-k">=</span> <span class="pl-c1">1.0</span>

<span class="pl-c"><span class="pl-c">#</span> true distribution is isotropic Gaussian</span>
p <span class="pl-k">=</span> <span class="pl-c1">MultivariateNormal</span>([μ, <span class="pl-c1">0</span>], [σ² <span class="pl-c1">0</span>; <span class="pl-c1">0</span> σ²])  <span class="pl-c"><span class="pl-c">#</span> true</span>

n <span class="pl-k">=</span> <span class="pl-c1">400</span>
xs <span class="pl-k">=</span> <span class="pl-c1">rand</span>(p, n)  <span class="pl-c"><span class="pl-c">#</span> draw samples from `p`</span>

<span class="pl-c"><span class="pl-c">#</span> Gaussian Mixture Model (GMM) with two components as our model distribution</span>
q <span class="pl-k">=</span> <span class="pl-c1">MixtureModel</span>(MvNormal[ 
    <span class="pl-c1">MvNormal</span>([μ, <span class="pl-k">-</span><span class="pl-c1">2.0</span>], [<span class="pl-c1">5</span> <span class="pl-k">*</span> σ² <span class="pl-c1">0</span>; <span class="pl-c1">0</span> σ²]),
    <span class="pl-c1">MvNormal</span>([μ, <span class="pl-c1">2.0</span>], [<span class="pl-c1">3</span> <span class="pl-k">*</span> σ² <span class="pl-c1">0</span>; <span class="pl-c1">0</span> <span class="pl-c1">3</span> <span class="pl-k">*</span> σ²]),
    ]
)

<span class="pl-c"><span class="pl-c">#</span> `Distributions.jl` does not provide `gradlogpdf` for mixture models; auto-differentiation using `ForwardDiff.jl` to the rescue!</span>
<span class="pl-k">using</span> ForwardDiff
<span class="pl-en">gradlogpdf</span>(d<span class="pl-k">::</span><span class="pl-c1">MixtureModel</span>, x<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>) <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">gradient</span>(z <span class="pl-k">-&gt;</span> <span class="pl-c1">log</span>(<span class="pl-c1">pdf</span>(d, z)), x)

res <span class="pl-k">=</span> <span class="pl-c1">FSSDopt</span>(xs, q; J <span class="pl-k">=</span> <span class="pl-c1">2</span>, β_H₁ <span class="pl-k">=</span> <span class="pl-c1">0.001</span>) <span class="pl-c"><span class="pl-c">#</span> using 2 test locations</span>
res</pre></div>
<p>This example lends itself nicely to interpretable test locations. Run the following code to visualize the results.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Plots


V <span class="pl-k">=</span> res<span class="pl-k">.</span>V

f <span class="pl-k">=</span> <span class="pl-c1">scatter</span>(xs[<span class="pl-c1">1</span>, :], xs[<span class="pl-c1">2</span>, :], markeralpha <span class="pl-k">=</span> <span class="pl-c1">0.5</span>, markerstrokewidth <span class="pl-k">=</span> <span class="pl-c1">0.1</span>, label <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\$</span>x_i <span class="pl-cce">\\</span>sim p<span class="pl-cce">\$</span><span class="pl-pds">"</span></span>)

<span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(V, <span class="pl-c1">2</span>)
    v <span class="pl-k">=</span> V[:, i]
    <span class="pl-c1">scatter!</span>([v[<span class="pl-c1">1</span>]], [v[<span class="pl-c1">2</span>]], markerstrokewidth <span class="pl-k">=</span> <span class="pl-c1">0.5</span>, label <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\$</span>v_<span class="pl-v">$i</span><span class="pl-cce">\$</span><span class="pl-pds">"</span></span>, color <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>red<span class="pl-pds">"</span></span>)
<span class="pl-k">end</span>

x <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">5</span>, <span class="pl-c1">5</span>, length<span class="pl-k">=</span><span class="pl-c1">100</span>)
y <span class="pl-k">=</span> x

<span class="pl-c1">contour!</span>(x, y, (x, y) <span class="pl-k">-&gt;</span> <span class="pl-c1">pdf</span>(q, [x; y]), nlevels <span class="pl-k">=</span> <span class="pl-c1">10</span>, width <span class="pl-k">=</span> <span class="pl-c1">0.2</span>)
f</pre></div>
<p>You should then see something similar to this plot:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/torfjelde/KernelGoodnessOfFit.jl/blob/master/GMM_2D_test.png"><img src="https://github.com/torfjelde/KernelGoodnessOfFit.jl/raw/master/GMM_2D_test.png" alt="Test locations and data" style="max-width:100%;"></a></p>
<h2><a id="user-content-different-kernels" class="anchor" aria-hidden="true" href="#different-kernels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Different kernels</h2>
<p>The package include the following (universal) kernels:</p>
<ul>
<li><code>GaussianRBF(γ)</code> (default)</li>
<li><code>InverseMultiQuadratic(c, b)</code> (<code>c &gt; 0</code> and <code>b &lt; 0</code>, enforced by optimization process even w/o specifying bounds)</li>
<li><code>Matern25Kernel(ν=2.5, ρ)</code> (the test requires once-differentiability, therefore <code>ν = 2.5</code> is fixed)</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributions
<span class="pl-k">using</span> KernelGoodnessOfFit

p <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>)
xs <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">rand</span>(p, <span class="pl-c1">100</span>), <span class="pl-c1">1</span>, <span class="pl-c1">100</span>);

q <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>)

<span class="pl-c1">FSSDopt</span>(xs, q, KernelGoodnessOfFit<span class="pl-k">.</span><span class="pl-c1">GaussianRBF</span>(<span class="pl-c1">1.0</span>))
<span class="pl-c1">FSSDopt</span>(xs, q, KernelGoodnessOfFit<span class="pl-k">.</span><span class="pl-c1">InverseMultiQuadratic</span>(<span class="pl-c1">1.0</span>, <span class="pl-k">-</span><span class="pl-c1">0.5</span>))</pre></div>
<h3><a id="user-content-custom-kernels" class="anchor" aria-hidden="true" href="#custom-kernels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Custom kernels</h3>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Distributions, KernelGoodnessOfFit

<span class="pl-k">import</span> KernelGoodnessOfFit<span class="pl-k">:</span> Kernel, kernel, get_params, set_params! <span class="pl-c"><span class="pl-c">#</span> allows extending `kernel` method</span>

p <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>)
xs <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">rand</span>(p, <span class="pl-c1">100</span>), <span class="pl-c1">1</span>, <span class="pl-c1">100</span>);

q <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>)

<span class="pl-c"><span class="pl-c">#</span> define kernel struct; `mutable` to allow updating parameters in optimization process</span>
<span class="pl-k">mutable struct</span> CustomGaussianRBF <span class="pl-k">&lt;:</span> <span class="pl-c1">Kernel</span>
    γ
    custom_constant  <span class="pl-c"><span class="pl-c">#</span> arbitrary constant to multiply with</span>
<span class="pl-k">end</span>
<span class="pl-en">CustomGaussianRBF</span>(γ) <span class="pl-k">=</span> <span class="pl-c1">CustomGaussianRBF</span>(γ, <span class="pl-c1">5.0</span>) <span class="pl-c"><span class="pl-c">#</span> default constructor</span>

<span class="pl-c"><span class="pl-c">#</span> defines how evaluation of kernel</span>
<span class="pl-en">kernel</span>(k<span class="pl-k">::</span><span class="pl-c1">CustomGaussianRBF</span>, x<span class="pl-k">::</span><span class="pl-c1">AbstractVector</span>, y<span class="pl-k">::</span><span class="pl-c1">AbstractVector</span>) <span class="pl-k">=</span> k<span class="pl-k">.</span>custom_constant <span class="pl-k">*</span> <span class="pl-c1">exp</span>(<span class="pl-k">-</span> <span class="pl-c1">0.5</span> <span class="pl-k">*</span> k<span class="pl-k">.</span>γ<span class="pl-k">^</span>(<span class="pl-k">-</span><span class="pl-c1">2</span>) <span class="pl-k">*</span> <span class="pl-c1">sum</span>((x <span class="pl-k">-</span> y)<span class="pl-k">.^</span><span class="pl-c1">2</span>))

<span class="pl-c"><span class="pl-c">#</span> following is required for optimization wrt. γ</span>
<span class="pl-c"><span class="pl-c">#</span> NOT required for fixed γ, e.g. `FSSDrand` or `FSSDopt(...; test_locations_only=true)`</span>
<span class="pl-en">get_params</span>(k<span class="pl-k">::</span><span class="pl-c1">CustomGaussianRBF</span>) <span class="pl-k">=</span> [k<span class="pl-k">.</span>γ]  <span class="pl-c"><span class="pl-c">#</span> array of parameters to optimize =&gt; `custom_constant` stays fixed</span>
<span class="pl-en">set_params!</span>(k<span class="pl-k">::</span><span class="pl-c1">CustomGaussianRBF</span>, γ) <span class="pl-k">=</span> <span class="pl-k">begin</span> k<span class="pl-k">.</span>γ <span class="pl-k">=</span> γ <span class="pl-k">end</span>

<span class="pl-c1">FSSDopt</span>(xs, q, <span class="pl-c1">CustomGaussianRBF</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">5.0</span>))</pre></div>
<p>Output:</p>
<pre><code>Finite-Set Stein Discrepancy optimized (FSSD-opt)
-------------------------------------------------
Population details:
    parameter of interest:   Finite-Set Stein Discrepancy (FSSD)
    value under h_0:         0
    point estimate:          3.8215122818979457

Test summary:
    outcome with 95% confidence: reject h_0
    p-value:                     &lt;1e-99

Details:
    kernel:           CustomGaussianRBF(2.770003377834852, 5.0)
    test locations:   [127.592 105.799 … -24.3751 85.3997]
    num. simulate H₀: 3000
</code></pre>
<h1><a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TODO</h1>
<ul>
<li>[ ] Rewrite <code>KSD</code> to implement the hypothesis-test interface from <code>HypothesisTests.jl</code></li>
</ul>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h1>
<p>[1] Liu, Q., Lee, J. D., &amp; Jordan, M. I., A kernelized stein discrepancy for goodness-of-fit tests and model evaluation, CoRR, (),  (2016).</p>
<p>[2] Jitkrittum, W., Xu, W., Szabo, Z., Fukumizu, K., &amp; Gretton, A., A Linear-Time Kernel Goodness-Of-Fit Test, CoRR, (),  (2017).</p>
</article></div>