<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-globalmatchingmodels" class="anchor" aria-hidden="true" href="#globalmatchingmodels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GlobalMatchingModels</h1>
<p dir="auto">This package will feature a variety of global matching models of recognition memory. Currently, the only models available are MINERVA 2 and REM. More to follow.</p>
<h2 dir="auto"><a id="user-content-minerva-2" class="anchor" aria-hidden="true" href="#minerva-2"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MINERVA 2</h2>
<p dir="auto">MINERVA 2 is a global matching model of recognition memory and frequency judgment. According to MINERVA 2, each experienced event is stored as a vector representing feature values. Feature values can take the
value -1, which is inhibatory, 0 which indicates an absence of information, or 1, which is excitatory. Event information is encoded into a long term memory store. Feature values are encoded accurately with probability <code>L</code>. Recognition memory and frequency judgments are based on the similarity between a memory probe or cue and stored traces in long term memory.</p>
<h3 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h3>
<p dir="auto">The following code illustrates how frequency of experience increases the echo intensity which is a measure
of memory "signal". During each simulated experiment, the model encodes 20 unique events into memory. Four events are encoded once, four events are encoded twice, and so on, until four events are encoded five times, resulting in 60 learning memory traces. The echo intensity distribution will be ploted for each frequency distribution.</p>
<p dir="auto">First, we will load the required packages and code. You will have to install <code>StatsPlots</code> in your global environment.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="cd(@__DIR__)
using Pkg
Pkg.activate(&quot;..&quot;)
using GlobalMatchingModels, StatsPlots, Random
include(&quot;MINERVA_Functions.jl&quot;)
Random.seed!(356355)"><pre><span class="pl-c1">cd</span>(<span class="pl-c1">@__DIR__</span>)
<span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">activate</span>(<span class="pl-s"><span class="pl-pds">"</span>..<span class="pl-pds">"</span></span>)
<span class="pl-k">using</span> GlobalMatchingModels, StatsPlots, Random
<span class="pl-c1">include</span>(<span class="pl-s"><span class="pl-pds">"</span>MINERVA_Functions.jl<span class="pl-pds">"</span></span>)
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">356355</span>)</pre></div>
<p dir="auto">Next, a model object with 60 memory traces and 20 features is generated. Encoding accuracy parameter <code>L</code>
is set to .5. The next line of code simulates the model 10,000 times. On each simulation, it generates four random event vectors for each of the zero to five frequency conditions.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# initialize model
model = MINERVA(n_features=20, n_traces=60, L=.50)
# simulate experiment
sim_data = mapreduce(x-&gt;simulate(model, 4, 5), hcat, 1:10000)'"><pre><span class="pl-c"><span class="pl-c">#</span> initialize model</span>
model <span class="pl-k">=</span> <span class="pl-c1">MINERVA</span>(n_features<span class="pl-k">=</span><span class="pl-c1">20</span>, n_traces<span class="pl-k">=</span><span class="pl-c1">60</span>, L<span class="pl-k">=</span>.<span class="pl-c1">50</span>)
<span class="pl-c"><span class="pl-c">#</span> simulate experiment</span>
sim_data <span class="pl-k">=</span> <span class="pl-c1">mapreduce</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">simulate</span>(model, <span class="pl-c1">4</span>, <span class="pl-c1">5</span>), hcat, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10000</span>)<span class="pl-k">'</span></pre></div>
<p dir="auto">The plot replicates the results reported in Figure 4 of Hintzman (1988), showing that the echo intensity distributions increase in mean and variance as a function of frequency.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pyplot()
density(sim_data, alpha=.9, grid=false, norm=true, legendtitle=&quot;Frequency&quot;, label=[0:5;]',
    legendtitlefontsize=9, xaxis=font(12), yaxis=font(12), xlabel=&quot;Echo Intensity&quot;, ylabel=&quot;Density&quot;, 
    linewidth=1.5, bins=50, size=(600,300))"><pre><span class="pl-c1">pyplot</span>()
<span class="pl-c1">density</span>(sim_data, alpha<span class="pl-k">=</span>.<span class="pl-c1">9</span>, grid<span class="pl-k">=</span><span class="pl-c1">false</span>, norm<span class="pl-k">=</span><span class="pl-c1">true</span>, legendtitle<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Frequency<span class="pl-pds">"</span></span>, label<span class="pl-k">=</span>[<span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">5</span>;]<span class="pl-k">'</span>,
    legendtitlefontsize<span class="pl-k">=</span><span class="pl-c1">9</span>, xaxis<span class="pl-k">=</span><span class="pl-c1">font</span>(<span class="pl-c1">12</span>), yaxis<span class="pl-k">=</span><span class="pl-c1">font</span>(<span class="pl-c1">12</span>), xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Echo Intensity<span class="pl-pds">"</span></span>, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Density<span class="pl-pds">"</span></span>, 
    linewidth<span class="pl-k">=</span><span class="pl-c1">1.5</span>, bins<span class="pl-k">=</span><span class="pl-c1">50</span>, size<span class="pl-k">=</span>(<span class="pl-c1">600</span>,<span class="pl-c1">300</span>))</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="Examples/MINERVA.png"><img src="Examples/MINERVA.png" alt="" width="600" height="300" style="max-width: 100%;"></a></p>
<p dir="auto"><em>References</em></p>
<ol dir="auto">
<li>Hintzman, D. L. (1988). Judgments of frequency and recognition model in a multiple-trace model model.
Psychological Review, 95(4), 528.</li>
</ol>
<p dir="auto"><em>Authors</em></p>
<ol dir="auto">
<li><a href="https://github.com/itsdfish">itsdfish</a></li>
</ol>
<h2 dir="auto"><a id="user-content-rem" class="anchor" aria-hidden="true" href="#rem"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>REM</h2>
<p dir="auto">REM (Retrieving Effectively from Memory) is a global matching model of recognition memory. The model assumes that each memory is stored as an "image" of feature values drawn from a geometric distribution. Errors occur during encoding, which results in a degraded memory. The accuracy of the encoded memory representation is controled by parameters <code>g</code> and <code>u</code>. Use <code>?</code> to see documentation for details.</p>
<h3 dir="auto"><a id="user-content-example-1" class="anchor" aria-hidden="true" href="#example-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h3>
<p dir="auto">The following example is taken from Figure 1 of Shiffrin &amp; Strevers (1997).</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="probe = [6,1,1,3];
memory = [0 2; 1 2; 0 1; 3 0];
model = REM(;memory, g=.40, c=.70)
activations = compute_activations(model, probe)
odds = compute_odds(activations)"><pre>probe <span class="pl-k">=</span> [<span class="pl-c1">6</span>,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>,<span class="pl-c1">3</span>];
memory <span class="pl-k">=</span> [<span class="pl-c1">0</span> <span class="pl-c1">2</span>; <span class="pl-c1">1</span> <span class="pl-c1">2</span>; <span class="pl-c1">0</span> <span class="pl-c1">1</span>; <span class="pl-c1">3</span> <span class="pl-c1">0</span>];
model <span class="pl-k">=</span> <span class="pl-c1">REM</span>(;memory, g<span class="pl-k">=</span>.<span class="pl-c1">40</span>, c<span class="pl-k">=</span>.<span class="pl-c1">70</span>)
activations <span class="pl-k">=</span> <span class="pl-c1">compute_activations</span>(model, probe)
odds <span class="pl-k">=</span> <span class="pl-c1">compute_odds</span>(activations)</pre></div>
<p dir="auto">which returns the following result:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; odds = compute_odds(activations)
5.3823888888888884"><pre>julia<span class="pl-k">&gt;</span> odds <span class="pl-k">=</span> <span class="pl-c1">compute_odds</span>(activations)
<span class="pl-c1">5.3823888888888884</span></pre></div>
<p dir="auto"><em>References</em></p>
<ol dir="auto">
<li>Shiffrin, R. M., &amp; Steyvers, M. (1997). A model for recognition memory:
REM—Retrieving Effectively From Memory. Psychonomic Bulletin &amp; Review,
4(2), 145-166.</li>
</ol>
<p dir="auto"><em>Authors</em></p>
<ol dir="auto">
<li><a href="https://github.com/tmc2737">tmc2737</a></li>
<li><a href="https://github.com/itsdfish">itsdfish</a></li>
</ol>
</article></div>