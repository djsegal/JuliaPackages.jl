<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-fann" class="anchor" aria-hidden="true" href="#fann"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://travis-ci.org/gasagna/FANN.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/94f20d06d4257533d1af415ee44847042b4c048d/68747470733a2f2f7472617669732d63692e6f72672f67617361676e612f46414e4e2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/gasagna/FANN.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/r/gasagna/FANN.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c15994a06287074c716e4de3f41e10dc78dfcdc3/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f67617361676e612f46414e4e2e6a6c2f62616467652e706e67" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/gasagna/FANN.jl/badge.png" style="max-width:100%;"></a>
FANN</h1>
<p>A Julia wrapper for the Fast Artificial Neural Network C library (FANN)</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>This package is not yet registered on METADATA, but it can be installed simply by running</p>
<pre><code>Pkg.clone("https://github.com/gasagna/FANN.git")
Pkg.build("FANN")
</code></pre>
<p>in the julia REPL.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p>To illustrate the available functionality we start by importing the package</p>
<pre><code>using FANN
</code></pre>
<h2><a id="user-content-data-set" class="anchor" aria-hidden="true" href="#data-set"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Data set</h2>
<p>Both input and output data that will be used has to be in a two dimensions array, even in the case where <code>n_inputs</code> or <code>n_outputs</code> is equal to one. For example let's generate some synthetic data</p>
<pre><code>X = rand(5, 100)
</code></pre>
<p>That is we have generate a hundred random observations of a problem with five inputs. Let's suppose there is only one output so we generate some data as</p>
<pre><code>y = 0.2*X[1, :].^2 + 0.8*X[2, :] + 0.01*randn(1, 100)
</code></pre>
<p>Now, we create a <code>DataSet</code> object. This does not do anything in particular, but only wraps the data that will be passed to the underlying c library. Unfortunately, a copy of the data is created in this step.</p>
<pre><code>dset = DataSet(X, y)
</code></pre>
<p>Datasets can be saved and loaded to/from file as</p>
<pre><code>savedset(dset, "dataset.dat")
dset = loaddset("dataset.dat")
</code></pre>
<h2><a id="user-content-artificial-neural-network" class="anchor" aria-hidden="true" href="#artificial-neural-network"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Artificial Neural Network</h2>
<p>Now we create a neural network, i.e. a multi-layer perceptron.</p>
<pre><code>net = ANN([5, 5, 1], [:sigmoid_symmetric, :linear]; b=0.1, errorfunc=:tanh, steepness=1.0)
</code></pre>
<p>The first input is an array of Ints, with the number of nodes in each of the network layers. A bias node is also present in each layer except for the last one (see FANN documentation). The second input is an array of <code>n_layers-1</code> symbols that specifies the type of activation of the nodes in each layer except for the first one, which is always linear. Available activation functions are documented in src/constants.jl. The third parameter <code>b</code> is a float that specifies the half-width of the interval around zero over which random initial values for the network weights are drawn. The fourth argument to <code>ANN</code> is a symbol that specifies the type of error function used for training, it can be either <code>:tanh</code> (default) or <code>:linear</code>. The last parameter specifies the steepness of the activation functions of each layer, except the input layer.</p>
<p>The network can be trained as</p>
<pre><code>train!(net, dset, max_epochs=1000, epochs_between_reports=100, desired_error=1e-8)
</code></pre>
<p>where <code>epochs_between_reports</code> determines the frequency at which training progress is displayed. We may also want to stop the training when some <code>desired_error</code> is reached. With this call, the neural network is trained using the default RPROP algorithm, with default values of the training parameters. Other training algorithm can be used, and the training parameters can be also varied, (if one knows what is doing). Four algorithms are available, and they can be specified by using one of these functions, before the call to <code>train!</code>:</p>
<pre><code>setup_rprop!(ann::ANN; increase_factor::Float64=1.2, decrease_factor::Float64=0.5, delta_min::Float64=0.0, delta_max::Float64=50.0, delta_zero::Float64=0.1)
setup_qprop!(ann::ANN; mu::Float64=1.75, decay::Float64=-0.0001)
setup_bprop!(ann::ANN; batch::Bool=true, learning_rate::Float64=0.7, learning_momentum::Float64=0.0)
</code></pre>
<p>The first function sets up the standard RPROP algorithm, and training parameters can be varied. The second sets up the QUICKPROP algorith, while the last function sets up the standard backpropagation algorithm in batch or incremental mode, depending on the value of the boolean parameter <code>batch</code>.</p>
<h2><a id="user-content-early-stopping" class="anchor" aria-hidden="true" href="#early-stopping"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Early stopping</h2>
<p>A simple form of early stopping to avoid over-fitting has been implemented. Suppose, the training and validation datasets <code>tset</code> and <code>vset</code> are available. Then the network can be trained with</p>
<pre><code>train!(net, tset, vset; max_epochs=100, desired_error=1e-5, epochs_between_checks=10, minratio=0.95)
</code></pre>
<p>with the additional optional parameter <code>minratio</code> to control early stopping. Typically, when over-fitting occurs the error on the training set decreases whereas the error on the validation set increases. The ratio between the errors on the training and the validation sets is evaluated every <code>epochs_between_checks</code> and if it is smaller then <code>minratio</code> the training is halted.</p>
<h2><a id="user-content-prediction" class="anchor" aria-hidden="true" href="#prediction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Prediction</h2>
<p>When the network is trained, predictions on new inputs can be obtained as</p>
<pre><code>predict(net, X)
</code></pre>
<p>where <code>X</code> is matrix with <code>n_inputs x n_observations</code> elements.</p>
<p>The mean square error on new data can be obtained with</p>
<pre><code>mse(net, dset)
</code></pre>
<p>but note that this mean square error might be different to that obtained with</p>
<pre><code>sumabs2(predict(net, X) - y)
</code></pre>
<p>since <code>mse</code> uses the internal error function, specified when the network is created.</p>
<p>Finally, networks can be saved and loaded to/from a file</p>
<pre><code>savenet(net, "net.net")
net = loadnet("net.net")
</code></pre>
<h2><a id="user-content-note" class="anchor" aria-hidden="true" href="#note"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Note</h2>
<p>A minimal amount of the FANN library has been wrapped and most of the options have been left to the default values that the c library sets. New features of this wrapper will be slowly added, as it is required.</p>
<h2><a id="user-content-links" class="anchor" aria-hidden="true" href="#links"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Links</h2>
<p>See these two links for full documentation</p>
<p><a href="https://github.com/libfann/fann">https://github.com/libfann/fann</a></p>
<p><a href="http://leenissen.dk/fann/wp/" rel="nofollow">http://leenissen.dk/fann/wp/</a></p>
<h2><a id="user-content-author" class="anchor" aria-hidden="true" href="#author"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Author</h2>
<p>This wrapper was written by Davide Lasagna</p>
</article></div>