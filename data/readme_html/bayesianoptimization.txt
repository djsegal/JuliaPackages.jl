<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-bayesianoptimization" class="anchor" aria-hidden="true" href="#bayesianoptimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>BayesianOptimization</h1>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ea6e0ff99602c3563e3dd684abf60b30edceaeef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6966656379636c652d6578706572696d656e74616c2d6f72616e67652e737667"><img src="https://camo.githubusercontent.com/ea6e0ff99602c3563e3dd684abf60b30edceaeef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6966656379636c652d6578706572696d656e74616c2d6f72616e67652e737667" alt="Lifecycle" data-canonical-src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.org/jbrea/BayesianOptimization.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c183c2ed139a8aa8b987a73892313cb6cf017151/68747470733a2f2f7472617669732d63692e6f72672f6a627265612f426179657369616e4f7074696d697a6174696f6e2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/jbrea/BayesianOptimization.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/jbrea/bayesianoptimization-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f636dc2fb96a0bcaa5769b53a67f3e76733ec528/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f6a627265612f426179657369616e4f7074696d697a6174696f6e2e6a6c3f6272616e63683d6d6173746572267376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/jbrea/BayesianOptimization.jl?branch=master&amp;svg=true" style="max-width:100%;"></a>
<a href="http://codecov.io/github/jbrea/BayesianOptimization.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/1d38f65bdd9122e0753941b296705afdf77492f3/687474703a2f2f636f6465636f762e696f2f6769746875622f6a627265612f426179657369616e4f7074696d697a6174696f6e2e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/jbrea/BayesianOptimization.jl/coverage.svg?branch=master" style="max-width:100%;"></a></p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> BayesianOptimization, GaussianProcesses, Distributions

<span class="pl-en">f</span>(x) <span class="pl-k">=</span> <span class="pl-c1">sum</span>((x <span class="pl-k">.-</span> <span class="pl-c1">1</span>)<span class="pl-k">.^</span><span class="pl-c1">2</span>) <span class="pl-k">+</span> <span class="pl-c1">randn</span>()                <span class="pl-c"><span class="pl-c">#</span> noisy function to minimize</span>

<span class="pl-c"><span class="pl-c">#</span> Choose as a model an elastic GP with input dimensions 2.</span>
<span class="pl-c"><span class="pl-c">#</span> The GP is called elastic, because data can be appended efficiently.</span>
model <span class="pl-k">=</span> <span class="pl-c1">ElasticGPE</span>(<span class="pl-c1">2</span>,                            <span class="pl-c"><span class="pl-c">#</span> 2 input dimensions</span>
                   mean <span class="pl-k">=</span> <span class="pl-c1">MeanConst</span>(<span class="pl-c1">0.</span>),         
                   kernel <span class="pl-k">=</span> <span class="pl-c1">SEArd</span>([<span class="pl-c1">0.</span>, <span class="pl-c1">0.</span>], <span class="pl-c1">5.</span>),
                   logNoise <span class="pl-k">=</span> <span class="pl-c1">0.</span>,
                   capacity <span class="pl-k">=</span> <span class="pl-c1">3000</span>)              <span class="pl-c"><span class="pl-c">#</span> the initial capacity of the GP is 3000 samples.</span>
<span class="pl-c1">set_priors!</span>(model<span class="pl-k">.</span>mean, [<span class="pl-c1">Normal</span>(<span class="pl-c1">1</span>, <span class="pl-c1">2</span>)])

<span class="pl-c"><span class="pl-c">#</span> Optimize the hyperparameters of the GP using maximum a posteriori (MAP) estimates every 50 steps</span>
modeloptimizer <span class="pl-k">=</span> <span class="pl-c1">MAPGPOptimizer</span>(every <span class="pl-k">=</span> <span class="pl-c1">50</span>, noisebounds <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">4</span>, <span class="pl-c1">3</span>],       <span class="pl-c"><span class="pl-c">#</span> bounds of the logNoise</span>
                                kernbounds <span class="pl-k">=</span> [[<span class="pl-k">-</span><span class="pl-c1">1</span>, <span class="pl-k">-</span><span class="pl-c1">1</span>, <span class="pl-c1">0</span>], [<span class="pl-c1">4</span>, <span class="pl-c1">4</span>, <span class="pl-c1">10</span>]],  <span class="pl-c"><span class="pl-c">#</span> bounds of the 3 parameters GaussianProcesses.get_param_names(model.kernel)</span>
                                maxeval <span class="pl-k">=</span> <span class="pl-c1">40</span>)
opt <span class="pl-k">=</span> <span class="pl-c1">BOpt</span>(f,
           model,
           <span class="pl-c1">UpperConfidenceBound</span>(),                   <span class="pl-c"><span class="pl-c">#</span> type of acquisition</span>
           modeloptimizer,                        
           [<span class="pl-k">-</span><span class="pl-c1">5.</span>, <span class="pl-k">-</span><span class="pl-c1">5.</span>], [<span class="pl-c1">5.</span>, <span class="pl-c1">5.</span>],                     <span class="pl-c"><span class="pl-c">#</span> lowerbounds, upperbounds         </span>
           repetitions <span class="pl-k">=</span> <span class="pl-c1">5</span>,                          <span class="pl-c"><span class="pl-c">#</span> evaluate the function for each input 5 times</span>
           maxiterations <span class="pl-k">=</span> <span class="pl-c1">100</span>,                      <span class="pl-c"><span class="pl-c">#</span> evaluate at 100 input positions</span>
           sense <span class="pl-k">=</span> Min,                              <span class="pl-c"><span class="pl-c">#</span> minimize the function</span>
           acquisitionoptions <span class="pl-k">=</span> (method <span class="pl-k">=</span> <span class="pl-c1">:LD_LBFGS</span>, <span class="pl-c"><span class="pl-c">#</span> run optimization of acquisition function with NLopts :LD_LBFGS method</span>
                                 restarts <span class="pl-k">=</span> <span class="pl-c1">5</span>,       <span class="pl-c"><span class="pl-c">#</span> run the NLopt method from 5 random initial conditions each time.</span>
                                 maxtime <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,      <span class="pl-c"><span class="pl-c">#</span> run the NLopt method for at most 0.1 second each time</span>
                                 maxeval <span class="pl-k">=</span> <span class="pl-c1">1000</span>),    <span class="pl-c"><span class="pl-c">#</span> run the NLopt methods for at most 1000 iterations (for other options see https://github.com/JuliaOpt/NLopt.jl)</span>
            verbosity <span class="pl-k">=</span> Progress)

result <span class="pl-k">=</span> <span class="pl-c1">boptimize!</span>(opt)</pre></div>
<h3><a id="user-content-resume-optimization" class="anchor" aria-hidden="true" href="#resume-optimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Resume optimization</h3>
<p>To continue the optimization, one can call <code>boptimize!(opt)</code> multiple times.</p>
<div class="highlight highlight-source-julia"><pre>result <span class="pl-k">=</span> <span class="pl-c1">boptimize!</span>(opt) <span class="pl-c"><span class="pl-c">#</span> first time (includes initialization)</span>
result <span class="pl-k">=</span> <span class="pl-c1">boptimize!</span>(opt) <span class="pl-c"><span class="pl-c">#</span> restart</span>
<span class="pl-c1">maxiterations!</span>(opt, <span class="pl-c1">50</span>)  <span class="pl-c"><span class="pl-c">#</span> set maxiterations for the next call</span>
result <span class="pl-k">=</span> <span class="pl-c1">boptimize!</span>(opt) <span class="pl-c"><span class="pl-c">#</span> restart again</span></pre></div>
<h3><a id="user-content-warm-start-with-some-known-function-values" class="anchor" aria-hidden="true" href="#warm-start-with-some-known-function-values"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>(Warm-)start with some known function values</h3>
<p>By default, the first <code>5*length(lowerbounds)</code> input points are sampled from a
Sobol sequence. If instead one has already some function values available and
wants to skip the initialization with the Sobol sequence, one can update the
model with the available data and set <code>initializer_iterations = 0</code>. For example
(continuing the above example after setting the <code>modeloptimizer</code>).</p>
<div class="highlight highlight-source-julia"><pre>x <span class="pl-k">=</span> [<span class="pl-c1">rand</span>(<span class="pl-c1">2</span>) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">20</span>]
y <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">f</span>.(x)
<span class="pl-c1">append!</span>(model, <span class="pl-c1">hcat</span>(x<span class="pl-k">...</span>), y)

opt <span class="pl-k">=</span> <span class="pl-c1">BOpt</span>(f,
           model,
           <span class="pl-c1">UpperConfidenceBound</span>(),
           modeloptimizer,                        
           [<span class="pl-k">-</span><span class="pl-c1">5.</span>, <span class="pl-k">-</span><span class="pl-c1">5.</span>], [<span class="pl-c1">5.</span>, <span class="pl-c1">5.</span>],
           maxiterations <span class="pl-k">=</span> <span class="pl-c1">100</span>,
           sense <span class="pl-k">=</span> Min,
           initializer_iterations <span class="pl-k">=</span> <span class="pl-c1">0</span>
          )

result <span class="pl-k">=</span> <span class="pl-c1">boptimize!</span>(opt)</pre></div>
<p>This package exports</p>
<ul>
<li><code>BOpt</code>, <code>boptimize!</code></li>
<li>acquisition types: <code>ExpectedImprovement</code>, <code>ProbabilityOfImprovement</code>, <code>UpperConfidenceBound</code>, <code>ThompsonSamplingSimple</code>, <code>MutualInformation</code></li>
<li>scaling of standard deviation in <code>UpperConfidenceBound</code>: <code>BrochuBetaScaling</code>, <code>NoBetaScaling</code></li>
<li>GP hyperparameter optimizer: <code>MAPGPOptimizer</code>, <code>NoModelOptimizer</code></li>
<li>Initializer: <code>ScaledSobolIterator</code>, <code>ScaledLHSIterator</code></li>
<li>optimization sense: <code>Min</code>, <code>Max</code></li>
<li>verbosity levels: <code>Silent</code>, <code>Timings</code>, <code>Progress</code></li>
<li>helper: <code>maxduration!</code>, <code>maxiterations!</code></li>
</ul>
<p>Use the REPL help, e.g. <code>?Bopt</code>, to get more information.</p>
<h2><a id="user-content-review-papers-on-bayesian-optimization" class="anchor" aria-hidden="true" href="#review-papers-on-bayesian-optimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Review papers on Bayesian optimization</h2>
<ul>
<li><a href="https://arxiv.org/abs/1012.2599v1" rel="nofollow">A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</a></li>
<li><a href="https://ieeexplore.ieee.org/document/7352306" rel="nofollow">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a></li>
</ul>
<h2><a id="user-content-similar-projects" class="anchor" aria-hidden="true" href="#similar-projects"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Similar Projects</h2>
<p><a href="https://github.com/jbrea/BayesOpt.jl">BayesOpt</a> is a wrapper of the established
<a href="https://github.com/rmcantin/bayesopt">BayesOpt</a> toolbox written in C++.</p>
<p><a href="https://github.com/dragonfly/dragonfly">Dragonfly</a> is a feature-rich package
for scalable Bayesian optimization written in Python. Use it in Julia with
<a href="https://github.com/JuliaPy/PyCall.jl">PyCall</a>.</p>
</article></div>