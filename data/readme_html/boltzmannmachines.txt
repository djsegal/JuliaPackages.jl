<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-boltzmannmachinesjl" class="anchor" aria-hidden="true" href="#boltzmannmachinesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BoltzmannMachines.jl</h1>
<p><a href="https://travis-ci.org/stefan-m-lenz/BoltzmannMachines.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/d20171288a13ea6032bbd10b69687061d36d72aab1554607c9dd1a6a5134dd0f/68747470733a2f2f7472617669732d63692e6f72672f73746566616e2d6d2d6c656e7a2f426f6c747a6d616e6e4d616368696e65732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/stefan-m-lenz/BoltzmannMachines.jl.svg?branch=master" style="max-width:100%;"></a></p>

<p>This Julia package implements algorithms for training and evaluating several types of Boltzmann Machines (BMs):</p>
<ul>
<li>Learning of Restricted Boltzmann Machines (RBMs) using Contrastive Divergence (CD)</li>
<li>Greedy layerwise pre-training of Deep Boltzmann Machines (DBMs)</li>
<li>Learning procedure for general Boltzmann Machines using mean-field inference and stochastic approximation. Applicable to DBMs and used for fine-tuning the weights after the pre-training</li>
<li>Exact calculation of the likelihood of BMs (only suitable for small models)</li>
<li>Annealed Importance Sampling (AIS) for estimating the likelihood of larger BMs</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>The package is contained in the official Julia package registry and can be installed via</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;BoltzmannMachines&quot;)
"><pre><code>using Pkg
Pkg.add("BoltzmannMachines")
</code></pre></div>
<h2><a id="user-content-types-of-boltzmann-machines" class="anchor" aria-hidden="true" href="#types-of-boltzmann-machines"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Types of Boltzmann Machines</h2>
<h3><a id="user-content-restricted-boltzmann-machines" class="anchor" aria-hidden="true" href="#restricted-boltzmann-machines"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Restricted Boltzmann Machines</h3>
<p>The package contains the following types of RBMs (subtypes of <code>AbstractRBM</code>):</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Distribution of visible units</th>
<th>Distribution of hidden units</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BernoulliRBM</code></td>
<td>Bernoulli</td>
<td>Bernoulli</td>
</tr>
<tr>
<td><code>Softmax0BernoulliRBM</code></td>
<td>Categorical (binary encoded)</td>
<td>Bernoulli</td>
</tr>
<tr>
<td><code>GaussianBernoulliRBM</code>, <code>GaussianBernoulliRBM2</code> ([6])</td>
<td>Gaussian</td>
<td>Bernoulli</td>
</tr>
<tr>
<td><code>Binomial2BernoulliRBM</code></td>
<td>Binomial distribution with n = 2</td>
<td>Bernoulli</td>
</tr>
<tr>
<td><code>BernoulliGaussianRBM</code></td>
<td>Bernoulli</td>
<td>Gaussian</td>
</tr>
</tbody>
</table>
<h3><a id="user-content-multimodal-deep-boltzmann-machines" class="anchor" aria-hidden="true" href="#multimodal-deep-boltzmann-machines"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>(Multimodal) Deep Boltzmann Machines</h3>
<p>DBMs are implemented as vectors of RBMs. <code>BasicDBM</code>s have only Bernoulli distributed nodes and therefore consist of a vector of <code>BernoulliRBM</code>s.
DBMs with different types of visible units can be constructed
by using the corresponding RBM type in the first layer.
Actual <code>MultimodalDBM</code>s can be formed by using <code>PartitionedRBM</code>s, which is a type of <code>AbstractRBM</code> that is able to encapsulate non-connected RBMs of different types into an RBM-like layer.</p>
<p>All these types of DBMs can be trained using layerwise pre-training and fine-tuning employing the mean-field approximation. It is also possible to estimate or calculate the likelihood for these DBM types.</p>
<h2><a id="user-content-overview-of-functions" class="anchor" aria-hidden="true" href="#overview-of-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview of functions</h2>
<p>The following tables provide an overview of the functions of the package, together with a short description. You can find more detailed descriptions for each function using the Julia help mode (entered by typing <code>?</code> at the beginning of the Julia command prompt).</p>
<h3><a id="user-content-data-preprocessing" class="anchor" aria-hidden="true" href="#data-preprocessing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Data preprocessing</h3>
<p>Continuously valued data or ordinal data can be transformed into probabilities via <code>intensities_encode</code> and then fed to <code>BernoulliRBM</code>s, like it is usually done when handling grayscale or color intensities in images.</p>
<p>Categorical data can be binary encoded as input for a <code>Softmax0BernoulliRBM</code> via <code>oneornone_encode</code>.</p>
<p>The back transformations are available via the functions <code>intensities_decode</code> and <code>oneornone_decode</code>.</p>
<h3><a id="user-content-functions-for-training" class="anchor" aria-hidden="true" href="#functions-for-training"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Functions for Training</h3>
<h4><a id="user-content-training-of-rbms" class="anchor" aria-hidden="true" href="#training-of-rbms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training of RBMs</h4>
<table>
<thead>
<tr>
<th>Function name</th>
<th>Short description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>initrbm</code></td>
<td>Initializes an RBM model.</td>
</tr>
<tr>
<td><code>trainrbm!</code></td>
<td>Performs CD-learning on an RBM model.</td>
</tr>
<tr>
<td><code>fitrbm</code></td>
<td>Fits a RBM model to a dataset using CD. (Wraps <code>initrbm</code> and <code>trainrbm!</code>)</td>
</tr>
<tr>
<td><code>samplevisible</code>, <code>samplevisible!</code> (<code>samplehidden</code>, <code>samplehidden!</code>)</td>
<td>Gibbs sampling of visible (hidden) nodes' states given the hidden (visible) nodes' states in an RBM.</td>
</tr>
<tr>
<td><code>visiblepotential</code>, <code>visiblepotential!</code> (<code>hiddenpotential</code>, <code>hiddenpotential!</code>)</td>
<td>Computes the deterministic potential for the activation of the visible (hidden) nodes of an RBM.</td>
</tr>
<tr>
<td><code>visibleinput</code>, <code>visibleinput!</code> (<code>hiddeninput</code>, <code>hiddeninput!</code>)</td>
<td>Computes the total input received by the visible (hidden) layer of an RBM.</td>
</tr>
</tbody>
</table>
<h4><a id="user-content-training-of-dbms" class="anchor" aria-hidden="true" href="#training-of-dbms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training of DBMs</h4>
<table>
<thead>
<tr>
<th>Function name</th>
<th>Short description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>fitdbm</code></td>
<td>Fits a DBM model to a dataset. This includes pre-training, followed by the general Boltzmann Machine learning procedure for fine-tuning.</td>
</tr>
<tr>
<td><code>gibbssample!</code></td>
<td>Performs Gibbs sampling in a DBM.</td>
</tr>
<tr>
<td><code>meanfield</code></td>
<td>Computes the mean-field inference of the hidden nodes' activations in a DBM.</td>
</tr>
<tr>
<td><code>stackrbms</code></td>
<td>Greedy layerwise pre-training of a DBM model or a Deep Belief Network.</td>
</tr>
<tr>
<td><code>traindbm!</code></td>
<td>Trains a DBM using the learning procedure for a general Boltzmann Machine.</td>
</tr>
</tbody>
</table>
<h3><a id="user-content-partitioned-training-and-joining-of-models" class="anchor" aria-hidden="true" href="#partitioned-training-and-joining-of-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Partitioned training and joining of models</h3>
<p>To fit <code>MultimodalDBM</code>s, the arguments for training its (partitioned) layers can be
specified using structs of type <code>TrainLayer</code> and <code>TrainPartitionedLayer</code>
(best see the <a href="test/examples.jl">examples</a> for how to use these arguments in <code>fitdbm</code> or <code>stackrbms</code>).</p>
<p>The functions <code>joindbms</code> and <code>joinrbms</code> can be used to join the weights of two
separately trained models.</p>
<h3><a id="user-content-functions-for-evaluating-a-trained-model" class="anchor" aria-hidden="true" href="#functions-for-evaluating-a-trained-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Functions for evaluating a trained model</h3>
<table>
<thead>
<tr>
<th>Function name</th>
<th>Short description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>aislogimpweights</code></td>
<td>Performs AIS on a BM and calculates the logarithmised importance weights for estimating the BM's partition function.</td>
</tr>
<tr>
<td><code>freeenergy</code></td>
<td>Computes the mean free energy of a data set in an RBM model.</td>
</tr>
<tr>
<td><code>loglikelihood</code></td>
<td>Estimates the mean loglikelihood of a dataset in a BM model using AIS.</td>
</tr>
<tr>
<td><code>logpartitionfunction</code></td>
<td>Estimates the log of the partition function of a BM.</td>
</tr>
<tr>
<td><code>logproblowerbound</code></td>
<td>Estimates the mean lower bound of the log probability of a dataset in a DBM model.</td>
</tr>
<tr>
<td><code>reconstructionerror</code></td>
<td>Computes the mean reconstruction error of a dataset in an RBM model.</td>
</tr>
<tr>
<td><code>samples</code></td>
<td>Generates samples from the distribution defined by a BM model. (See also <code>gibbssample!</code> and <code>gibbsamplecond!</code> for (conditional) Gibbs sampling.)</td>
</tr>
</tbody>
</table>
<h3><a id="user-content-monitoring-the-learning-process" class="anchor" aria-hidden="true" href="#monitoring-the-learning-process"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Monitoring the learning process</h3>
<p>The functions of the form <code>monitor*!</code> can be used for monitoring a property of the model during the learning process.
The following words, corresponding to the denominated properties, may stand in place of <code>*</code>:</p>
<ul>
<li><code>freeenergy</code></li>
<li><code>exactloglikelihood</code></li>
<li><code>loglikelihood</code></li>
<li><code>logproblowerbound</code></li>
<li><code>reconstructionerror</code></li>
</ul>
<p>The results of evaluations are stored in <code>Monitor</code> objects. The evaluations can be plotted by calling the function <code>plotevaluation</code> of the external plotting package <code>BoltzmannMachinesPlots</code>.</p>
<p>The monitoring mechanism is very flexible and allows the specification of callback functions that can be passed to the training functions <code>fitrbm</code>, <code>stackrbms</code>, <code>traindbm!</code>, and <code>fitdbm</code>.
Monitoring can be streamlined with the functions <code>monitored_fitrbm</code>,
<code>monitored_stackrbms</code>, <code>monitored_traindbm!</code> and <code>monitored_fitdbm</code>.
These functions also allow user-defined monitoring functions that conform to the same argument schema as the above mentioned predefined monitoring functions.</p>
<p>To see how these functions can be used together, best take a look at the <a href="test/examples.jl">examples</a>.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<p>You can find <a href="test/examples.jl">example code here</a>.</p>
<p>If you want to use the plotting functionality, you need to install the package <a href="https://github.com/stefan-m-lenz/BoltzmannMachinesPlots.jl"><code>BoltzmannMachinesPlots</code></a>
in addition.</p>
<h3><a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Applications</h3>
<p>The package has been used for an approach to uncover patterns in high-dimensional genetic data, described in the article</p>
<blockquote>
<p>Hess M., Lenz S., Blätte T. J., Bullinger L., Binder H. <em>Partitioned learning of deep Boltzmann machines for SNP data</em>. Bioinformatics 2017 btx408. doi: <a href="https://doi.org/10.1093/bioinformatics/btx408" rel="nofollow">https://doi.org/10.1093/bioinformatics/btx408</a>.</p>
</blockquote>
<p>The code for the analyses presented there is available in the article supplement.</p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<p>[1] Salakhutdinov, R. (2015). <em>Learning Deep Generative Models</em>. Annual Review of Statistics and Its Application, 2, 361-385.</p>
<p>[2] Salakhutdinov, R. Hinton, G. (2012). <em>An Efficient Learning Procedure for Deep Boltzmann Machines</em>. Neural computation, 24(8), 1967-2006.</p>
<p>[3] Salakhutdinov. R. (2008). <em>Learning and Evaluating Boltzmann Machines</em>. Technical Report UTML TR 2008-002, Department of Computer Science, University of Toronto.</p>
<p>[4] Krizhevsky, A., Hinton, G. (2009). <em>Learning Multiple Layers of Features from Tiny Images</em>.</p>
<p>[5] Srivastava, N., Salakhutdinov R. (2014). <em>Multimodal Learning with Deep Boltzmann Machines</em>. Journal of Machine Learning Research, 15, 2949-2980.</p>
<p>[6] Cho, K., Ilin A., Raiko, T. (2011) <em>Improved learning of Gaussian-Bernoulli restricted Boltzmann machines</em>. Artificial Neural Networks and Machine Learning – ICANN 2011.</p>
</article></div>