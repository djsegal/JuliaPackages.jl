<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://codecov.io/gh/LeeoBianchi/HealpixMPI.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/b26d36e9d0b57666bf273d3c433b7f24783af8265ee01dd19ad073a69e29e9df/68747470733a2f2f636f6465636f762e696f2f67682f4c65656f4269616e6368692f4865616c7069784d50492e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/LeeoBianchi/HealpixMPI.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://github.com/LeeoBianchi/HealpixMPI.jl/actions/workflows/UnitTest.yml"><img src="https://github.com/LeeoBianchi/HealpixMPI.jl/workflows/Unit%20tests/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://leeobianchi.github.io/HealpixMPI.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="docs/src/assets/logo.png"><img src="docs/src/assets/logo.png" width="180" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-healpixmpijl-an-mpi-parallel-implementation-of-the-healpix-tessellation-scheme-in-julia" class="anchor" aria-hidden="true" href="#healpixmpijl-an-mpi-parallel-implementation-of-the-healpix-tessellation-scheme-in-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>HealpixMPI.jl: an MPI-parallel implementation of the Healpix tessellation scheme in Julia</h1>
<p dir="auto">Welcome to HealpixMPI.jl, an MPI-parallel implementation of the main functionalities of <a href="https://healpix.sourceforge.io/" rel="nofollow">HEALPix</a> spherical tessellation scheme, entirely coded in Julia.</p>
<p dir="auto">This package constitutes a natural extension of the package <a href="https://github.com/ziotom78/Healpix.jl">Healpix.jl</a>, providing an MPI integration of its main functionalities, allowing for simultaneous shared-memory (multithreading) and distributed-memory (MPI) parallelization leading to high performance sperical harmonic transforms.</p>
<p dir="auto">Read the full <a href="https://leeobianchi.github.io/HealpixMPI.jl/dev" rel="nofollow">documentation</a> for further details.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">From the Julia REPL, run</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Pkg
Pkg.add(&quot;HealpixMPI&quot;)"><pre><span class="pl-k">import</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>HealpixMPI<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-usage-example" class="anchor" aria-hidden="true" href="#usage-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage Example</h2>
<p dir="auto">The example shows the necessary steps to set up and perform an MPI-parallel <code>alm2map</code> SHT with HealpixMPI.jl.</p>
<h3 dir="auto"><a id="user-content-set-up" class="anchor" aria-hidden="true" href="#set-up"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Set up</h3>
<p dir="auto">We set up the necessary MPI communication and initialize Healpix.jl structures:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MPI
using Random
using Healpix
using HealpixMPI

#MPI set-up
MPI.Init()
comm = MPI.COMM_WORLD
crank = MPI.Comm_rank(comm)
csize = MPI.Comm_size(comm)
root = 0

#initialize Healpix structures
NSIDE = 64
lmax = 3*NSIDE - 1
if crank == root
  h_map = HealpixMap{Float64, RingOrder}(NSIDE)   #empty map
  h_alm = Alm(lmax, lmax, randn(ComplexF64, numberOfAlms(lmax)))  #random alm
else
  h_map = nothing
  h_alm = nothing
end"><pre><span class="pl-k">using</span> MPI
<span class="pl-k">using</span> Random
<span class="pl-k">using</span> Healpix
<span class="pl-k">using</span> HealpixMPI

<span class="pl-c"><span class="pl-c">#</span>MPI set-up</span>
MPI<span class="pl-k">.</span><span class="pl-c1">Init</span>()
comm <span class="pl-k">=</span> MPI<span class="pl-k">.</span>COMM_WORLD
crank <span class="pl-k">=</span> MPI<span class="pl-k">.</span><span class="pl-c1">Comm_rank</span>(comm)
csize <span class="pl-k">=</span> MPI<span class="pl-k">.</span><span class="pl-c1">Comm_size</span>(comm)
root <span class="pl-k">=</span> <span class="pl-c1">0</span>

<span class="pl-c"><span class="pl-c">#</span>initialize Healpix structures</span>
NSIDE <span class="pl-k">=</span> <span class="pl-c1">64</span>
lmax <span class="pl-k">=</span> <span class="pl-c1">3</span><span class="pl-k">*</span>NSIDE <span class="pl-k">-</span> <span class="pl-c1">1</span>
<span class="pl-k">if</span> crank <span class="pl-k">==</span> root
  h_map <span class="pl-k">=</span> <span class="pl-c1">HealpixMap</span><span class="pl-c1">{Float64, RingOrder}</span>(NSIDE)   <span class="pl-c"><span class="pl-c">#</span>empty map</span>
  h_alm <span class="pl-k">=</span> <span class="pl-c1">Alm</span>(lmax, lmax, <span class="pl-c1">randn</span>(ComplexF64, <span class="pl-c1">numberOfAlms</span>(lmax)))  <span class="pl-c"><span class="pl-c">#</span>random alm</span>
<span class="pl-k">else</span>
  h_map <span class="pl-k">=</span> <span class="pl-c1">nothing</span>
  h_alm <span class="pl-k">=</span> <span class="pl-c1">nothing</span>
<span class="pl-k">end</span></pre></div>
<h3 dir="auto"><a id="user-content-distribution" class="anchor" aria-hidden="true" href="#distribution"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Distribution</h3>
<p dir="auto">The distributed HealpixMPI.jl data types are filled through an overload of <code>MPI.Scatter!</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="#initialize empty HealpixMPI structures 
d_map = DMap{RR}(comm)
d_alm = DAlm{RR}(comm)

#fill them
MPI.Scatter!(h_map, d_map)
MPI.Scatter!(h_alm, d_alm)"><pre><span class="pl-c"><span class="pl-c">#</span>initialize empty HealpixMPI structures </span>
d_map <span class="pl-k">=</span> <span class="pl-c1">DMap</span><span class="pl-c1">{RR}</span>(comm)
d_alm <span class="pl-k">=</span> <span class="pl-c1">DAlm</span><span class="pl-c1">{RR}</span>(comm)

<span class="pl-c"><span class="pl-c">#</span>fill them</span>
MPI<span class="pl-k">.</span><span class="pl-c1">Scatter!</span>(h_map, d_map)
MPI<span class="pl-k">.</span><span class="pl-c1">Scatter!</span>(h_alm, d_alm)</pre></div>
<h3 dir="auto"><a id="user-content-sht" class="anchor" aria-hidden="true" href="#sht"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SHT</h3>
<p dir="auto">We perform the SHT through an overload of <code>Healpix.alm2map</code> and, if needed, we <code>Gather!</code> the result in a <code>HealpixMap</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="alm2map!(d_alm, d_map; nthreads = 16)
MPI.Gather!(d_map, h_map)"><pre><span class="pl-c1">alm2map!</span>(d_alm, d_map; nthreads <span class="pl-k">=</span> <span class="pl-c1">16</span>)
MPI<span class="pl-k">.</span><span class="pl-c1">Gather!</span>(d_map, h_map)</pre></div>
<p dir="auto">This allows the user to adjust at run time the number of threads to use, typically to be set to the number of cores of your machine.</p>
<h2 dir="auto"><a id="user-content-run" class="anchor" aria-hidden="true" href="#run"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run</h2>
<p dir="auto">In order to exploit MPI parallelization run the code through <code>mpirun</code> or <code>mpiexec</code> as</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="$ mpiexec -n {Ntask} julia {your_script.jl}"><pre>$ mpiexec -n {Ntask} julia {your_script.jl}</pre></div>
<p dir="auto">To run a code on multiple nodes, specify a machine file <code>machines.txt</code> as</p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="$ mpiexec -machinefile machines.txt julia {your_script.jl}"><pre>$ mpiexec -machinefile machines.txt julia {your_script.jl}</pre></div>
</article></div>