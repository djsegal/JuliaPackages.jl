<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-anomaly-detection" class="anchor" aria-hidden="true" href="#anomaly-detection"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Anomaly Detection</h1>
<p>Implementation of various generative neural network models for anomaly detection in Julia, using the Flux framework. Serves as a codebase for the comparative study presented in the <a href="https://arxiv.org/abs/1807.05027" rel="nofollow">paper</a></p>
<p>Škvára, Vít, Tomáš Pevný, and Václav Šmídl. <em>Are generative deep models for novelty detection truly better?</em> arXiv preprint arXiv:1807.05027 (2018).</p>
<h2><a id="user-content-models-implemented" class="anchor" aria-hidden="true" href="#models-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Models implemented:</h2>
<table>
<thead>
<tr>
<th>acronym</th>
<th>name</th>
<th>paper</th>
</tr>
</thead>
<tbody>
<tr>
<td>AE</td>
<td>Autoencoder</td>
<td>Vincent, Pascal, et al. "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion." Journal of Machine Learning Research 11.Dec (2010): 3371-3408. <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" rel="nofollow">link</a></td>
</tr>
<tr>
<td>VAE</td>
<td>Variational Autoencoder</td>
<td>Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013). <a href="arxiv.org/abs/1312.6114">link</a></td>
</tr>
<tr>
<td>sVAE</td>
<td>symetric Variational Autoencoder</td>
<td>Pu, Yunchen, et al. "Symmetric variational autoencoder and connections to adversarial learning." arXiv preprint arXiv:1709.01846 (2017). <a href="https://arxiv.org/abs/1709.01846" rel="nofollow">link</a></td>
</tr>
<tr>
<td>GAN</td>
<td>Generative Adversarial Network</td>
<td>Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014. <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="nofollow">link</a></td>
</tr>
<tr>
<td>fmGAN</td>
<td>GAN with feature-matching loss</td>
<td>Salimans, Tim, et al. "Improved techniques for training gans." Advances in Neural Information Processing Systems. 2016. <a href="http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans" rel="nofollow">link</a></td>
</tr>
</tbody>
</table>
<h2><a id="user-content-experiments" class="anchor" aria-hidden="true" href="#experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Experiments:</h2>
<p>Experiments are executed on the Loda (Lightweight on-line detector of anomalies) datasets that are in the experiments directory. The sampling method is based on <a href="http://web.engr.oregonstate.edu/~tgd/publications/emmott-das-dietterich-fern-wong-systematic-construction-of-anomaly-detection-benchmarks-from-real-data-odd13.pdf" rel="nofollow">this paper</a>. After downloading the datasets, you can create your own using the experiments/prepare_data.jl function. For experimental evaluation, you need the EvalCurves package:</p>
<p><code>&gt;&gt; Pkg.clone("https://github.com/vitskvara/EvalCurves.jl.git")</code></p>
</article></div>