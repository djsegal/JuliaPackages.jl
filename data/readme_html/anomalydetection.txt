<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-anomaly-detection" class="anchor" aria-hidden="true" href="#anomaly-detection"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Anomaly Detection</h1>
<p>Implementation of various generative neural network models for anomaly detection in Julia, using the Flux framework. Serves as a codebase for the comparative study presented in the <a href="https://arxiv.org/abs/1807.05027" rel="nofollow">paper</a></p>
<p>Škvára, Vít, Tomáš Pevný, and Václav Šmídl. <em>Are generative deep models for novelty detection truly better?</em> arXiv preprint arXiv:1807.05027 (2018).</p>
<h2><a id="user-content-models-implemented" class="anchor" aria-hidden="true" href="#models-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models implemented:</h2>
<table>
<thead>
<tr>
<th>acronym</th>
<th>name</th>
<th>paper</th>
</tr>
</thead>
<tbody>
<tr>
<td>AE</td>
<td>Autoencoder</td>
<td>Vincent, Pascal, et al. "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion." Journal of Machine Learning Research 11.Dec (2010): 3371-3408. <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" rel="nofollow">link</a></td>
</tr>
<tr>
<td>VAE</td>
<td>Variational Autoencoder</td>
<td>Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013). <a href="arxiv.org/abs/1312.6114">link</a></td>
</tr>
<tr>
<td>sVAE</td>
<td>symetric Variational Autoencoder</td>
<td>Pu, Yunchen, et al. "Symmetric variational autoencoder and connections to adversarial learning." arXiv preprint arXiv:1709.01846 (2017). <a href="https://arxiv.org/abs/1709.01846" rel="nofollow">link</a></td>
</tr>
<tr>
<td>GAN</td>
<td>Generative Adversarial Network</td>
<td>Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014. <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="nofollow">link</a></td>
</tr>
<tr>
<td>fmGAN</td>
<td>GAN with feature-matching loss</td>
<td>Salimans, Tim, et al. "Improved techniques for training gans." Advances in Neural Information Processing Systems. 2016. <a href="http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans" rel="nofollow">link</a></td>
</tr>
</tbody>
</table>
<h2><a id="user-content-experiments" class="anchor" aria-hidden="true" href="#experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Experiments:</h2>
<p>Experiments are executed on the Loda (Lightweight on-line detector of anomalies) datasets that are in the experiments directory. The sampling method is based on <a href="http://web.engr.oregonstate.edu/~tgd/publications/emmott-das-dietterich-fern-wong-systematic-construction-of-anomaly-detection-benchmarks-from-real-data-odd13.pdf" rel="nofollow">this paper</a>. After downloading the datasets, you can create your own using the experiments/prepare_data.jl function. For experimental evaluation, you need the EvalCurves package:</p>
<p><code>&gt;&gt; Pkg.clone("https://github.com/vitskvara/EvalCurves.jl.git")</code></p>
</article></div>