<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-symbolicmdpsjl" class="anchor" aria-hidden="true" href="#symbolicmdpsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SymbolicMDPs.jl</h1>
<p dir="auto"><strong>SymbolicMDPs.jl</strong> wraps the  <a href="https://github.com/JuliaPlanners/PDDL.jl">PDDL.jl</a> interface for <a href="https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language" rel="nofollow">PDDL</a> domains and problems within the <a href="https://juliapomdp.github.io/POMDPs.jl/latest/" rel="nofollow">POMDPs.jl</a> interface for <a href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="nofollow">Markov decision processes (MDPs)</a>.</p>
<p dir="auto">Since POMDPs.jl supports the reinforcement learning interface defined by <a href="https://github.com/JuliaReinforcementLearning/CommonRLInterface.jl">CommonRLInterface.jl</a>, this package also allows PDDL domains to be treated as RL environments compatible with libraries such as <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a>.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">Install via the <code>Pkg</code> REPL by running:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="add https://github.com/JuliaPlanners/SymbolicMDPs.jl.git"><pre>add https<span class="pl-k">:</span><span class="pl-k">//</span>github<span class="pl-k">.</span>com<span class="pl-k">/</span>JuliaPlanners<span class="pl-k">/</span>SymbolicMDPs<span class="pl-k">.</span>jl<span class="pl-k">.</span>git</pre></div>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Once a PDDL <code>domain</code> and <code>problem</code> are loaded (e.g. using <code>PDDL.load_domain</code> and <code>PDDL.load_problem</code>), a corresponding MDP can be constructed as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="mdp = SymbolicMDP(domain, problem)"><pre>mdp <span class="pl-k">=</span> <span class="pl-c1">SymbolicMDP</span>(domain, problem)</pre></div>
<p dir="auto">Alternatively, we can manually specify an initial <code>state</code>, <code>goal</code> formula and cost <code>metric</code> formula:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="mdp = SymbolicMDP(domain, state, goal, metric)"><pre>mdp <span class="pl-k">=</span> <span class="pl-c1">SymbolicMDP</span>(domain, state, goal, metric)</pre></div>
<p dir="auto">By default, every action leads to <code>-1</code> reward. If <code>metric</code> is specified, then the reward will be equal to the difference in the value of the metric between consecutive states. More customization of e.g. reward functions will be supported in the future. States that satisfy the <code>goal</code> condition are considered terminal.</p>
<p dir="auto">We can also construct RL environments as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="env = SymbolicRLEnv(domain, problem)"><pre>env <span class="pl-k">=</span> <span class="pl-c1">SymbolicRLEnv</span>(domain, problem)</pre></div>
<p dir="auto">For many reinforcement learning methods, the underlying PDDL state has to be converted to an integer or vector representation (eg. for tabular or deep reinforcement learning respectively). We can do this by specifying a conversion type:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="tabular_env = SymbolicRLEnv(Int, domain, problem)
vector_env = SymbolicRLEnv(Vector, domain, problem)"><pre>tabular_env <span class="pl-k">=</span> <span class="pl-c1">SymbolicRLEnv</span>(Int, domain, problem)
vector_env <span class="pl-k">=</span> <span class="pl-c1">SymbolicRLEnv</span>(Vector, domain, problem)</pre></div>
<p dir="auto">To use these environments with <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a>, we have to perform one further step of conversion:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ReinforcementLearning
env = convert(RLBase.AbstractEnv, env)"><pre><span class="pl-k">using</span> ReinforcementLearning
env <span class="pl-k">=</span> <span class="pl-c1">convert</span>(RLBase<span class="pl-k">.</span>AbstractEnv, env)</pre></div>
<p dir="auto">For an example of tabular Q-learning on a small Blocksworld problem with two blocks, see <a href="examples/tabular_q.jl"><code>examples/tabular_q.jl</code></a>.</p>
</article></div>