<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a href="https://doi.org/10.5281/zenodo.4301017" rel="nofollow"><img src="https://camo.githubusercontent.com/a7f4827bde18d73545a50901837ea71d798a396ecedcb9d65d862c5504e103aa/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343330313031372e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.4301017.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-judi4flux-seismic-modeling-for-deep-learning" class="anchor" aria-hidden="true" href="#judi4flux-seismic-modeling-for-deep-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>JUDI4Flux: Seismic modeling for deep learning</h2>
<p>JUDI4Flux enables compositions of seismic modeling operators with (convolutional) neural networks. JUDI4Flux is an extension of <a href="https://github.com/slimgroup/JUDI.jl">JUDI</a>, a framework for seismic modeling and inversion with automatic code generation and performance optimization based on <a href="https://www.devitoproject.org/" rel="nofollow">Devito</a>. JUDI4Flux integrates JUDI's linear and non-linear modeling operators into the <a href="https://github.com/FluxML/Flux.jl">Flux</a> deep learning library, thus allowing the implementation of <em>physics-driven</em> neural networks. For backpropagation, JUDI4Flux calls JUDI's adjoint PDE solvers, thus making it possible to backpropagate efficiently through single or multiple PDE layers and scale to large problem sizes.</p>
<p><strong>Features:</strong></p>
<ul>
<li>
<p>Compatibility with the Julia <a href="https://github.com/FluxML/Flux.jl">Flux</a> deep learning library. Both Flux and JUDI are based on abstract high-level mathematical expressions that enable <em>clean</em> coding.</p>
</li>
<li>
<p>Blazingly fast seismic modeling routines using stencil-based finite-difference C code, which is automatically generated and optimized by <a href="https://www.devitoproject.org/" rel="nofollow">Devito</a>.</p>
</li>
<li>
<p>Supported operators: forward/adjoint modeling, linearized Born scattering/RTM</p>
</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>User installation:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="] add https://github.com/slimgroup/JUDI4Flux.jl
"><pre><code>] add https://github.com/slimgroup/JUDI4Flux.jl
</code></pre></div>
<p>Developer installation:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="] dev https://github.com/slimgroup/JUDI4Flux.jl
"><pre><code>] dev https://github.com/slimgroup/JUDI4Flux.jl
</code></pre></div>
<h2><a id="user-content-linear-and-nonlinear-judi-operators-with-flux" class="anchor" aria-hidden="true" href="#linear-and-nonlinear-judi-operators-with-flux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Linear and nonlinear JUDI operators with Flux</h2>
<p>JUDI4Flux enables compositions of neural network layers (e.g. convolutional or fully-connected layers) with operators for seismic modeling. Instead of having to re-implement seismic modeling operators with convolutions from machine learning libraries, this makes it possible to use existing modeling operators, namely JUDI operators for Born- and nonlinear modeling. Even more importantly, we can evaluate these operators during backpropagation by calling the corresponding adjoint operators, but fully integrate them into Flux's automatic differentiation (AD) module (Flux.Tracker).</p>
<p>This allows combining JUDI operators with Flux misfit functions to compute gradients for least squares RTM (LS-RTM) or full waveform inversion (FWI) and combinations of modeling and neural network layers. For example, the following code show an example of combining the Born scattering operator with a dense neural network layer (check <a href="https://github.com/slimgroup/JUDI4Flux.jl/blob/master/examples/example_born_fully_connected.jl">here</a> for the full example):</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="# Test demigration operator w/ Flux Dense Layer
y = randn(Float32, 100)
W = randn(Float32, 100, length(y))
b = randn(Float32, 100)

# Linearized Born operator
J = judiJacobian(F, q)

# Example image
x = vec(image)

predict(x) = W*(J*x) .+ b
loss(x,y) = Flux.mse(predict(x), y)

# Compute gradient w/ Flux
gs = gradient(() -&gt; loss(x, y), params(W, b, x))
gs[x]   # evalute gradient of x
"><pre lang="{#example_lin}"><code># Test demigration operator w/ Flux Dense Layer
y = randn(Float32, 100)
W = randn(Float32, 100, length(y))
b = randn(Float32, 100)

# Linearized Born operator
J = judiJacobian(F, q)

# Example image
x = vec(image)

predict(x) = W*(J*x) .+ b
loss(x,y) = Flux.mse(predict(x), y)

# Compute gradient w/ Flux
gs = gradient(() -&gt; loss(x, y), params(W, b, x))
gs[x]   # evalute gradient of x
</code></pre></div>
<p>Using non-linear modeling operators and convolutional layers is possible as well! For example, the following code shows how to integrate an extended source modeling JUDI operator into a shallow CNN, consisting of two convolutional layers, with a nonlinear forward modeling layer ℱ in-between them. As before, we can define a loss function using Flux utilities and compute derivatives with respect to various parameters, such as the squared slowness vector <code>m</code>. Once again, gradients of layers containing JUDI operators are computed using the corresponding adjoints or JUDI gradients, instead of Flux's Tracker module (full example <a href="https://github.com/slimgroup/JUDI4Flux.jl/blob/master/examples/example_extended_source_cnn.jl">here</a>):</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="# Extended source modeling operator
model = Model(n, d, o, m)
Pr = judiProjection(info, recGeometry)
A_inv = judiModeling(info, model; options=opt)
Pw = judiLRWF(info, wavelet)

# Network layers
ℱ = ExtendedQForward(Pr*A_inv*adjoint(Pw))
conv1 = Conv((3, 3), 1=&gt;1, pad=1, stride=1)
conv2 = Conv((3, 3), 1=&gt;1, pad=1, stride=1)

# Network and loss
function predict(x, m)
    x = conv1(x)
    x = ℱ(x, m)    # m is the velocity model
    x = conv2(x)
    return x
end

loss(x, m, y) = Flux.mse(predict(x, m), y)

# Compute gradient w/ Flux
p = params(x, m, conv1, conv2)
gs = gradient(() -&gt; loss(x, m, y), p)

gs[m]   # evalute gradient w.r.t. m
gs[conv1.weight]   # evalute gradient w.r.t. the convolution kernel

"><pre lang="{#example_nonlin}"><code># Extended source modeling operator
model = Model(n, d, o, m)
Pr = judiProjection(info, recGeometry)
A_inv = judiModeling(info, model; options=opt)
Pw = judiLRWF(info, wavelet)

# Network layers
ℱ = ExtendedQForward(Pr*A_inv*adjoint(Pw))
conv1 = Conv((3, 3), 1=&gt;1, pad=1, stride=1)
conv2 = Conv((3, 3), 1=&gt;1, pad=1, stride=1)

# Network and loss
function predict(x, m)
    x = conv1(x)
    x = ℱ(x, m)    # m is the velocity model
    x = conv2(x)
    return x
end

loss(x, m, y) = Flux.mse(predict(x, m), y)

# Compute gradient w/ Flux
p = params(x, m, conv1, conv2)
gs = gradient(() -&gt; loss(x, m, y), p)

gs[m]   # evalute gradient w.r.t. m
gs[conv1.weight]   # evalute gradient w.r.t. the convolution kernel

</code></pre></div>
<h2><a id="user-content-example-applications" class="anchor" aria-hidden="true" href="#example-applications"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example applications</h2>
<p>A possible application of JUDI4Flux is the implementation of loop-unrolled LS-RTM algorithms - physics-augmented convolutional neural networks for seismic imaging. By training a loop unrolled LS-RTM network using pairs of true images and observed data, this makes it possible to obtain high-fidelity images from noisy simultaneous shot records. The below figure compares RTM, standard LS-RTM with gradient descent and loop unrolled LS-RTM. Each image is obtained from a single simultaneous shot record only. (<em>Full preprint to be added to arXiv shortly.</em>)</p>
<p><a target="_blank" rel="noopener noreferrer" href="docs/loop_unrolling.png"><img src="docs/loop_unrolling.png" alt="" style="max-width:100%;"></a></p>
<h2><a id="user-content-related-work" class="anchor" aria-hidden="true" href="#related-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Related work</h2>
<ul>
<li>
<p>For a similar framework in Python that interfaces PyTorch (and actually predates <code>JUDI4Flux</code>), please check out Alan Richardson very cool package <a href="https://github.com/ar4/deepwave">deepwave</a>. Similar to <code>JUDI4Flux</code>, this package integrates seismic modeling and linearized modeling functions based on finite-difference stencil code into a deep learning framework. Alan's package supports seismic modeling on both CPUs and GPUs.</p>
</li>
<li>
<p><a href="https://github.com/JuliaDiffEq/DiffEqFlux.jl">DiffEqFlux.jl</a> is a Julia package that integrates the  <a href="https://github.com/JuliaDiffEq/DiffEqDocs.jl/blob/master/docs/src/index.md">DifferentialEquations.jl</a> package for more generic ODEs/PDEs into Flux.</p>
</li>
</ul>
<h2><a id="user-content-author" class="anchor" aria-hidden="true" href="#author"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Author</h2>
<p>This package was written by <a href="https://www.slim.eos.ubc.ca/philipp" rel="nofollow">Philipp Witte</a> from the Seismic Laboratory for Imaging and Modeling (SLIM) at the Georgia Institute of Technology.</p>
<ul>
<li>Contact Philipp at <a href="mailto:pwitte3@gatech.edu">pwitte3@gatech.edu</a></li>
</ul>
</article></div>