<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-sigmaridgeregressionjl" class="anchor" aria-hidden="true" href="#sigmaridgeregressionjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SigmaRidgeRegression.jl</h1>
<p><a target="_blank" rel="noopener noreferrer" href="sigmaridge_logo.png"><img src="sigmaridge_logo.png" width="205" style="max-width:100%;"></a></p>
<p><a href="https://github.com/nignatiadis/SigmaRidgeRegression.jl/actions"><img src="https://github.com/nignatiadis/SigmaRidgeRegression.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/nignatiadis/SigmaRidgeRegression.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/58900bbee03cc584545571e42effda4846d0943c988c3eb90194960bf0db39f8/68747470733a2f2f636f6465636f762e696f2f67682f6e69676e617469616469732f5369676d61526964676552656772657373696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/nignatiadis/SigmaRidgeRegression.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>Automatically and optimally-tuned Ridge regression when the features may be partitioned into groups.</p>
<p>See the manuscript below for a theoretical description of the method.</p>
<blockquote>
<p>Ignatiadis, Nikolaos, and Panagiotis Lolas. "σ-Ridge: group regularized ridge regression via empirical Bayes noise level cross-validation." <a href="https://arxiv.org/abs/2010.15817" rel="nofollow">arXiv:2010.15817</a> (2020+)</p>
</blockquote>
<p>The folder <code>reproduction_code</code> in this repository contains code to reproduce the results of the paper.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>The package is available on the Julia registry (for Julia version 1.5) and may be installed as follows:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;SigmaRidgeRegression&quot;)
"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>SigmaRidgeRegression<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example usage</h2>
<p>SigmaRidgeRegression.jl can be used alongside the <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> framework for machine learning in Julia.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using MLJ
using SigmaRidgeRegression
using Random

# Suppose we have three groups of features, each with n observations
# and 25, 50 and 100 features respectively
n = 400
Random.seed!(1)
p1 = 25 ; X1 = randn(n, p1)
p2 = 50 ; X2 = randn(n, p2)
p3 = 100; X3 = randn(n, p3)

# The signal in the regression of the coefficients across these groups varies
α1_sq = 4.0 ;  βs1 = randn(p1) .* sqrt(α1_sq / p1)
α2_sq = 8.0 ;  βs2 = randn(p2) .* sqrt(α2_sq / p2)
α3_sq = 12.0;  βs3 = randn(p3) .* sqrt(α3_sq / p3)

# Let us concatenate the results and create a response
X = [X1 X2 X3]
βs = [βs1; βs2; βs3]
σ = 4.0
Y = X*βs .+ σ .* randn(n)

# Let us make a `GroupedFeatures` object that describes the feature grouping
# !!NOTE!! Right now the features are expected to be ordered consecutively in groups
# i.e., the first p1 features belong to group 1 etc.
groups = GroupedFeatures([p1;p2;p3])

# Create MLJ machine and fit SigmaRidgeRegression:
sigma_model = LooSigmaRidgeRegressor(;groups=groups)
mach_sigma_model = machine(sigma_model,  MLJ.table(X), Y)
fit!(mach_sigma_model)

# How well are we estimating the true X*βs in mean squared error?
mean(abs2, X*βs .- predict(mach_sigma_model))  # 4.612726430034071

# In this case we may compare also to the Bayes risk
λs_opt = σ^2 ./ [α1_sq; α2_sq; α3_sq] .* groups.ps ./n
bayes = MultiGroupRidgeRegressor(;groups=groups, λs=λs_opt, center=false, scale=false)


mach_bayes = machine(bayes, MLJ.table(X), Y)
fit!(mach_bayes)
mean(abs2, X*βs .- predict(mach_bayes)) #4.356913540118585
"><pre><span class="pl-k">using</span> MLJ
<span class="pl-k">using</span> SigmaRidgeRegression
<span class="pl-k">using</span> Random

<span class="pl-c"><span class="pl-c">#</span> Suppose we have three groups of features, each with n observations</span>
<span class="pl-c"><span class="pl-c">#</span> and 25, 50 and 100 features respectively</span>
n <span class="pl-k">=</span> <span class="pl-c1">400</span>
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">1</span>)
p1 <span class="pl-k">=</span> <span class="pl-c1">25</span> ; X1 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n, p1)
p2 <span class="pl-k">=</span> <span class="pl-c1">50</span> ; X2 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n, p2)
p3 <span class="pl-k">=</span> <span class="pl-c1">100</span>; X3 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n, p3)

<span class="pl-c"><span class="pl-c">#</span> The signal in the regression of the coefficients across these groups varies</span>
α1_sq <span class="pl-k">=</span> <span class="pl-c1">4.0</span> ;  βs1 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(p1) <span class="pl-k">.*</span> <span class="pl-c1">sqrt</span>(α1_sq <span class="pl-k">/</span> p1)
α2_sq <span class="pl-k">=</span> <span class="pl-c1">8.0</span> ;  βs2 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(p2) <span class="pl-k">.*</span> <span class="pl-c1">sqrt</span>(α2_sq <span class="pl-k">/</span> p2)
α3_sq <span class="pl-k">=</span> <span class="pl-c1">12.0</span>;  βs3 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(p3) <span class="pl-k">.*</span> <span class="pl-c1">sqrt</span>(α3_sq <span class="pl-k">/</span> p3)

<span class="pl-c"><span class="pl-c">#</span> Let us concatenate the results and create a response</span>
X <span class="pl-k">=</span> [X1 X2 X3]
βs <span class="pl-k">=</span> [βs1; βs2; βs3]
σ <span class="pl-k">=</span> <span class="pl-c1">4.0</span>
Y <span class="pl-k">=</span> X<span class="pl-k">*</span>βs <span class="pl-k">.+</span> σ <span class="pl-k">.*</span> <span class="pl-c1">randn</span>(n)

<span class="pl-c"><span class="pl-c">#</span> Let us make a `GroupedFeatures` object that describes the feature grouping</span>
<span class="pl-c"><span class="pl-c">#</span> !!NOTE!! Right now the features are expected to be ordered consecutively in groups</span>
<span class="pl-c"><span class="pl-c">#</span> i.e., the first p1 features belong to group 1 etc.</span>
groups <span class="pl-k">=</span> <span class="pl-c1">GroupedFeatures</span>([p1;p2;p3])

<span class="pl-c"><span class="pl-c">#</span> Create MLJ machine and fit SigmaRidgeRegression:</span>
sigma_model <span class="pl-k">=</span> <span class="pl-c1">LooSigmaRidgeRegressor</span>(;groups<span class="pl-k">=</span>groups)
mach_sigma_model <span class="pl-k">=</span> <span class="pl-c1">machine</span>(sigma_model,  MLJ<span class="pl-k">.</span><span class="pl-c1">table</span>(X), Y)
<span class="pl-c1">fit!</span>(mach_sigma_model)

<span class="pl-c"><span class="pl-c">#</span> How well are we estimating the true X*βs in mean squared error?</span>
<span class="pl-c1">mean</span>(abs2, X<span class="pl-k">*</span>βs <span class="pl-k">.-</span> <span class="pl-c1">predict</span>(mach_sigma_model))  <span class="pl-c"><span class="pl-c">#</span> 4.612726430034071</span>

<span class="pl-c"><span class="pl-c">#</span> In this case we may compare also to the Bayes risk</span>
λs_opt <span class="pl-k">=</span> σ<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">./</span> [α1_sq; α2_sq; α3_sq] <span class="pl-k">.*</span> groups<span class="pl-k">.</span>ps <span class="pl-k">./</span>n
bayes <span class="pl-k">=</span> <span class="pl-c1">MultiGroupRidgeRegressor</span>(;groups<span class="pl-k">=</span>groups, λs<span class="pl-k">=</span>λs_opt, center<span class="pl-k">=</span><span class="pl-c1">false</span>, scale<span class="pl-k">=</span><span class="pl-c1">false</span>)


mach_bayes <span class="pl-k">=</span> <span class="pl-c1">machine</span>(bayes, MLJ<span class="pl-k">.</span><span class="pl-c1">table</span>(X), Y)
<span class="pl-c1">fit!</span>(mach_bayes)
<span class="pl-c1">mean</span>(abs2, X<span class="pl-k">*</span>βs <span class="pl-k">.-</span> <span class="pl-c1">predict</span>(mach_bayes)) <span class="pl-c"><span class="pl-c">#</span>4.356913540118585</span></pre></div>
<h3><a id="user-content-todos" class="anchor" aria-hidden="true" href="#todos"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TODOs</h3>
<ul>
<li>Fully implement the MLJ interface.</li>
<li>Wait for the following MLJ issue to be fixed: <a href="https://github.com/alan-turing-institute/MLJBase.jl/issues/428#issuecomment-708141459">https://github.com/alan-turing-institute/MLJBase.jl/issues/428#issuecomment-708141459</a>, in the meantime this package uses type piracy as in the linked comment to accommodate a large number of features.</li>
</ul>
</article></div>