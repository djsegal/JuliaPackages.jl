<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-covariancematricesjl" class="anchor" aria-hidden="true" href="#covariancematricesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CovarianceMatrices.jl</h1>
<p><a href="https://travis-ci.org/gragusa/CovarianceMatrices.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9500b79d6146d9521cbdb9053e701e1347677160/68747470733a2f2f7472617669732d63692e6f72672f677261677573612f436f76617269616e63654d617472696365732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/gragusa/CovarianceMatrices.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/gragusa/CovarianceMatrices.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/5c2978bda3776b259a3e1716a2b1c00252a69207/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f677261677573612f436f76617269616e63654d617472696365732e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/gragusa/CovarianceMatrices.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a>
<a href="http://codecov.io/github/gragusa/CovarianceMatrices.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/9119d07dc075a12c8a27133b60e66d505c5e94e4/687474703a2f2f636f6465636f762e696f2f6769746875622f677261677573612f436f76617269616e63654d617472696365732e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/gragusa/CovarianceMatrices.jl/coverage.svg?branch=master" style="max-width:100%;"></a></p>
<p>Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation for Julia.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>The package is registered on <a href="http::/github.com/JuliaLang/METADATA.jl">METADATA</a>, so to install</p>
<div class="highlight highlight-source-julia"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>CovarianceMatrices<span class="pl-pds">"</span></span>)</pre></div>
<hr>
<h2><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Introduction</h2>
<p>This package provides types and methods useful to obtain consistent estimates of the long run covariance matrix of a random process.</p>
<p>Three classes of estimators are considered:</p>
<ol>
<li><strong>HAC</strong> - heteroskedasticity and autocorrelation consistent (Andrews, 1996; Newey and West, 1994)</li>
<li><strong>HC</strong>  - hetheroskedasticity (White, 1982)</li>
<li><strong>CRVE</strong> - cluster robust (Arellano, 1986)</li>
</ol>
<p>The typical application of these estimators is to conduct robust inference about parameters of a model. This is accomplished by extending methods defined in <a href="http://github.com/JuliaStat/StatsBase.jl">StatsBase.jl</a> and <a href="http://github.com/JuliaStat/GLM.jl">GLM.jl</a>.</p>
<h1><a id="user-content-quick-tour" class="anchor" aria-hidden="true" href="#quick-tour"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick tour</h1>
<h2><a id="user-content-hac-heteroskedasticity-and-autocorrelation-consistent" class="anchor" aria-hidden="true" href="#hac-heteroskedasticity-and-autocorrelation-consistent"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>HAC (Heteroskedasticity and Autocorrelation Consistent)</h2>
<p>Available kernel types are:</p>
<ul>
<li><code>TruncatedKernel</code></li>
<li><code>BartlettKernel</code></li>
<li><code>ParzenKernel</code></li>
<li><code>TukeyHanningKernel</code></li>
<li><code>QuadraticSpectralKernel</code></li>
</ul>
<p>For example, <code>ParzenKernel(NeweyWest)</code> return an instance of <code>TruncatedKernel</code> parametrized by <code>NeweyWest</code>, the type that corresponds to the optimal bandwidth calculated following Newey and West (1994).  Similarly, <code>ParzenKernel(Andrews)</code> corresponds to the optimal bandwidth obtained in Andrews (1991). If the bandwidth is known, it can be directly passed, i.e. <code>TruncatedKernel(2)</code>.</p>
<p>The examples below clarify the API, that is however relatively easy to use.</p>
<h3><a id="user-content-long-run-variance-of-the-regression-coefficient" class="anchor" aria-hidden="true" href="#long-run-variance-of-the-regression-coefficient"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Long run variance of the regression coefficient</h3>
<p>In the regression context, the function <code>vcov</code> does all the work:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">vcov</span>(<span class="pl-k">::</span><span class="pl-c1">DataFrameRegressionModel</span>, <span class="pl-k">::</span><span class="pl-c1">HAC</span>; prewhite <span class="pl-k">=</span> <span class="pl-c1">true</span>)</pre></div>
<p>Consider the following artificial data (a regression with autoregressive error component):</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> CovarianceMatrices
<span class="pl-k">using</span> DataFrames
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">1</span>)
n <span class="pl-k">=</span> <span class="pl-c1">500</span>
x <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n,<span class="pl-c1">5</span>)
u <span class="pl-k">=</span> <span class="pl-c1">Array</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">2</span><span class="pl-k">*</span>n)
u[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-c1">rand</span>()
<span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">*</span>n
    u[j] <span class="pl-k">=</span> <span class="pl-c1">0.78</span><span class="pl-k">*</span>u[j<span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">+</span> <span class="pl-c1">randn</span>()
<span class="pl-k">end</span>
u <span class="pl-k">=</span> u[n<span class="pl-k">+</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">*</span>n]    
y <span class="pl-k">=</span> <span class="pl-c1">0.1</span> <span class="pl-k">+</span> x<span class="pl-k">*</span>[<span class="pl-c1">0.2</span>, <span class="pl-c1">0.3</span>, <span class="pl-c1">0.0</span>, <span class="pl-c1">0.0</span>, <span class="pl-c1">0.5</span>] <span class="pl-k">+</span> u            

df <span class="pl-k">=</span> <span class="pl-c1">DataFrame</span>()
df[<span class="pl-c1">:y</span>] <span class="pl-k">=</span> y
<span class="pl-k">for</span> j <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>([<span class="pl-c1">:x1</span>, <span class="pl-c1">:x2</span>, <span class="pl-c1">:x3</span>, <span class="pl-c1">:x4</span>, <span class="pl-c1">:x5</span>])
    df[j[<span class="pl-c1">2</span>]] <span class="pl-k">=</span> x[:,j[<span class="pl-c1">1</span>]]
<span class="pl-k">end</span></pre></div>
<p>Using the data in <code>df</code>, the coefficient of the regression can be estimated using <code>GLM</code></p>
<div class="highlight highlight-source-julia"><pre>lm1 <span class="pl-k">=</span> <span class="pl-c1">glm</span>(y<span class="pl-k">~</span>x1<span class="pl-k">+</span>x2<span class="pl-k">+</span>x3<span class="pl-k">+</span>x4<span class="pl-k">+</span>x5, df, <span class="pl-c1">Normal</span>(), <span class="pl-c1">IdentityLink</span>())</pre></div>
<p>To get a consistent estimate of the long run variance of the estimated coefficients using a Quadratic Spectral kernel with automatic bandwidth selection  <em>à la</em> Andrews</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">vcov</span>(lm1, <span class="pl-c1">QuadraticSpectralKernel</span>(Andrews), prewhite <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<p>If one wants to estimate the long-time variance using the same kernel, but with a bandwidth selected <em>à la</em> Newey-West</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">vcov</span>(lm1, <span class="pl-c1">QuadraticSpectralKernel</span>(NeweyWest), prewhite <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<p>The standard errors can be obtained by the <code>stderror</code> method</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">stderror</span>(<span class="pl-k">::</span><span class="pl-c1">DataFrameRegressionModel</span>, <span class="pl-k">::</span><span class="pl-c1">HAC</span>; prewhite<span class="pl-k">::</span><span class="pl-c1">Bool</span>)</pre></div>
<p>Sometime is useful to access the bandwidth selected by the automatic procedures. This can be done using the <code>optimalbw</code> method</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">optimalbw</span>(NeweyWest, QuadraticSpectralKernel, lm1; prewhite <span class="pl-k">=</span> <span class="pl-c1">false</span>)
<span class="pl-c1">optimalbw</span>(Andrews, QuadraticSpectralKernel, lm1; prewhite <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<h3><a id="user-content-long-run-variance-of-the-average-of-the-process" class="anchor" aria-hidden="true" href="#long-run-variance-of-the-average-of-the-process"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Long run variance of the average of the process</h3>
<p>Sometime interest lies in estimating the long-run variance of the average of the process. At the moment this can be done by carrying out a regression on a constant (the sample mean of the realization of the process) and using <code>vcov</code> or <code>stderror</code> to obtain a consistent variance estimate (or its diagonal elements).</p>
<div class="highlight highlight-source-julia"><pre>lm2 <span class="pl-k">=</span> <span class="pl-c1">glm</span>(u<span class="pl-k">~</span><span class="pl-c1">1</span>, df, <span class="pl-c1">Normal</span>(), <span class="pl-c1">IdentityLink</span>())
<span class="pl-c1">vcov</span>(lm1, <span class="pl-c1">ParzenKernel</span>(NeweyWest), prewhite <span class="pl-k">=</span> <span class="pl-c1">false</span>)
<span class="pl-c1">stderr</span>(lm1, <span class="pl-c1">ParzenKernel</span>(NeweyWest), prewhite <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<h2><a id="user-content-hc-heteroskedastic-consistent" class="anchor" aria-hidden="true" href="#hc-heteroskedastic-consistent"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>HC (Heteroskedastic consistent)</h2>
<p>As in the HAC case, <code>vcov</code> and <code>stderr</code> are the main functions. They know get as argument the type of robust variance being sought</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">vcov</span>(<span class="pl-k">::</span><span class="pl-c1">DataFrameRegressionModel</span>, <span class="pl-k">::</span><span class="pl-c1">HC</span>)</pre></div>
<p>Where HC is an abstract type with the following concrete types:</p>
<ul>
<li><code>HC0</code></li>
<li><code>HC1</code> (this is <code>HC0</code> with the degree of freedom adjustment)</li>
<li><code>HC2</code></li>
<li><code>HC3</code></li>
<li><code>HC4</code></li>
<li><code>HC4m</code></li>
<li><code>HC5</code></li>
</ul>
<pre><code>using CovarianceMatrices
using DataFrames
using GLM

# A Gamma example, from McCullagh &amp; Nelder (1989, pp. 300-2)
# The weights are added just to test the interface and are not part
# of the original data
clotting = DataFrame(
    u    = log([5,10,15,20,30,40,60,80,100]),
    lot1 = [118,58,42,35,27,25,21,19,18],
    lot2 = [69,35,26,21,18,16,13,12,12],
    w    = 9*[1/8, 1/9, 1/25, 1/6, 1/14, 1/25, 1/15, 1/13, 0.3022039]
)
wOLS = fit(GeneralizedLinearModel, lot1~u, clotting, Normal(), wts = array(clotting[:w]))

vcov(wOLS, HC0
vcov(wOLS, HC1)
vcov(wOLS, HC2)
vcov(wOLS, HC3)
vcov(wOLS, HC4)
vcov(wOLS, HC4m)
vcov(wOLS, HC5)
</code></pre>
<h2><a id="user-content-crhc-cluster-robust-heteroskedasticty-consistent" class="anchor" aria-hidden="true" href="#crhc-cluster-robust-heteroskedasticty-consistent"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CRHC (Cluster robust heteroskedasticty consistent)</h2>
<p>The API of this class of variance estimators is subject to change, so please use with care. The difficulty is that <code>CRHC</code> type needs to have access to the variable along which dimension the clustering mast take place. For the moment, the following approach works --- as long as no missing values are present in the original dataframe.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> RDatasets
df <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>plm<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Grunfeld<span class="pl-pds">"</span></span>)
lm <span class="pl-k">=</span> <span class="pl-c1">glm</span>(Inv<span class="pl-k">~</span>Value<span class="pl-k">+</span>Capital, df, <span class="pl-c1">Normal</span>(), <span class="pl-c1">IdentityLink</span>())
<span class="pl-c1">vcov</span>(lm, <span class="pl-c1">CRHC1</span>(<span class="pl-c1">convert</span>(Array, df[<span class="pl-c1">:Firm</span>])))
<span class="pl-c1">stderr</span>(lm, <span class="pl-c1">CRHC1</span>(<span class="pl-c1">convert</span>(Array, df[<span class="pl-c1">:Firm</span>])))</pre></div>
</article></div>