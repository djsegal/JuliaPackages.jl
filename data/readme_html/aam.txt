<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0f6b6ac93f91d45e0de6119296a03b780dd4305333058131fe6683541431ddb4/68747470733a2f2f7472617669732d63692e6f72672f646664782f416374697665417070656172616e63654d6f64656c732e6a6c2e737667"><img src="https://camo.githubusercontent.com/0f6b6ac93f91d45e0de6119296a03b780dd4305333058131fe6683541431ddb4/68747470733a2f2f7472617669732d63692e6f72672f646664782f416374697665417070656172616e63654d6f64656c732e6a6c2e737667" alt="travis" data-canonical-src="https://travis-ci.org/dfdx/ActiveAppearanceModels.jl.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-active-appearance-models" class="anchor" aria-hidden="true" href="#active-appearance-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Active Appearance Models</h1>
<p dir="auto">Port of Luca Vezzaro's <a href="http://www.mathworks.com/matlabcentral/fileexchange/32704-icaam-inverse-compositional-active-appearance-models" rel="nofollow">ICAAM</a>.</p>
<h2 dir="auto"><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto">Active appearance models provide a way to find a set of related points on an image. AAMs are based on 2 main concepts: shape and appearance.</p>
<p dir="auto"><strong>Shape</strong> consists of a fixed number of points (so-called landmarks) that describe configuration of some object on an image. For example, here's a shape describing some human's face:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_shape.png"><img src="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_shape.png" alt="Shape" style="max-width: 100%;"></a></p>
<p dir="auto"><strong>Appearance</strong> is made of all pixels on the image inside the shape. E.g. appearance, corresponding to the shape above looks like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_app.png"><img src="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_app.png" alt="Appearance" style="max-width: 100%;"></a></p>
<p dir="auto">Active appearance models are first trained on a bunch of <code>(image, shape)</code> pairs	and then, given a new image and initial guess for a shape, are fitted to this image to find exact location of landmarks. Let's take a concrete example.</p>
<p dir="auto">First, we need some data to train a model on. <code>FaceDatasets</code> package contains a simple dataset from original research by Tim Cootes et al. that fits our needs:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using FaceDatasets
imgs = load_images(CootesDataset)
shapes = load_shapes(CootesDataset)"><pre class="notranslate"><code>using FaceDatasets
imgs = load_images(CootesDataset)
shapes = load_shapes(CootesDataset)
</code></pre></div>
<p dir="auto">We will use simple leave-one-one cross-validation to see how training and testing works:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="tst = 6                                      # index of a test image
all_but_tst = [1:tst-1, tst+1:length(imgs)]  # all other indexes"><pre class="notranslate"><code>tst = 6                                      # index of a test image
all_but_tst = [1:tst-1, tst+1:length(imgs)]  # all other indexes
</code></pre></div>
<p dir="auto"><strong>Training</strong> is simple:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using ActiveAppearanceModels
aam = AAModel()
train(aam, imgs[all_but_tst], shapes[all_but_tst])"><pre class="notranslate"><code>using ActiveAppearanceModels
aam = AAModel()
train(aam, imgs[all_but_tst], shapes[all_but_tst])
</code></pre></div>
<p dir="auto">Fitting model to a new image requires 2 more parameters: initial shape and number of iterations:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="init_shape = shapes[3]
n_iter = 20"><pre class="notranslate"><code>init_shape = shapes[3]
n_iter = 20
</code></pre></div>
<p dir="auto">Before fitting let's see initial landmark position:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="viewshape(imgs[tst], init_shape)    "><pre class="notranslate"><code>viewshape(imgs[tst], init_shape)    
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_init_shape.png"><img src="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_init_shape.png" alt="Init Shape" style="max-width: 100%;"></a></p>
<p dir="auto"><strong>Fitting</strong> itself is straightforward:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="fitted_shape, fitted_app = fit(aam, imgs[tst], init_shape, n_iter)"><pre class="notranslate"><code>fitted_shape, fitted_app = fit(aam, imgs[tst], init_shape, n_iter)
</code></pre></div>
<p dir="auto"><code>fitted_shape</code> is what AAM believes is true position of landmarks, and <code>fitted_app</code> is corresponding appearance. Here's they are:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using ImageView
viewshape(imgs[tst], fitted_shape)
view(fitted_app)"><pre class="notranslate"><code>using ImageView
viewshape(imgs[tst], fitted_shape)
view(fitted_app)
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_fitted_shape.png"><img src="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_fitted_shape.png" alt="Fitted Shape" style="max-width: 100%;"></a></p>
<p align="center" dir="auto">
   <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_fitted_app.png"><img src="https://raw.githubusercontent.com/dfdx/ActiveAppearanceModels.jl/master/docs/data/readme_fitted_app.png" alt="Fitted App" style="max-width: 100%;"></a>
</p>
<p dir="auto">For interactive example of using AAMs see <a href="https://github.com/dfdx/ActiveAppearanceModels.jl/blob/master/examples/multi.jl"><code>multu.jl</code></a>.</p>
<h2 dir="auto"><a id="user-content-when-fitting-diverges" class="anchor" aria-hidden="true" href="#when-fitting-diverges"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>When fitting diverges</h2>
<p dir="auto">Active appearance models use a variant of Lucas-Kanade algorithm and thus expect relatively small difference between initial and target shape. If difference is too large, fitting process will diverge (most often ending with <code>BoundsError</code>). This is easy to overcome, though, by repeating fitting with several variants of init shape.</p>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<p dir="auto">This package closely follows original code in <a href="http://www.mathworks.com/matlabcentral/fileexchange/32704-icaam-inverse-compositional-active-appearance-models" rel="nofollow">ICAAM</a> project. ICAAM, in its turn, implements inverse compositional approach to AAMs first described in:</p>
<blockquote>
<p dir="auto">Matthews, I., Baker, S. Active appearance models revisited. International Journal of Computer Vision 60 (2004) 135 – 164</p>
</blockquote>
</article></div>