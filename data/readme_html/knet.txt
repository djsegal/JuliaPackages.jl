<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-knet" class="anchor" aria-hidden="true" href="#knet"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Knet</h1>
<p dir="auto"><a href="https://denizyuret.github.io/Knet.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width: 100%;"></a>
<a href="https://travis-ci.org/denizyuret/Knet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ab4f6035cb414673e326cb8a9655d9e7587e8104b4adb8e54497fbc0c7da90fb/68747470733a2f2f7472617669732d63692e6f72672f64656e697a79757265742f4b6e65742e6a6c2e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://travis-ci.org/denizyuret/Knet.jl.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://coveralls.io/github/denizyuret/Knet.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/cf3a2f7e0d51f7157f7ad3ef1558aa3da70d6c2ac181f0b0b2df6be6334e71b6/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f64656e697a79757265742f4b6e65742e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://coveralls.io/repos/github/denizyuret/Knet.jl/badge.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/denizyuret/Knet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e0528e7660d6d8337092cc4dd592092778cdef27f6044dd27619f08062297f48/68747470733a2f2f636f6465636f762e696f2f67682f64656e697a79757265742f4b6e65742e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/denizyuret/Knet.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><a href="https://denizyuret.github.io/Knet.jl/latest" rel="nofollow">Knet</a> (pronounced "kay-net") is the <a href="http://www.ku.edu.tr/en" rel="nofollow">Ko√ß
University</a> deep learning framework implemented in
<a href="http://docs.julialang.org" rel="nofollow">Julia</a> by <a href="http://www.denizyuret.com" rel="nofollow">Deniz Yuret</a> and
collaborators.  It supports GPU operation and automatic differentiation using dynamic
computational graphs for models defined in plain Julia. You can install Knet with the
following at the julia prompt: <code>using Pkg; Pkg.add("Knet")</code>. Some starting points:</p>
<ul dir="auto">
<li><a href="tutorial">Tutorial:</a>
introduces Julia and Knet via examples.</li>
<li><a href="https://denizyuret.github.io/Knet.jl/latest" rel="nofollow">Documentation:</a>
installation, introduction, design, implementation, full reference and deep learning chapters.</li>
<li><a href="examples">Examples:</a>
more tutorials and example models.</li>
<li><a href="http://denizyuret.github.io/Knet.jl/latest/tutorial/#Benchmarks-1" rel="nofollow">Benchmarks:</a>
comparison of Knet's speed with TensorFlow, PyTorch, DyNet etc.</li>
<li><a href="https://goo.gl/zeUBFr" rel="nofollow">Paper:</a>
Yuret, D. "Knet: beginning deep learning with 100 lines of julia." In <em>Machine Learning Systems Workshop</em> at NIPS 2016.</li>
<li><a href="https://github.com/KnetML">KnetML:</a>
github organization with Knet repos of models, tutorials, layer collections and other resources.</li>
<li><a href="http://denizyuret.github.io/Knet.jl/latest/install/#Using-Amazon-AWS-1" rel="nofollow">Images:</a>
Knet machine images are available for <a href="http://denizyuret.github.io/Knet.jl/latest/install/#Using-Amazon-AWS-1" rel="nofollow">AWS</a>, <a href="https://github.com/KnetML/singularity-images">Singularity</a> and <a href="https://github.com/JuliaGPU/docker">Docker</a>.</li>
<li><a href="https://github.com/denizyuret/Knet.jl/issues">Issues:</a>
if you find a bug, please open a github issue.</li>
<li><a href="https://groups.google.com/forum/#!forum/knet-users" rel="nofollow">knet-users:</a>
if you need help or would like to request a feature, please join this mailing list.</li>
<li><a href="https://groups.google.com/forum/#!forum/knet-dev" rel="nofollow">knet-dev:</a>
if you would like to contribute to Knet development, please join this mailing list and check out these <a href="https://denizyuret.github.io/Knet.jl/latest/install/#Tips-for-developers-1" rel="nofollow">tips</a>.</li>
<li><a href="https://julialang.slack.com/messages/CDLKQ92P3/details" rel="nofollow">knet-slack:</a> Slack channel for Knet.</li>
<li>Related work: Please check out <a href="https://github.com/FLuxML">Flux</a>, <a href="https://github.com/pluskid/Mocha.jl">Mocha</a>, <a href="https://github.com/JuliaML">JuliaML</a>, <a href="https://github.com/JuliaDiff">JuliaDiff</a>, <a href="https://github.com/JuliaGPU">JuliaGPU</a>, <a href="https://github.com/JuliaOpt">JuliaOpt</a> for related packages.</li>
</ul>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<p dir="auto">Here is a simple example where we define, train and test the
<a href="http://yann.lecun.com/exdb/lenet" rel="nofollow">LeNet</a> model for the
<a href="http://yann.lecun.com/exdb/mnist" rel="nofollow">MNIST</a> handwritten digit recognition dataset from scratch
using 15 lines of code and 10 seconds of GPU computation.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Install packages before first run: using Pkg; pkg&quot;add Knet IterTools MLDatasets&quot;
using Knet, IterTools, MLDatasets

# Define convolutional layer:
struct Conv; w; b; end
Conv(w1,w2,nx,ny) = Conv(param(w1,w2,nx,ny), param0(1,1,ny,1))
(c::Conv)(x) = relu.(pool(conv4(c.w, x) .+ c.b))

# Define dense layer:
struct Dense; w; b; f; end
Dense(i,o; f=identity) = Dense(param(o,i), param0(o), f)
(d::Dense)(x) = d.f.(d.w * mat(x) .+ d.b)

# Define a chain of layers and a loss function:
struct Chain; layers; end
(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)
(c::Chain)(x,y) = nll(c(x),y)

# Load MNIST data:
xtrn,ytrn = MNIST.traindata(Float32); ytrn[ytrn.==0] .= 10
xtst,ytst = MNIST.testdata(Float32);  ytst[ytst.==0] .= 10
dtrn = minibatch(xtrn, ytrn, 100; xsize = (28,28,1,:))
dtst = minibatch(xtst, ytst, 100; xsize = (28,28,1,:))

# Define and train LeNet (~10 secs on a GPU or ~3 mins on a CPU to reach ~99% accuracy)
LeNet = Chain((Conv(5,5,1,20), Conv(5,5,20,50), Dense(800,500,f=relu), Dense(500,10)))
progress!(adam(LeNet, ncycle(dtrn,3)))
accuracy(LeNet,data=dtst)"><pre><span class="pl-c"><span class="pl-c">#</span> Install packages before first run: using Pkg; pkg"add Knet IterTools MLDatasets"</span>
<span class="pl-k">using</span> Knet, IterTools, MLDatasets

<span class="pl-c"><span class="pl-c">#</span> Define convolutional layer:</span>
<span class="pl-k">struct</span> Conv; w; b; <span class="pl-k">end</span>
<span class="pl-en">Conv</span>(w1,w2,nx,ny) <span class="pl-k">=</span> <span class="pl-c1">Conv</span>(<span class="pl-c1">param</span>(w1,w2,nx,ny), <span class="pl-c1">param0</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>,ny,<span class="pl-c1">1</span>))
(c<span class="pl-k">::</span><span class="pl-c1">Conv</span>)(x) <span class="pl-k">=</span> <span class="pl-c1">relu</span>.(<span class="pl-c1">pool</span>(<span class="pl-c1">conv4</span>(c<span class="pl-k">.</span>w, x) <span class="pl-k">.+</span> c<span class="pl-k">.</span>b))

<span class="pl-c"><span class="pl-c">#</span> Define dense layer:</span>
<span class="pl-k">struct</span> Dense; w; b; f; <span class="pl-k">end</span>
<span class="pl-en">Dense</span>(i,o; f<span class="pl-k">=</span>identity) <span class="pl-k">=</span> <span class="pl-c1">Dense</span>(<span class="pl-c1">param</span>(o,i), <span class="pl-c1">param0</span>(o), f)
(d<span class="pl-k">::</span><span class="pl-c1">Dense</span>)(x) <span class="pl-k">=</span> d<span class="pl-k">.</span><span class="pl-c1">f</span>.(d<span class="pl-k">.</span>w <span class="pl-k">*</span> <span class="pl-c1">mat</span>(x) <span class="pl-k">.+</span> d<span class="pl-k">.</span>b)

<span class="pl-c"><span class="pl-c">#</span> Define a chain of layers and a loss function:</span>
<span class="pl-k">struct</span> Chain; layers; <span class="pl-k">end</span>
(c<span class="pl-k">::</span><span class="pl-c1">Chain</span>)(x) <span class="pl-k">=</span> (<span class="pl-k">for</span> l <span class="pl-k">in</span> c<span class="pl-k">.</span>layers; x <span class="pl-k">=</span> <span class="pl-c1">l</span>(x); <span class="pl-k">end</span>; x)
(c<span class="pl-k">::</span><span class="pl-c1">Chain</span>)(x,y) <span class="pl-k">=</span> <span class="pl-c1">nll</span>(<span class="pl-c1">c</span>(x),y)

<span class="pl-c"><span class="pl-c">#</span> Load MNIST data:</span>
xtrn,ytrn <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>(Float32); ytrn[ytrn<span class="pl-k">.==</span><span class="pl-c1">0</span>] <span class="pl-k">.=</span> <span class="pl-c1">10</span>
xtst,ytst <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">testdata</span>(Float32);  ytst[ytst<span class="pl-k">.==</span><span class="pl-c1">0</span>] <span class="pl-k">.=</span> <span class="pl-c1">10</span>
dtrn <span class="pl-k">=</span> <span class="pl-c1">minibatch</span>(xtrn, ytrn, <span class="pl-c1">100</span>; xsize <span class="pl-k">=</span> (<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,:))
dtst <span class="pl-k">=</span> <span class="pl-c1">minibatch</span>(xtst, ytst, <span class="pl-c1">100</span>; xsize <span class="pl-k">=</span> (<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,:))

<span class="pl-c"><span class="pl-c">#</span> Define and train LeNet (~10 secs on a GPU or ~3 mins on a CPU to reach ~99% accuracy)</span>
LeNet <span class="pl-k">=</span> <span class="pl-c1">Chain</span>((<span class="pl-c1">Conv</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">1</span>,<span class="pl-c1">20</span>), <span class="pl-c1">Conv</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">20</span>,<span class="pl-c1">50</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">800</span>,<span class="pl-c1">500</span>,f<span class="pl-k">=</span>relu), <span class="pl-c1">Dense</span>(<span class="pl-c1">500</span>,<span class="pl-c1">10</span>)))
<span class="pl-c1">progress!</span>(<span class="pl-c1">adam</span>(LeNet, <span class="pl-c1">ncycle</span>(dtrn,<span class="pl-c1">3</span>)))
<span class="pl-c1">accuracy</span>(LeNet,data<span class="pl-k">=</span>dtst)</pre></div>
<h2 dir="auto"><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">Knet is an open-source project and we are always open to new contributions: bug reports and
fixes, feature requests and contributions, new machine learning models and operators,
inspiring examples, benchmarking results are all welcome. See <a href="https://denizyuret.github.io/Knet.jl/latest/install/#Tips-for-developers" rel="nofollow">Tips for Developers</a> for instructions.</p>
<p dir="auto">Contributors: Can G√ºmeli, Carlo Lucibello, Ege Onat, Ekin Aky√ºrek, Ekrem Emre Yurdakul, Emre √únal, Emre Yolcu, Enis Berk, Erenay Dayanƒ±k, ƒ∞lker Kesen, Kai Xu, Meri√ß Melike Softa, Mike Innes, Onur Kuru, Ozan Arkan Can, √ñmer Kƒ±rnap, Phuoc Nguyen, Rene Donner, Tim Besard, Zhang Shiwei.</p>
</article></div>