<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-knet" class="anchor" aria-hidden="true" href="#knet"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Knet</h1>
<p dir="auto"><a href="https://denizyuret.github.io/Knet.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width: 100%;"></a>
<a href="https://travis-ci.org/denizyuret/Knet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ab4f6035cb414673e326cb8a9655d9e7587e8104b4adb8e54497fbc0c7da90fb/68747470733a2f2f7472617669732d63692e6f72672f64656e697a79757265742f4b6e65742e6a6c2e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://travis-ci.org/denizyuret/Knet.jl.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://coveralls.io/github/denizyuret/Knet.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/cf3a2f7e0d51f7157f7ad3ef1558aa3da70d6c2ac181f0b0b2df6be6334e71b6/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f64656e697a79757265742f4b6e65742e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://coveralls.io/repos/github/denizyuret/Knet.jl/badge.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/denizyuret/Knet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e0528e7660d6d8337092cc4dd592092778cdef27f6044dd27619f08062297f48/68747470733a2f2f636f6465636f762e696f2f67682f64656e697a79757265742f4b6e65742e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/denizyuret/Knet.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><a href="https://denizyuret.github.io/Knet.jl/latest" rel="nofollow">Knet</a> (pronounced "kay-net") is the <a href="http://www.ku.edu.tr/en" rel="nofollow">Koç
University</a> deep learning framework implemented in
<a href="http://docs.julialang.org" rel="nofollow">Julia</a> by <a href="http://www.denizyuret.com" rel="nofollow">Deniz Yuret</a> and
collaborators.  It supports GPU operation and automatic differentiation using dynamic
computational graphs for models defined in plain Julia. You can install Knet with the
following at the julia prompt: <code>using Pkg; Pkg.add("Knet")</code>. Some starting points:</p>
<ul dir="auto">
<li><a href="tutorial">Tutorial:</a>
introduces Julia and Knet via examples.</li>
<li><a href="https://denizyuret.github.io/Knet.jl/latest" rel="nofollow">Documentation:</a>
installation, introduction, design, implementation, full reference and deep learning chapters.</li>
<li><a href="examples">Examples:</a>
more tutorials and example models.</li>
<li><a href="http://denizyuret.github.io/Knet.jl/latest/tutorial/#Benchmarks-1" rel="nofollow">Benchmarks:</a>
comparison of Knet's speed with TensorFlow, PyTorch, DyNet etc.</li>
<li><a href="https://goo.gl/zeUBFr" rel="nofollow">Paper:</a>
Yuret, D. "Knet: beginning deep learning with 100 lines of julia." In <em>Machine Learning Systems Workshop</em> at NIPS 2016.</li>
<li><a href="https://github.com/KnetML">KnetML:</a>
github organization with Knet repos of models, tutorials, layer collections and other resources.</li>
<li><a href="http://denizyuret.github.io/Knet.jl/latest/install/#Using-Amazon-AWS-1" rel="nofollow">Images:</a>
Knet machine images are available for <a href="http://denizyuret.github.io/Knet.jl/latest/install/#Using-Amazon-AWS-1" rel="nofollow">AWS</a>, <a href="https://github.com/KnetML/singularity-images">Singularity</a> and <a href="https://github.com/JuliaGPU/docker">Docker</a>.</li>
<li><a href="https://github.com/denizyuret/Knet.jl/issues">Issues:</a>
if you find a bug, please open a github issue.</li>
<li><a href="https://groups.google.com/forum/#!forum/knet-users" rel="nofollow">knet-users:</a>
if you need help or would like to request a feature, please join this mailing list.</li>
<li><a href="https://groups.google.com/forum/#!forum/knet-dev" rel="nofollow">knet-dev:</a>
if you would like to contribute to Knet development, please join this mailing list and check out these <a href="https://denizyuret.github.io/Knet.jl/latest/install/#Tips-for-developers-1" rel="nofollow">tips</a>.</li>
<li><a href="https://julialang.slack.com/messages/CDLKQ92P3/details" rel="nofollow">knet-slack:</a> Slack channel for Knet.</li>
<li>Related work: Please check out <a href="https://github.com/FLuxML">Flux</a>, <a href="https://github.com/pluskid/Mocha.jl">Mocha</a>, <a href="https://github.com/JuliaML">JuliaML</a>, <a href="https://github.com/JuliaDiff">JuliaDiff</a>, <a href="https://github.com/JuliaGPU">JuliaGPU</a>, <a href="https://github.com/JuliaOpt">JuliaOpt</a> for related packages.</li>
</ul>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<p dir="auto">Here is a simple example where we define, train and test the
<a href="http://yann.lecun.com/exdb/lenet" rel="nofollow">LeNet</a> model for the
<a href="http://yann.lecun.com/exdb/mnist" rel="nofollow">MNIST</a> handwritten digit recognition dataset from scratch
using 15 lines of code and 10 seconds of GPU computation.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Install packages before first run: using Pkg; pkg&quot;add Knet IterTools MLDatasets&quot;
using Knet, IterTools, MLDatasets

# Define convolutional layer:
struct Conv; w; b; end
Conv(w1,w2,nx,ny) = Conv(param(w1,w2,nx,ny), param0(1,1,ny,1))
(c::Conv)(x) = relu.(pool(conv4(c.w, x) .+ c.b))

# Define dense layer:
struct Dense; w; b; f; end
Dense(i,o; f=identity) = Dense(param(o,i), param0(o), f)
(d::Dense)(x) = d.f.(d.w * mat(x) .+ d.b)

# Define a chain of layers and a loss function:
struct Chain; layers; end
(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)
(c::Chain)(x,y) = nll(c(x),y)

# Load MNIST data:
xtrn,ytrn = MNIST.traindata(Float32); ytrn[ytrn.==0] .= 10
xtst,ytst = MNIST.testdata(Float32);  ytst[ytst.==0] .= 10
dtrn = minibatch(xtrn, ytrn, 100; xsize = (28,28,1,:))
dtst = minibatch(xtst, ytst, 100; xsize = (28,28,1,:))

# Define and train LeNet (~10 secs on a GPU or ~3 mins on a CPU to reach ~99% accuracy)
LeNet = Chain((Conv(5,5,1,20), Conv(5,5,20,50), Dense(800,500,f=relu), Dense(500,10)))
progress!(adam(LeNet, ncycle(dtrn,3)))
accuracy(LeNet,data=dtst)"><pre><span class="pl-c"><span class="pl-c">#</span> Install packages before first run: using Pkg; pkg"add Knet IterTools MLDatasets"</span>
<span class="pl-k">using</span> Knet, IterTools, MLDatasets

<span class="pl-c"><span class="pl-c">#</span> Define convolutional layer:</span>
<span class="pl-k">struct</span> Conv; w; b; <span class="pl-k">end</span>
<span class="pl-en">Conv</span>(w1,w2,nx,ny) <span class="pl-k">=</span> <span class="pl-c1">Conv</span>(<span class="pl-c1">param</span>(w1,w2,nx,ny), <span class="pl-c1">param0</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>,ny,<span class="pl-c1">1</span>))
(c<span class="pl-k">::</span><span class="pl-c1">Conv</span>)(x) <span class="pl-k">=</span> <span class="pl-c1">relu</span>.(<span class="pl-c1">pool</span>(<span class="pl-c1">conv4</span>(c<span class="pl-k">.</span>w, x) <span class="pl-k">.+</span> c<span class="pl-k">.</span>b))

<span class="pl-c"><span class="pl-c">#</span> Define dense layer:</span>
<span class="pl-k">struct</span> Dense; w; b; f; <span class="pl-k">end</span>
<span class="pl-en">Dense</span>(i,o; f<span class="pl-k">=</span>identity) <span class="pl-k">=</span> <span class="pl-c1">Dense</span>(<span class="pl-c1">param</span>(o,i), <span class="pl-c1">param0</span>(o), f)
(d<span class="pl-k">::</span><span class="pl-c1">Dense</span>)(x) <span class="pl-k">=</span> d<span class="pl-k">.</span><span class="pl-c1">f</span>.(d<span class="pl-k">.</span>w <span class="pl-k">*</span> <span class="pl-c1">mat</span>(x) <span class="pl-k">.+</span> d<span class="pl-k">.</span>b)

<span class="pl-c"><span class="pl-c">#</span> Define a chain of layers and a loss function:</span>
<span class="pl-k">struct</span> Chain; layers; <span class="pl-k">end</span>
(c<span class="pl-k">::</span><span class="pl-c1">Chain</span>)(x) <span class="pl-k">=</span> (<span class="pl-k">for</span> l <span class="pl-k">in</span> c<span class="pl-k">.</span>layers; x <span class="pl-k">=</span> <span class="pl-c1">l</span>(x); <span class="pl-k">end</span>; x)
(c<span class="pl-k">::</span><span class="pl-c1">Chain</span>)(x,y) <span class="pl-k">=</span> <span class="pl-c1">nll</span>(<span class="pl-c1">c</span>(x),y)

<span class="pl-c"><span class="pl-c">#</span> Load MNIST data:</span>
xtrn,ytrn <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>(Float32); ytrn[ytrn<span class="pl-k">.==</span><span class="pl-c1">0</span>] <span class="pl-k">.=</span> <span class="pl-c1">10</span>
xtst,ytst <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">testdata</span>(Float32);  ytst[ytst<span class="pl-k">.==</span><span class="pl-c1">0</span>] <span class="pl-k">.=</span> <span class="pl-c1">10</span>
dtrn <span class="pl-k">=</span> <span class="pl-c1">minibatch</span>(xtrn, ytrn, <span class="pl-c1">100</span>; xsize <span class="pl-k">=</span> (<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,:))
dtst <span class="pl-k">=</span> <span class="pl-c1">minibatch</span>(xtst, ytst, <span class="pl-c1">100</span>; xsize <span class="pl-k">=</span> (<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,:))

<span class="pl-c"><span class="pl-c">#</span> Define and train LeNet (~10 secs on a GPU or ~3 mins on a CPU to reach ~99% accuracy)</span>
LeNet <span class="pl-k">=</span> <span class="pl-c1">Chain</span>((<span class="pl-c1">Conv</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">1</span>,<span class="pl-c1">20</span>), <span class="pl-c1">Conv</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">20</span>,<span class="pl-c1">50</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">800</span>,<span class="pl-c1">500</span>,f<span class="pl-k">=</span>relu), <span class="pl-c1">Dense</span>(<span class="pl-c1">500</span>,<span class="pl-c1">10</span>)))
<span class="pl-c1">progress!</span>(<span class="pl-c1">adam</span>(LeNet, <span class="pl-c1">ncycle</span>(dtrn,<span class="pl-c1">3</span>)))
<span class="pl-c1">accuracy</span>(LeNet,data<span class="pl-k">=</span>dtst)</pre></div>
<h2 dir="auto"><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">Knet is an open-source project and we are always open to new contributions: bug reports and
fixes, feature requests and contributions, new machine learning models and operators,
inspiring examples, benchmarking results are all welcome. See <a href="https://denizyuret.github.io/Knet.jl/latest/install/#Tips-for-developers" rel="nofollow">Tips for Developers</a> for instructions.</p>
<p dir="auto">Contributors: Can Gümeli, Carlo Lucibello, Ege Onat, Ekin Akyürek, Ekrem Emre Yurdakul, Emre Ünal, Emre Yolcu, Enis Berk, Erenay Dayanık, İlker Kesen, Kai Xu, Meriç Melike Softa, Mike Innes, Onur Kuru, Ozan Arkan Can, Ömer Kırnap, Phuoc Nguyen, Rene Donner, Tim Besard, Zhang Shiwei.</p>
</article></div>