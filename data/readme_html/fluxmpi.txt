<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-fluxmpijl" class="anchor" aria-hidden="true" href="#fluxmpijl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FluxMPI.jl</h1>
<p dir="auto"><a href="https://avik-pal.github.io/FluxMPI.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://avik-pal.github.io/FluxMPI.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Latest" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/avik-pal/FluxMPI.jl/actions/workflows/CI.yml"><img src="https://github.com/avik-pal/FluxMPI.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/avik-pal/FluxMPI.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/0844a7dc0331ea763baf633e99b9cce53196434437a7168e6d41a8ca4dda4fee/68747470733a2f2f636f6465636f762e696f2f67682f6176696b2d70616c2f466c75784d50492e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d314c3365506d7179506f" alt="codecov" data-canonical-src="https://codecov.io/gh/avik-pal/FluxMPI.jl/branch/master/graph/badge.svg?token=1L3ePmqyPo" style="max-width: 100%;"></a>
<a href="https://github.com/SciML/ColPrac"><img src="https://camo.githubusercontent.com/a6c1efcb19a957860ecb25966a730260b03d6e05380d0c27992ee7f9e3b1feb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6c507261632d436f6e7472696275746f72277325323047756964652d626c756576696f6c6574" alt="ColPrac: Contributor's Guide on Collaborative Practices for Community Packages" data-canonical-src="https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet" style="max-width: 100%;"></a>
<a href="https://github.com/SciML/SciMLStyle"><img src="https://camo.githubusercontent.com/3e16f03bad047817fbc07f49307817ed7919ef79c339dc75ad4ce813012c3e0b/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d636f64652532307374796c65266d6573736167653d5363694d4c26636f6c6f723d393535386232266c6162656c436f6c6f723d333839383236" alt="SciML Code Style" data-canonical-src="https://img.shields.io/static/v1?label=code%20style&amp;message=SciML&amp;color=9558b2&amp;labelColor=389826" style="max-width: 100%;"></a>
<a href="https://pkgs.genieframework.com?packages=FluxMPI" rel="nofollow"><img src="https://camo.githubusercontent.com/1e715dbfe2b59aae9790e7a7bcfb56b1d22c0e3e9ed5210ea5be498a6c520bad/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f706b67732e67656e69656672616d65776f726b2e636f6d2f6170692f76312f62616467652f466c75784d5049" alt="Package Downloads" data-canonical-src="https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/FluxMPI" style="max-width: 100%;"></a></p>
<p dir="auto">Distributed Data Parallel Training of Neural Networks</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">Stable release:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="] add FluxMPI"><pre>] add FluxMPI</pre></div>
<p dir="auto">Latest development version:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="] add FluxMPI#main"><pre>] add FluxMPI<span class="pl-c"><span class="pl-c">#</span>main</span></pre></div>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using CUDA, FluxMPI, Lux, Optimisers, Random, Zygote

FluxMPI.Init()
CUDA.allowscalar(false)

model = Chain(Dense(1 =&gt; 256, tanh), Dense(256 =&gt; 512, tanh), Dense(512 =&gt; 256, tanh),
              Dense(256 =&gt; 1))
rng = Random.default_rng()
Random.seed!(rng, local_rank())
ps, st = Lux.setup(rng, model) .|&gt; gpu

ps = FluxMPI.synchronize!(ps; root_rank = 0)
st = FluxMPI.synchronize!(st; root_rank = 0)

x = rand(rng, 1, 16) |&gt; gpu
y = x .^ 2

opt = DistributedOptimizer(Adam(0.001f0))
st_opt = Optimisers.setup(opt, ps)

loss(p) = sum(abs2, model(x, p, st)[1] .- y)

st_opt = FluxMPI.synchronize!(st_opt; root_rank = 0)

gs_ = gradient(loss, ps)[1]
Optimisers.update(st_opt, ps, gs_)

t1 = time()

for epoch in 1:100
  global ps, st_opt
  l, back = Zygote.pullback(loss, ps)
  FluxMPI.fluxmpi_println(&quot;Epoch $epoch: Loss $l&quot;)
  gs = back(one(l))[1]
  st_opt, ps = Optimisers.update(st_opt, ps, gs)
end

FluxMPI.fluxmpi_println(time() - t1)"><pre><span class="pl-k">using</span> CUDA, FluxMPI, Lux, Optimisers, Random, Zygote

FluxMPI<span class="pl-k">.</span><span class="pl-c1">Init</span>()
CUDA<span class="pl-k">.</span><span class="pl-c1">allowscalar</span>(<span class="pl-c1">false</span>)

model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">256</span>, tanh), <span class="pl-c1">Dense</span>(<span class="pl-c1">256</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">512</span>, tanh), <span class="pl-c1">Dense</span>(<span class="pl-c1">512</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">256</span>, tanh),
              <span class="pl-c1">Dense</span>(<span class="pl-c1">256</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span>))
rng <span class="pl-k">=</span> Random<span class="pl-k">.</span><span class="pl-c1">default_rng</span>()
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(rng, <span class="pl-c1">local_rank</span>())
ps, st <span class="pl-k">=</span> Lux<span class="pl-k">.</span><span class="pl-c1">setup</span>(rng, model) <span class="pl-k">.|</span><span class="pl-k">&gt;</span> gpu

ps <span class="pl-k">=</span> FluxMPI<span class="pl-k">.</span><span class="pl-c1">synchronize!</span>(ps; root_rank <span class="pl-k">=</span> <span class="pl-c1">0</span>)
st <span class="pl-k">=</span> FluxMPI<span class="pl-k">.</span><span class="pl-c1">synchronize!</span>(st; root_rank <span class="pl-k">=</span> <span class="pl-c1">0</span>)

x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, <span class="pl-c1">1</span>, <span class="pl-c1">16</span>) <span class="pl-k">|&gt;</span> gpu
y <span class="pl-k">=</span> x <span class="pl-k">.^</span> <span class="pl-c1">2</span>

opt <span class="pl-k">=</span> <span class="pl-c1">DistributedOptimizer</span>(<span class="pl-c1">Adam</span>(<span class="pl-c1">0.001f0</span>))
st_opt <span class="pl-k">=</span> Optimisers<span class="pl-k">.</span><span class="pl-c1">setup</span>(opt, ps)

<span class="pl-en">loss</span>(p) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(abs2, <span class="pl-c1">model</span>(x, p, st)[<span class="pl-c1">1</span>] <span class="pl-k">.-</span> y)

st_opt <span class="pl-k">=</span> FluxMPI<span class="pl-k">.</span><span class="pl-c1">synchronize!</span>(st_opt; root_rank <span class="pl-k">=</span> <span class="pl-c1">0</span>)

gs_ <span class="pl-k">=</span> <span class="pl-c1">gradient</span>(loss, ps)[<span class="pl-c1">1</span>]
Optimisers<span class="pl-k">.</span><span class="pl-c1">update</span>(st_opt, ps, gs_)

t1 <span class="pl-k">=</span> <span class="pl-c1">time</span>()

<span class="pl-k">for</span> epoch <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
  <span class="pl-k">global</span> ps, st_opt
  l, back <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">pullback</span>(loss, ps)
  FluxMPI<span class="pl-k">.</span><span class="pl-c1">fluxmpi_println</span>(<span class="pl-s"><span class="pl-pds">"</span>Epoch <span class="pl-v">$epoch</span>: Loss <span class="pl-v">$l</span><span class="pl-pds">"</span></span>)
  gs <span class="pl-k">=</span> <span class="pl-c1">back</span>(<span class="pl-c1">one</span>(l))[<span class="pl-c1">1</span>]
  st_opt, ps <span class="pl-k">=</span> Optimisers<span class="pl-k">.</span><span class="pl-c1">update</span>(st_opt, ps, gs)
<span class="pl-k">end</span>

FluxMPI<span class="pl-k">.</span><span class="pl-c1">fluxmpi_println</span>(<span class="pl-c1">time</span>() <span class="pl-k">-</span> t1)</pre></div>
<p dir="auto">Run the code using <code>mpiexecjl -n 3 julia --project=. &lt;filename&gt;.jl</code>.</p>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<ul dir="auto">
<li><a href="https://github.com/SciML/FastDEQ.jl">Deep Equilibrium Models</a> -- Deep Implicit Neural
Networks &amp; Infinite Time Neural ODEs</li>
<li><a href="https://github.com/avik-pal/Lux.jl/tree/main/examples/ImageNet">ImageNet Training with Lux.jl</a></li>
</ul>
<h2 dir="auto"><a id="user-content-style-guide" class="anchor" aria-hidden="true" href="#style-guide"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Style Guide</h2>
<p dir="auto">We follow the <a href="http://lux.csail.mit.edu/stable/devdocs/style_guide/" rel="nofollow">Lux Style Guide</a>. All
contributions must adhere to this style guide.</p>
<h2 dir="auto"><a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Changelog</h2>
<h3 dir="auto"><a id="user-content-v07" class="anchor" aria-hidden="true" href="#v07"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.7</h3>
<ul dir="auto">
<li>Dropped support for MPI v0.19.</li>
<li><code>FLUXMPI_DISABLE_CUDAMPI_SUPPORT</code> is no longer used. Instead use
<code>FluxMPI.disable_cudampi_support()</code> to setup a LocalPreferences.toml file.</li>
<li><code>clean_(print/println)</code> functions are now <code>fluxmpi_(print/println)</code>.</li>
</ul>
<details>
<summary><h3 dir="auto"><a id="user-content-v06" class="anchor" aria-hidden="true" href="#v06"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.6</h3></summary>
<ul dir="auto">
<li>Dropped support for <code>LearnBase</code>, aka <code>DataLoaders.jl</code>. <code>DistributedDataContainer</code> is now
the only compatible with <code>MLUtils.jl</code>.</li>
<li><code>DistributedOptimiser</code> name changed to <code>DistributedOptimizer</code>.</li>
</ul>
</details>
<details>
<summary><h3 dir="auto"><a id="user-content-v05" class="anchor" aria-hidden="true" href="#v05"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.5</h3></summary>
<h4 dir="auto"><a id="user-content-v053" class="anchor" aria-hidden="true" href="#v053"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.5.3</h4>
<ul dir="auto">
<li>Introduces a new API for gradient synchronization
<ul dir="auto">
<li>Don't wrap in <code>DistributedOptimiser</code></li>
<li>Instead just add a line <code>allreduce_gradients(gs::NamedTuple)</code></li>
</ul>
</li>
</ul>
<h4 dir="auto"><a id="user-content-v051" class="anchor" aria-hidden="true" href="#v051"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.5.1</h4>
<ul dir="auto">
<li>Internal <code>MPIExtensions</code> functions renamed
<ul dir="auto">
<li><code>Allreduce!</code> --&gt; <code>allreduce!</code></li>
<li><code>Bcast!</code> --&gt; <code>bcast!</code></li>
<li><code>Reduce!</code> --&gt; <code>reduce!</code></li>
</ul>
</li>
<li>CUDA-unaware MPI bug resolved <a class="issue-link js-issue-link" data-error-text="Failed to load title" data-id="1236638124" data-permission-text="Title is private" data-url="https://github.com/LuxDL/Lux.jl/issues/18" data-hovercard-type="issue" data-hovercard-url="/LuxDL/Lux.jl/issues/18/hovercard" href="https://github.com/LuxDL/Lux.jl/issues/18">LuxDL/Lux.jl#18</a></li>
<li>Disable CUDA-aware MPI support from <code>FluxMPI</code> using <code>FLUXMPI_DISABLE_CUDAMPI_SUPPORT=true</code></li>
<li>Temporarily re-added dependencies on <code>MLDataUtils</code> and <code>LearnBase</code> to ensure
<code>DataLoaders.jl</code> still works -- This will be dropped in a future release</li>
</ul>
<h4 dir="auto"><a id="user-content-v050" class="anchor" aria-hidden="true" href="#v050"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.5.0</h4>
<ul dir="auto">
<li><code>DistributedOptimiser</code> no longer averages the gradients. Instead, the values are summed
across the processes. To ensure averaging divide the loss by <code>total_workers()</code></li>
<li><code>rrule</code>s and <code>frule</code>s defined for <code>local_rank()</code> and <code>total_workers</code> -- they can now be
safely used inside loss functions.</li>
</ul>
</details>
<details>
<summary><h3 dir="auto"><a id="user-content-v04" class="anchor" aria-hidden="true" href="#v04"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.4</h3></summary>
<ul dir="auto">
<li><code>fluxmpi_print</code> and <code>fluxmpi_println</code> print the current time even if <code>FluxMPI</code> has not been
initialized.</li>
<li>Calling <code>local_rank</code> or <code>total_workers</code> before <code>FluxMPI.Init</code> doesn't lead to a segfault.
Rather we throw an error.</li>
<li><code>MLDataUtils</code> and <code>LearnBase</code> dependencies have been dropped
(See <a class="issue-link js-issue-link" data-error-text="Failed to load title" data-id="1216274730" data-permission-text="Title is private" data-url="https://github.com/avik-pal/FluxMPI.jl/issues/17" data-hovercard-type="issue" data-hovercard-url="/avik-pal/FluxMPI.jl/issues/17/hovercard" href="https://github.com/avik-pal/FluxMPI.jl/issues/17">#17</a>)</li>
<li><code>Zygote</code> and <code>Flux</code> dependencies have been removed
<ul dir="auto">
<li>No dispatch for <code>FluxMPI.synchronize!</code> is now available for <code>Zygote.Params</code>. Instead
users should be manually broadcasting the function over <code>Zygote.Params</code></li>
</ul>
</li>
</ul>
</details>
<details>
<summary><h3 dir="auto"><a id="user-content-v03" class="anchor" aria-hidden="true" href="#v03"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>v0.3</h3></summary>
<ul dir="auto">
<li><code>broadcast_parameters</code> has been renamed to <code>FluxMPI.synchronize!</code> since it synchronizes
a lot more than trainable parameters now.</li>
<li>DistributedOptimiser is no longer tied with Flux. We can essentially deal with any
training as long as it is compatible with
<a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a></li>
</ul>
</details>
</article></div>