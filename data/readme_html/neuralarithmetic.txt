<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-neural-arithmetic" class="anchor" aria-hidden="true" href="#neural-arithmetic"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Neural Arithmetic</h1>
<p><a href="https://travis-ci.com/nmheim/NeuralArithmetic.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dac5023edd2d60db00a8ba5ecaebf01b1a046fea42d7246a31d7b07cd01159e4/68747470733a2f2f7472617669732d63692e636f6d2f6e6d6865696d2f4e657572616c41726974686d657469632e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/nmheim/NeuralArithmetic.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/nmheim/NeuralArithmetic.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6a5fed2d01c1c88a4d561e53c25c827d6304e3996eb368c2124107c929aa0552/68747470733a2f2f636f6465636f762e696f2f67682f6e6d6865696d2f4e657572616c41726974686d657469632e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/nmheim/NeuralArithmetic.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>Collection of layers that can perform arithmetic operations such as addition,
subtraction, multiplication, and division in a single layer.  Implements
<a href="https://openreview.net/forum?id=H1gNOeHKPS" rel="nofollow">NMU &amp; NAU</a>,
<a href="https://arxiv.org/abs/2006.01681" rel="nofollow">NPU (ours)</a>,
<a href="https://arxiv.org/abs/1808.00508" rel="nofollow">NALU</a>, and
<a href="https://arxiv.org/abs/2003.07629" rel="nofollow">iNALU</a>.
They can all be used with <a href="https://github.com/FluxML/Flux.jl"><code>Flux.jl</code></a>.</p>
<p>Additionally there are <code>FastNAU</code> and <code>FastNPU</code> for use with <a href="https://github.com/SciML/DiffEqFlux.jl"><code>DiffEqFlux.jl</code></a>.</p>
<h1><a id="user-content-a-simple-example" class="anchor" aria-hidden="true" href="#a-simple-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>A Simple Example</h1>
<p>The script <a href="examples/npu.jl">examples/npu.jl</a> illustrates how to learn a the function <code>f</code></p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="f(x,y) = (x+y, x*y, x/y, sqrt(x))
"><pre><span class="pl-en">f</span>(x,y) <span class="pl-k">=</span> (x<span class="pl-k">+</span>y, x<span class="pl-k">*</span>y, x<span class="pl-k">/</span>y, <span class="pl-c1">sqrt</span>(x))</pre></div>
<p>with a stack of NPU and NAU.
An NPU with two inputs <code>x</code> and <code>y</code> can perform <code>x^a * y^b</code> for each hidden variable
(i.e. multiplication, division, and other power functions of its inputs).
The NAU is just a matmul, so it can perform <code>a*x + b*y</code> (i.e. addition/subtraction).</p>
<p>The image below depicts the learned weights of the model compared to the perfect solution.
The first plot shows the real weights of the NPU, where the first
row forms the first hidden activation <code>h1 = x^1*y^(-1) = x/y</code>, the second row
forms the second hidden activation <code>h2 = x^1*y^1 = x*y</code>, and the sqaure root is computed in
rows 3 and 4 (which can be pruned with a more effective regularisation).
The imaginary weights of the NPU are not needed in this application, so they
are all zero.
The NAU performs the remaining addition in the first row of the plot on the right.</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/npu_example.png"><img src="img/npu_example.png" alt="npu" style="max-width:100%;"></a></p>
<h1><a id="user-content-comparing-neural-arithmetic-units" class="anchor" aria-hidden="true" href="#comparing-neural-arithmetic-units"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Comparing Neural Arithmetic Units</h1>
<p>The figure below compares the extrapolation performance of the existing Neural Arithmetic Units
on the same task as above. Bright colors indicate low error.
All units were trained on the input range U(0.1,2). For more
details take a look at <a href="https://arxiv.org/abs/2006.01681" rel="nofollow">our paper</a> and
the <a href="https://github.com/nmheim/NeuralPowerUnits">code to reproduce</a> the image below.</p>
<p><a target="_blank" rel="noopener noreferrer" href="img/layers.png"><img src="img/layers.png" alt="layers" style="max-width:100%;"></a></p>
</article></div>