<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/wesselb/NeuralProcesses.jl/raw/master/loop.gif"><img src="https://github.com/wesselb/NeuralProcesses.jl/raw/master/loop.gif" width="800px" style="max-width:100%;"></a></p>
<h1><a id="user-content-neuralprocessesjl" class="anchor" aria-hidden="true" href="#neuralprocessesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NeuralProcesses.jl</h1>
<p>NeuralProcesses.jl is a framework for composing
<a href="https://arxiv.org/abs/1807.01622" rel="nofollow">Neural Processes</a> built on top of
<a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>.</p>
<p><a href="https://www.youtube.com/watch?v=nq6X-w5xgLo" rel="nofollow">NeuralProcesses.jl was presented at JuliaCon 2020 [link to video (7:41)].</a></p>
<p>See the <a href="https://github.com/YannDubs/Neural-Process-Family">Neural Process Family</a> for code to reproduce the image experiments from <a href="https://openreview.net/forum?id=Skey4eBYPS" rel="nofollow">Convolutional Conditional Neural Processes (Gordon et al., 2020)</a>.</p>
<p>Contents:</p>
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#predefined-experimental-setup-trainjl">Predefined Experimental Setup: <code>train.jl</code></a></li>
</ul>
</li>
<li><a href="#manual">Manual</a>
<ul>
<li><a href="#principles">Principles</a></li>
<li><a href="#available-models-for-1d-regression">Available Models for 1D Regression</a></li>
<li><a href="#building-blocks">Building Blocks</a></li>
<li><a href="#data-generators">Data Generators</a></li>
<li><a href="#training-and-evaluation">Training and Evaluation</a></li>
</ul>
</li>
<li><a href="#examples">Examples</a>
<ul>
<li><a href="#the-conditional-neural-process">The Conditional Neural Process</a></li>
<li><a href="#the-neural-process">The Neural Process</a></li>
<li><a href="#the-attentive-neural-process">The Attentive Neural Process</a></li>
<li><a href="#the-convolutional-conditional-neural-process">The Convolutional Conditional Neural Process</a></li>
</ul>
</li>
<li><a href="#state-of-the-package">State of the Package</a></li>
<li><a href="#implementation-details">Implementation Details</a></li>
</ul>
<h2><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Introduction</h2>
<p>The setting of NeuralProcesses.jl is <em>meta-learning</em>. Meta-learning is concerned
with learning a map from data sets directly to predictive distributions.
Neural processes are a powerful class of parametrisations of this map based on
an <em>encoding</em> of the data.</p>
<h3><a id="user-content-predefined-experimental-setup-trainjl" class="anchor" aria-hidden="true" href="#predefined-experimental-setup-trainjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Predefined Experimental Setup: <code>train.jl</code></h3>
<p>Eager to get started?!
The file <code>train.jl</code> contains an predefined experimental setup that gets
you going immediately!
Example:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="$ julia --project=. train.jl --model convcnp --data matern52 --loss loglik --epochs 20
"><pre><code>$ julia --project=. train.jl --model convcnp --data matern52 --loss loglik --epochs 20
</code></pre></div>
<p>Here's what it can do:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="usage: train.jl --data DATA --model MODEL [--num-samples NUM-SAMPLES]
                [--batch-size BATCH-SIZE] --loss LOSS
                [--starting-epoch STARTING-EPOCH] [--epochs EPOCHS]
                [--evaluate] [--evaluate-iw] [--evaluate-no-iw]
                [--evaluate-num-samples EVALUATE-NUM-SAMPLES]
                [--evaluate-only-within] [--models-dir MODELS-DIR]
                [--bson BSON] [-h]

optional arguments:
  --data DATA           Data set: eq-small, eq, matern52, eq-mixture,
                        noisy-mixture, weakly-periodic, sawtooth, or
                        mixture. Append &quot;-noisy&quot; to a data set to make
                        it noisy.
  --model MODEL         Model: conv[c]np, corconvcnp, a[c]np, or
                        [c]np. Append &quot;-global-{mean,sum}&quot; to
                        introduce a global latent variable. Append
                        &quot;-amortised-{mean,sum}&quot; to use amortised
                        observation noise. Append &quot;-het&quot; to use
                        heterogeneous observation noise.
  --num-samples NUM-SAMPLES
                        Number of samples to estimate the training
                        loss. Defaults to 20 for &quot;loglik&quot; and 5 for
                        &quot;elbo&quot;. (type: Int64)
  --batch-size BATCH-SIZE
                        Batch size. (type: Int64, default: 16)
  --loss LOSS           Loss: loglik, loglik-iw, or elbo.
  --starting-epoch STARTING-EPOCH
                        Set to a number greater than one to continue
                        training. (type: Int64, default: 1)
  --epochs EPOCHS       Number of epochs to training for. (type:
                        Int64, default: 20)
  --evaluate            Evaluate model.
  --evaluate-iw         Force to use importance weighting for the
                        evaluation objective.
  --evaluate-no-iw      Force to NOT use importance weighting for the
                        evaluation objective.
  --evaluate-num-samples EVALUATE-NUM-SAMPLES
                        Number of samples to estimate the evaluation
                        loss. (type: Int64, default: 4096)
  --evaluate-only-within
                        Evaluate with only the task of interpolation
                        within training range.
  --models-dir MODELS-DIR
                        Directory to store models in. (default:
                        &quot;models&quot;)
  --bson BSON           Directly specify the file to save the model to
                        and load it from.
  -h, --help            show this help message and exit
"><pre><code>usage: train.jl --data DATA --model MODEL [--num-samples NUM-SAMPLES]
                [--batch-size BATCH-SIZE] --loss LOSS
                [--starting-epoch STARTING-EPOCH] [--epochs EPOCHS]
                [--evaluate] [--evaluate-iw] [--evaluate-no-iw]
                [--evaluate-num-samples EVALUATE-NUM-SAMPLES]
                [--evaluate-only-within] [--models-dir MODELS-DIR]
                [--bson BSON] [-h]

optional arguments:
  --data DATA           Data set: eq-small, eq, matern52, eq-mixture,
                        noisy-mixture, weakly-periodic, sawtooth, or
                        mixture. Append "-noisy" to a data set to make
                        it noisy.
  --model MODEL         Model: conv[c]np, corconvcnp, a[c]np, or
                        [c]np. Append "-global-{mean,sum}" to
                        introduce a global latent variable. Append
                        "-amortised-{mean,sum}" to use amortised
                        observation noise. Append "-het" to use
                        heterogeneous observation noise.
  --num-samples NUM-SAMPLES
                        Number of samples to estimate the training
                        loss. Defaults to 20 for "loglik" and 5 for
                        "elbo". (type: Int64)
  --batch-size BATCH-SIZE
                        Batch size. (type: Int64, default: 16)
  --loss LOSS           Loss: loglik, loglik-iw, or elbo.
  --starting-epoch STARTING-EPOCH
                        Set to a number greater than one to continue
                        training. (type: Int64, default: 1)
  --epochs EPOCHS       Number of epochs to training for. (type:
                        Int64, default: 20)
  --evaluate            Evaluate model.
  --evaluate-iw         Force to use importance weighting for the
                        evaluation objective.
  --evaluate-no-iw      Force to NOT use importance weighting for the
                        evaluation objective.
  --evaluate-num-samples EVALUATE-NUM-SAMPLES
                        Number of samples to estimate the evaluation
                        loss. (type: Int64, default: 4096)
  --evaluate-only-within
                        Evaluate with only the task of interpolation
                        within training range.
  --models-dir MODELS-DIR
                        Directory to store models in. (default:
                        "models")
  --bson BSON           Directly specify the file to save the model to
                        and load it from.
  -h, --help            show this help message and exit
</code></pre></div>
<h2><a id="user-content-manual" class="anchor" aria-hidden="true" href="#manual"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Manual</h2>
<h3><a id="user-content-principles" class="anchor" aria-hidden="true" href="#principles"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Principles</h3>
<h4><a id="user-content-models" class="anchor" aria-hidden="true" href="#models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Models</h4>
<p>In NeuralProcesses.jl, models consists of an <em>encoder</em> and a <em>decoder</em>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model = Model(encoder, decoder)
"><pre>model <span class="pl-k">=</span> <span class="pl-c1">Model</span>(encoder, decoder)</pre></div>
<p>An encoder takes in the data and produces an abstract representation of the
data.
A decoder then takes in this representation and produces a prediction at target
inputs.</p>
<h4><a id="user-content-functional-representations-and-coding" class="anchor" aria-hidden="true" href="#functional-representations-and-coding"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Functional Representations and Coding</h4>
<p>In the package, the three objects — the data, encoding, and prediction —
have a common representation. In particular, everything is represented as a
<em>function</em>: a tuple <code>(x, y)</code> that corresponds to the function <code>f(x[i]) = y[i]</code>
for all indices <code>i</code>.
Encoding and decoding, which we will collectively call <em>coding</em>, then become
<em>transformations of functions</em>.</p>
<p>Coding is implemented by the function <code>code</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="xz, z = code(encoder, xc, yc, xt)
"><pre>xz, z <span class="pl-k">=</span> <span class="pl-c1">code</span>(encoder, xc, yc, xt)</pre></div>
<p>Here <code>encoder</code> transforms the function <code>(xc, yc)</code>, the <em>context set</em>, into
another function <code>(xz, z)</code>, the abstract representation. The <em>target inputs</em>
<code>xt</code> express the desire that <code>encoder</code> <em>should</em> (not must) output a function
that maps <em>from</em> <code>xt</code>.
If indeed <code>xz == xt</code>, then the coding operation is called <em>complete</em>.
If, on the other hand, <code>xz != xt</code>, then the coding operation is called
<em>partial</em>.
Encoders and decoders are complete coders.
However, encoders and decoders are often composed from simpler coders, and these
coders could be partial.</p>
<h4><a id="user-content-compositional-coder-design" class="anchor" aria-hidden="true" href="#compositional-coder-design"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Compositional Coder Design</h4>
<p>Coders can be composed using <code>Chain</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="encoder = Chain(
    coder1,
    coder2,
    coder3
)
"><pre>encoder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    coder1,
    coder2,
    coder3
)</pre></div>
<p>Coders can also be put in <em>parallel</em> using <code>Parallel</code>.
For example, this is useful if an encoder should output multiple encodings,
e.g. in a multi-headed architecture:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="encoder = Chain(
    ...,
    # Split the output into two parts, which can then be processed by two heads.
    Splitter(...),
    Parallel(
        ...,  # Head 1
        ...   # Head 2
    )
)
"><pre>encoder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    <span class="pl-k">...</span>,
    <span class="pl-c"><span class="pl-c">#</span> Split the output into two parts, which can then be processed by two heads.</span>
    <span class="pl-c1">Splitter</span>(<span class="pl-k">...</span>),
    <span class="pl-c1">Parallel</span>(
        <span class="pl-k">...</span>,  <span class="pl-c"><span class="pl-c">#</span> Head 1</span>
        <span class="pl-k">...</span>   <span class="pl-c"><span class="pl-c">#</span> Head 2</span>
    )
)</pre></div>
<p>By default, parallel representations are combined with concatenation along the
channel dimension (see <code>Materialise</code> in the section below), but this is
readily extended to additional designs (see <code>?Materialise</code>).</p>
<h4><a id="user-content-coder-likelihoods" class="anchor" aria-hidden="true" href="#coder-likelihoods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Coder Likelihoods</h4>
<p>A coder should output either a <em>deterministic</em> coding or a <em>stochastic</em>
coding, which can be achieved by appending a <em>likelihood</em>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="deterministic_coder = Chain(
    ...,
    DeterministicLikelihood()
)

stochastic_coder = Chain(
    ...,
    HeterogeneousGaussianLikelihood()
)
"><pre>deterministic_coder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    <span class="pl-k">...</span>,
    <span class="pl-c1">DeterministicLikelihood</span>()
)

stochastic_coder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    <span class="pl-k">...</span>,
    <span class="pl-c1">HeterogeneousGaussianLikelihood</span>()
)</pre></div>
<p>When a model is run, the output of the <em>encoder</em> is <em>sampled</em>.
The resulting sample is then fed to the <em>decoder</em>.
In scenarios where the encoder outputs multiple encodings in parallel, it may be
desirable to concatenate those encodings into one big tensor which can then
be processed by the decoder.
This is achieved by prepending <code>Materialise()</code> to the decoder:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="decoder = Chain(
    Materialise(),
    ...,
    HeterogenousGaussian()
)
"><pre>decoder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    <span class="pl-c1">Materialise</span>(),
    <span class="pl-k">...</span>,
    <span class="pl-c1">HeterogenousGaussian</span>()
)</pre></div>
<p>The decoder outputs the prediction for the data.
In the above example, <code>decoder</code> produces means and variances at test inputs.</p>
<h3><a id="user-content-available-models-for-1d-regression" class="anchor" aria-hidden="true" href="#available-models-for-1d-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Available Models for 1D Regression</h3>
<p>The package exports constructors for a number of architectures from the
literature.</p>
<table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Constructor</th>
<th align="left">Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Conditional Neural Process</td>
<td align="left"><code>cnp_1d</code></td>
<td align="left"><a href="https://arxiv.org/abs/1807.01613" rel="nofollow">Garnelo, Rosenbaum, et al. (2018)</a></td>
</tr>
<tr>
<td align="left">Neural Process</td>
<td align="left"><code>np_1d</code></td>
<td align="left"><a href="https://arxiv.org/abs/1807.01622" rel="nofollow">Garnelo, Schwarz, et al. (2018)</a></td>
</tr>
<tr>
<td align="left">Attentive Conditional Neural Process</td>
<td align="left"><code>acnp_1d</code></td>
<td align="left"><a href="https://openreview.net/forum?id=SkE6PjC9KX" rel="nofollow">Kim, Mnih, et al. (2019)</a></td>
</tr>
<tr>
<td align="left">Attentive Neural Process</td>
<td align="left"><code>anp_1d</code></td>
<td align="left"><a href="https://openreview.net/forum?id=SkE6PjC9KX" rel="nofollow">Kim, Mnih, et al. (2019)</a></td>
</tr>
<tr>
<td align="left">Convolutional Conditional Neural Process</td>
<td align="left"><code>convcnp_1d</code></td>
<td align="left"><a href="https://openreview.net/forum?id=Skey4eBYPS" rel="nofollow">Gordon, Bruinsma, et al. (2020)</a></td>
</tr>
<tr>
<td align="left">Convolutional Neural Process</td>
<td align="left"><code>convnp_1d</code></td>
<td align="left"><a href="https://arxiv.org/abs/2007.01332" rel="nofollow">Foong, Bruinsma, et al. (2020)</a></td>
</tr>
<tr>
<td align="left">Gaussian Neural Process</td>
<td align="left"><code>corconvcnp_1d</code></td>
<td align="left"><a href="https://openreview.net/forum?id=rzsDn7Vzxf" rel="nofollow">Bruinsma, Requeima et al. (2021)</a></td>
</tr>
</tbody>
</table>
<p>Download links for pretrained models are below.
The instructions for how a pretrained model can be run are as follows:</p>
<ol>
<li>
<p>Download some <a href="https://www.dropbox.com/s/ua40uc9ttzq9t18/models.tar.gz?dl=1" rel="nofollow">pretrained models</a>.</p>
</li>
<li>
<p>Extract the models:</p>
</li>
</ol>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="$ tar -xzvf models.tar.gz
"><pre>$ tar -xzvf models.tar.gz</pre></div>
<ol start="3">
<li>Open Julia and load the model:</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NeuralProcesses, NeuralProcesses.Experiment, Flux

convnp = best_model(&quot;models/convnp-het/loglik/matern52.bson&quot;)
"><pre><span class="pl-k">using</span> NeuralProcesses, NeuralProcesses<span class="pl-k">.</span>Experiment, Flux

convnp <span class="pl-k">=</span> <span class="pl-c1">best_model</span>(<span class="pl-s"><span class="pl-pds">"</span>models/convnp-het/loglik/matern52.bson<span class="pl-pds">"</span></span>)</pre></div>
<ol start="4">
<li>Run the model:</li>
</ol>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="means, lowers, uppers, samples = predict(
    convnp,
    randn(Float32, 10),  # Random context inputs
    randn(Float32, 10),  # Random context outputs
    randn(Float32, 10)   # Random target inputs
)
"><pre>means, lowers, uppers, samples <span class="pl-k">=</span> <span class="pl-c1">predict</span>(
    convnp,
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context inputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context outputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>)   <span class="pl-c"><span class="pl-c">#</span> Random target inputs</span>
)</pre></div>
<h4><a id="user-content-pretrained-models-for-foong-bruinsma-et-al-2020" class="anchor" aria-hidden="true" href="#pretrained-models-for-foong-bruinsma-et-al-2020"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pretrained Models for <a href="https://arxiv.org/abs/2007.01332" rel="nofollow">Foong, Bruinsma, et al. (2020)</a></h4>
<p><a href="https://www.dropbox.com/s/ua40uc9ttzq9t18/models.tar.gz?dl=1" rel="nofollow">Download link</a></p>
<p>Interpolation results for the pretrained models are as follows:</p>
<h5><a id="user-content-loglik" class="anchor" aria-hidden="true" href="#loglik"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>loglik</code></h5>
<table>
<thead>
<tr>
<th>Model</th>
<th align="center"><code>eq</code></th>
<th align="center"><code>matern52</code></th>
<th align="center"><code>noisy-mixture</code></th>
<th align="center"><code>weakly-periodic</code></th>
<th align="center"><code>sawtooth</code></th>
<th align="center"><code>mixture</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cnp</code></td>
<td align="center">-1.09</td>
<td align="center">-1.17</td>
<td align="center">-1.28</td>
<td align="center">-1.34</td>
<td align="center">-0.16</td>
<td align="center">-1.17</td>
</tr>
<tr>
<td><code>acnp</code></td>
<td align="center">-0.83</td>
<td align="center">-0.93</td>
<td align="center">-1.00</td>
<td align="center">-1.29</td>
<td align="center">-0.17</td>
<td align="center">-1.09</td>
</tr>
<tr>
<td><code>convcnp</code></td>
<td align="center">-0.69</td>
<td align="center">-0.88</td>
<td align="center">-0.93</td>
<td align="center">-1.19</td>
<td align="center">1.09</td>
<td align="center">-0.94</td>
</tr>
<tr>
<td><code>np-het</code></td>
<td align="center">-0.76</td>
<td align="center">-0.90</td>
<td align="center">-0.94</td>
<td align="center">-1.23</td>
<td align="center">-0.16</td>
<td align="center">-0.88</td>
</tr>
<tr>
<td><code>anp-het</code></td>
<td align="center">-0.53</td>
<td align="center">-0.74</td>
<td align="center">-0.66</td>
<td align="center">-1.17</td>
<td align="center">-0.10</td>
<td align="center">-0.63</td>
</tr>
<tr>
<td><code>convnp-het</code></td>
<td align="center">-0.34</td>
<td align="center">-0.61</td>
<td align="center">-0.59</td>
<td align="center">-1.01</td>
<td align="center">2.24</td>
<td align="center">-0.40</td>
</tr>
</tbody>
</table>
<h5><a id="user-content-elbo" class="anchor" aria-hidden="true" href="#elbo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>elbo</code></h5>
<table>
<thead>
<tr>
<th>Model</th>
<th align="center"><code>eq</code></th>
<th align="center"><code>matern52</code></th>
<th align="center"><code>noisy-mixture</code></th>
<th align="center"><code>weakly-periodic</code></th>
<th align="center"><code>sawtooth</code></th>
<th align="center"><code>mixture</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>np-het</code></td>
<td align="center">-0.34</td>
<td align="center">-0.66</td>
<td align="center">-0.66</td>
<td align="center">-1.21</td>
<td align="center">-0.12</td>
<td align="center">-0.71</td>
</tr>
<tr>
<td><code>anp-het</code></td>
<td align="center">-0.71</td>
<td align="center">-0.88</td>
<td align="center">-0.80</td>
<td align="center">-1.27</td>
<td align="center">-0.00</td>
<td align="center">-0.86</td>
</tr>
<tr>
<td><code>convnp-het</code></td>
<td align="center">-0.61</td>
<td align="center">-0.59</td>
<td align="center">-2.19</td>
<td align="center">-1.07</td>
<td align="center">2.40</td>
<td align="center">-1.27</td>
</tr>
</tbody>
</table>
<h5><a id="user-content-loglik-iw" class="anchor" aria-hidden="true" href="#loglik-iw"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>loglik-iw</code></h5>
<table>
<thead>
<tr>
<th>Model</th>
<th align="center"><code>eq</code></th>
<th align="center"><code>matern52</code></th>
<th align="center"><code>noisy-mixture</code></th>
<th align="center"><code>weakly-periodic</code></th>
<th align="center"><code>sawtooth</code></th>
<th align="center"><code>mixture</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>np-het</code></td>
<td align="center">-0.28</td>
<td align="center">-0.57</td>
<td align="center">-0.47</td>
<td align="center">-1.20</td>
<td align="center">0.32</td>
<td align="center">-0.59</td>
</tr>
<tr>
<td><code>anp-het</code></td>
<td align="center">-0.45</td>
<td align="center">-0.67</td>
<td align="center">-0.57</td>
<td align="center">-1.19</td>
<td align="center">-0.16</td>
<td align="center">-0.61</td>
</tr>
<tr>
<td><code>convnp-het</code></td>
<td align="center">-0.09</td>
<td align="center">-0.29</td>
<td align="center">-0.34</td>
<td align="center">-0.98</td>
<td align="center">2.39</td>
<td align="center">-0.31</td>
</tr>
</tbody>
</table>
<h4><a id="user-content-pretrained-models-for-bruinsma-requeima-et-al-2021" class="anchor" aria-hidden="true" href="#pretrained-models-for-bruinsma-requeima-et-al-2021"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pretrained Models for <a href="https://openreview.net/forum?id=rzsDn7Vzxf" rel="nofollow">Bruinsma, Requeima et al. (2021)</a></h4>
<p><a href="https://www.dropbox.com/s/knlydai66aroorh/models.tar.gz?dl=1" rel="nofollow">Download link</a></p>
<p>Interpolation results for the pretrained models are as follows:</p>
<h5><a id="user-content-loglik-1" class="anchor" aria-hidden="true" href="#loglik-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>loglik</code></h5>
<table>
<thead>
<tr>
<th>Model</th>
<th align="center"><code>eq-noisy</code></th>
<th align="center"><code>matern52-noisy</code></th>
<th align="center"><code>noisy-mixture</code></th>
<th align="center"><code>weakly-periodic-noisy</code></th>
<th align="center"><code>sawtooth-noisy</code></th>
<th align="center"><code>mixture-noisy</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>convcnp</code></td>
<td align="center">-0.80</td>
<td align="center">-0.95</td>
<td align="center">-0.95</td>
<td align="center">-1.20</td>
<td align="center">0.55</td>
<td align="center">-0.93</td>
</tr>
<tr>
<td><code>corconvcnp</code></td>
<td align="center">0.70</td>
<td align="center">0.30</td>
<td align="center">0.96</td>
<td align="center">-0.47</td>
<td align="center">0.42</td>
<td align="center">0.10</td>
</tr>
<tr>
<td><code>anp-het</code></td>
<td align="center">-0.61</td>
<td align="center">-0.75</td>
<td align="center">-0.73</td>
<td align="center">-1.19</td>
<td align="center">0.34</td>
<td align="center">-0.69</td>
</tr>
<tr>
<td><code>convnp-het</code></td>
<td align="center">-0.46</td>
<td align="center">-0.67</td>
<td align="center">-0.53</td>
<td align="center">-1.02</td>
<td align="center">1.20</td>
<td align="center">-0.50</td>
</tr>
</tbody>
</table>
<h3><a id="user-content-building-blocks" class="anchor" aria-hidden="true" href="#building-blocks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Building Blocks</h3>
<p>The package provides various building blocks that can be used to compose
encoders and decoders.
For some building blocks, there is a constructor function available that can be
used to more easily construct the block.
More information about a block can be obtained by using the built-in help
function, e.g. <code>?LayerNorm</code>.</p>
<h4><a id="user-content-glue" class="anchor" aria-hidden="true" href="#glue"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Glue</h4>
<table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Constructor</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Chain</code></td>
<td align="left"></td>
<td align="left">Put things in sequence.</td>
</tr>
<tr>
<td align="left"><code>Parallel</code></td>
<td align="left"></td>
<td align="left">Put things in parallel.</td>
</tr>
</tbody>
</table>
<h4><a id="user-content-basic-blocks" class="anchor" aria-hidden="true" href="#basic-blocks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic Blocks</h4>
<table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Constructor</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>BatchedMLP</code></td>
<td align="left"><code>batched_mlp</code></td>
<td align="left">Batched MLP.</td>
</tr>
<tr>
<td align="left"><code>BatchedConv</code></td>
<td align="left"><code>build_conv</code></td>
<td align="left">Batched CNN.</td>
</tr>
<tr>
<td align="left"><code>Splitter</code></td>
<td align="left"></td>
<td align="left">Split off a given number of channels.</td>
</tr>
<tr>
<td align="left"><code>LayerNorm</code></td>
<td align="left"></td>
<td align="left">Layer normalisation.</td>
</tr>
<tr>
<td align="left"><code>MeanPooling</code></td>
<td align="left"></td>
<td align="left">Mean pooling.</td>
</tr>
<tr>
<td align="left"><code>SumPooling</code></td>
<td align="left"></td>
<td align="left">Sum pooling.</td>
</tr>
</tbody>
</table>
<h4><a id="user-content-advanced-blocks" class="anchor" aria-hidden="true" href="#advanced-blocks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Advanced Blocks</h4>
<table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Constructor</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Attention</code></td>
<td align="left"><code>attention</code></td>
<td align="left">Attentive mechanism.</td>
</tr>
<tr>
<td align="left"><code>SetConv</code></td>
<td align="left"><code>set_conv</code></td>
<td align="left">Set convolution.</td>
</tr>
<tr>
<td align="left"><code>SetConvPD</code></td>
<td align="left"><code>set_conv</code></td>
<td align="left">Set convolution for kernel functions.</td>
</tr>
</tbody>
</table>
<h4><a id="user-content-likelihoods" class="anchor" aria-hidden="true" href="#likelihoods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Likelihoods</h4>
<table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Constructor</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>DeterministicLikelihood</code></td>
<td align="left"></td>
<td align="left">DeterministicLikelihood output.</td>
</tr>
<tr>
<td align="left"><code>FixedGaussianLikelihood</code></td>
<td align="left"></td>
<td align="left">Gaussian likelihood with a fixed variance.</td>
</tr>
<tr>
<td align="left"><code>AmortisedGaussianLikelihood</code></td>
<td align="left"></td>
<td align="left">Gaussian likelihood with a fixed variance that is calculated from split-off channels.</td>
</tr>
<tr>
<td align="left"><code>HeterogeneousGaussianLikelihood</code></td>
<td align="left"></td>
<td align="left">Gaussian likelihood with input-dependent variance.</td>
</tr>
</tbody>
</table>
<h4><a id="user-content-coders" class="anchor" aria-hidden="true" href="#coders"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Coders</h4>
<table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Constructor</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Materialise</code></td>
<td align="left"></td>
<td align="left">Materialise a sample.</td>
</tr>
<tr>
<td align="left"><code>FunctionalCoder</code></td>
<td align="left"></td>
<td align="left">Code into a function space: make the target inputs a discretisation.</td>
</tr>
<tr>
<td align="left"><code>UniformDiscretisation1D</code></td>
<td align="left"></td>
<td align="left">Discretise uniformly at a given density of points.</td>
</tr>
<tr>
<td align="left"><code>InputsCoder</code></td>
<td align="left"></td>
<td align="left">Code with the target inputs.</td>
</tr>
<tr>
<td align="left"><code>MLPCoder</code></td>
<td align="left"></td>
<td align="left">Rho-sum-phi coder.</td>
</tr>
</tbody>
</table>
<h3><a id="user-content-data-generators" class="anchor" aria-hidden="true" href="#data-generators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Data Generators</h3>
<p>Models can be trained with data generators.
Data generators are callables that take in an integer (number of batches) and give
back an iterator that generates four-tuples: context inputs, context outputs,
target inputs, and target outputs.
All tensors should be of rank three where the first dimension is the data
dimension, the second dimension is the feature dimension, and the third
dimension is the batch dimension.</p>
<p>Data generators can be constructed with <code>DataGenerator</code> which takes in an
underlying <em>stochastic process</em>.
<a href="https://github.com/willtebbutt/Stheno.jl">Stheno.jl</a> can be used to build
Gaussian processes.
In addition, the package exports the following processes:</p>
<table>
<thead>
<tr>
<th align="left">Type</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Sawtooth</code></td>
<td align="left">Sawtooth process.</td>
</tr>
<tr>
<td align="left"><code>BayesianConvNP</code></td>
<td align="left">A Convolutional Neural Process with a prior on the weights.</td>
</tr>
<tr>
<td align="left"><code>Mixture</code></td>
<td align="left">Mixture of processes.</td>
</tr>
</tbody>
</table>
<h3><a id="user-content-training-and-evaluation" class="anchor" aria-hidden="true" href="#training-and-evaluation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training and Evaluation</h3>
<p>Experimentation functionality is exported by <code>NeuralProcesses.Experiment</code>.</p>
<h4><a id="user-content-running-models" class="anchor" aria-hidden="true" href="#running-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Running Models</h4>
<p>A model can be run forward by calling it with three arguments:
context inputs, context outputs, and target inputs.
All arguments to models should be tensors of rank three where the first
dimension is the data dimension, the second dimension is the feature dimension,
and the third dimension is the batch dimension.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="convcnp(
    # Use a batch size of 16.
    randn(Float32, 10, 1, 16),  # Random context inputs
    randn(Float32, 10, 1, 16),  # Random context outputs
    randn(Float32, 15, 1, 16)   # Random target inputs
)
"><pre><span class="pl-c1">convcnp</span>(
    <span class="pl-c"><span class="pl-c">#</span> Use a batch size of 16.</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>, <span class="pl-c1">1</span>, <span class="pl-c1">16</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context inputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>, <span class="pl-c1">1</span>, <span class="pl-c1">16</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context outputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">15</span>, <span class="pl-c1">1</span>, <span class="pl-c1">16</span>)   <span class="pl-c"><span class="pl-c">#</span> Random target inputs</span>
)</pre></div>
<p>For convenience, the package also exports the function <code>predict</code>, which
runs a model from inputs of type <code>Vector</code> and produces predictive means,
lower and upper credible bounds, and predictive samples.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="means, lowers, uppers, samples = predict(
    convcnp,
    randn(Float32, 10),  # Random context inputs
    randn(Float32, 10),  # Random context outputs
    randn(Float32, 10)   # Random target inputs
)
"><pre>means, lowers, uppers, samples <span class="pl-k">=</span> <span class="pl-c1">predict</span>(
    convcnp,
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context inputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context outputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>)   <span class="pl-c"><span class="pl-c">#</span> Random target inputs</span>
)</pre></div>
<h4><a id="user-content-training" class="anchor" aria-hidden="true" href="#training"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training</h4>
<p>To train a model, use <code>train!</code>, which, amongst other things, requires a
loss function and optimiser.
Loss functions are described below, and
optimisers can be found in <code>Flux.Optimiser</code>;
for most applications, <code>ADAM(5e-4)</code> probably suffices.
After training, a model can be evaluated with <code>eval_model</code>.</p>
<p>See <code>train.jl</code> for an example of <code>train!</code>.</p>
<h4><a id="user-content-losses" class="anchor" aria-hidden="true" href="#losses"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Losses</h4>
<p>The following loss functions are exported:</p>
<table>
<thead>
<tr>
<th align="left">Function</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>loglik</code></td>
<td align="left">Biased estimate of the log-expected-likelihood. Exact for models with a deterministic encoder.</td>
</tr>
<tr>
<td align="left"><code>elbo</code></td>
<td align="left">Neural process ELBO-style loss.</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# 1-sample log-EL loss. This is exact for models with a deterministic encoder.
loss(xs...) = loglik(xs..., num_samples=1)   

# 20-sample log-EL loss. This is probably what you want if you are training
# a model with a stochastic encoder.
loss(xs...) = loglik(xs..., num_samples=20)

# 20-sample ELBO loss. This is an alternative to `loglik`.
loss(xs...) = elbo(xs..., num_samples=20)    
"><pre><span class="pl-c"><span class="pl-c">#</span> 1-sample log-EL loss. This is exact for models with a deterministic encoder.</span>
<span class="pl-en">loss</span>(xs<span class="pl-k">...</span>) <span class="pl-k">=</span> <span class="pl-c1">loglik</span>(xs<span class="pl-k">...</span>, num_samples<span class="pl-k">=</span><span class="pl-c1">1</span>)   

<span class="pl-c"><span class="pl-c">#</span> 20-sample log-EL loss. This is probably what you want if you are training</span>
<span class="pl-c"><span class="pl-c">#</span> a model with a stochastic encoder.</span>
<span class="pl-en">loss</span>(xs<span class="pl-k">...</span>) <span class="pl-k">=</span> <span class="pl-c1">loglik</span>(xs<span class="pl-k">...</span>, num_samples<span class="pl-k">=</span><span class="pl-c1">20</span>)

<span class="pl-c"><span class="pl-c">#</span> 20-sample ELBO loss. This is an alternative to `loglik`.</span>
<span class="pl-en">loss</span>(xs<span class="pl-k">...</span>) <span class="pl-k">=</span> <span class="pl-c1">elbo</span>(xs<span class="pl-k">...</span>, num_samples<span class="pl-k">=</span><span class="pl-c1">20</span>)    </pre></div>
<p>See <code>train.jl</code> for more examples.</p>
<h4><a id="user-content-saving-and-loading" class="anchor" aria-hidden="true" href="#saving-and-loading"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Saving and Loading</h4>
<p>After every epoch, the current model and top five best models are saved.
To file to which the model is written is determined by the keyword <code>bson</code>
of <code>train!</code>.
After training, the best model can be loaded with <code>best_model(path)</code>.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<h3><a id="user-content-the-conditional-neural-process" class="anchor" aria-hidden="true" href="#the-conditional-neural-process"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Conditional Neural Process</h3>
<p>Perhaps the simplest member of the NP family is the
<a href="http://proceedings.mlr.press/v80/garnelo18a.html" rel="nofollow">Conditional Neural Process</a>
(CNP).
CNPs employ a deterministic MLP-based encoder, and an MLP-based decoder.
As a first example, we provide an implementation of a simple CNP in the
framework:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# The encoder maps into a finite-dimensional vector space, and produces a
# global (deterministic) representation, which is then concatenated to every
# test point. We use a `Parallel` object to achieve this.
encoder = Parallel(
    # The `InputsEncoder` simply outputs the target locations. We `Chain` this
    # with a `DeterministicLikelihood` to form a complete coder.
    Chain(
        InputsCoder(),
        DeterministicLikelihood()
    ),
    Chain(
        # The representation is given by a deep-set network, which is
        # implemented with the `MLPCoder` object. This object receives two MLPs
        # upon construction, a pre-pooling network and post-pooling network,
        # and produces a vector representation for each context set in the
        # batch.
        MLPCoder(
            batched_mlp(
                dim_in    =dim_x + dim_y,
                dim_hidden=dim_embedding,
                dim_out   =dim_embedding,
                num_layers=num_encoder_layers
            ),
            batched_mlp(
                dim_in    =dim_embedding,
                dim_hidden=dim_embedding,
                dim_out   =dim_embedding,
                num_layers=num_encoder_layers
            )
        ),
        # The resulting representation is also chained with a
        # `DeterministicLikelihood` as we are interested in a conditional model.
        DeterministicLikelihood()
    )
)

# The CNP decoder is also MLP based. It first `materialises` the encoder output
# (concatenates the target inputs and context set representation), and then
# passes these through an MLP that outputs a mean and standard deviation at
# every target location.
decoder = Chain(
        # First, concatenate target inputs and context set representation. By
        # default, `Materialise` uses concatenation to combine the different
        # representations in a `Parallel` object, but alternative designs (e.g.,
        # summation or multiplicative flows) could also be considered in
        # NeuralProcesses.jl.
        Materialise(),
        # Pass the resulting representations through an MLP-based decoder. The
        # input dimensionality is the dimensionality of the target inputs plus
        # the dimensionality of the representation. The output dimension is
        # twice the output dimensionality, since we require a mean and standard
        # deviation for every location.
        batched_mlp(
            dim_in    =dim_x + dim_embedding,
            dim_hidden=dim_embedding,
            dim_out   =2dim_y,
            num_layers=num_decoder_layers
        ),
        # The `HeterogeneousGaussianLikelihood` automatically splits its inputs
        # in two along the feature dimension, and treats the first half as the
        # mean and second half as the standard deviation of a Gaussian
        # distribution.
        HeterogeneousGaussianLikelihood()
    )

cnp = Model(encoder, decoder)
"><pre><span class="pl-c"><span class="pl-c">#</span> The encoder maps into a finite-dimensional vector space, and produces a</span>
<span class="pl-c"><span class="pl-c">#</span> global (deterministic) representation, which is then concatenated to every</span>
<span class="pl-c"><span class="pl-c">#</span> test point. We use a `Parallel` object to achieve this.</span>
encoder <span class="pl-k">=</span> <span class="pl-c1">Parallel</span>(
    <span class="pl-c"><span class="pl-c">#</span> The `InputsEncoder` simply outputs the target locations. We `Chain` this</span>
    <span class="pl-c"><span class="pl-c">#</span> with a `DeterministicLikelihood` to form a complete coder.</span>
    <span class="pl-c1">Chain</span>(
        <span class="pl-c1">InputsCoder</span>(),
        <span class="pl-c1">DeterministicLikelihood</span>()
    ),
    <span class="pl-c1">Chain</span>(
        <span class="pl-c"><span class="pl-c">#</span> The representation is given by a deep-set network, which is</span>
        <span class="pl-c"><span class="pl-c">#</span> implemented with the `MLPCoder` object. This object receives two MLPs</span>
        <span class="pl-c"><span class="pl-c">#</span> upon construction, a pre-pooling network and post-pooling network,</span>
        <span class="pl-c"><span class="pl-c">#</span> and produces a vector representation for each context set in the</span>
        <span class="pl-c"><span class="pl-c">#</span> batch.</span>
        <span class="pl-c1">MLPCoder</span>(
            <span class="pl-c1">batched_mlp</span>(
                dim_in    <span class="pl-k">=</span>dim_x <span class="pl-k">+</span> dim_y,
                dim_hidden<span class="pl-k">=</span>dim_embedding,
                dim_out   <span class="pl-k">=</span>dim_embedding,
                num_layers<span class="pl-k">=</span>num_encoder_layers
            ),
            <span class="pl-c1">batched_mlp</span>(
                dim_in    <span class="pl-k">=</span>dim_embedding,
                dim_hidden<span class="pl-k">=</span>dim_embedding,
                dim_out   <span class="pl-k">=</span>dim_embedding,
                num_layers<span class="pl-k">=</span>num_encoder_layers
            )
        ),
        <span class="pl-c"><span class="pl-c">#</span> The resulting representation is also chained with a</span>
        <span class="pl-c"><span class="pl-c">#</span> `DeterministicLikelihood` as we are interested in a conditional model.</span>
        <span class="pl-c1">DeterministicLikelihood</span>()
    )
)

<span class="pl-c"><span class="pl-c">#</span> The CNP decoder is also MLP based. It first `materialises` the encoder output</span>
<span class="pl-c"><span class="pl-c">#</span> (concatenates the target inputs and context set representation), and then</span>
<span class="pl-c"><span class="pl-c">#</span> passes these through an MLP that outputs a mean and standard deviation at</span>
<span class="pl-c"><span class="pl-c">#</span> every target location.</span>
decoder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
        <span class="pl-c"><span class="pl-c">#</span> First, concatenate target inputs and context set representation. By</span>
        <span class="pl-c"><span class="pl-c">#</span> default, `Materialise` uses concatenation to combine the different</span>
        <span class="pl-c"><span class="pl-c">#</span> representations in a `Parallel` object, but alternative designs (e.g.,</span>
        <span class="pl-c"><span class="pl-c">#</span> summation or multiplicative flows) could also be considered in</span>
        <span class="pl-c"><span class="pl-c">#</span> NeuralProcesses.jl.</span>
        <span class="pl-c1">Materialise</span>(),
        <span class="pl-c"><span class="pl-c">#</span> Pass the resulting representations through an MLP-based decoder. The</span>
        <span class="pl-c"><span class="pl-c">#</span> input dimensionality is the dimensionality of the target inputs plus</span>
        <span class="pl-c"><span class="pl-c">#</span> the dimensionality of the representation. The output dimension is</span>
        <span class="pl-c"><span class="pl-c">#</span> twice the output dimensionality, since we require a mean and standard</span>
        <span class="pl-c"><span class="pl-c">#</span> deviation for every location.</span>
        <span class="pl-c1">batched_mlp</span>(
            dim_in    <span class="pl-k">=</span>dim_x <span class="pl-k">+</span> dim_embedding,
            dim_hidden<span class="pl-k">=</span>dim_embedding,
            dim_out   <span class="pl-k">=</span><span class="pl-c1">2</span>dim_y,
            num_layers<span class="pl-k">=</span>num_decoder_layers
        ),
        <span class="pl-c"><span class="pl-c">#</span> The `HeterogeneousGaussianLikelihood` automatically splits its inputs</span>
        <span class="pl-c"><span class="pl-c">#</span> in two along the feature dimension, and treats the first half as the</span>
        <span class="pl-c"><span class="pl-c">#</span> mean and second half as the standard deviation of a Gaussian</span>
        <span class="pl-c"><span class="pl-c">#</span> distribution.</span>
        <span class="pl-c1">HeterogeneousGaussianLikelihood</span>()
    )

cnp <span class="pl-k">=</span> <span class="pl-c1">Model</span>(encoder, decoder)</pre></div>
<p>Then, after training, we can make predictions as follows:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="means, lowers, uppers, samples = predict(
    cnp,
    randn(Float32, 10),  # Random context inputs
    randn(Float32, 10),  # Random context outputs
    randn(Float32, 10)   # Random target inputs
)
"><pre>means, lowers, uppers, samples <span class="pl-k">=</span> <span class="pl-c1">predict</span>(
    cnp,
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context inputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>),  <span class="pl-c"><span class="pl-c">#</span> Random context outputs</span>
    <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">10</span>)   <span class="pl-c"><span class="pl-c">#</span> Random target inputs</span>
)</pre></div>
<h3><a id="user-content-the-neural-process" class="anchor" aria-hidden="true" href="#the-neural-process"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Neural Process</h3>
<p><a href="https://arxiv.org/abs/1807.01622" rel="nofollow">Neural Processes</a> (NP) extend CNPs by adding
a latent variable to the model.
This enables NPs to capture joint, non-Gaussian marginal distributions for
target sets, which in turn allows producing coherent samples.
Extending CNPs to NPs in NeuralProcceses.jl is extremely easy: we simply
replace the <code>DeterministicLikelihood</code> component of the <code>MLPCoder</code> with a
<code>HeterogenousGaussian</code>, and adjust the output dimension of the encoder to
produce both means and variances!</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# The only change to the encoder is replacing the `DeterministicLikelihood`
# following the `MLPCoder` with a `HeterogenousGaussian`!
encoder = Parallel(
    Chain(
        InputsCoder(),
        DeterministicLikelihood()
    ),
    Chain(
        MLPCoder(
            batched_mlp(
                dim_in    =dim_x + dim_y,
                dim_hidden=dim_embedding,
                dim_out   =dim_embedding,
                num_layers=num_encoder_layers
            ),
            batched_mlp(
                dim_in    =dim_embedding,
                dim_hidden=dim_embedding,
                # Since `HeterogenousGaussian` splits its inputs along the
                # channel dimension, we increase the output dimension of the set
                # encoder accordingly.
                dim_out   =2dim_embedding,
                num_layers=num_encoder_layers
            )
        ),
        # This is the main change required to switch between a CNP and an NP.
        HeterogeneousGaussianLikelihood()
    )
)

# We can then reuse the previously defined decoder as is!
np = Model(encoder, decoder)
"><pre><span class="pl-c"><span class="pl-c">#</span> The only change to the encoder is replacing the `DeterministicLikelihood`</span>
<span class="pl-c"><span class="pl-c">#</span> following the `MLPCoder` with a `HeterogenousGaussian`!</span>
encoder <span class="pl-k">=</span> <span class="pl-c1">Parallel</span>(
    <span class="pl-c1">Chain</span>(
        <span class="pl-c1">InputsCoder</span>(),
        <span class="pl-c1">DeterministicLikelihood</span>()
    ),
    <span class="pl-c1">Chain</span>(
        <span class="pl-c1">MLPCoder</span>(
            <span class="pl-c1">batched_mlp</span>(
                dim_in    <span class="pl-k">=</span>dim_x <span class="pl-k">+</span> dim_y,
                dim_hidden<span class="pl-k">=</span>dim_embedding,
                dim_out   <span class="pl-k">=</span>dim_embedding,
                num_layers<span class="pl-k">=</span>num_encoder_layers
            ),
            <span class="pl-c1">batched_mlp</span>(
                dim_in    <span class="pl-k">=</span>dim_embedding,
                dim_hidden<span class="pl-k">=</span>dim_embedding,
                <span class="pl-c"><span class="pl-c">#</span> Since `HeterogenousGaussian` splits its inputs along the</span>
                <span class="pl-c"><span class="pl-c">#</span> channel dimension, we increase the output dimension of the set</span>
                <span class="pl-c"><span class="pl-c">#</span> encoder accordingly.</span>
                dim_out   <span class="pl-k">=</span><span class="pl-c1">2</span>dim_embedding,
                num_layers<span class="pl-k">=</span>num_encoder_layers
            )
        ),
        <span class="pl-c"><span class="pl-c">#</span> This is the main change required to switch between a CNP and an NP.</span>
        <span class="pl-c1">HeterogeneousGaussianLikelihood</span>()
    )
)

<span class="pl-c"><span class="pl-c">#</span> We can then reuse the previously defined decoder as is!</span>
np <span class="pl-k">=</span> <span class="pl-c1">Model</span>(encoder, decoder)</pre></div>
<p>Note that typical NPs consider both a deterministic and latent representation.
This is easily achieved in NeuralProcesses.jl by adding an additional encoder to
the <code>Parallel</code> object (with a <code>DeterministicLikelihood</code>), and increasing the
decoder <code>dim_in</code> accordingly.
In this repo, the built-in NP model uses this form.
This example does not include a deterministic path to emphasise the ease of
switching between conditional and latent variable models in NeuralProcesses.jl.</p>
<h3><a id="user-content-the-attentive-neural-processes" class="anchor" aria-hidden="true" href="#the-attentive-neural-processes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Attentive Neural Processes</h3>
<p>Next, we consider a more complicated model, and demonstrate how easy it is to
implement with NeuralProcesses.jl.
<a href="https://openreview.net/forum?id=SkE6PjC9KX" rel="nofollow">Attentive Neural Processes</a> (ANPs)
extend NPs by considering an attentive mechanism for the deterministic
representation.
Attention comes built-in with NeuralProcesses.jl, and so we can deploy it within
a <code>Chain</code> or <code>Parallel</code> like other building blocks.
Below is an example implementation of an ANP with a deterministic attentive
representation, and a stochastic (Gaussian) global representation.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# The encoder now aggregates three separate representations:
#   (i) the target inputs, like the (C)NP,
#   (ii) a deterministic attentive representation, and
#   (iii) a stochastic global representation.
encoder = Parallel(
    # First, include the `InputsCoder` to represent the target set inputs.
    Chain(
        InputsCoder(),
        DeterministicLikelihood()
    ),
    # NeuralProcesses.jl uses a transformer-style multi-head architecture for
    # attention. It first embeds the inputs and outputs into a
    # finite-dimensional vector space with an MLP, and applies the attention in
    # the embedding space. The constructor requires the dimensionalities of the
    # inputs and outputs, the desired dimensionality of the embedding, and the
    # number of heads to employ (each head will use a
    # `div(dim_embedding, num_heads)`-dimensional embedding), and the number of
    # layers in the embedding MLPs. As ANPs employ attention for the
    # deterministic representations, this is chained with a
    # `DeterministicLikelihood`.
    Chain(
        attention(
            dim_x             =dim_x,
            dim_y             =dim_y,
            dim_embedding     =dim_embedding,
            num_heads         =num_encoder_heads,
            num_encoder_layers=num_encoder_layers
        ),
        DeterministicLikelihood()
    ),
    # The latent path uses the same form as for the NP.
    Chain(
        MLPCoder(
            batched_mlp(
                dim_in    =dim_x + dim_y,
                dim_hidden=dim_embedding,
                dim_out   =dim_embedding,
                num_layers=num_encoder_layers
            ),
            batched_mlp(
                dim_in    =dim_embedding,
                dim_hidden=dim_embedding,
                dim_out   =2dim_embedding,
                num_layers=num_encoder_layers
            )
        ),
        HeterogeneousGaussianLikelihood()
    )
)

# The decoder for the ANP is again MLP-based, and so has the same form as the
# (C)NP decoder. The only required change is to account for the dimensionality
# of the latent representation.
decoder = Chain(
    Materialise(),
    batched_mlp(
        dim_in    =dim_x + 2dim_embedding,
        dim_hidden=dim_embedding,
        dim_out   =num_noise_channels,
        num_layers=num_decoder_layers
    ),
    noise
)

anp = Model(encoder, decoder)
"><pre><span class="pl-c"><span class="pl-c">#</span> The encoder now aggregates three separate representations:</span>
<span class="pl-c"><span class="pl-c">#</span>   (i) the target inputs, like the (C)NP,</span>
<span class="pl-c"><span class="pl-c">#</span>   (ii) a deterministic attentive representation, and</span>
<span class="pl-c"><span class="pl-c">#</span>   (iii) a stochastic global representation.</span>
encoder <span class="pl-k">=</span> <span class="pl-c1">Parallel</span>(
    <span class="pl-c"><span class="pl-c">#</span> First, include the `InputsCoder` to represent the target set inputs.</span>
    <span class="pl-c1">Chain</span>(
        <span class="pl-c1">InputsCoder</span>(),
        <span class="pl-c1">DeterministicLikelihood</span>()
    ),
    <span class="pl-c"><span class="pl-c">#</span> NeuralProcesses.jl uses a transformer-style multi-head architecture for</span>
    <span class="pl-c"><span class="pl-c">#</span> attention. It first embeds the inputs and outputs into a</span>
    <span class="pl-c"><span class="pl-c">#</span> finite-dimensional vector space with an MLP, and applies the attention in</span>
    <span class="pl-c"><span class="pl-c">#</span> the embedding space. The constructor requires the dimensionalities of the</span>
    <span class="pl-c"><span class="pl-c">#</span> inputs and outputs, the desired dimensionality of the embedding, and the</span>
    <span class="pl-c"><span class="pl-c">#</span> number of heads to employ (each head will use a</span>
    <span class="pl-c"><span class="pl-c">#</span> `div(dim_embedding, num_heads)`-dimensional embedding), and the number of</span>
    <span class="pl-c"><span class="pl-c">#</span> layers in the embedding MLPs. As ANPs employ attention for the</span>
    <span class="pl-c"><span class="pl-c">#</span> deterministic representations, this is chained with a</span>
    <span class="pl-c"><span class="pl-c">#</span> `DeterministicLikelihood`.</span>
    <span class="pl-c1">Chain</span>(
        <span class="pl-c1">attention</span>(
            dim_x             <span class="pl-k">=</span>dim_x,
            dim_y             <span class="pl-k">=</span>dim_y,
            dim_embedding     <span class="pl-k">=</span>dim_embedding,
            num_heads         <span class="pl-k">=</span>num_encoder_heads,
            num_encoder_layers<span class="pl-k">=</span>num_encoder_layers
        ),
        <span class="pl-c1">DeterministicLikelihood</span>()
    ),
    <span class="pl-c"><span class="pl-c">#</span> The latent path uses the same form as for the NP.</span>
    <span class="pl-c1">Chain</span>(
        <span class="pl-c1">MLPCoder</span>(
            <span class="pl-c1">batched_mlp</span>(
                dim_in    <span class="pl-k">=</span>dim_x <span class="pl-k">+</span> dim_y,
                dim_hidden<span class="pl-k">=</span>dim_embedding,
                dim_out   <span class="pl-k">=</span>dim_embedding,
                num_layers<span class="pl-k">=</span>num_encoder_layers
            ),
            <span class="pl-c1">batched_mlp</span>(
                dim_in    <span class="pl-k">=</span>dim_embedding,
                dim_hidden<span class="pl-k">=</span>dim_embedding,
                dim_out   <span class="pl-k">=</span><span class="pl-c1">2</span>dim_embedding,
                num_layers<span class="pl-k">=</span>num_encoder_layers
            )
        ),
        <span class="pl-c1">HeterogeneousGaussianLikelihood</span>()
    )
)

<span class="pl-c"><span class="pl-c">#</span> The decoder for the ANP is again MLP-based, and so has the same form as the</span>
<span class="pl-c"><span class="pl-c">#</span> (C)NP decoder. The only required change is to account for the dimensionality</span>
<span class="pl-c"><span class="pl-c">#</span> of the latent representation.</span>
decoder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    <span class="pl-c1">Materialise</span>(),
    <span class="pl-c1">batched_mlp</span>(
        dim_in    <span class="pl-k">=</span>dim_x <span class="pl-k">+</span> <span class="pl-c1">2</span>dim_embedding,
        dim_hidden<span class="pl-k">=</span>dim_embedding,
        dim_out   <span class="pl-k">=</span>num_noise_channels,
        num_layers<span class="pl-k">=</span>num_decoder_layers
    ),
    noise
)

anp <span class="pl-k">=</span> <span class="pl-c1">Model</span>(encoder, decoder)</pre></div>
<h3><a id="user-content-the-convolutional-conditional-neural-process" class="anchor" aria-hidden="true" href="#the-convolutional-conditional-neural-process"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Convolutional Conditional Neural Process</h3>
<p>As a final example, we consider the
<a href="https://openreview.net/forum?id=Skey4eBYPS" rel="nofollow">Convolutional Conditional Neural Process</a>
(ConvCNP).
The key difference between the ConvCNP and other NPs in terms of implementation
is that it encodes the data into an infinite-dimensional function space, rather
than a finite-dimensional vector space.
This is handled in NeuralPeocesses.jl with <code>FunctionalCoder</code>s, which, in
addition to complete coders, also expect <code>Discretisation</code> objects on construction.
Below is an implementation of the
<a href="https://openreview.net/forum?id=Skey4eBYPS" rel="nofollow">Convolutional Conditional Neural Process</a>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# The encoder maps into a function space, which is what `FunctionalCoder`
# indicates.
encoder = FunctionalCoder(
    # We cannot exactly represent a function, so we represent a discretisation
    # of the function instead. We use a discretisation of 64 points per unit.
    # The discretisation will span from the minimal context or target input
    # to the maximal context or target input with a margin of 1 on either side.
    UniformDiscretisation1D(64f0, 1f0),
    Chain(
        # The encoder is given by a so-called set convolution, which directly
        # maps the data set to the discretised functional representation. The
        # data consists of one channel. We also specify a length scale of
        # twice the inter-point spacing of the discretisation. The function
        # space that we map into is a reproducing kernel Hilbert space (RKHS),
        # and the length scale corresponds to the length scale of the kernel of
        # the RKHS. We also append a density channel, which ensures that the
        # encoder is injective.
        set_conv(1, 2 / 64f0; density=true),
        # The encoding will be deterministic. We could also use a stochastic
        # encoding.
        DeterministicLikelihood()
    )
)

decoder = Chain(
    # The decoder first transforms the functional representation with a CNN.
    build_conv(
        4f0,  # Receptive field size
        10,   # Number of layers
        64,   # Number of channels
        points_per_unit =64f0,  # Density of the discretisation
        dimensionality  =1,     # This is a 1D model.
        num_in_channels =2,     # Account for density channel.
        num_out_channels=2      # Produce a mean and standard deviation.
    ),
    # Use another set convolution to map back from the space of the encoding
    # to the space of the data.
    set_conv(2, 2 / 64f0),
    # Predict means and variances.
    HeterogeneousGaussianLikelihood()
)

convcnp = Model(encoder, decoder)
"><pre><span class="pl-c"><span class="pl-c">#</span> The encoder maps into a function space, which is what `FunctionalCoder`</span>
<span class="pl-c"><span class="pl-c">#</span> indicates.</span>
encoder <span class="pl-k">=</span> <span class="pl-c1">FunctionalCoder</span>(
    <span class="pl-c"><span class="pl-c">#</span> We cannot exactly represent a function, so we represent a discretisation</span>
    <span class="pl-c"><span class="pl-c">#</span> of the function instead. We use a discretisation of 64 points per unit.</span>
    <span class="pl-c"><span class="pl-c">#</span> The discretisation will span from the minimal context or target input</span>
    <span class="pl-c"><span class="pl-c">#</span> to the maximal context or target input with a margin of 1 on either side.</span>
    <span class="pl-c1">UniformDiscretisation1D</span>(<span class="pl-c1">64f0</span>, <span class="pl-c1">1f0</span>),
    <span class="pl-c1">Chain</span>(
        <span class="pl-c"><span class="pl-c">#</span> The encoder is given by a so-called set convolution, which directly</span>
        <span class="pl-c"><span class="pl-c">#</span> maps the data set to the discretised functional representation. The</span>
        <span class="pl-c"><span class="pl-c">#</span> data consists of one channel. We also specify a length scale of</span>
        <span class="pl-c"><span class="pl-c">#</span> twice the inter-point spacing of the discretisation. The function</span>
        <span class="pl-c"><span class="pl-c">#</span> space that we map into is a reproducing kernel Hilbert space (RKHS),</span>
        <span class="pl-c"><span class="pl-c">#</span> and the length scale corresponds to the length scale of the kernel of</span>
        <span class="pl-c"><span class="pl-c">#</span> the RKHS. We also append a density channel, which ensures that the</span>
        <span class="pl-c"><span class="pl-c">#</span> encoder is injective.</span>
        <span class="pl-c1">set_conv</span>(<span class="pl-c1">1</span>, <span class="pl-c1">2</span> <span class="pl-k">/</span> <span class="pl-c1">64f0</span>; density<span class="pl-k">=</span><span class="pl-c1">true</span>),
        <span class="pl-c"><span class="pl-c">#</span> The encoding will be deterministic. We could also use a stochastic</span>
        <span class="pl-c"><span class="pl-c">#</span> encoding.</span>
        <span class="pl-c1">DeterministicLikelihood</span>()
    )
)

decoder <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
    <span class="pl-c"><span class="pl-c">#</span> The decoder first transforms the functional representation with a CNN.</span>
    <span class="pl-c1">build_conv</span>(
        <span class="pl-c1">4f0</span>,  <span class="pl-c"><span class="pl-c">#</span> Receptive field size</span>
        <span class="pl-c1">10</span>,   <span class="pl-c"><span class="pl-c">#</span> Number of layers</span>
        <span class="pl-c1">64</span>,   <span class="pl-c"><span class="pl-c">#</span> Number of channels</span>
        points_per_unit <span class="pl-k">=</span><span class="pl-c1">64f0</span>,  <span class="pl-c"><span class="pl-c">#</span> Density of the discretisation</span>
        dimensionality  <span class="pl-k">=</span><span class="pl-c1">1</span>,     <span class="pl-c"><span class="pl-c">#</span> This is a 1D model.</span>
        num_in_channels <span class="pl-k">=</span><span class="pl-c1">2</span>,     <span class="pl-c"><span class="pl-c">#</span> Account for density channel.</span>
        num_out_channels<span class="pl-k">=</span><span class="pl-c1">2</span>      <span class="pl-c"><span class="pl-c">#</span> Produce a mean and standard deviation.</span>
    ),
    <span class="pl-c"><span class="pl-c">#</span> Use another set convolution to map back from the space of the encoding</span>
    <span class="pl-c"><span class="pl-c">#</span> to the space of the data.</span>
    <span class="pl-c1">set_conv</span>(<span class="pl-c1">2</span>, <span class="pl-c1">2</span> <span class="pl-k">/</span> <span class="pl-c1">64f0</span>),
    <span class="pl-c"><span class="pl-c">#</span> Predict means and variances.</span>
    <span class="pl-c1">HeterogeneousGaussianLikelihood</span>()
)

convcnp <span class="pl-k">=</span> <span class="pl-c1">Model</span>(encoder, decoder)</pre></div>
<h2><a id="user-content-state-of-the-package" class="anchor" aria-hidden="true" href="#state-of-the-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>State of the Package</h2>
<p>The package is currently mostly a port from academic code.
There are a still a number of important things to do:</p>
<ul>
<li>
<p><strong>Support for 2D data:</strong>
The package is currently built around 1D tasks.
We plan to add support for 2D data, e.g. images.
This should not require big changes, but it should be implemented carefully.</p>
</li>
<li>
<p><strong>Tests:</strong>
The important components of the package are tested, but test coverage is
nowhere near where it should be.</p>
</li>
<li>
<p><strong>Regression tests:</strong>
For the package, GPU performance is crucial, so regression tests are
necessary.</p>
</li>
<li>
<p><strong>Documentation:</strong>
Documentation needs to be improved.</p>
</li>
</ul>
<h2><a id="user-content-implementation-details" class="anchor" aria-hidden="true" href="#implementation-details"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Implementation Details</h2>
<h3><a id="user-content-automatic-differentiation" class="anchor" aria-hidden="true" href="#automatic-differentiation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Automatic Differentiation</h3>
<p><a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a> is used to automatically
compute gradients.
A number of custom gradients are implemented in <code>src/util.jl</code>.</p>
<h3><a id="user-content-parameter-handling" class="anchor" aria-hidden="true" href="#parameter-handling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Parameter Handling</h3>
<p>The package uses <a href="https://github.com/FluxML/Functors.jl">Functors.jl</a> to handle
parameters, like <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>, and adheres
to <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>'s principles.
This means that <em>only</em> <code>AbstractArray{&lt;:Number}</code>s are parameters.
Nothing else will be trained, not even <code>Float32</code> or <code>Float64</code> scalars.</p>
<p>To not train an array, wrap it with <code>NeuralProcesses.Fixed(array)</code>,
and unwrap it with <code>NeuralProcesses.unwrap(fixed)</code> at runtime.</p>
<h3><a id="user-content-gpu-acceleration" class="anchor" aria-hidden="true" href="#gpu-acceleration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPU Acceleration</h3>
<p>CUDA support for depthwise separable convolutions (<code>DepthwiseConv</code> from
<a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>) is implemented in <code>src/gpu.jl</code>.</p>
<p>Loop fusion can cause issues on the GPU, so oftentimes computations are
unrolled.</p>
</article></div>