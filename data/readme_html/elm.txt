<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-elmjl" class="anchor" aria-hidden="true" href="#elmjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ELM.jl</h1>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a7cba9668456ba56444f44f208575844b698b3a3/68747470733a2f2f7472617669732d63692e6f72672f6c657069736d612f454c4d2e6a6c2e737667"><img src="https://camo.githubusercontent.com/a7cba9668456ba56444f44f208575844b698b3a3/68747470733a2f2f7472617669732d63692e6f72672f6c657069736d612f454c4d2e6a6c2e737667" alt="Travis-CI Build Status" data-canonical-src="https://travis-ci.org/lepisma/ELM.jl.svg" style="max-width:100%;"></a></p>
<p>Extreme Learning Machine in julia</p>
<p>Extreme Learning Machines [1] are a variant of Single-Hidden Layer Feedforward Networks (SLFNs) with a significant departure as their weights aren't iteratively tuned. This boosts the speed of neurals nets heavily.</p>
<p><strong>According to ELM theory:</strong></p>
<p>The hidden node / neuron parameters are not only independent of the training data but also of each other, standard feedforward neural networks with such hidden nodes have universial approximation capability and separation capability. Such hidden nodes and their related mappings are terms ELM random nodes, ELM random neurons or ELM random features.</p>
<p>Unlike conventional learning methods which MUST see the training data before generating the hidden node / neuron parameters, ELM could randomly generate the hidden node / neuron parameters before seeing the training data.</p>
<h3><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h3>
<div class="highlight highlight-source-julia"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>git://github.com/lepisma/ELM.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<h3><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h3>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ELM

X_train <span class="pl-k">=</span> [<span class="pl-c1">1</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span>; <span class="pl-c1">1</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span>; <span class="pl-c1">0</span> <span class="pl-c1">0</span> <span class="pl-c1">0</span>; <span class="pl-c1">1</span> <span class="pl-c1">0</span> <span class="pl-c1">0</span>]
Y_train <span class="pl-k">=</span> [<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>]

elm <span class="pl-k">=</span> <span class="pl-c1">ExtremeLearningMachine</span>(<span class="pl-c1">100</span>) <span class="pl-c"><span class="pl-c">#</span> number of nodes</span>
<span class="pl-c1">fit!</span>(elm, <span class="pl-c1">float</span>(X_train), <span class="pl-c1">float</span>(Y_train))

X_test <span class="pl-k">=</span> [<span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span>; <span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">0</span>]
<span class="pl-c1">predict</span>(elm, <span class="pl-c1">float</span>(X_test))</pre></div>
<h3><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h3>
<p><strong>[1]</strong> G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, "Extreme Learning Machine: A New Learning Scheme of Feedforward Neural
Networks", Proc. Int. Joint Conf.
Neural Networks (IJCNN2004), vol. 2,
IEEE, 2004, pp. 985-990.</p>
<p><strong>License</strong> MIT</p>
</article></div>