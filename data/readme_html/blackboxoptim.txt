<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-blackboxoptimjl" class="anchor" aria-hidden="true" href="#blackboxoptimjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BlackBoxOptim.jl</h1>
<p><a href="https://travis-ci.com/robertfeldt/BlackBoxOptim.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/0995c119d363692485bef0900fce4d17fa7398fb9c0ac6102503db18e51b0027/68747470733a2f2f7472617669732d63692e636f6d2f726f6265727466656c64742f426c61636b426f784f7074696d2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/robertfeldt/BlackBoxOptim.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p><code>BlackBoxOptim</code> is a global optimization package for Julia (<a href="http://julialang.org/" rel="nofollow">http://julialang.org/</a>). It supports both multi- and single-objective optimization problems and is focused on (meta-)heuristic/stochastic algorithms (DE, NES etc) that do NOT require the function being optimized to be differentiable. This is in contrast to more traditional, deterministic algorithms that are often based on gradients/differentiability. It also supports parallel evaluation to speed up optimization for functions that are slow to evaluate.</p>
<h1><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h1>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg; Pkg.add(&quot;BlackBoxOptim&quot;)
"><pre><span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>BlackBoxOptim<span class="pl-pds">"</span></span>)</pre></div>
<p>or latest master directly from github:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg; Pkg.clone(&quot;https://github.com/robertfeldt/BlackBoxOptim.jl&quot;)
"><pre><span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/robertfeldt/BlackBoxOptim.jl<span class="pl-pds">"</span></span>)</pre></div>
<p>from a Julia repl.</p>
<h1><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h1>
<p>To show how the <code>BlackBoxOptim</code> package can be used, let's implement the Rosenbrock function, a classic problem in numerical optimization. We'll assume that you have already installed <code>BlackBoxOptim</code> as described above.</p>
<p>First, we'll load <code>BlackBoxOptim</code> and define the Rosenbrock function (in 2 dimensions):</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using BlackBoxOptim

function rosenbrock2d(x)
  return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
end
"><pre><span class="pl-k">using</span> BlackBoxOptim

<span class="pl-k">function</span> <span class="pl-en">rosenbrock2d</span>(x)
  <span class="pl-k">return</span> (<span class="pl-c1">1.0</span> <span class="pl-k">-</span> x[<span class="pl-c1">1</span>])<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">+</span> <span class="pl-c1">100.0</span> <span class="pl-k">*</span> (x[<span class="pl-c1">2</span>] <span class="pl-k">-</span> x[<span class="pl-c1">1</span>]<span class="pl-k">^</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span>
<span class="pl-k">end</span></pre></div>
<p>We can now call the <code>bboptimize()</code> function, specifying the function to be optimized (here: <code>rosenbrock2d()</code>) and the range of values allowed for each of the dimensions of the input:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="res = bboptimize(rosenbrock2d; SearchRange = (-5.0, 5.0), NumDimensions = 2)
"><pre>res <span class="pl-k">=</span> <span class="pl-c1">bboptimize</span>(rosenbrock2d; SearchRange <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>), NumDimensions <span class="pl-k">=</span> <span class="pl-c1">2</span>)</pre></div>
<p>We get back an optimization result object that we can query to, for example, get the best fitness and solution candidate:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="best_fitness(res) &lt; 0.001        # Fitness is typically very close to zero here...
length(best_candidate(res)) == 2 # We get back a Float64 vector of dimension 2
"><pre><span class="pl-c1">best_fitness</span>(res) <span class="pl-k">&lt;</span> <span class="pl-c1">0.001</span>        <span class="pl-c"><span class="pl-c">#</span> Fitness is typically very close to zero here...</span>
<span class="pl-c1">length</span>(<span class="pl-c1">best_candidate</span>(res)) <span class="pl-k">==</span> <span class="pl-c1">2</span> <span class="pl-c"><span class="pl-c">#</span> We get back a Float64 vector of dimension 2</span></pre></div>
<p><code>BlackBoxOptim</code> will default to using an adaptive differential evolution optimizer in this case and use it to try to locate a solution where both elements can be Floats in the range -5.0:5.0. If you wanted a different range of allowed values for the second dimension of the solution you can specify that with a range of allowed values. In this case you do not need to specify the number of dimensions since that is implicit from the number of ranges supplied:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="bboptimize(rosenbrock2d; SearchRange = [(-5.0, 5.0), (-2.0, 2.0)])
"><pre><span class="pl-c1">bboptimize</span>(rosenbrock2d; SearchRange <span class="pl-k">=</span> [(<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>), (<span class="pl-k">-</span><span class="pl-c1">2.0</span>, <span class="pl-c1">2.0</span>)])</pre></div>
<p>If you want to use a different optimizer that can be specified with the <code>Method</code> keyword. For example, using the standard differential evolution optimizer <code>DE/rand/1/bin</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="bboptimize(rosenbrock2d; SearchRange = (-5.0, 5.0), NumDimensions = 2, Method = :de_rand_1_bin)
"><pre><span class="pl-c1">bboptimize</span>(rosenbrock2d; SearchRange <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>), NumDimensions <span class="pl-k">=</span> <span class="pl-c1">2</span>, Method <span class="pl-k">=</span> <span class="pl-c1">:de_rand_1_bin</span>)</pre></div>
<p>You can give a starting (initial candidate) point for the search when calling <code>bboptimize</code> but beware
that very little checking is done on it so be sure to provide a candidate of the right length and
inside the search space:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="x0 = [1.0, 1.0] # starting point (aka initial candidate)
res = bboptimize(rosenbrock2d, x0; SearchRange = (-5.0, 5.0), NumDimensions = 2, MaxTime = 0.1)
isapprox(best_fitness(res), 0.0)
"><pre>x0 <span class="pl-k">=</span> [<span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>] <span class="pl-c"><span class="pl-c">#</span> starting point (aka initial candidate)</span>
res <span class="pl-k">=</span> <span class="pl-c1">bboptimize</span>(rosenbrock2d, x0; SearchRange <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>), NumDimensions <span class="pl-k">=</span> <span class="pl-c1">2</span>, MaxTime <span class="pl-k">=</span> <span class="pl-c1">0.1</span>)
<span class="pl-c1">isapprox</span>(<span class="pl-c1">best_fitness</span>(res), <span class="pl-c1">0.0</span>)</pre></div>
<p>Note that the <code>rosenbrock2d()</code> function is quite easy to optimize. Even a random search will come close if we give it more time:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="bboptimize(rosenbrock2d; SearchRange = (-5.0, 5.0), NumDimensions = 2, Method = :random_search, MaxTime = 10.0)
"><pre><span class="pl-c1">bboptimize</span>(rosenbrock2d; SearchRange <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>), NumDimensions <span class="pl-k">=</span> <span class="pl-c1">2</span>, Method <span class="pl-k">=</span> <span class="pl-c1">:random_search</span>, MaxTime <span class="pl-k">=</span> <span class="pl-c1">10.0</span>)</pre></div>
<p>But if we optimize the same rosenbrock function in, say, 30 dimensions that will be very hard for a random searcher while sNES or DE can find good solutions if we give them some time. We can compare optimizers using the <code>compare_optimizers()</code> function:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function rosenbrock(x)
  return( sum( 100*( x[2:end] .- x[1:end-1].^2 ).^2 .+ ( x[1:end-1] .- 1 ).^2 ) )
end

res = compare_optimizers(rosenbrock; SearchRange = (-5.0, 5.0), NumDimensions = 30, MaxTime = 3.0);
"><pre><span class="pl-k">function</span> <span class="pl-en">rosenbrock</span>(x)
  <span class="pl-c1">return</span>( <span class="pl-c1">sum</span>( <span class="pl-c1">100</span><span class="pl-k">*</span>( x[<span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">end</span>] <span class="pl-k">.-</span> x[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>]<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span> )<span class="pl-k">.^</span><span class="pl-c1">2</span> <span class="pl-k">.+</span> ( x[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">.-</span> <span class="pl-c1">1</span> )<span class="pl-k">.^</span><span class="pl-c1">2</span> ) )
<span class="pl-k">end</span>

res <span class="pl-k">=</span> <span class="pl-c1">compare_optimizers</span>(rosenbrock; SearchRange <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>), NumDimensions <span class="pl-k">=</span> <span class="pl-c1">30</span>, MaxTime <span class="pl-k">=</span> <span class="pl-c1">3.0</span>);</pre></div>
<p>You can find more examples of using <code>BlackBoxOptim</code> in <a href="examples">the examples directory</a>.</p>
<h1><a id="user-content-multi-objective-optimization" class="anchor" aria-hidden="true" href="#multi-objective-optimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multi-objective optimization</h1>
<p>Multi-objective evaluation is supported by the BorgMOEA algorithm. Your fitness function should return a tuple of the objective values and you should indicate the fitness scheme to be (typically) Pareto fitness and specify the number of objectives. Otherwise the use is similar, here we optimize the Schaffer1 function:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="fitness_schaffer1(x) = (sum(abs2, x), sum(abs2, x .- 2.0))
res = bboptimize(fitness_schaffer1; Method=:borg_moea,
            FitnessScheme=ParetoFitnessScheme{2}(is_minimizing=true),
            SearchRange=(-10.0, 10.0), NumDimensions=3, ϵ=0.05,
            MaxSteps=50000, TraceInterval=1.0, TraceMode=:verbose);
"><pre><span class="pl-en">fitness_schaffer1</span>(x) <span class="pl-k">=</span> (<span class="pl-c1">sum</span>(abs2, x), <span class="pl-c1">sum</span>(abs2, x <span class="pl-k">.-</span> <span class="pl-c1">2.0</span>))
res <span class="pl-k">=</span> <span class="pl-c1">bboptimize</span>(fitness_schaffer1; Method<span class="pl-k">=</span><span class="pl-c1">:borg_moea</span>,
            FitnessScheme<span class="pl-k">=</span><span class="pl-c1">ParetoFitnessScheme</span><span class="pl-c1">{2}</span>(is_minimizing<span class="pl-k">=</span><span class="pl-c1">true</span>),
            SearchRange<span class="pl-k">=</span>(<span class="pl-k">-</span><span class="pl-c1">10.0</span>, <span class="pl-c1">10.0</span>), NumDimensions<span class="pl-k">=</span><span class="pl-c1">3</span>, ϵ<span class="pl-k">=</span><span class="pl-c1">0.05</span>,
            MaxSteps<span class="pl-k">=</span><span class="pl-c1">50000</span>, TraceInterval<span class="pl-k">=</span><span class="pl-c1">1.0</span>, TraceMode<span class="pl-k">=</span><span class="pl-c1">:verbose</span>);</pre></div>
<p><code>pareto_frontier(res)</code> would give a vector of all Pareto-optimal solutions and corresponding fitness values.
If we simply want to get one individual with the best aggregated fitness:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="bs = best_candidate(res)
bf = best_fitness(res)
"><pre>bs <span class="pl-k">=</span> <span class="pl-c1">best_candidate</span>(res)
bf <span class="pl-k">=</span> <span class="pl-c1">best_fitness</span>(res)</pre></div>
<p>By default, the aggregated fitness is the sum of the individual objective values, but this could be changed when declaring the fitness scheme, e.g.
the weighted sum with weights <code>(0.3, 0.7)</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="weightedfitness(f) = f[1]*0.3 + f[2]*0.7

...
    FitnessScheme=ParetoFitnessScheme{2}(is_minimizing=true, aggregator=weightedfitness)
...
"><pre><span class="pl-en">weightedfitness</span>(f) <span class="pl-k">=</span> f[<span class="pl-c1">1</span>]<span class="pl-k">*</span><span class="pl-c1">0.3</span> <span class="pl-k">+</span> f[<span class="pl-c1">2</span>]<span class="pl-k">*</span><span class="pl-c1">0.7</span>

<span class="pl-k">...</span>
    FitnessScheme<span class="pl-k">=</span><span class="pl-c1">ParetoFitnessScheme</span><span class="pl-c1">{2}</span>(is_minimizing<span class="pl-k">=</span><span class="pl-c1">true</span>, aggregator<span class="pl-k">=</span>weightedfitness)
<span class="pl-k">...</span></pre></div>
<p>Of course, once the Pareto set (<code>pareto_frontier(res)</code>) is found, one
can apply different criteria to filter the solutions.
For example, to find the solution with the minimal first objective:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="pf = pareto_frontier(res)
best_obj1, idx_obj1 = findmin(map(elm -&gt; fitness(elm)[1], pf))
bo1_solution = params(pf[idx_obj1]) # get the solution candidate itself...
"><pre>pf <span class="pl-k">=</span> <span class="pl-c1">pareto_frontier</span>(res)
best_obj1, idx_obj1 <span class="pl-k">=</span> <span class="pl-c1">findmin</span>(<span class="pl-c1">map</span>(elm <span class="pl-k">-&gt;</span> <span class="pl-c1">fitness</span>(elm)[<span class="pl-c1">1</span>], pf))
bo1_solution <span class="pl-k">=</span> <span class="pl-c1">params</span>(pf[idx_obj1]) <span class="pl-c"><span class="pl-c">#</span> get the solution candidate itself...</span></pre></div>
<p>or to use different weighted sums:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="weighedfitness(f, w) = f[1]*w + f[2]*(1.0-w)
weight = 0.4 # Weight on first objective, so second objective will have weight 1-0.4=0.6
best_wfitness, idx = findmin(map(elm -&gt; weighedfitness(fitness(elm), weight), pf))
bsw = params(pf[idx])
"><pre><span class="pl-en">weighedfitness</span>(f, w) <span class="pl-k">=</span> f[<span class="pl-c1">1</span>]<span class="pl-k">*</span>w <span class="pl-k">+</span> f[<span class="pl-c1">2</span>]<span class="pl-k">*</span>(<span class="pl-c1">1.0</span><span class="pl-k">-</span>w)
weight <span class="pl-k">=</span> <span class="pl-c1">0.4</span> <span class="pl-c"><span class="pl-c">#</span> Weight on first objective, so second objective will have weight 1-0.4=0.6</span>
best_wfitness, idx <span class="pl-k">=</span> <span class="pl-c1">findmin</span>(<span class="pl-c1">map</span>(elm <span class="pl-k">-&gt;</span> <span class="pl-c1">weighedfitness</span>(<span class="pl-c1">fitness</span>(elm), weight), pf))
bsw <span class="pl-k">=</span> <span class="pl-c1">params</span>(pf[idx])</pre></div>
<h1><a id="user-content-configurable-options" class="anchor" aria-hidden="true" href="#configurable-options"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Configurable Options</h1>
<p>The section above described the basic API for the <code>BlackBoxOptim</code> package. There is a large number of different optimization algorithms that you can select with the <code>Method</code> keyword (<code>adaptive_de_rand_1_bin</code>, <code>adaptive_de_rand_1_bin_radiuslimited</code>, <code>separable_nes</code>, <code>xnes</code>, <code>de_rand_1_bin</code>, <code>de_rand_2_bin</code>, <code>de_rand_1_bin_radiuslimited</code>, <code>de_rand_2_bin_radiuslimited</code>, <code>random_search</code>, <code>generating_set_search</code>, <code>probabilistic_descent</code>, <code>borg_moea</code>).</p>
<p>In addition to the <code>Method</code> parameter, there are many other parameters you can change. Some key ones are:</p>
<ul>
<li><code>MaxTime</code>: For how long can the optimization run? Defaults to <code>false</code> which means that number of iterations is the given budget, rather than time.</li>
<li><code>MaxFuncEvals</code>: How many evaluations that are allowed of the function being optimized.</li>
<li><code>TraceMode</code>: How optimization progress should be displayed (<code>:silent</code>, <code>:compact</code>, <code>:verbose</code>). Defaults to <code>:compact</code> that outputs current number of fitness evaluations and best value each <code>TraceInterval</code> seconds.</li>
<li><code>PopulationSize</code>: How large is the initial population for population-based optimizers? Defaults to <code>50</code>.</li>
<li><code>TargetFitness</code>. Allows to specify the value of the best fitness for a given problem. The algorithm stops as soon as the distance between the current <code>best_fitness()</code> and <code>TargetFitness</code> is less than <code>FitnessTolerance</code>.
This list is not complete though, please refer to the <code>examples</code> and <code>tests</code> directories for additional examples.</li>
</ul>
<h1><a id="user-content-state-of-the-library" class="anchor" aria-hidden="true" href="#state-of-the-library"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>State of the Library</h1>
<h2><a id="user-content-existing-optimizers" class="anchor" aria-hidden="true" href="#existing-optimizers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Existing Optimizers</h2>
<ul>
<li>Natural Evolution Strategies:
<ul>
<li>Separable NES: <code>separable_nes</code></li>
<li>Exponential NES: <code>xnes</code></li>
<li>Distance-weighted Exponential NES: <code>dxnes</code></li>
</ul>
</li>
<li>Differential Evolution optimizers, 5 different:
<ul>
<li>Adaptive DE/rand/1/bin: <code>adaptive_de_rand_1_bin</code></li>
<li>Adaptive DE/rand/1/bin with radius limited sampling: <code>adaptive_de_rand_1_bin_radiuslimited</code></li>
<li>DE/rand/1/bin: <code>de_rand_1_bin</code></li>
<li>DE/rand/1/bin with radius limited sampling (a type of trivial geography): <code>de_rand_1_bin_radiuslimited</code></li>
<li>DE/rand/2/bin: <code>de_rand_2_bin</code></li>
<li>DE/rand/2/bin with radius limited sampling (a type of trivial geography): <code>de_rand_2_bin_radiuslimited</code></li>
</ul>
</li>
<li>Direct search:
<ul>
<li>Generating set search:
<ul>
<li>Compass/coordinate search: <code>generating_set_search</code></li>
<li>Direct search through probabilistic descent: <code>probabilistic_descent</code></li>
</ul>
</li>
</ul>
</li>
<li>Resampling Memetic Searchers:
<ul>
<li>Resampling Memetic Search (RS): <code>resampling_memetic_search</code></li>
<li>Resampling Inheritance Memetic Search (RIS): <code>resampling_inheritance_memetic_search</code></li>
</ul>
</li>
<li>Stochastic Approximation:
<ul>
<li>Simultaneous Perturbation Stochastic Approximation (SPSA): <code>simultaneous_perturbation_stochastic_approximation</code></li>
</ul>
</li>
<li>RandomSearch (to compare to): <code>random_search</code></li>
</ul>
<p>For multi-objective optimization only the <a href="http://borgmoea.org/" rel="nofollow">BorgMOEA</a> (<code>borg_moea</code>) is supported but it is a good one. :)</p>
<h1><a id="user-content-multithreaded-and-parallel-function-evaluation" class="anchor" aria-hidden="true" href="#multithreaded-and-parallel-function-evaluation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multithreaded and Parallel Function Evaluation</h1>
<p><strong>NB!</strong> There are problems with the multi-threaded evaluation on Julia 1.6 and later. We will be investigating this and hope to fix in a future release. For now the related tests have been deactivated. Sorry for the inconvenience.</p>
<p>For some (slow) functions being optimized and if you have a multi-core CPU you can gain performance by using multithreaded or parallel evaluation. This typically requires an optimization algorithm that evaluates many candidate points in one batch. The NES family (<code>xnes</code>, <code>dxnes</code> etc), for example. See the file</p>
<p><a href="examples/rosenbrock_parallel.jl">examples/rosenbrock_parallel.jl</a></p>
<p>for one example of this. On Julia 1.3 and later it is typically better to use multithreading see the file</p>
<p><a href="examples/multithreaded_optimization.jl">examples/multithreaded_optimization.jl</a></p>
<p>for some examples.</p>
<h1><a id="user-content-guide-to-selecting-an-optimizer" class="anchor" aria-hidden="true" href="#guide-to-selecting-an-optimizer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Guide to selecting an optimizer</h1>
<p>In our experiments the radius limited DE's perform better than the classic <code>de_rand_1_bin DE</code> in almost all cases. And combining it with adaptive setting of the weights makes it even better. So for now <code>adaptive_de_rand_1_bin_radiuslimited()</code> is our recommended "goto" optimizer. However, the difference between the top performing DE's is slight.</p>
<p>Some NES variants (separable or dx) can sometimes beat the DE optimizers in the tests we have done. But xnes and dxnes are often very slow and while the separable NES isn't it is less robust. So we don't recommend it as a default, robust choice.</p>
<p>We maintain a <a href="examples/benchmarking/latest_toplist.csv">list of optimizers ranked by performance</a> when tested on a large set of problems. From the list we can see that the adaptive differential evolution optimizers (<code>adaptive_de_rand_1_bin</code> and/or <code>adaptive_de_rand_1_bin_radiuslimited</code>) are typically on top when it comes to mean rank. The <code>generating_set_search</code> often gives best results (its <code>MedianLogTimesWorseFitness</code> is typically in the range <code>0.3</code>-<code>0.6</code>, which means its median fitness value is <code>10^0.3=2.0</code> to <code>10^0.6=4.0</code> times worse than the best fitness found on a problem) and is faster (ranked first on run time, typically) but it is not as robust as the DE optimizers and thus is ranked lower on mean rank (per problem).</p>
<p>Overall we recommend one of the DE optimizers as a generally good choice since their runtime is often good and they are robust and works well both for simpler, separable problems as well as for more complex ones. They also tend to scale better to high-dimensional settings. Of course, optimizer performance varies depending on the problem and the dimensionality so YMMV.</p>
</article></div>