<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><strong>warning package under development:</strong> it will break. But renewed shall be the code that was broken, the crashless again shall compile.</p>
<h1><a id="user-content-download-the-datasets" class="anchor" aria-hidden="true" href="#download-the-datasets"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Download the datasets</h1>
<p>After cloning it, go into the <code>StochOpt.jl</code> repository and download the datasets by running</p>
<div class="highlight highlight-source-shell"><pre>./download_datasets.sh</pre></div>
<p>If it crashes, data can be downloaded manually <a href="https://partage.mines-telecom.fr/index.php/s/9MreP5y6evFWyJP" rel="nofollow">here</a>. Then, datasets <code>.jld</code> files should be placed in the directory <code>./data</code>.</p>
<h1><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dependencies</h1>
<p>Launch the  Julia REPL and press <code>]</code> to enter the package REPL mode, then install the required packages with</p>
<div class="highlight highlight-source-shell"><pre>(v1.0) pkg<span class="pl-k">&gt;</span> add JLD
(v1.0) pkg<span class="pl-k">&gt;</span> add Plots
(v1.0) pkg<span class="pl-k">&gt;</span> add StatsBase
(v1.0) pkg<span class="pl-k">&gt;</span> add Match
(v1.0) pkg<span class="pl-k">&gt;</span> add Combinatorics
(v1.0) pkg<span class="pl-k">&gt;</span> add Formatting
(v1.0) pkg<span class="pl-k">&gt;</span> add LaTeXStrings
(v1.0) pkg<span class="pl-k">&gt;</span> add PyPlot
(v1.0) pkg<span class="pl-k">&gt;</span> add Distributions
...
(v1.0) pkg<span class="pl-k">&gt;</span> add Distributed</pre></div>
<p>if any problem with PyPlot, try the following manipulation</p>
<div class="highlight highlight-source-shell"><pre>$ julia
julia<span class="pl-k">&gt;</span> ENV[<span class="pl-s"><span class="pl-pds">"</span>PYTHON<span class="pl-pds">"</span></span>]=<span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
julia<span class="pl-k">&gt;</span> ]
(v1.0) pkg<span class="pl-k">&gt;</span> build PyCall</pre></div>
<h1><a id="user-content-stochopt" class="anchor" aria-hidden="true" href="#stochopt"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>StochOpt</h1>
<p>A suite of stochastic optimization methods for solving the empirical risk minimization problem. <br></p>
<h1><a id="user-content-demo" class="anchor" aria-hidden="true" href="#demo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Demo</h1>
<p>For a simple demo of the use of the package</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>test<span class="pl-k">/</span>demo<span class="pl-k">.</span>jl</pre></div>
<p><em>WARNING : DEBUG THIS</em></p>
<p>For a demo of the SVRG2 type methods from [1]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>test<span class="pl-k">/</span>test<span class="pl-k">/</span>demo_SVRG2<span class="pl-k">.</span>jl</pre></div>
<p>For a demo of the BFGS and accelerated BFGS methods from [2]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>test<span class="pl-k">/</span>demo_BFGS<span class="pl-k">.</span>jl</pre></div>
<p>For a demo of SAGA with optimized probabilities from [4]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>test<span class="pl-k">/</span>demo_SAGA<span class="pl-k">.</span>jl</pre></div>
<p>For a demo of SAGA nice from [5]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>test<span class="pl-k">/</span>demo_SAGA_nice<span class="pl-k">.</span>jl</pre></div>
<h1><a id="user-content-repeating-paper-results" class="anchor" aria-hidden="true" href="#repeating-paper-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Repeating paper results</h1>
<h2><a id="user-content-tracking-the-gradients-using-the-hessian-a-new-look-at-variance-reducing-stochastic-methods" class="anchor" aria-hidden="true" href="#tracking-the-gradients-using-the-hessian-a-new-look-at-variance-reducing-stochastic-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><em>Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods</em></h2>
<p>To re-generate all of the experiments from [1]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>repeat_SVRG2_paper_experiments<span class="pl-k">.</span>jl</pre></div>
<h2><a id="user-content-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization" class="anchor" aria-hidden="true" href="#accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><em>Accelerated stochastic matrix inversion: general theory and speeding up BFGS rules for faster second-order optimization</em></h2>
<p>To re-generate all of the experiments from [2]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>repeat_BFGS_accel_paper_results<span class="pl-k">.</span>jl</pre></div>
<h2><a id="user-content-stochastic-quasi-gradient-methods-variance-reduction-via-jacobian-sketching" class="anchor" aria-hidden="true" href="#stochastic-quasi-gradient-methods-variance-reduction-via-jacobian-sketching"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><em>Stochastic Quasi-Gradient Methods: Variance Reduction via Jacobian Sketching</em></h2>
<p>To re-generate the experiments from Section 6.1 of [4]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>compare_SAGA_importance_opt_Lis<span class="pl-k">.</span>jl</pre></div>
<p>To re-generate the experiments from Section 6.1 of [4]</p>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>test_optimal_minibatch_SAGA_nice<span class="pl-k">.</span>jl</pre></div>
<h2><a id="user-content-optimal-mini-batch-and-step-sizes-for-saga" class="anchor" aria-hidden="true" href="#optimal-mini-batch-and-step-sizes-for-saga"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><em>Optimal mini-batch and step sizes for SAGA</em></h2>
<ul>
<li>To re-generate the experiments from Section 5.1 &amp; 5.2 of [5] (~1h 30min)</li>
</ul>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">-</span>p <span class="pl-k">&lt;</span>number_of_processors<span class="pl-k">&gt;</span> <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_1_and_2_parallel<span class="pl-k">.</span>jl all_problems</pre></div>
<p>setting <code>all_problems</code> to <code>false</code> to run the code only on the first problem, unscaled <em>uniform</em> synthetic dataset with $\lambda =10^{-1}$, (~XXXXXmin) or to <code>true</code> to run it on all of them (~XXXXXXXXh XXXXXmin).</p>
<ul>
<li>To re-generate experiments from Section 5.3 of [5]</li>
</ul>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">-</span>p <span class="pl-k">&lt;</span>number_of_processors<span class="pl-k">&gt;</span> <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_3_parallel<span class="pl-k">.</span>jl all_problems</pre></div>
<p>setting <code>all_problems</code> to <code>false</code> to run the code only on the first problem, scaled <em>ijcnn1_full</em> with $\lambda =10^{-1}$, (~1min) or to <code>true</code> to run it on all of them (~1h 30min).</p>
<ul>
<li>To re-generate the experiments from Section 5.4 of [5]</li>
</ul>
<div class="highlight highlight-source-julia"><pre>julia <span class="pl-k">-</span>p <span class="pl-k">&lt;</span>number_of_processors<span class="pl-k">&gt;</span> <span class="pl-k">./</span>repeat_paper_experiments<span class="pl-k">/</span>repeat_optimal_minibatch_step_sizes_SAGA_paper_experiment_4_parallel<span class="pl-k">.</span>jl all_problems</pre></div>
<p>setting <code>all_problems</code> to <code>false</code> to run the code only on the first problem, scaled <em>ijcnn1_full</em> with $\lambda =10^{-1}$, (~2min) or to <code>true</code> to run it on all of them (~XXh XXmin).</p>
<h2><a id="user-content-towards-closing-the-gap-between-the-theory-and-practice-of-svrg" class="anchor" aria-hidden="true" href="#towards-closing-the-gap-between-the-theory-and-practice-of-svrg"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><em>Towards closing the gap between the theory and practice of SVRG</em></h2>
<ul>
<li>To re-generate experiments of [6], enter the <code>StochOpt.jl</code> folder and run</li>
</ul>
<div class="highlight highlight-source-shell"><pre>./repeat_paper_experiments/theory_practice_svrg.sh <span class="pl-k">&lt;</span>path_to_julia<span class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>absolute_path_to_StochOpt.jl<span class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>exp_number<span class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>all_problems<span class="pl-k">&gt;</span> <span class="pl-k">&lt;</span>number_of_processors<span class="pl-k">&gt;</span></pre></div>
<p>setting <code>exp_number</code> to 1a, 1b, 1c, 2a or 2b to select the desired experiment, <code>all_problems</code> to <code>false</code> to run the code only on the first problem, scaled <em>ijcnn1_full</em> with $\lambda =10^{-1}$, (~10 to 20min) or to <code>true</code> to run it on all the eight problems (several hours). If you run an experiment on all the problems, set <code>number_of_processors</code> (1 by default) to 8 in order to run them in parallel.</p>

<h1><a id="user-content-methods-implemented" class="anchor" aria-hidden="true" href="#methods-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Methods implemented</h1>
<p>SVRG, the original SVRG algorithm. <br>
SVRG2, which tracks the gradients using the full Hessian. <br>
2D, which tracks the gradients using the diagonal of the Hessian. <br>
2Dsec, which tracks the gradients using the robust secant equation. <br>
SVRG2emb, which tracks the gradients using a low-rank approximation of the Hessians. <br>
CM, which tracks the gradients using the low-rank curvature matching approximation of the Hessian. <br>
AM, which uses the low-rank action matching approximation of the Hessian. <br>
BFGS, the standard, full memory BFGS method. <br>
BFGS_accel, an accelerated BFGS method. <br>
SAGA, stochastic average gradient descent, with several options of samplings (including optimal probabilities). <br>
SAGA nice, mini-batch version of SAGA with nice sampling. <br></p>
<p>More details on the methods can be found in [1], [2], [4] and [5] <br></p>
<h1><a id="user-content-code-philosophy" class="anchor" aria-hidden="true" href="#code-philosophy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code Philosophy</h1>
<p>To provide en environment where competing stochastic methods can be compared on equal footing. This is why all methods are called by the same wrapper function "minimizeFunc" (or it's extension minimizeFunc_grid_stepsize). All performance measures such as time taken, test error or epochs are calculated by these wrapper functions. Each new method need only supply a <em>stepmethod</em> and a <em>bootmethod</em>. The stepmethod returns an update vector d which is then added to x_k to give the next iterate x_{k+1}. The bootmethod is called once to initialize the method.</p>
<h1><a id="user-content-adding-more-data" class="anchor" aria-hidden="true" href="#adding-more-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding more data</h1>
<p>To test a new data set, download the raw data of a binary classification from LIBSVM [3] and place it in the folder ./data.
Then replace "liver-disorders" in the code <em>src/load_new_LIBSVM_data.jl</em> and execute. In other words, run the code</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">include</span>(<span class="pl-s"><span class="pl-pds">"</span>dataLoad.jl<span class="pl-pds">"</span></span>)
<span class="pl-c1">initDetails</span>()

datasets <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>liver-disorders<span class="pl-pds">"</span></span>]
<span class="pl-k">for</span>  dataset <span class="pl-k">in</span> datasets
<span class="pl-c1">transformDataJLD</span>(dataset)
X,y <span class="pl-k">=</span> <span class="pl-c1">loadDataset</span>(dataset)
<span class="pl-c1">showDetails</span>(dataset)
<span class="pl-k">end</span></pre></div>
<p>where "liver-disorders" has been replaced with the name of the new raw data file.</p>
<h1><a id="user-content-adding-new-loss-functions" class="anchor" aria-hidden="true" href="#adding-new-loss-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding new loss functions</h1>
<p>to include new objective function, see load_logistic.jl and copy the same structure</p>
<h1><a id="user-content-adding-new-methods" class="anchor" aria-hidden="true" href="#adding-new-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding new methods</h1>
<p>to include a new method X, you need to write a descent_X.jl and boot_X.jl function. See descent_grad and boot_grad for an example. I also recommend writing your type and including it in StochOpt or using one of the types there defined already.</p>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h1>
<p>[1] <em>Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods</em> <br>
Robert M. Gower, Nicolas Le Roux and Francis Bach.
To appear in AISTATS 2018</p>
<p>[2] <em>Accelerated stochastic matrix inversion: general theory and speeding up BFGS rules for faster second-order optimization</em> <br>
Robert M. Gower, Filip Hanzely, P. Richtárik and S. Stich.
arXiv:1801.05490, 2018</p>
<p>[3] <em>LIBSVM : a library for support vector machines.</em> <br>
Chih-Chung Chang and Chih-Jen Lin, ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011. 
Software available at <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm" rel="nofollow">http://www.csie.ntu.edu.tw/~cjlin/libsvm</a></p>
<p>[4] <em>Stochastic Quasi-Gradient Methods:
Variance Reduction via Jacobian Sketching</em> <br>
Robert M. Gower, Peter Richtárik, Francis Bach</p>
<p>[5] <em>Optimal mini-batch and step sizes for SAGA</em> <br>
Nidham Gazagnadou, Robert M. Gower and Joseph Salmon.
arXiv:1902.00071, 2019</p>
<p>[6] <em>Towards closing the gap between the theory and practice of SVRG</em> <br>
Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis Bach, Robert M. Gower. arXiv:1908.02725, 2019</p>
<p>For up-to-date references see <a href="https://perso.telecom-paristech.fr/rgower/publications.html" rel="nofollow">https://perso.telecom-paristech.fr/rgower/publications.html</a></p>
<h1><a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TODO</h1>
<ul>
<li>change output type to have a testerrors field. Think where best to load a test problem. Probably outside of minimizeFunc. Where best to place code for test_error ?  Probably best to start a new src file for error calculations? or testing related things?</li>
<li>Implement the calculation of the Jacobian.</li>
<li>The code for SVRG2 type methods (AMprev, AMgauss, CMprev, CMgauss) should have their own type. Right now they are definied using the generic Method type, which is why the code for these functions is illegible.</li>
</ul>
</article></div>