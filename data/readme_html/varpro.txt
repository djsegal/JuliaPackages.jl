<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-varpro-variable-projection-for-nonlinear-least-squares-problems" class="anchor" aria-hidden="true" href="#varpro-variable-projection-for-nonlinear-least-squares-problems"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Varpro: Variable Projection for Nonlinear Least Squares Problems</h1>
<p>Model fitting is often approached as an optimization problem where the
sum of the model errors are minimized by optimizing the model
parameters.  If some of the model parameters are non-linear, then a
non-linear optimization algorithm must be used.  This can be
inefficient if some of the parameters are linear.</p>
<p>The Varpro algorithm recasts the problem so that only the nonlinear
parameters need to be considered by the nonlinear optimizer.  For more
details see the references below and the embedded docs in the source
code.</p>
<p>This Julia code is a translation and extension the of the Matlab code
that can be found <a href="http://www.cs.umd.edu/~oleary/software/varpro.m" rel="nofollow">here</a>.
The extensions involve handling complex inputs and a complex model (although
the optimization objective function remains real since the objective is
essentially the L2 norm of the residual (error) vector).</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<p>The best way to learn how to use varpro is to read reference [4].  The usage
in Julia differs somewhat from the MATLAB version.  With this version, we
first set up a FitContext by calling the constructor as in the following</p>
<pre><code>ctx = FitContext(y, t, w, x_init, n, ind, f_exp, g_exp)
</code></pre>
<p>All of these are required parameters.  The vector <strong>y</strong> is the data we wish to
fit sampled at the times <strong>t</strong>.  The vector <strong>w</strong> is a weight vector for selectively
weighting the data. The vector <strong>x_init</strong> is the starting guess.  Note that both
<strong>y</strong> and <strong>x_init</strong> can be either real or complex, but they both must share the same
type.  The integer <strong>n</strong> is the number of basis functions (ie the number of linear
parameters).  The matrix <strong>ind</strong> specifies the structure of the dphi matrix (see [4]).
The functions <strong>f_exp</strong> and <strong>g_exp</strong> calculate the phi and dphi matrices.</p>
<p>The following is a complete example of fitting the H1 strain ringdown values of the
recently discovered gravity wave GW150914 [5].</p>
<pre><code>using Varpro
using PyPlot
using DelimitedFiles

function f_exp(alpha, ctx)
    for j = 1:ctx.n
        for i in 1:ctx.m
            ctx.phi[i, j] = exp(-alpha[j] * ctx.t[i])
        end
    end
    ctx.phi
end

function g_exp(alpha, ctx)
    for i in 1:ctx.m
        ctx.dphi[i, :] = -ctx.t[i] * ctx.phi[i, :]
    end
    ctx.dphi
end

""" Fit n complex exponentials to the measured data """
function exp_fit(n, y, t)
    w = ones(length(t))
    ind = [collect(1:n)'; collect(1:n)']
    x_init = complex.(0.1*rand(n), 2.0*rand(n))  
    ctx = FitContext(y, t, w, x_init, n, ind, f_exp, g_exp)
    (alpha, c, wresid, resid_norm, y_est, regression) = varpro(ctx)
end

function main()
    h1 = readdlm("h1_whitened.txt")
    t = h1[:, 1]
    y = complex.(h1[:, 2])  # must be complex to match x_init
    x, c, r, r_norm, y_est, reg = exp_fit(6, y, t)
    println("Norm of residual error: ", r_norm)
    plot(t, real(y), "o", label="measured H1 strain")
    plot(t, real(y_est), label="modeled H1 strain")
    xlabel("Time")
    ylabel("Strain")
    title("H1 Ringdown Model")
    legend(loc="upper right")
    savefig("modeled_GW150914_strain.png")
end

main()
</code></pre>
<p>The above code produces the following figure:</p>
<p><a target="_blank" rel="noopener noreferrer" href="modeled_GW150914_strain.png"><img src="modeled_GW150914_strain.png" alt="alt-text" title="Greetings Programs!" style="max-width:100%;"></a></p>
<h2><a id="user-content-limitations" class="anchor" aria-hidden="true" href="#limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limitations</h2>
<p>Only supported in Julia 0.7 (dev)</p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2>
<p>[1] Golub, G.H., Pereyra, V.: "The differentiation of pseudoinverses and
nonlinear least squares problems whose variables separate". SIAM Journal
on Numerical Analysis 10, pp 413-432 (1973)</p>
<p>[2] Golub, G.H., Pereyra, V.: "Separable nonlinear least squares: The variable
projection method and its applications". Inverse Problems 19 (2), R1â€“R26 (2003)</p>
<p>[3] Pereyra, V., Scherer, eds:  "Exponential Data Fitting and its Applications"
Bentham Books, ISBN: 978-1-60805-048-2 (2010)</p>
<p>[4] Dianne P. O'Leary, Bert W. Rust: "Variable projection for nonlinear least squares
problems".  Computational Optimization and Applications April 2013, Volume 54,
Issue 3, pp 579-593  Available <a href="http://www.cs.umd.edu/~oleary/software/varpro.pdf" rel="nofollow">here</a></p>
<p>[5] B. P. Abbott el. al. "ASTROPHYSICAL IMPLICATIONS OF THE BINARY BLACK HOLE MERGER GW150914"
The Astrophysical Journal Letters, Volume 818, Number 2</p>
</article></div>