<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-adaptivedesignoptimization" class="anchor" aria-hidden="true" href="#adaptivedesignoptimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AdaptiveDesignOptimization</h1>
<p>This package is a grid-based approach for performing Bayesian adaptive design optimization. After each observation, the optimizer chooses an experimental design that maximizes mutual information between model parameters and design parameters. In so doing, the optimizer selects designs that minimize the variance in the posterior distribution of model parameters.</p>
<h1><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h1>
<p>In this example, we will optimize a decision making experiment for the model called Transfer of Attention Exchange (TAX; Birnbaum, 2008). Additional examples can be found in the folder titled Examples.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using AdaptiveDesignOptimization, Random, UtilityModels, Distributions
include(&quot;TAX_Model.jl&quot;)
Random.seed!(25974)
"><pre><span class="pl-k">using</span> AdaptiveDesignOptimization, Random, UtilityModels, Distributions
<span class="pl-c1">include</span>(<span class="pl-s"><span class="pl-pds">"</span>TAX_Model.jl<span class="pl-pds">"</span></span>)
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">25974</span>)</pre></div>
<h2><a id="user-content-define-model" class="anchor" aria-hidden="true" href="#define-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Define Model</h2>
<p>The model object contains a log likelihood function and optional prior distributions. Unless an array of distribution objects is passed as <code>prior</code>, uniform distributions will be used by default. Arguments in the log likelihood function must be ordered as follows:</p>
<ul>
<li>loglike(model_parameters..., design_parameters..., data..., args...; kwargs...)</li>
</ul>
<p><code>args...</code> and <code>kwargs...</code> are optional arguments that may be preloaded through the <code>Model</code> constructor. Enter</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="? Model
"><pre>? Model</pre></div>
<p>for additional details. The likelihood function for the TAX model (see <code>TAX_Model.jl</code>) is defined as:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function loglike(δ, β, γ, θ, pa, va, pb, vb, data)
    eua,eub = expected_utilities(δ, β, γ, θ, pa, va, pb, vb)
    p = choice_prob(eua, eub, θ)
    p = max(p, eps())
    return logpdf(Bernoulli(p), data)
end
"><pre><span class="pl-k">function</span> <span class="pl-en">loglike</span>(δ, β, γ, θ, pa, va, pb, vb, data)
    eua,eub <span class="pl-k">=</span> <span class="pl-c1">expected_utilities</span>(δ, β, γ, θ, pa, va, pb, vb)
    p <span class="pl-k">=</span> <span class="pl-c1">choice_prob</span>(eua, eub, θ)
    p <span class="pl-k">=</span> <span class="pl-c1">max</span>(p, <span class="pl-c1">eps</span>())
    <span class="pl-k">return</span> <span class="pl-c1">logpdf</span>(<span class="pl-c1">Bernoulli</span>(p), data)
<span class="pl-k">end</span></pre></div>
<p>The model object is contructed with default uniform prior distributions.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# model with default uniform prior
model = Model(;loglike)
"><pre><span class="pl-c"><span class="pl-c">#</span> model with default uniform prior</span>
model <span class="pl-k">=</span> <span class="pl-c1">Model</span>(;loglike)</pre></div>
<h2><a id="user-content-define-parameters" class="anchor" aria-hidden="true" href="#define-parameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Define Parameters</h2>
<p>Define a <code>NamedTuple</code> of parameter value ranges. Note that the parameters listed in the same order that they appear in <code>loglike</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="parm_list = (
    δ = range(-2, 2, length=10),
    β = range(.5, 1.5, length=10),
    γ = range(.5, 1.2, length=10),
    θ = range(.5, 3, length=10)
)
"><pre>parm_list <span class="pl-k">=</span> (
    δ <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">2</span>, <span class="pl-c1">2</span>, length<span class="pl-k">=</span><span class="pl-c1">10</span>),
    β <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">.5</span>, <span class="pl-c1">1.5</span>, length<span class="pl-k">=</span><span class="pl-c1">10</span>),
    γ <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">.5</span>, <span class="pl-c1">1.2</span>, length<span class="pl-k">=</span><span class="pl-c1">10</span>),
    θ <span class="pl-k">=</span> <span class="pl-c1">range</span>(<span class="pl-c1">.5</span>, <span class="pl-c1">3</span>, length<span class="pl-k">=</span><span class="pl-c1">10</span>)
)</pre></div>
<p>The experiment will consist of two gambles with three outcomes each. The number of dimensions in the design space is large (2X2X3X3 = 36). In this case, we will sample random gambles and select a subset of 100 with high distributional overlap. In this case, the <code>design_list</code> will be a <code>Tuple</code> of design names and design values.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# outcome distribution
dist = Normal(0,10)
n_vals = 3
n_choices = 2
design_vals = map(x-&gt;random_design(dist, n_vals, n_choices), 1:1000)
# select gambles with overlapping distributions
filter!(x-&gt;abs_zscore(x) ≤ .4, design_vals)
design_names = (:p1,:v1,:p2,:v2)
design_list = (design_names,design_vals[1:100])
"><pre><span class="pl-c"><span class="pl-c">#</span> outcome distribution</span>
dist <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(<span class="pl-c1">0</span>,<span class="pl-c1">10</span>)
n_vals <span class="pl-k">=</span> <span class="pl-c1">3</span>
n_choices <span class="pl-k">=</span> <span class="pl-c1">2</span>
design_vals <span class="pl-k">=</span> <span class="pl-c1">map</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">random_design</span>(dist, n_vals, n_choices), <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1000</span>)
<span class="pl-c"><span class="pl-c">#</span> select gambles with overlapping distributions</span>
<span class="pl-c1">filter!</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">abs_zscore</span>(x) <span class="pl-k">≤</span> <span class="pl-c1">.4</span>, design_vals)
design_names <span class="pl-k">=</span> (<span class="pl-c1">:p1</span>,<span class="pl-c1">:v1</span>,<span class="pl-c1">:p2</span>,<span class="pl-c1">:v2</span>)
design_list <span class="pl-k">=</span> (design_names,design_vals[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>])</pre></div>
<p>Lastly, we will define a list for the data. The value true indicates gamble A was choosen and false indicates gamble B was chosen.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="data_list = (choice=[true, false],)
"><pre>data_list <span class="pl-k">=</span> (choice<span class="pl-k">=</span>[<span class="pl-c1">true</span>, <span class="pl-c1">false</span>],)</pre></div>
<h2><a id="user-content-optimize-exeriment" class="anchor" aria-hidden="true" href="#optimize-exeriment"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Optimize Exeriment</h2>
<p>In the following code blocks, we will run an optimized experiment and a random experiment. The first step is to generate the optimizer with the contructor <code>Optimizer</code>. Next, we specify true parameters for generating data from the model and initialize a <code>DataFrame</code> to collect the results on each simulated trial. In the experiment loop, data are generated with <code>simulate</code>. The data are passed to <code>update</code> in order to optimize the experiment for the next trial. Finally, the mean and standard deviation are added to the <code>DataFrame</code> for each parameter. A similar process is used to perform the random experiment.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using DataFrames
true_parms = (δ=-1.0, β=1.0, γ=.7, θ=1.5)
n_trials = 100
optimizer = Optimizer(;design_list, parm_list, data_list, model)
design = optimizer.best_design
df = DataFrame(design=Symbol[], trial=Int[], mean_δ=Float64[], mean_β=Float64[],
    mean_γ=Float64[], mean_θ=Float64[], std_δ=Float64[], std_β=Float64[],
    std_γ=Float64[], std_θ=Float64[])
new_data = [:optimal, 0, mean_post(optimizer)..., std_post(optimizer)...]
push!(df, new_data)

for trial in 1:n_trials
    data = simulate(true_parms..., design...)
    design = update!(optimizer, data)
    new_data = [:optimal, trial, mean_post(optimizer)..., std_post(optimizer)...]
    push!(df, new_data)
end
"><pre><span class="pl-k">using</span> DataFrames
true_parms <span class="pl-k">=</span> (δ<span class="pl-k">=</span><span class="pl-k">-</span><span class="pl-c1">1.0</span>, β<span class="pl-k">=</span><span class="pl-c1">1.0</span>, γ<span class="pl-k">=</span><span class="pl-c1">.7</span>, θ<span class="pl-k">=</span><span class="pl-c1">1.5</span>)
n_trials <span class="pl-k">=</span> <span class="pl-c1">100</span>
optimizer <span class="pl-k">=</span> <span class="pl-c1">Optimizer</span>(;design_list, parm_list, data_list, model)
design <span class="pl-k">=</span> optimizer<span class="pl-k">.</span>best_design
df <span class="pl-k">=</span> <span class="pl-c1">DataFrame</span>(design<span class="pl-k">=</span>Symbol[], trial<span class="pl-k">=</span>Int[], mean_δ<span class="pl-k">=</span>Float64[], mean_β<span class="pl-k">=</span>Float64[],
    mean_γ<span class="pl-k">=</span>Float64[], mean_θ<span class="pl-k">=</span>Float64[], std_δ<span class="pl-k">=</span>Float64[], std_β<span class="pl-k">=</span>Float64[],
    std_γ<span class="pl-k">=</span>Float64[], std_θ<span class="pl-k">=</span>Float64[])
new_data <span class="pl-k">=</span> [<span class="pl-c1">:optimal</span>, <span class="pl-c1">0</span>, <span class="pl-c1">mean_post</span>(optimizer)<span class="pl-k">...</span>, <span class="pl-c1">std_post</span>(optimizer)<span class="pl-k">...</span>]
<span class="pl-c1">push!</span>(df, new_data)

<span class="pl-k">for</span> trial <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n_trials
    data <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(true_parms<span class="pl-k">...</span>, design<span class="pl-k">...</span>)
    design <span class="pl-k">=</span> <span class="pl-c1">update!</span>(optimizer, data)
    new_data <span class="pl-k">=</span> [<span class="pl-c1">:optimal</span>, trial, <span class="pl-c1">mean_post</span>(optimizer)<span class="pl-k">...</span>, <span class="pl-c1">std_post</span>(optimizer)<span class="pl-k">...</span>]
    <span class="pl-c1">push!</span>(df, new_data)
<span class="pl-k">end</span></pre></div>
<h2><a id="user-content-random-experiment" class="anchor" aria-hidden="true" href="#random-experiment"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Random Experiment</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="randomizer = Optimizer(;design_list, parm_list, data_list, model, design_type=Randomize);
design = randomizer.best_design
new_data = [:random, 0, mean_post(randomizer)..., std_post(randomizer)...]
push!(df, new_data)

for trial in 1:n_trials
    data = simulate(true_parms..., design...)
    design = update!(randomizer, data)
    new_data = [:random, trial, mean_post(randomizer)..., std_post(randomizer)...]
    push!(df, new_data)
end
"><pre>randomizer <span class="pl-k">=</span> <span class="pl-c1">Optimizer</span>(;design_list, parm_list, data_list, model, design_type<span class="pl-k">=</span>Randomize);
design <span class="pl-k">=</span> randomizer<span class="pl-k">.</span>best_design
new_data <span class="pl-k">=</span> [<span class="pl-c1">:random</span>, <span class="pl-c1">0</span>, <span class="pl-c1">mean_post</span>(randomizer)<span class="pl-k">...</span>, <span class="pl-c1">std_post</span>(randomizer)<span class="pl-k">...</span>]
<span class="pl-c1">push!</span>(df, new_data)

<span class="pl-k">for</span> trial <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n_trials
    data <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(true_parms<span class="pl-k">...</span>, design<span class="pl-k">...</span>)
    design <span class="pl-k">=</span> <span class="pl-c1">update!</span>(randomizer, data)
    new_data <span class="pl-k">=</span> [<span class="pl-c1">:random</span>, trial, <span class="pl-c1">mean_post</span>(randomizer)<span class="pl-k">...</span>, <span class="pl-c1">std_post</span>(randomizer)<span class="pl-k">...</span>]
    <span class="pl-c1">push!</span>(df, new_data)
<span class="pl-k">end</span></pre></div>
<h2><a id="user-content-results" class="anchor" aria-hidden="true" href="#results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Results</h2>
<p>As expected, in the figure below, the posterior standard deviation of δ is smaller for the optimal experiment compared to the random experiment.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using StatsPlots
@df df plot(:trial, :std_δ, xlabel=&quot;trial&quot;, ylabel=&quot;σ of δ&quot;, grid=false, group=:design, linewidth=2, ylims=(0,1.5), size=(600,400))
"><pre><span class="pl-k">using</span> StatsPlots
<span class="pl-c1">@df</span> df <span class="pl-c1">plot</span>(<span class="pl-c1">:trial</span>, <span class="pl-c1">:std_δ</span>, xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>trial<span class="pl-pds">"</span></span>, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>σ of δ<span class="pl-pds">"</span></span>, grid<span class="pl-k">=</span><span class="pl-c1">false</span>, group<span class="pl-k">=</span><span class="pl-c1">:design</span>, linewidth<span class="pl-k">=</span><span class="pl-c1">2</span>, ylims<span class="pl-k">=</span>(<span class="pl-c1">0</span>,<span class="pl-c1">1.5</span>), size<span class="pl-k">=</span>(<span class="pl-c1">600</span>,<span class="pl-c1">400</span>))</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="Examples/Monetary_Gambles/results.png"><img src="Examples/Monetary_Gambles/results.png" alt="" width="500" height="300" style="max-width:100%;"></a></p>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h1>
<ul>
<li>
<p>Birnbaum, M. H., &amp; Chavez, A. (1997). Tests of theories of decision making: Violations of branch independence and distribution   independence. Organizational Behavior and human decision Processes, 71(2), 161-194. Birnbaum, M. H. (2008). New paradoxes of risky decision making. Psychological review, 115(2), 463.</p>
</li>
<li>
<p>Myung, J. I., Cavagnaro, D. R., and Pitt, M. A. (2013). A tutorial on adaptive design optimization. Journal of Mathematical Psychology, 57, 53–67.</p>
</li>
<li>
<p>Yang, J., Pitt, M. A., Ahn, W., &amp; Myung, J. I. (2020). ADOpy: A Python Package for Adaptive Design Optimization. Behavior Research Methods, 1--24. <a href="https://doi.org/10.3758/s13428-020-01386-4" rel="nofollow">https://doi.org/10.3758/s13428-020-01386-4</a></p>
</li>
</ul>
</article></div>