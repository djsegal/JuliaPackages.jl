<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-naivenasflux" class="anchor" aria-hidden="true" href="#naivenasflux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NaiveNASflux</h1>
<p><a href="https://github.com/DrChainsaw/NaiveNASflux.jl/actions"><img src="https://github.com/DrChainsaw/NaiveNASflux.jl/workflows/CI/badge.svg?branch=master" alt="Build status" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/DrChainsaw/NaiveNASflux-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/12459e821442576f218423717c923e8abfdb41509bdeefe8ffd3a8747686d5f4/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f4472436861696e7361772f4e616976654e4153666c75782e6a6c3f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/DrChainsaw/NaiveNASflux.jl?svg=true" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/DrChainsaw/NaiveNASflux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4036e117ff388fd31c144a317fe45829b5861e3d275f8f3cc560c4f8b347c006/68747470733a2f2f636f6465636f762e696f2f67682f4472436861696e7361772f4e616976654e4153666c75782e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/DrChainsaw/NaiveNASflux.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>NaiveNASflux uses <a href="https://github.com/DrChainsaw/NaiveNASlib.jl">NaiveNASlib</a> to enable mutation operations of arbitrary <a href="https://github.com/FluxML/Flux.jl">Flux</a> computation graphs. It is designed with Neural Architecture Search (NAS) in mind, but can be used for any purpose where doing changes to a model is desired.</p>
<p>The following operations are supported:</p>
<ul>
<li>Change the input/output size of layers</li>
<li>Parameter pruning, including methods to rank neurons</li>
<li>Add layers to the model</li>
<li>Remove layers from the model</li>
<li>Add inputs to layers</li>
<li>Remove inputs to layers</li>
</ul>
<p>Note that NaiveNASflux does not have any functionality to search for a model architecture. Check out <a href="https://github.com/DrChainsaw/NaiveGAflux.jl">NaiveGAflux</a> for a simple proof of concept.</p>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic Usage</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="]add NaiveNASflux
"><pre>]add NaiveNASflux</pre></div>
<p>Check out the basic usage of <a href="https://github.com/DrChainsaw/NaiveNASlib.jl">NaiveNASlib</a> for less verbose examples.</p>
<p>Here is a quick rundown of some common operations:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Input type: 3 channels 2D image
invertex = inputvertex(&quot;in&quot;, 3, FluxConv{2}())

# Mutable layers
conv = mutable(Conv((3,3), 3 =&gt; 5, pad=(1,1)), invertex)
batchnorm = mutable(BatchNorm(nout(conv), relu), conv)

# Explore graph
@test inputs(conv) == [invertex]
@test outputs(conv) == [batchnorm]

@test nin(conv) == [3]
@test nout(conv) == 5

@test layertype(conv) isa FluxConv{2}
@test layertype(batchnorm) isa FluxBatchNorm

# naming vertices is a good idea for debugging and logging purposes
namedconv = mutable(&quot;namedconv&quot;, Conv((5,5), 3=&gt;7, pad=(2,2)), invertex)

@test name(namedconv) == &quot;namedconv&quot;

# Concatenate activations
conc = concat(&quot;conc&quot;, namedconv, batchnorm)

@test nout(conc) == nout(namedconv) + nout(batchnorm)

residualconv = mutable(&quot;residualconv&quot;, Conv((3,3), nout(conc) =&gt; nout(conc), pad=(1,1)), conc)

# Elementwise addition. '&gt;&gt;' operation can be used to add metadata, such as a name in this case
add = &quot;add&quot; &gt;&gt; conc + residualconv

@test name(add) == &quot;add&quot;
@test inputs(add) == [conc, residualconv]

# Computation graph for evaluation
graph = CompGraph(invertex, add)

# Can be evaluated just like any function
x = ones(Float32, 7, 7, nout(invertex), 2)
@test size(graph(x)) == (7, 7, nout(add), 2) == (7 ,7, 12 ,2)

# Graphs can be copied
graphcopy = copy(graph)

# Mutate number of neurons
@test nout(add) == nout(residualconv) == nout(conv) + nout(namedconv) == 12
Δnout(add, -3)
@test nout(add) == nout(residualconv) == nout(conv) + nout(namedconv) == 9

# Remove layer
@test nv(graph) == 7
remove!(batchnorm)
@test nv(graph) == 6

# Add layer
insert!(residualconv, v -&gt; mutable(BatchNorm(nout(v), relu), v))
@test nv(graph) == 7


# Change kernel size (and supply new padding)
let Δsize = (-2, -2), pad = (1,1)
    mutate_weights(namedconv, KernelSizeAligned(Δsize, pad))
end

# Apply all mutations
apply_mutation(graph)

# Note: Parameters not changed yet...
size(NaiveNASflux.weights(layer(namedconv))) == (5, 5, 3, 7)

@test size(graph(x)) == (7, 7, nout(add), 2) == (7, 7, 9, 2)

# ... because mutations are lazy by default so that no new layers are created until the graph is evaluated
size(NaiveNASflux.weights(layer(namedconv))) == (3, 3, 3, 7)

# Btw, the copy we made above is of course unaffected
@test size(graphcopy(x)) == (7, 7, 12, 2)
"><pre><span class="pl-c"><span class="pl-c">#</span> Input type: 3 channels 2D image</span>
invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>in<span class="pl-pds">"</span></span>, <span class="pl-c1">3</span>, <span class="pl-c1">FluxConv</span><span class="pl-c1">{2}</span>())

<span class="pl-c"><span class="pl-c">#</span> Mutable layers</span>
conv <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">5</span>, pad<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)), invertex)
batchnorm <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">BatchNorm</span>(<span class="pl-c1">nout</span>(conv), relu), conv)

<span class="pl-c"><span class="pl-c">#</span> Explore graph</span>
<span class="pl-c1">@test</span> <span class="pl-c1">inputs</span>(conv) <span class="pl-k">==</span> [invertex]
<span class="pl-c1">@test</span> <span class="pl-c1">outputs</span>(conv) <span class="pl-k">==</span> [batchnorm]

<span class="pl-c1">@test</span> <span class="pl-c1">nin</span>(conv) <span class="pl-k">==</span> [<span class="pl-c1">3</span>]
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(conv) <span class="pl-k">==</span> <span class="pl-c1">5</span>

<span class="pl-c1">@test</span> <span class="pl-c1">layertype</span>(conv) <span class="pl-k">isa</span> FluxConv{<span class="pl-c1">2</span>}
<span class="pl-c1">@test</span> <span class="pl-c1">layertype</span>(batchnorm) <span class="pl-k">isa</span> FluxBatchNorm

<span class="pl-c"><span class="pl-c">#</span> naming vertices is a good idea for debugging and logging purposes</span>
namedconv <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>namedconv<span class="pl-pds">"</span></span>, <span class="pl-c1">Conv</span>((<span class="pl-c1">5</span>,<span class="pl-c1">5</span>), <span class="pl-c1">3</span><span class="pl-k">=&gt;</span><span class="pl-c1">7</span>, pad<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)), invertex)

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>(namedconv) <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>namedconv<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Concatenate activations</span>
conc <span class="pl-k">=</span> <span class="pl-c1">concat</span>(<span class="pl-s"><span class="pl-pds">"</span>conc<span class="pl-pds">"</span></span>, namedconv, batchnorm)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(conc) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(namedconv) <span class="pl-k">+</span> <span class="pl-c1">nout</span>(batchnorm)

residualconv <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>residualconv<span class="pl-pds">"</span></span>, <span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), <span class="pl-c1">nout</span>(conc) <span class="pl-k">=&gt;</span> <span class="pl-c1">nout</span>(conc), pad<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)), conc)

<span class="pl-c"><span class="pl-c">#</span> Elementwise addition. '&gt;&gt;' operation can be used to add metadata, such as a name in this case</span>
add <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>add<span class="pl-pds">"</span></span> <span class="pl-k">&gt;&gt;</span> conc <span class="pl-k">+</span> residualconv

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>(add) <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>add<span class="pl-pds">"</span></span>
<span class="pl-c1">@test</span> <span class="pl-c1">inputs</span>(add) <span class="pl-k">==</span> [conc, residualconv]

<span class="pl-c"><span class="pl-c">#</span> Computation graph for evaluation</span>
graph <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, add)

<span class="pl-c"><span class="pl-c">#</span> Can be evaluated just like any function</span>
x <span class="pl-k">=</span> <span class="pl-c1">ones</span>(Float32, <span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">nout</span>(invertex), <span class="pl-c1">2</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graph</span>(x)) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">nout</span>(add), <span class="pl-c1">2</span>) <span class="pl-k">==</span> (<span class="pl-c1">7</span> ,<span class="pl-c1">7</span>, <span class="pl-c1">12</span> ,<span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> Graphs can be copied</span>
graphcopy <span class="pl-k">=</span> <span class="pl-c1">copy</span>(graph)

<span class="pl-c"><span class="pl-c">#</span> Mutate number of neurons</span>
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(add) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(residualconv) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(conv) <span class="pl-k">+</span> <span class="pl-c1">nout</span>(namedconv) <span class="pl-k">==</span> <span class="pl-c1">12</span>
<span class="pl-c1">Δnout</span>(add, <span class="pl-k">-</span><span class="pl-c1">3</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(add) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(residualconv) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(conv) <span class="pl-k">+</span> <span class="pl-c1">nout</span>(namedconv) <span class="pl-k">==</span> <span class="pl-c1">9</span>

<span class="pl-c"><span class="pl-c">#</span> Remove layer</span>
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph) <span class="pl-k">==</span> <span class="pl-c1">7</span>
<span class="pl-c1">remove!</span>(batchnorm)
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph) <span class="pl-k">==</span> <span class="pl-c1">6</span>

<span class="pl-c"><span class="pl-c">#</span> Add layer</span>
<span class="pl-c1">insert!</span>(residualconv, v <span class="pl-k">-&gt;</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">BatchNorm</span>(<span class="pl-c1">nout</span>(v), relu), v))
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph) <span class="pl-k">==</span> <span class="pl-c1">7</span>


<span class="pl-c"><span class="pl-c">#</span> Change kernel size (and supply new padding)</span>
<span class="pl-k">let</span> Δsize <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">2</span>, <span class="pl-k">-</span><span class="pl-c1">2</span>), pad <span class="pl-k">=</span> (<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)
    <span class="pl-c1">mutate_weights</span>(namedconv, <span class="pl-c1">KernelSizeAligned</span>(Δsize, pad))
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Apply all mutations</span>
<span class="pl-c1">apply_mutation</span>(graph)

<span class="pl-c"><span class="pl-c">#</span> Note: Parameters not changed yet...</span>
<span class="pl-c1">size</span>(NaiveNASflux<span class="pl-k">.</span><span class="pl-c1">weights</span>(<span class="pl-c1">layer</span>(namedconv))) <span class="pl-k">==</span> (<span class="pl-c1">5</span>, <span class="pl-c1">5</span>, <span class="pl-c1">3</span>, <span class="pl-c1">7</span>)

<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graph</span>(x)) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">nout</span>(add), <span class="pl-c1">2</span>) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">9</span>, <span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> ... because mutations are lazy by default so that no new layers are created until the graph is evaluated</span>
<span class="pl-c1">size</span>(NaiveNASflux<span class="pl-k">.</span><span class="pl-c1">weights</span>(<span class="pl-c1">layer</span>(namedconv))) <span class="pl-k">==</span> (<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">7</span>)

<span class="pl-c"><span class="pl-c">#</span> Btw, the copy we made above is of course unaffected</span>
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graphcopy</span>(x)) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">12</span>, <span class="pl-c1">2</span>)</pre></div>
<p>While NaiveNASflux does not come with any built in search policies, it is still possible to do some cool stuff with it. Below is a very simple example of parameter pruning of a model trained on the <code>xor</code> function.</p>
<p>First we need some boilerplate to create the model and do the training:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveNASflux, Test
import Flux: train!, mse
import Random
Random.seed!(666)
niters = 50

# First lets create a simple model
# layerfun=ActivationContribution will wrap the layer and compute a pruning metric for it while the model trains
mdense(in, outsize, act) = mutable(Dense(nout(in),outsize, act), in, layerfun=ActivationContribution)

invertex = inputvertex(&quot;input&quot;, 2, FluxDense())
layer1 = mdense(invertex, 32, relu)
layer2 = mdense(layer1, 1, sigmoid)
original = CompGraph(invertex, layer2)

# Training params, nothing to see here
opt = ADAM(0.1)
loss(g) = (x, y) -&gt; mse(g(x), y)

# Training data: xor truth table: y = xor(x)
x = Float32[0 0 1 1;
            0 1 0 1]
y = Float32[0 1 1 0]

# Train the model
for iter in 1:niters
    train!(loss(original), params(original), [(x,y)], opt)
end
@test loss(original)(x, y) &lt; 0.001
"><pre><span class="pl-k">using</span> NaiveNASflux, Test
<span class="pl-k">import</span> Flux<span class="pl-k">:</span> train!, mse
<span class="pl-k">import</span> Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">666</span>)
niters <span class="pl-k">=</span> <span class="pl-c1">50</span>

<span class="pl-c"><span class="pl-c">#</span> First lets create a simple model</span>
<span class="pl-c"><span class="pl-c">#</span> layerfun=ActivationContribution will wrap the layer and compute a pruning metric for it while the model trains</span>
<span class="pl-en">mdense</span>(in, outsize, act) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(in),outsize, act), in, layerfun<span class="pl-k">=</span>ActivationContribution)

invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span>, <span class="pl-c1">2</span>, <span class="pl-c1">FluxDense</span>())
layer1 <span class="pl-k">=</span> <span class="pl-c1">mdense</span>(invertex, <span class="pl-c1">32</span>, relu)
layer2 <span class="pl-k">=</span> <span class="pl-c1">mdense</span>(layer1, <span class="pl-c1">1</span>, sigmoid)
original <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, layer2)

<span class="pl-c"><span class="pl-c">#</span> Training params, nothing to see here</span>
opt <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.1</span>)
<span class="pl-en">loss</span>(g) <span class="pl-k">=</span> (x, y) <span class="pl-k">-&gt;</span> <span class="pl-c1">mse</span>(<span class="pl-c1">g</span>(x), y)

<span class="pl-c"><span class="pl-c">#</span> Training data: xor truth table: y = xor(x)</span>
x <span class="pl-k">=</span> Float32[<span class="pl-c1">0</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span>;
            <span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span>]
y <span class="pl-k">=</span> Float32[<span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span> <span class="pl-c1">0</span>]

<span class="pl-c"><span class="pl-c">#</span> Train the model</span>
<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>niters
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(original), <span class="pl-c1">params</span>(original), [(x,y)], opt)
<span class="pl-k">end</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(original)(x, y) <span class="pl-k">&lt;</span> <span class="pl-c1">0.001</span></pre></div>
<p>With that out of the way, lets try three different ways to prune the hidden layer:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="nprune = 16

# Prune randomly selected neurons
pruned_random = copy(original)
Δnin(pruned_random.outputs[], -nprune)
Δoutputs(pruned_random.outputs[], v -&gt; rand(nout_org(v)))
apply_mutation(pruned_random)

# Prune the least valuable neurons according to the metric in ActivationContribution
pruned_least = copy(original)
Δnin(pruned_least.outputs[], -nprune)
Δoutputs(pruned_least.outputs[], neuron_value)
apply_mutation(pruned_least)

# Prune the most valuable neurons according to the metric in ActivationContribution
pruned_most = copy(original)
Δnin(pruned_most.outputs[], -nprune)
Δoutputs(pruned_most.outputs[], v -&gt; -neuron_value(v))
apply_mutation(pruned_most)

# Can I have my free lunch now please?!
@test loss(pruned_most)(x, y) &gt; loss(pruned_random)(x, y) &gt; loss(pruned_least)(x, y) &gt; loss(original)(x, y)

# The metric calculated by ActivationContribution is actually quite good (in this case).
@test loss(pruned_least)(x, y) ≈ loss(original)(x, y) atol = 1e-5
"><pre>nprune <span class="pl-k">=</span> <span class="pl-c1">16</span>

<span class="pl-c"><span class="pl-c">#</span> Prune randomly selected neurons</span>
pruned_random <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
<span class="pl-c1">Δnin</span>(pruned_random<span class="pl-k">.</span>outputs[], <span class="pl-k">-</span>nprune)
<span class="pl-c1">Δoutputs</span>(pruned_random<span class="pl-k">.</span>outputs[], v <span class="pl-k">-&gt;</span> <span class="pl-c1">rand</span>(<span class="pl-c1">nout_org</span>(v)))
<span class="pl-c1">apply_mutation</span>(pruned_random)

<span class="pl-c"><span class="pl-c">#</span> Prune the least valuable neurons according to the metric in ActivationContribution</span>
pruned_least <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
<span class="pl-c1">Δnin</span>(pruned_least<span class="pl-k">.</span>outputs[], <span class="pl-k">-</span>nprune)
<span class="pl-c1">Δoutputs</span>(pruned_least<span class="pl-k">.</span>outputs[], neuron_value)
<span class="pl-c1">apply_mutation</span>(pruned_least)

<span class="pl-c"><span class="pl-c">#</span> Prune the most valuable neurons according to the metric in ActivationContribution</span>
pruned_most <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
<span class="pl-c1">Δnin</span>(pruned_most<span class="pl-k">.</span>outputs[], <span class="pl-k">-</span>nprune)
<span class="pl-c1">Δoutputs</span>(pruned_most<span class="pl-k">.</span>outputs[], v <span class="pl-k">-&gt;</span> <span class="pl-k">-</span><span class="pl-c1">neuron_value</span>(v))
<span class="pl-c1">apply_mutation</span>(pruned_most)

<span class="pl-c"><span class="pl-c">#</span> Can I have my free lunch now please?!</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(pruned_most)(x, y) <span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(pruned_random)(x, y) <span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(pruned_least)(x, y) <span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(original)(x, y)

<span class="pl-c"><span class="pl-c">#</span> The metric calculated by ActivationContribution is actually quite good (in this case).</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(pruned_least)(x, y) <span class="pl-k">≈</span> <span class="pl-c1">loss</span>(original)(x, y) atol <span class="pl-k">=</span> <span class="pl-c1">1e-5</span></pre></div>
<p>Another toy example where the model has too few layers to efficiently fit the data.</p>
<p>Create model and train it just to have something to mutate:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NaiveNASflux, Test
import Flux.Losses: logitbinarycrossentropy
import Flux: train!, glorot_uniform
using Statistics
import Random
Random.seed!(666)
niters = 20

# Layers used in this example
mconv(in, outsize, act; init=glorot_uniform) = mutable(Conv((3,3),nout(in)=&gt;outsize, act, pad=(1,1), init=init), in)
mavgpool(in, h, w) = mutable(MeanPool((h, w)), in)
mdense(in, outsize, act) = mutable(Dense(nout(in),outsize, act), in)

# Size of the input
height = 4
width = 4

function model(nconv)
    invertex = inputvertex(&quot;in&quot;, 1, FluxConv{2}())
    l = invertex
    for i in 1:nconv
        l = mconv(l, 16, relu)
    end
    l = mavgpool(l, height, width)
    l = invariantvertex(Flux.flatten, l)
    l = mdense(l, 2, identity)
    return CompGraph(invertex, l)
end
original = model(1)

# Training params, nothing to see here
opt_org = ADAM(0.01)
loss(g) = (x, y) -&gt; logitbinarycrossentropy(g(x), y, agg=mean)

# Training data: 2D xor(-ish)
# Class 1 are matrices A where A[n,m] ⊻ A[n+h÷2, m]) ⩓ A[n,m] ⊻ A[n, m+w÷2] ∀ n&lt;h÷2, m&lt;w÷2 is true, e.g. [1 0; 0, 1]
function xy1(h,w)
    q1_3 = rand(Bool,h÷2,w÷2)
    q2_4 = .!q1_3
    return (Float32.(vcat(hcat(q2_4, q1_3), hcat(q1_3, q2_4))), Float32[0, 1])
end
xy0(h,w) = (rand(Float32[0,1], h,w), Float32[1, 0]) #Joke's on me when this generates false negatives :)
# Generate 50% class 1 and 50% class 0 examples in one batch
catbatch((x1,y1)::Tuple, (x2,y2)::Tuple) = (cat(x1, x2, dims=4), hcat(y1,y2))
batch(h,w,batchsize) = mapfoldl(i -&gt; i==0 ? xy0(h,w) : xy1(h,w), catbatch, (1:batchsize) .% 2)

x_test, y_test = batch(height, width, 1024)
startloss = loss(original)(x_test, y_test)

# Train the model
for iter in 1:niters
    train!(loss(original), params(original), [batch(height, width, 64)], opt_org)
end
# That didn't work so well...
@test loss(original)(x_test, y_test) ≈ startloss atol=1e-1

# Lets try three things:
# 1. Just train the same model some more
# 2. Add two more conv-layers to the already trained model
# 3. Create a new model with three conv layers from scratch

# Disclaimer: This experiment is intended to show usage of this library.
# It is not meant to give evidence that method 2 is the better option.
# Hyperparameters are tuned to strongly favor 2 in order to avoid sporadic failures

# Add two layers after the conv layer
add_layers = copy(original)
function add2conv(in)
    l = mconv(in, nout(in), relu, init=idmapping)
    return mconv(l, nout(in), relu, init=idmapping)
end
insert!(vertices(add_layers)[2], add2conv)

# New layers are initialized to identity mapping weights
# We basically have the same model as before, just with more potential
# Not guaranteed to be a good idea as it relies on existing layers to provide gradient diversity
@test add_layers(x_test) == original(x_test)

# Create a new model with three conv layers
new_model = model(3)

opt_add = deepcopy(opt_org)
opt_new = ADAM(0.01)

# Lets try again
for iter in 1:niters
    b = batch(height, width, 64)
    train!(loss(original), params(original), [b], opt_org)
    train!(loss(add_layers), params(add_layers), [b], opt_add)
    train!(loss(new_model), params(new_model), [b], opt_new)
end

@test loss(add_layers)(x_test,y_test) &lt; loss(new_model)(x_test,y_test) &lt; loss(original)(x_test,y_test)
"><pre><span class="pl-k">using</span> NaiveNASflux, Test
<span class="pl-k">import</span> Flux<span class="pl-k">.</span>Losses<span class="pl-k">:</span> logitbinarycrossentropy
<span class="pl-k">import</span> Flux<span class="pl-k">:</span> train!, glorot_uniform
<span class="pl-k">using</span> Statistics
<span class="pl-k">import</span> Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">666</span>)
niters <span class="pl-k">=</span> <span class="pl-c1">20</span>

<span class="pl-c"><span class="pl-c">#</span> Layers used in this example</span>
<span class="pl-en">mconv</span>(in, outsize, act; init<span class="pl-k">=</span>glorot_uniform) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>),<span class="pl-c1">nout</span>(in)<span class="pl-k">=&gt;</span>outsize, act, pad<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>), init<span class="pl-k">=</span>init), in)
<span class="pl-en">mavgpool</span>(in, h, w) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">MeanPool</span>((h, w)), in)
<span class="pl-en">mdense</span>(in, outsize, act) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(in),outsize, act), in)

<span class="pl-c"><span class="pl-c">#</span> Size of the input</span>
height <span class="pl-k">=</span> <span class="pl-c1">4</span>
width <span class="pl-k">=</span> <span class="pl-c1">4</span>

<span class="pl-k">function</span> <span class="pl-en">model</span>(nconv)
    invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>in<span class="pl-pds">"</span></span>, <span class="pl-c1">1</span>, <span class="pl-c1">FluxConv</span><span class="pl-c1">{2}</span>())
    l <span class="pl-k">=</span> invertex
    <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>nconv
        l <span class="pl-k">=</span> <span class="pl-c1">mconv</span>(l, <span class="pl-c1">16</span>, relu)
    <span class="pl-k">end</span>
    l <span class="pl-k">=</span> <span class="pl-c1">mavgpool</span>(l, height, width)
    l <span class="pl-k">=</span> <span class="pl-c1">invariantvertex</span>(Flux<span class="pl-k">.</span>flatten, l)
    l <span class="pl-k">=</span> <span class="pl-c1">mdense</span>(l, <span class="pl-c1">2</span>, identity)
    <span class="pl-k">return</span> <span class="pl-c1">CompGraph</span>(invertex, l)
<span class="pl-k">end</span>
original <span class="pl-k">=</span> <span class="pl-c1">model</span>(<span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> Training params, nothing to see here</span>
opt_org <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.01</span>)
<span class="pl-en">loss</span>(g) <span class="pl-k">=</span> (x, y) <span class="pl-k">-&gt;</span> <span class="pl-c1">logitbinarycrossentropy</span>(<span class="pl-c1">g</span>(x), y, agg<span class="pl-k">=</span>mean)

<span class="pl-c"><span class="pl-c">#</span> Training data: 2D xor(-ish)</span>
<span class="pl-c"><span class="pl-c">#</span> Class 1 are matrices A where A[n,m] ⊻ A[n+h÷2, m]) ⩓ A[n,m] ⊻ A[n, m+w÷2] ∀ n&lt;h÷2, m&lt;w÷2 is true, e.g. [1 0; 0, 1]</span>
<span class="pl-k">function</span> <span class="pl-en">xy1</span>(h,w)
    q1_3 <span class="pl-k">=</span> <span class="pl-c1">rand</span>(Bool,h<span class="pl-k">÷</span><span class="pl-c1">2</span>,w<span class="pl-k">÷</span><span class="pl-c1">2</span>)
    q2_4 <span class="pl-k">=</span> <span class="pl-k">.!</span>q1_3
    <span class="pl-k">return</span> (<span class="pl-c1">Float32</span>.(<span class="pl-c1">vcat</span>(<span class="pl-c1">hcat</span>(q2_4, q1_3), <span class="pl-c1">hcat</span>(q1_3, q2_4))), Float32[<span class="pl-c1">0</span>, <span class="pl-c1">1</span>])
<span class="pl-k">end</span>
<span class="pl-en">xy0</span>(h,w) <span class="pl-k">=</span> (<span class="pl-c1">rand</span>(Float32[<span class="pl-c1">0</span>,<span class="pl-c1">1</span>], h,w), Float32[<span class="pl-c1">1</span>, <span class="pl-c1">0</span>]) <span class="pl-c"><span class="pl-c">#</span>Joke's on me when this generates false negatives :)</span>
<span class="pl-c"><span class="pl-c">#</span> Generate 50% class 1 and 50% class 0 examples in one batch</span>
<span class="pl-en">catbatch</span>((x1,y1)<span class="pl-k">::</span><span class="pl-c1">Tuple</span>, (x2,y2)<span class="pl-k">::</span><span class="pl-c1">Tuple</span>) <span class="pl-k">=</span> (<span class="pl-c1">cat</span>(x1, x2, dims<span class="pl-k">=</span><span class="pl-c1">4</span>), <span class="pl-c1">hcat</span>(y1,y2))
<span class="pl-en">batch</span>(h,w,batchsize) <span class="pl-k">=</span> <span class="pl-c1">mapfoldl</span>(i <span class="pl-k">-&gt;</span> i<span class="pl-k">==</span><span class="pl-c1">0</span> <span class="pl-k">?</span> <span class="pl-c1">xy0</span>(h,w) <span class="pl-k">:</span> <span class="pl-c1">xy1</span>(h,w), catbatch, (<span class="pl-c1">1</span><span class="pl-k">:</span>batchsize) <span class="pl-k">.%</span> <span class="pl-c1">2</span>)

x_test, y_test <span class="pl-k">=</span> <span class="pl-c1">batch</span>(height, width, <span class="pl-c1">1024</span>)
startloss <span class="pl-k">=</span> <span class="pl-c1">loss</span>(original)(x_test, y_test)

<span class="pl-c"><span class="pl-c">#</span> Train the model</span>
<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>niters
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(original), <span class="pl-c1">params</span>(original), [<span class="pl-c1">batch</span>(height, width, <span class="pl-c1">64</span>)], opt_org)
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> That didn't work so well...</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(original)(x_test, y_test) <span class="pl-k">≈</span> startloss atol<span class="pl-k">=</span><span class="pl-c1">1e-1</span>

<span class="pl-c"><span class="pl-c">#</span> Lets try three things:</span>
<span class="pl-c"><span class="pl-c">#</span> 1. Just train the same model some more</span>
<span class="pl-c"><span class="pl-c">#</span> 2. Add two more conv-layers to the already trained model</span>
<span class="pl-c"><span class="pl-c">#</span> 3. Create a new model with three conv layers from scratch</span>

<span class="pl-c"><span class="pl-c">#</span> Disclaimer: This experiment is intended to show usage of this library.</span>
<span class="pl-c"><span class="pl-c">#</span> It is not meant to give evidence that method 2 is the better option.</span>
<span class="pl-c"><span class="pl-c">#</span> Hyperparameters are tuned to strongly favor 2 in order to avoid sporadic failures</span>

<span class="pl-c"><span class="pl-c">#</span> Add two layers after the conv layer</span>
add_layers <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
<span class="pl-k">function</span> <span class="pl-en">add2conv</span>(in)
    l <span class="pl-k">=</span> <span class="pl-c1">mconv</span>(in, <span class="pl-c1">nout</span>(in), relu, init<span class="pl-k">=</span>idmapping)
    <span class="pl-k">return</span> <span class="pl-c1">mconv</span>(l, <span class="pl-c1">nout</span>(in), relu, init<span class="pl-k">=</span>idmapping)
<span class="pl-k">end</span>
<span class="pl-c1">insert!</span>(<span class="pl-c1">vertices</span>(add_layers)[<span class="pl-c1">2</span>], add2conv)

<span class="pl-c"><span class="pl-c">#</span> New layers are initialized to identity mapping weights</span>
<span class="pl-c"><span class="pl-c">#</span> We basically have the same model as before, just with more potential</span>
<span class="pl-c"><span class="pl-c">#</span> Not guaranteed to be a good idea as it relies on existing layers to provide gradient diversity</span>
<span class="pl-c1">@test</span> <span class="pl-c1">add_layers</span>(x_test) <span class="pl-k">==</span> <span class="pl-c1">original</span>(x_test)

<span class="pl-c"><span class="pl-c">#</span> Create a new model with three conv layers</span>
new_model <span class="pl-k">=</span> <span class="pl-c1">model</span>(<span class="pl-c1">3</span>)

opt_add <span class="pl-k">=</span> <span class="pl-c1">deepcopy</span>(opt_org)
opt_new <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.01</span>)

<span class="pl-c"><span class="pl-c">#</span> Lets try again</span>
<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>niters
    b <span class="pl-k">=</span> <span class="pl-c1">batch</span>(height, width, <span class="pl-c1">64</span>)
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(original), <span class="pl-c1">params</span>(original), [b], opt_org)
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(add_layers), <span class="pl-c1">params</span>(add_layers), [b], opt_add)
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(new_model), <span class="pl-c1">params</span>(new_model), [b], opt_new)
<span class="pl-k">end</span>

<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(add_layers)(x_test,y_test) <span class="pl-k">&lt;</span> <span class="pl-c1">loss</span>(new_model)(x_test,y_test) <span class="pl-k">&lt;</span> <span class="pl-c1">loss</span>(original)(x_test,y_test)</pre></div>
<h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<p>All contributions are welcome. Please file an issue before creating a PR.</p>
</article></div>