<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-naivenasflux" class="anchor" aria-hidden="true" href="#naivenasflux"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>NaiveNASflux</h1>
<p><a href="https://travis-ci.org/DrChainsaw/NaiveNASflux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a21f39cca77d1980c626c19e61c333e6b04317b8/68747470733a2f2f7472617669732d63692e6f72672f4472436861696e7361772f4e616976654e4153666c75782e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/DrChainsaw/NaiveNASflux.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/DrChainsaw/NaiveNASflux-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/1c2b2e304440ac9b80b6cad71da9d93fd9ea6eeb/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f4472436861696e7361772f4e616976654e4153666c75782e6a6c3f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/DrChainsaw/NaiveNASflux.jl?svg=true" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/DrChainsaw/NaiveNASflux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/720f97404ab14a809d6aeb2a1fd29e135ed0b1ca/68747470733a2f2f636f6465636f762e696f2f67682f4472436861696e7361772f4e616976654e4153666c75782e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/DrChainsaw/NaiveNASflux.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>NaiveNASflux uses <a href="https://github.com/DrChainsaw/NaiveNASlib.jl">NaiveNASlib</a> to enable mutation operations of arbitrary <a href="https://github.com/FluxML/Flux.jl">Flux</a> computation graphs. It is designed with Neural Architecture Search (NAS) in mind, but can be used for any purpose where doing changes to a model is desired.</p>
<p>The following operations are supported:</p>
<ul>
<li>Change the input/output size of layers</li>
<li>Parameter pruning, including methods to rank neurons</li>
<li>Add layers to the model</li>
<li>Remove layers from the model</li>
<li>Add inputs to layers</li>
<li>Remove inputs to layers</li>
</ul>
<p>Note that NaiveNASflux does not have any functionality to search for a model architecture. Check out <a href="https://github.com/DrChainsaw/NaiveGAflux.jl">NaiveGAflux</a> for a simple proof of concept.</p>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Basic Usage</h2>
<div class="highlight highlight-source-julia"><pre>]add NaiveNASflux</pre></div>
<p>Check out the basic usage of <a href="https://github.com/DrChainsaw/NaiveNASlib.jl">NaiveNASlib</a> for less verbose examples.</p>
<p>Here is a quick rundown of some common operations:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Input type: 3 channels 2D image</span>
invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>in<span class="pl-pds">"</span></span>, <span class="pl-c1">3</span>, <span class="pl-c1">FluxConv</span><span class="pl-c1">{2}</span>())

<span class="pl-c"><span class="pl-c">#</span> Mutable layers</span>
conv <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), <span class="pl-c1">3</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">5</span>, pad<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)), invertex)
batchnorm <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">BatchNorm</span>(<span class="pl-c1">nout</span>(conv), relu), conv)

<span class="pl-c"><span class="pl-c">#</span> Explore graph</span>
<span class="pl-c1">@test</span> <span class="pl-c1">inputs</span>(conv) <span class="pl-k">==</span> [invertex]
<span class="pl-c1">@test</span> <span class="pl-c1">outputs</span>(conv) <span class="pl-k">==</span> [batchnorm]

<span class="pl-c1">@test</span> <span class="pl-c1">nin</span>(conv) <span class="pl-k">==</span> [<span class="pl-c1">3</span>]
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(conv) <span class="pl-k">==</span> <span class="pl-c1">5</span>

<span class="pl-c1">@test</span> <span class="pl-c1">layertype</span>(conv) <span class="pl-k">isa</span> FluxConv{<span class="pl-c1">2</span>}
<span class="pl-c1">@test</span> <span class="pl-c1">layertype</span>(batchnorm) <span class="pl-k">isa</span> FluxBatchNorm

<span class="pl-c"><span class="pl-c">#</span> naming vertices is a good idea for debugging and logging purposes</span>
namedconv <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>namedconv<span class="pl-pds">"</span></span>, <span class="pl-c1">Conv</span>((<span class="pl-c1">5</span>,<span class="pl-c1">5</span>), <span class="pl-c1">3</span><span class="pl-k">=&gt;</span><span class="pl-c1">7</span>, pad<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)), invertex)

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>(namedconv) <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>namedconv<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Concatenate activations</span>
conc <span class="pl-k">=</span> <span class="pl-c1">concat</span>(<span class="pl-s"><span class="pl-pds">"</span>conc<span class="pl-pds">"</span></span>, namedconv, batchnorm)

<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(conc) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(namedconv) <span class="pl-k">+</span> <span class="pl-c1">nout</span>(batchnorm)

residualconv <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-s"><span class="pl-pds">"</span>residualconv<span class="pl-pds">"</span></span>, <span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), <span class="pl-c1">nout</span>(conc) <span class="pl-k">=&gt;</span> <span class="pl-c1">nout</span>(conc), pad<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)), conc)

<span class="pl-c"><span class="pl-c">#</span> Elementwise addition. '&gt;&gt;' operation can be used to add metadata, such as a name in this case</span>
add <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>add<span class="pl-pds">"</span></span> <span class="pl-k">&gt;&gt;</span> conc <span class="pl-k">+</span> residualconv

<span class="pl-c1">@test</span> <span class="pl-c1">name</span>(add) <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>add<span class="pl-pds">"</span></span>
<span class="pl-c1">@test</span> <span class="pl-c1">inputs</span>(add) <span class="pl-k">==</span> [conc, residualconv]

<span class="pl-c"><span class="pl-c">#</span> Computation graph for evaluation</span>
graph <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, add)

<span class="pl-c"><span class="pl-c">#</span> Can be evaluated just like any function</span>
x <span class="pl-k">=</span> <span class="pl-c1">ones</span>(Float32, <span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">nout</span>(invertex), <span class="pl-c1">2</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graph</span>(x)) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">nout</span>(add), <span class="pl-c1">2</span>) <span class="pl-k">==</span> (<span class="pl-c1">7</span> ,<span class="pl-c1">7</span>, <span class="pl-c1">12</span> ,<span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> Graphs can be copied</span>
graphcopy <span class="pl-k">=</span> <span class="pl-c1">copy</span>(graph)

<span class="pl-c"><span class="pl-c">#</span> Mutate number of neurons</span>
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(add) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(residualconv) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(conv) <span class="pl-k">+</span> <span class="pl-c1">nout</span>(namedconv) <span class="pl-k">==</span> <span class="pl-c1">12</span>
Δ<span class="pl-c1">nout</span>(add, <span class="pl-k">-</span><span class="pl-c1">3</span>)
<span class="pl-c1">@test</span> <span class="pl-c1">nout</span>(add) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(residualconv) <span class="pl-k">==</span> <span class="pl-c1">nout</span>(conv) <span class="pl-k">+</span> <span class="pl-c1">nout</span>(namedconv) <span class="pl-k">==</span> <span class="pl-c1">9</span>

<span class="pl-c"><span class="pl-c">#</span> Remove layer</span>
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph) <span class="pl-k">==</span> <span class="pl-c1">7</span>
<span class="pl-c1">remove!</span>(batchnorm)
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph) <span class="pl-k">==</span> <span class="pl-c1">6</span>

<span class="pl-c"><span class="pl-c">#</span> Add layer</span>
<span class="pl-c1">insert!</span>(residualconv, v <span class="pl-k">-&gt;</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">BatchNorm</span>(<span class="pl-c1">nout</span>(v), relu), v))
<span class="pl-c1">@test</span> <span class="pl-c1">nv</span>(graph) <span class="pl-k">==</span> <span class="pl-c1">7</span>


<span class="pl-c"><span class="pl-c">#</span> Change kernel size (and supply new padding)</span>
<span class="pl-k">let</span> Δsize <span class="pl-k">=</span> (<span class="pl-k">-</span><span class="pl-c1">2</span>, <span class="pl-k">-</span><span class="pl-c1">2</span>), pad <span class="pl-k">=</span> (<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)
    <span class="pl-c1">mutate_weights</span>(namedconv, <span class="pl-c1">KernelSizeAligned</span>(Δsize, pad))
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Apply all mutations</span>
<span class="pl-c1">apply_mutation</span>(graph)

<span class="pl-c"><span class="pl-c">#</span> Note: Parameters not changed yet...</span>
<span class="pl-c1">size</span>(NaiveNASflux<span class="pl-k">.</span><span class="pl-c1">weights</span>(<span class="pl-c1">layer</span>(namedconv))) <span class="pl-k">==</span> (<span class="pl-c1">5</span>, <span class="pl-c1">5</span>, <span class="pl-c1">3</span>, <span class="pl-c1">7</span>)

<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graph</span>(x)) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">nout</span>(add), <span class="pl-c1">2</span>) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">9</span>, <span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> ... because mutations are lazy by default so that no new layers are created until the graph is evaluated</span>
<span class="pl-c1">size</span>(NaiveNASflux<span class="pl-k">.</span><span class="pl-c1">weights</span>(<span class="pl-c1">layer</span>(namedconv))) <span class="pl-k">==</span> (<span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">7</span>)

<span class="pl-c"><span class="pl-c">#</span> Btw, the copy we made above is of course unaffected</span>
<span class="pl-c1">@test</span> <span class="pl-c1">size</span>(<span class="pl-c1">graphcopy</span>(x)) <span class="pl-k">==</span> (<span class="pl-c1">7</span>, <span class="pl-c1">7</span>, <span class="pl-c1">12</span>, <span class="pl-c1">2</span>)</pre></div>
<p>While NaiveNASflux does not come with any built in search policies, it is still possible to do some cool stuff with it. Below is a very simple example of parameter pruning of a model trained on the <code>xor</code> function.</p>
<p>First we need some boilerplate to create the model and do the training:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> NaiveNASflux, Test
<span class="pl-k">import</span> Flux<span class="pl-k">:</span> train!, mse
<span class="pl-k">import</span> Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">666</span>)
niters <span class="pl-k">=</span> <span class="pl-c1">50</span>

<span class="pl-c"><span class="pl-c">#</span> First lets create a simple model</span>
<span class="pl-c"><span class="pl-c">#</span> layerfun=ActivationContribution will wrap the layer and compute a pruning metric for it while the model trains</span>
<span class="pl-en">mdense</span>(in, outsize, act) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(in),outsize, act), in, layerfun<span class="pl-k">=</span>ActivationContribution)

invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span>, <span class="pl-c1">2</span>, <span class="pl-c1">FluxDense</span>())
layer1 <span class="pl-k">=</span> <span class="pl-c1">mdense</span>(invertex, <span class="pl-c1">32</span>, relu)
layer2 <span class="pl-k">=</span> <span class="pl-c1">mdense</span>(layer1, <span class="pl-c1">1</span>, sigmoid)
original <span class="pl-k">=</span> <span class="pl-c1">CompGraph</span>(invertex, layer2)

<span class="pl-c"><span class="pl-c">#</span> Training params, nothing to see here</span>
opt <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.1</span>)
<span class="pl-en">loss</span>(g) <span class="pl-k">=</span> (x, y) <span class="pl-k">-&gt;</span> <span class="pl-c1">mse</span>(<span class="pl-c1">g</span>(x), y)

<span class="pl-c"><span class="pl-c">#</span> Training data: xor truth table: y = xor(x)</span>
x <span class="pl-k">=</span> Float32[<span class="pl-c1">0</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span>;
            <span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">0</span> <span class="pl-c1">1</span>]
y <span class="pl-k">=</span> Float32[<span class="pl-c1">0</span> <span class="pl-c1">1</span> <span class="pl-c1">1</span> <span class="pl-c1">0</span>]

<span class="pl-c"><span class="pl-c">#</span> Train the model</span>
<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>niters
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(original), <span class="pl-c1">params</span>(original), [(x,y)], opt)
<span class="pl-k">end</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(original)(x, y) <span class="pl-k">&lt;</span> <span class="pl-c1">0.001</span></pre></div>
<p>With that out of the way, lets try three different ways to prune the hidden layer:</p>
<div class="highlight highlight-source-julia"><pre>nprune <span class="pl-k">=</span> <span class="pl-c1">16</span>

<span class="pl-c"><span class="pl-c">#</span> Prune randomly selected neurons</span>
pruned_random <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
Δ<span class="pl-c1">nin</span>(pruned_random<span class="pl-k">.</span>outputs[], <span class="pl-k">-</span>nprune)
Δ<span class="pl-c1">outputs</span>(pruned_random<span class="pl-k">.</span>outputs[], v <span class="pl-k">-&gt;</span> <span class="pl-c1">rand</span>(<span class="pl-c1">nout_org</span>(v)))
<span class="pl-c1">apply_mutation</span>(pruned_random)

<span class="pl-c"><span class="pl-c">#</span> Prune the least valuable neurons according to the metric in ActivationContribution</span>
pruned_least <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
Δ<span class="pl-c1">nin</span>(pruned_least<span class="pl-k">.</span>outputs[], <span class="pl-k">-</span>nprune)
Δ<span class="pl-c1">outputs</span>(pruned_least<span class="pl-k">.</span>outputs[], neuron_value)
<span class="pl-c1">apply_mutation</span>(pruned_least)

<span class="pl-c"><span class="pl-c">#</span> Prune the most valuable neurons according to the metric in ActivationContribution</span>
pruned_most <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
Δ<span class="pl-c1">nin</span>(pruned_most<span class="pl-k">.</span>outputs[], <span class="pl-k">-</span>nprune)
Δ<span class="pl-c1">outputs</span>(pruned_most<span class="pl-k">.</span>outputs[], v <span class="pl-k">-&gt;</span> <span class="pl-k">-</span><span class="pl-c1">neuron_value</span>(v))
<span class="pl-c1">apply_mutation</span>(pruned_most)

<span class="pl-c"><span class="pl-c">#</span> Can I have my free lunch now please?!</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(pruned_most)(x, y) <span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(pruned_random)(x, y) <span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(pruned_least)(x, y) <span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(original)(x, y)

<span class="pl-c"><span class="pl-c">#</span> The metric calculated by ActivationContribution is actually quite good (in this case).</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(pruned_least)(x, y) <span class="pl-k">≈</span> <span class="pl-c1">loss</span>(original)(x, y) atol <span class="pl-k">=</span> <span class="pl-c1">1e-5</span></pre></div>
<p>Another toy example where the model has too few layers to efficiently fit the data.</p>
<p>Create model and train it just to have something to mutate:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> NaiveNASflux, Test
<span class="pl-k">import</span> Flux<span class="pl-k">:</span> train!, logitbinarycrossentropy, glorot_uniform
<span class="pl-k">using</span> Statistics
<span class="pl-k">import</span> Random
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">666</span>)
niters <span class="pl-k">=</span> <span class="pl-c1">20</span>

<span class="pl-c"><span class="pl-c">#</span> Layers used in this example</span>
<span class="pl-en">mconv</span>(in, outsize, act; init<span class="pl-k">=</span>glorot_uniform) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Conv</span>((<span class="pl-c1">3</span>,<span class="pl-c1">3</span>),<span class="pl-c1">nout</span>(in)<span class="pl-k">=&gt;</span>outsize, act, pad<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">1</span>), init<span class="pl-k">=</span>init), in)
<span class="pl-en">mavgpool</span>(in, h, w) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">MeanPool</span>((h, w)), in)
<span class="pl-en">mdense</span>(in, outsize, act) <span class="pl-k">=</span> <span class="pl-c1">mutable</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">nout</span>(in),outsize, act), in)

<span class="pl-c"><span class="pl-c">#</span> Size of the input</span>
height <span class="pl-k">=</span> <span class="pl-c1">4</span>
width <span class="pl-k">=</span> <span class="pl-c1">4</span>

<span class="pl-k">function</span> <span class="pl-en">model</span>(nconv)
    invertex <span class="pl-k">=</span> <span class="pl-c1">inputvertex</span>(<span class="pl-s"><span class="pl-pds">"</span>in<span class="pl-pds">"</span></span>, <span class="pl-c1">1</span>, <span class="pl-c1">FluxConv</span><span class="pl-c1">{2}</span>())
    l <span class="pl-k">=</span> invertex
    <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>nconv
        l <span class="pl-k">=</span> <span class="pl-c1">mconv</span>(l, <span class="pl-c1">16</span>, relu)
    <span class="pl-k">end</span>
    l <span class="pl-k">=</span> <span class="pl-c1">mavgpool</span>(l, height, width)
    l <span class="pl-k">=</span> <span class="pl-c1">invariantvertex</span>(x <span class="pl-k">-&gt;</span> x[<span class="pl-c1">1</span>,<span class="pl-c1">1</span>,:,:], l)
    l <span class="pl-k">=</span> <span class="pl-c1">mdense</span>(l, <span class="pl-c1">2</span>, identity)
    <span class="pl-k">return</span> <span class="pl-c1">CompGraph</span>(invertex, l)
<span class="pl-k">end</span>
original <span class="pl-k">=</span> <span class="pl-c1">model</span>(<span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> Training params, nothing to see here</span>
opt_org <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.01</span>)
<span class="pl-en">loss</span>(g) <span class="pl-k">=</span> (x, y) <span class="pl-k">-&gt;</span> <span class="pl-c1">mean</span>(<span class="pl-c1">logitbinarycrossentropy</span>.(<span class="pl-c1">g</span>(x), y))

<span class="pl-c"><span class="pl-c">#</span> Training data: 2D xor(-ish)</span>
<span class="pl-c"><span class="pl-c">#</span> Class 1 are matrices A where A[n,m] ⊻ A[n+h÷2, m]) ⩓ A[n,m] ⊻ A[n, m+w÷2] ∀ n&lt;h÷2, m&lt;w÷2 is true, e.g. [1 0; 0, 1]</span>
<span class="pl-k">function</span> <span class="pl-en">xy1</span>(h,w)
    q1_3 <span class="pl-k">=</span> <span class="pl-c1">rand</span>(Bool,h<span class="pl-k">÷</span><span class="pl-c1">2</span>,w<span class="pl-k">÷</span><span class="pl-c1">2</span>)
    q2_4 <span class="pl-k">=</span> <span class="pl-k">.!</span>q1_3
    <span class="pl-k">return</span> (x <span class="pl-k">=</span> <span class="pl-c1">Float32</span>.(<span class="pl-c1">vcat</span>(<span class="pl-c1">hcat</span>(q2_4, q1_3), <span class="pl-c1">hcat</span>(q1_3, q2_4))), y <span class="pl-k">=</span> Float32[<span class="pl-c1">0</span>, <span class="pl-c1">1</span>])
<span class="pl-k">end</span>
<span class="pl-en">xy0</span>(h,w) <span class="pl-k">=</span> (x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(Float32[<span class="pl-c1">0</span>,<span class="pl-c1">1</span>], h,w), y <span class="pl-k">=</span> Float32[<span class="pl-c1">1</span>, <span class="pl-c1">0</span>]) <span class="pl-c"><span class="pl-c">#</span>Joke's on me when this generates false negatives :)</span>
<span class="pl-c"><span class="pl-c">#</span> Generate 50% class 1 and 50% class 0 examples in one batch</span>
<span class="pl-en">batch</span>(h,w,batchsize) <span class="pl-k">=</span> <span class="pl-c1">mapfoldl</span>(i <span class="pl-k">-&gt;</span> i<span class="pl-k">==</span><span class="pl-c1">0</span> ? <span class="pl-c1">xy0</span>(h,w) : <span class="pl-c1">xy1</span>(h,w), (ex1, ex2) <span class="pl-k">-&gt;</span> (x <span class="pl-k">=</span> <span class="pl-c1">cat</span>(ex1<span class="pl-k">.</span>x, ex2<span class="pl-k">.</span>x, dims<span class="pl-k">=</span><span class="pl-c1">4</span>), y <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(ex1<span class="pl-k">.</span>y,ex2<span class="pl-k">.</span>y)), (<span class="pl-c1">1</span><span class="pl-k">:</span>batchsize) <span class="pl-k">.%</span> <span class="pl-c1">2</span>)

x_test, y_test <span class="pl-k">=</span> <span class="pl-c1">batch</span>(height, width, <span class="pl-c1">1024</span>)
startloss <span class="pl-k">=</span> <span class="pl-c1">loss</span>(original)(x_test, y_test)

<span class="pl-c"><span class="pl-c">#</span> Train the model</span>
<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>niters
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(original), <span class="pl-c1">params</span>(original), [<span class="pl-c1">batch</span>(height, width, <span class="pl-c1">64</span>)], opt_org)
<span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#</span> That didn't work so well...</span>
<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(original)(x_test, y_test) <span class="pl-k">≈</span> startloss atol<span class="pl-k">=</span><span class="pl-c1">1e-1</span></pre></div>
<p>Lets see if more layers help:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Lets try three things:</span>
<span class="pl-c"><span class="pl-c">#</span> 1. Just train the same model some more</span>
<span class="pl-c"><span class="pl-c">#</span> 2. Add two more conv-layers to the already trained model and train some more</span>
<span class="pl-c"><span class="pl-c">#</span> 3. Create a new model with three conv layers from scratch and train it</span>

<span class="pl-c"><span class="pl-c">#</span> Disclaimer: This experiment is intended to show usage of this library.</span>
<span class="pl-c"><span class="pl-c">#</span> It is not meant to give evidence that method 2 is always the better option.</span>
<span class="pl-c"><span class="pl-c">#</span> Hyperparameters are tuned to strongly favor 2 in order to avoid sporadic failures</span>

<span class="pl-c"><span class="pl-c">#</span> Add two layers after the conv layer</span>
add_layers <span class="pl-k">=</span> <span class="pl-c1">copy</span>(original)
<span class="pl-k">function</span> <span class="pl-en">add2conv</span>(in)
    l <span class="pl-k">=</span> <span class="pl-c1">mconv</span>(in, <span class="pl-c1">nout</span>(in), relu, init<span class="pl-k">=</span>idmapping)
    <span class="pl-k">return</span> <span class="pl-c1">mconv</span>(l, <span class="pl-c1">nout</span>(in), relu, init<span class="pl-k">=</span>idmapping)
<span class="pl-k">end</span>
<span class="pl-c1">insert!</span>(<span class="pl-c1">vertices</span>(add_layers)[<span class="pl-c1">2</span>], add2conv)

<span class="pl-c"><span class="pl-c">#</span> New layers are initialized to identity mapping weights</span>
<span class="pl-c"><span class="pl-c">#</span> We basically have the same model as before, just with more potential</span>
<span class="pl-c"><span class="pl-c">#</span> Not guaranteed to be a good idea as it relies on existing layers to provide gradient diversity</span>
<span class="pl-c1">@test</span> <span class="pl-c1">add_layers</span>(x_test) <span class="pl-k">==</span> <span class="pl-c1">original</span>(x_test)

<span class="pl-c"><span class="pl-c">#</span> Create a new model with three conv layers</span>
new_model <span class="pl-k">=</span> <span class="pl-c1">model</span>(<span class="pl-c1">3</span>)

opt_add <span class="pl-k">=</span> <span class="pl-c1">deepcopy</span>(opt_org)
opt_new <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.01</span>)

<span class="pl-c"><span class="pl-c">#</span> Lets try again</span>
<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>niters
    b <span class="pl-k">=</span> <span class="pl-c1">batch</span>(height, width, <span class="pl-c1">64</span>)
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(original), <span class="pl-c1">params</span>(original), [b], opt_org)
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(add_layers), <span class="pl-c1">params</span>(add_layers), [b], opt_add)
    <span class="pl-c1">train!</span>(<span class="pl-c1">loss</span>(new_model), <span class="pl-c1">params</span>(new_model), [b], opt_new)
<span class="pl-k">end</span>

<span class="pl-c1">@test</span> <span class="pl-c1">loss</span>(add_layers)(x_test,y_test) <span class="pl-k">&lt;</span> <span class="pl-c1">loss</span>(new_model)(x_test,y_test) <span class="pl-k">&lt;</span> <span class="pl-c1">loss</span>(original)(x_test,y_test)</pre></div>
<h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributing</h2>
<p>All contributions are welcome. Please file an issue before creating a PR.</p>
</article></div>