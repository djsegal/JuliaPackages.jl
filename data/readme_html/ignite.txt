<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-ignitejl" class="anchor" aria-hidden="true" href="#ignitejl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Ignite.jl</h1>
<p dir="auto"><a href="https://jondeuce.github.io/Ignite.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/jondeuce/Ignite.jl/actions/workflows/CI.yml?query=branch%3Amaster"><img src="https://github.com/jondeuce/Ignite.jl/actions/workflows/CI.yml/badge.svg?branch=master" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/jondeuce/Ignite.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/31f874aacfac60f216b9cc16a68e8d4b050ce243c14c81e8acd0ce8c87e77d4a/68747470733a2f2f636f6465636f762e696f2f67682f6a6f6e64657563652f49676e6974652e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/jondeuce/Ignite.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://github.com/JuliaTesting/Aqua.jl"><img src="https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg" alt="Aqua QA" style="max-width: 100%;"></a></p>
<p dir="auto">Welcome to <code>Ignite.jl</code>, a Julia port of the Python library <a href="https://github.com/pytorch/ignite"><code>ignite</code></a> for simplifying neural network training and validation loops using events and handlers.</p>
<p dir="auto"><code>Ignite.jl</code> provides a simple yet flexible engine and event system, allowing for the easy composition of training pipelines with various events such as artifact saving, metric logging, and model validation. Event-based training abstracts away the training loop, replacing it with:</p>
<ol dir="auto">
<li>An <em>engine</em> which wraps a <em>process function</em> that consumes a single batch of data,</li>
<li>An iterable data loader which produces said batches of data, and</li>
<li>Events and corresponding event handlers which are attached to the engine, configured to fire at specific points during training.</li>
</ol>
<p dir="auto">Event handlers are much more flexibile compared to other approaches like callbacks: handlers can be any callable; multiple handlers can be attached to a single event; multiple events can trigger the same handler; and custom events can be defined to fire at user-specified points during training. This makes adding functionality to your training pipeline easy, minimizing the need to modify existing code.</p>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto">The example below demonstrates how to use <code>Ignite.jl</code> to train a simple neural network. Key features to note:</p>
<ul dir="auto">
<li>The training step is factored out of the training loop: the <code>train_step</code> process function takes a batch of training data and computes the training loss, gradients, and updates the model parameters.</li>
<li>Data loaders can be any iterable collection. Here, we use a <a href="https://juliaml.github.io/MLUtils.jl/stable/api/#MLUtils.DataLoader" rel="nofollow"><code>DataLoader</code></a> from <a href="https://github.com/JuliaML/MLUtils.jl"><code>MLUtils.jl</code></a></li>
</ul>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Ignite
using Flux, Zygote, Optimisers, MLUtils # for training a neural network

# Build simple neural network and initialize Adam optimizer
model = Chain(Dense(1 =&gt; 32, tanh), Dense(32 =&gt; 1))
optim = Optimisers.setup(Optimisers.Adam(1f-3), model)

# Create mock data and data loaders
f(x) = 2x-x^3
xtrain, xtest = 2 * rand(1, 10_000) .- 1, reshape(range(-1, 1, length = 100), 1, :)
ytrain, ytest = f.(xtrain), f.(xtest)
train_data_loader = DataLoader((; x = xtrain, y = ytrain); batchsize = 64, shuffle = true, partial = false)
eval_data_loader = DataLoader((; x = xtest, y = ytest); batchsize = 10, shuffle = false)

# Create training engine:
#   - `engine` is a reference to the parent `trainer` engine, created below
#   - `batch` is a batch of training data, retrieved by iterating `train_data_loader`
#   - (optional) return value is stored in `trainer.state.output`
function train_step(engine, batch)
    x, y = batch
    l, gs = Zygote.withgradient(m -&gt; sum(abs2, m(x) .- y), model)
    global optim, model = Optimisers.update!(optim, model, gs[1])
    return Dict(&quot;loss&quot; =&gt; l)
end
trainer = Engine(train_step)

# Start the training
Ignite.run!(trainer, train_data_loader; max_epochs = 25, epoch_length = 100)"><pre><span class="pl-k">using</span> Ignite
<span class="pl-k">using</span> Flux, Zygote, Optimisers, MLUtils <span class="pl-c"><span class="pl-c">#</span> for training a neural network</span>

<span class="pl-c"><span class="pl-c">#</span> Build simple neural network and initialize Adam optimizer</span>
model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">1</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">32</span>, tanh), <span class="pl-c1">Dense</span>(<span class="pl-c1">32</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span>))
optim <span class="pl-k">=</span> Optimisers<span class="pl-k">.</span><span class="pl-c1">setup</span>(Optimisers<span class="pl-k">.</span><span class="pl-c1">Adam</span>(<span class="pl-c1">1f-3</span>), model)

<span class="pl-c"><span class="pl-c">#</span> Create mock data and data loaders</span>
<span class="pl-en">f</span>(x) <span class="pl-k">=</span> <span class="pl-c1">2</span>x<span class="pl-k">-</span>x<span class="pl-k">^</span><span class="pl-c1">3</span>
xtrain, xtest <span class="pl-k">=</span> <span class="pl-c1">2</span> <span class="pl-k">*</span> <span class="pl-c1">rand</span>(<span class="pl-c1">1</span>, <span class="pl-c1">10_000</span>) <span class="pl-k">.-</span> <span class="pl-c1">1</span>, <span class="pl-c1">reshape</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">1</span>, <span class="pl-c1">1</span>, length <span class="pl-k">=</span> <span class="pl-c1">100</span>), <span class="pl-c1">1</span>, :)
ytrain, ytest <span class="pl-k">=</span> <span class="pl-c1">f</span>.(xtrain), <span class="pl-c1">f</span>.(xtest)
train_data_loader <span class="pl-k">=</span> <span class="pl-c1">DataLoader</span>((; x <span class="pl-k">=</span> xtrain, y <span class="pl-k">=</span> ytrain); batchsize <span class="pl-k">=</span> <span class="pl-c1">64</span>, shuffle <span class="pl-k">=</span> <span class="pl-c1">true</span>, partial <span class="pl-k">=</span> <span class="pl-c1">false</span>)
eval_data_loader <span class="pl-k">=</span> <span class="pl-c1">DataLoader</span>((; x <span class="pl-k">=</span> xtest, y <span class="pl-k">=</span> ytest); batchsize <span class="pl-k">=</span> <span class="pl-c1">10</span>, shuffle <span class="pl-k">=</span> <span class="pl-c1">false</span>)

<span class="pl-c"><span class="pl-c">#</span> Create training engine:</span>
<span class="pl-c"><span class="pl-c">#</span>   - `engine` is a reference to the parent `trainer` engine, created below</span>
<span class="pl-c"><span class="pl-c">#</span>   - `batch` is a batch of training data, retrieved by iterating `train_data_loader`</span>
<span class="pl-c"><span class="pl-c">#</span>   - (optional) return value is stored in `trainer.state.output`</span>
<span class="pl-k">function</span> <span class="pl-en">train_step</span>(engine, batch)
    x, y <span class="pl-k">=</span> batch
    l, gs <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">withgradient</span>(m <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(abs2, <span class="pl-c1">m</span>(x) <span class="pl-k">.-</span> y), model)
    <span class="pl-k">global</span> optim, model <span class="pl-k">=</span> Optimisers<span class="pl-k">.</span><span class="pl-c1">update!</span>(optim, model, gs[<span class="pl-c1">1</span>])
    <span class="pl-k">return</span> <span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>loss<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> l)
<span class="pl-k">end</span>
trainer <span class="pl-k">=</span> <span class="pl-c1">Engine</span>(train_step)

<span class="pl-c"><span class="pl-c">#</span> Start the training</span>
Ignite<span class="pl-k">.</span><span class="pl-c1">run!</span>(trainer, train_data_loader; max_epochs <span class="pl-k">=</span> <span class="pl-c1">25</span>, epoch_length <span class="pl-k">=</span> <span class="pl-c1">100</span>)</pre></div>
<h3 dir="auto"><a id="user-content-periodically-evaluate-model" class="anchor" aria-hidden="true" href="#periodically-evaluate-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Periodically evaluate model</h3>
<p dir="auto">The real power of <code>Ignite.jl</code> comes when adding <em>events</em> to our training engine.</p>
<p dir="auto">Let's evaluate our model after every 5th training epoch. This can be easily incorporated without needing to modify any of the above training code:</p>
<ol dir="auto">
<li>Create an <code>evaluator</code> engine which consumes batches of evaluation data</li>
<li>Add <em>event handlers</em> to the <code>evaluator</code> engine which accumulate a running average of evaluation metrics over batches of evaluation data; we use <a href="https://github.com/joshday/OnlineStats.jl"><code>OnlineStats.jl</code></a> to make this easy.</li>
<li>Add an event handler to the <code>trainer</code> which runs the <code>evaluator</code> on the evaluation data loader every 5 training epochs.</li>
</ol>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using OnlineStats: Mean, fit! # for tracking evaluation metrics

# Create an evaluation engine using `do` syntax:
evaluator = Engine() do engine, batch
    x, y = batch
    ypred = model(x) # evaluate model on a single batch of validation data
    return Dict(&quot;ytrue&quot; =&gt; y, &quot;ypred&quot; =&gt; ypred) # result is stored in `evaluator.state.output`
end

# Add events to the evaluation engine to track metrics:
add_event_handler!(evaluator, STARTED()) do engine
    # When `evaluator` starts, initialize the running mean
    engine.state.metrics = Dict(&quot;abs_err&quot; =&gt; Mean()) # new fields can be added to `engine.state` dynamically
end

add_event_handler!(evaluator, ITERATION_COMPLETED()) do engine
    # Each iteration, compute eval metrics from predictions
    o = engine.state.output
    m = engine.state.metrics[&quot;abs_err&quot;]
    fit!(m, abs.(o[&quot;ytrue&quot;] .- o[&quot;ypred&quot;]) |&gt; vec)
end

# Add an event to `trainer` which runs `evaluator` every 5 epochs:
add_event_handler!(trainer, EPOCH_COMPLETED(every = 5)) do engine
    Ignite.run!(evaluator, eval_data_loader)
    @info &quot;Evaluation metrics: abs_err = $(evaluator.state.metrics[&quot;abs_err&quot;])&quot;
end

# Run the trainer with periodic evaluation
Ignite.run!(trainer, train_data_loader; max_epochs = 25, epoch_length = 100)"><pre><span class="pl-k">using</span> OnlineStats<span class="pl-k">:</span> Mean, fit! <span class="pl-c"><span class="pl-c">#</span> for tracking evaluation metrics</span>

<span class="pl-c"><span class="pl-c">#</span> Create an evaluation engine using `do` syntax:</span>
evaluator <span class="pl-k">=</span> <span class="pl-c1">Engine</span>() <span class="pl-k">do</span> engine, batch
    x, y <span class="pl-k">=</span> batch
    ypred <span class="pl-k">=</span> <span class="pl-c1">model</span>(x) <span class="pl-c"><span class="pl-c">#</span> evaluate model on a single batch of validation data</span>
    <span class="pl-k">return</span> <span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>ytrue<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> y, <span class="pl-s"><span class="pl-pds">"</span>ypred<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> ypred) <span class="pl-c"><span class="pl-c">#</span> result is stored in `evaluator.state.output`</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Add events to the evaluation engine to track metrics:</span>
<span class="pl-c1">add_event_handler!</span>(evaluator, <span class="pl-c1">STARTED</span>()) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> When `evaluator` starts, initialize the running mean</span>
    engine<span class="pl-k">.</span>state<span class="pl-k">.</span>metrics <span class="pl-k">=</span> <span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>abs_err<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">Mean</span>()) <span class="pl-c"><span class="pl-c">#</span> new fields can be added to `engine.state` dynamically</span>
<span class="pl-k">end</span>

<span class="pl-c1">add_event_handler!</span>(evaluator, <span class="pl-c1">ITERATION_COMPLETED</span>()) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> Each iteration, compute eval metrics from predictions</span>
    o <span class="pl-k">=</span> engine<span class="pl-k">.</span>state<span class="pl-k">.</span>output
    m <span class="pl-k">=</span> engine<span class="pl-k">.</span>state<span class="pl-k">.</span>metrics[<span class="pl-s"><span class="pl-pds">"</span>abs_err<span class="pl-pds">"</span></span>]
    <span class="pl-c1">fit!</span>(m, <span class="pl-c1">abs</span>.(o[<span class="pl-s"><span class="pl-pds">"</span>ytrue<span class="pl-pds">"</span></span>] <span class="pl-k">.-</span> o[<span class="pl-s"><span class="pl-pds">"</span>ypred<span class="pl-pds">"</span></span>]) <span class="pl-k">|&gt;</span> vec)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Add an event to `trainer` which runs `evaluator` every 5 epochs:</span>
<span class="pl-c1">add_event_handler!</span>(trainer, <span class="pl-c1">EPOCH_COMPLETED</span>(every <span class="pl-k">=</span> <span class="pl-c1">5</span>)) <span class="pl-k">do</span> engine
    Ignite<span class="pl-k">.</span><span class="pl-c1">run!</span>(evaluator, eval_data_loader)
    <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Evaluation metrics: abs_err = <span class="pl-v">$(evaluator<span class="pl-k">.</span>state<span class="pl-k">.</span>metrics[<span class="pl-s"><span class="pl-pds">"</span>abs_err<span class="pl-pds">"</span></span>])</span><span class="pl-pds">"</span></span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Run the trainer with periodic evaluation</span>
Ignite<span class="pl-k">.</span><span class="pl-c1">run!</span>(trainer, train_data_loader; max_epochs <span class="pl-k">=</span> <span class="pl-c1">25</span>, epoch_length <span class="pl-k">=</span> <span class="pl-c1">100</span>)</pre></div>
<h3 dir="auto"><a id="user-content-artifact-saving" class="anchor" aria-hidden="true" href="#artifact-saving"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Artifact saving</h3>
<p dir="auto">Logging artifacts can be easily added to the trainer, again without modifying the above code. For example, save the current model and optimizer state to disk every 10 epochs using <a href="https://github.com/JuliaIO/BSON.jl"><code>BSON.jl</code></a>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using BSON: @save

# Save model and optimizer state every 10 epochs
add_event_handler!(trainer, EPOCH_COMPLETED(every = 10)) do engine
    @save &quot;model_and_optim.bson&quot; model optim
    @info &quot;Saved model and optimizer state to disk&quot;
end"><pre><span class="pl-k">using</span> BSON<span class="pl-k">:</span> <span class="pl-c1">@save</span>

<span class="pl-c"><span class="pl-c">#</span> Save model and optimizer state every 10 epochs</span>
<span class="pl-c1">add_event_handler!</span>(trainer, <span class="pl-c1">EPOCH_COMPLETED</span>(every <span class="pl-k">=</span> <span class="pl-c1">10</span>)) <span class="pl-k">do</span> engine
    <span class="pl-c1">@save</span> <span class="pl-s"><span class="pl-pds">"</span>model_and_optim.bson<span class="pl-pds">"</span></span> model optim
    <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Saved model and optimizer state to disk<span class="pl-pds">"</span></span>
<span class="pl-k">end</span></pre></div>
<h3 dir="auto"><a id="user-content-trigger-multiple-functions-per-event" class="anchor" aria-hidden="true" href="#trigger-multiple-functions-per-event"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Trigger multiple functions per event</h3>
<p dir="auto">Multiple event handlers can be added to the same event:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="add_event_handler!(trainer, COMPLETED()) do engine
    # Runs after training has completed
end
add_event_handler!(trainer, COMPLETED()) do engine
    # Also runs after training has completed, after the above function runs
end"><pre><span class="pl-c1">add_event_handler!</span>(trainer, <span class="pl-c1">COMPLETED</span>()) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> Runs after training has completed</span>
<span class="pl-k">end</span>
<span class="pl-c1">add_event_handler!</span>(trainer, <span class="pl-c1">COMPLETED</span>()) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> Also runs after training has completed, after the above function runs</span>
<span class="pl-k">end</span></pre></div>
<h3 dir="auto"><a id="user-content-attach-the-same-handler-to-multiple-events" class="anchor" aria-hidden="true" href="#attach-the-same-handler-to-multiple-events"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Attach the same handler to multiple events</h3>
<p dir="auto">The boolean operators <code>|</code> and <code>&amp;</code> can be used to combine events together:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="add_event_handler!(trainer, EPOCH_COMPLETED(every = 10) | COMPLETED()) do engine
    # Runs at the end of every 10th epoch, or when training is completed
end

throttled_event = EPOCH_COMPLETED(; every = 3) &amp; EPOCH_COMPLETED(; event_filter = throttle_filter(30.0))
add_event_handler!(trainer, throttled_event) do engine
    # Runs at the end of every 3rd epoch if at least 30s has passed since the last firing
end"><pre><span class="pl-c1">add_event_handler!</span>(trainer, <span class="pl-c1">EPOCH_COMPLETED</span>(every <span class="pl-k">=</span> <span class="pl-c1">10</span>) <span class="pl-k">|</span> <span class="pl-c1">COMPLETED</span>()) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> Runs at the end of every 10th epoch, or when training is completed</span>
<span class="pl-k">end</span>

throttled_event <span class="pl-k">=</span> <span class="pl-c1">EPOCH_COMPLETED</span>(; every <span class="pl-k">=</span> <span class="pl-c1">3</span>) <span class="pl-k">&amp;</span> <span class="pl-c1">EPOCH_COMPLETED</span>(; event_filter <span class="pl-k">=</span> <span class="pl-c1">throttle_filter</span>(<span class="pl-c1">30.0</span>))
<span class="pl-c1">add_event_handler!</span>(trainer, throttled_event) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> Runs at the end of every 3rd epoch if at least 30s has passed since the last firing</span>
<span class="pl-k">end</span></pre></div>
<h3 dir="auto"><a id="user-content-define-custom-events" class="anchor" aria-hidden="true" href="#define-custom-events"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Define custom events</h3>
<p dir="auto">Custom events can be created and fired at user-defined stages in the training process.</p>
<p dir="auto">For example, suppose we want to define events that fire at the start and finish of both the backward pass and the optimizer step. All we need to do is define new event types that subtype <code>AbstractLoopEvent</code>, and then fire them at appropriate points in the <code>train_step</code> process function using <code>fire_event!</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="struct BACKWARD_STARTED &lt;: AbstractLoopEvent end
struct BACKWARD_COMPLETED &lt;: AbstractLoopEvent end
struct OPTIM_STEP_STARTED &lt;: AbstractLoopEvent end
struct OPTIM_STEP_COMPLETED &lt;: AbstractLoopEvent end

function train_step(engine, batch)
    x, y = batch

    # Compute the gradients of the loss with respect to the model
    fire_event!(engine, BACKWARD_STARTED())
    l, gs = Zygote.withgradient(m -&gt; sum(abs2, m(x) .- y), model)
    engine.state.gradients = gs # the engine state can be accessed by event handlers
    fire_event!(engine, BACKWARD_COMPLETED())

    # Update the model's parameters
    fire_event!(engine, OPTIM_STEP_STARTED())
    global optim, model = Optimisers.update!(optim, model, gs[1])
    fire_event!(engine, OPTIM_STEP_COMPLETED())

    return Dict(&quot;loss&quot; =&gt; l)
end
trainer = Engine(train_step)"><pre><span class="pl-k">struct</span> BACKWARD_STARTED <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractLoopEvent</span> <span class="pl-k">end</span>
<span class="pl-k">struct</span> BACKWARD_COMPLETED <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractLoopEvent</span> <span class="pl-k">end</span>
<span class="pl-k">struct</span> OPTIM_STEP_STARTED <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractLoopEvent</span> <span class="pl-k">end</span>
<span class="pl-k">struct</span> OPTIM_STEP_COMPLETED <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractLoopEvent</span> <span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">train_step</span>(engine, batch)
    x, y <span class="pl-k">=</span> batch

    <span class="pl-c"><span class="pl-c">#</span> Compute the gradients of the loss with respect to the model</span>
    <span class="pl-c1">fire_event!</span>(engine, <span class="pl-c1">BACKWARD_STARTED</span>())
    l, gs <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">withgradient</span>(m <span class="pl-k">-&gt;</span> <span class="pl-c1">sum</span>(abs2, <span class="pl-c1">m</span>(x) <span class="pl-k">.-</span> y), model)
    engine<span class="pl-k">.</span>state<span class="pl-k">.</span>gradients <span class="pl-k">=</span> gs <span class="pl-c"><span class="pl-c">#</span> the engine state can be accessed by event handlers</span>
    <span class="pl-c1">fire_event!</span>(engine, <span class="pl-c1">BACKWARD_COMPLETED</span>())

    <span class="pl-c"><span class="pl-c">#</span> Update the model's parameters</span>
    <span class="pl-c1">fire_event!</span>(engine, <span class="pl-c1">OPTIM_STEP_STARTED</span>())
    <span class="pl-k">global</span> optim, model <span class="pl-k">=</span> Optimisers<span class="pl-k">.</span><span class="pl-c1">update!</span>(optim, model, gs[<span class="pl-c1">1</span>])
    <span class="pl-c1">fire_event!</span>(engine, <span class="pl-c1">OPTIM_STEP_COMPLETED</span>())

    <span class="pl-k">return</span> <span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>loss<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> l)
<span class="pl-k">end</span>
trainer <span class="pl-k">=</span> <span class="pl-c1">Engine</span>(train_step)</pre></div>
<p dir="auto">Then, add event handlers for these custom events as usual:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="add_event_handler!(trainer, BACKWARD_COMPLETED(every = 10)) do engine
    # This code runs after every 10th backward pass is completed
end"><pre><span class="pl-c1">add_event_handler!</span>(trainer, <span class="pl-c1">BACKWARD_COMPLETED</span>(every <span class="pl-k">=</span> <span class="pl-c1">10</span>)) <span class="pl-k">do</span> engine
    <span class="pl-c"><span class="pl-c">#</span> This code runs after every 10th backward pass is completed</span>
<span class="pl-k">end</span></pre></div>
<hr>
<p dir="auto"><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p>
</article></div>