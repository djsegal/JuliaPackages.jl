<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-kernelestimator" class="anchor" aria-hidden="true" href="#kernelestimator"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>KernelEstimator</h1>
<p dir="auto">Linux: <a href="https://travis-ci.org/panlanfeng/KernelEstimator.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/5545e89bce2a88777dbf2ea9b568111c4cab29dba7d7583ac629bd084a68983c/68747470733a2f2f7472617669732d63692e6f72672f70616e6c616e66656e672f4b65726e656c457374696d61746f722e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/panlanfeng/KernelEstimator.jl.svg?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">The Julia package for nonparametric kernel density estimate and regression. This package currently includes univariate kernel density estimate, local constant regression (Nadaraya-Watson regression) and local linear regression. It can also compute the Bootstrap confidence band [4].</p>
<p dir="auto">This package provides Gamma and Beta kernel to deal with bounded density estimation and regression. These two kernels are free of boundary bias for one side and two sides bounded data respectively, see [2, 3]. In particular, this package provide least square cross validation (LSCV) bandwidth selection functions for Gamma and Beta kernels.</p>
<p dir="auto">Bandwidth selection is critical in kernel estimation. LSCV is always recommended. Likelihood cross validation is provided but should be avoided because of known drawbacks. For regression problem, the bandwidth of local constant regression is selected using LSCV while that for local linear regression is chosen by AIC [6].</p>
<p dir="auto">To install and use this package in Julia,</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Pkg.add(&quot;KernelEstimator&quot;)
using KernelEstimator"><pre class="notranslate"><code>Pkg.add("KernelEstimator")
using KernelEstimator
</code></pre></div>
<p dir="auto">See usage under <a href="examples/"><code>examples/</code></a> directory.</p>
<p dir="auto">This package calculate densities via direct approach, i.e. adding kernel functions together. To define new kernel, you need to define a new function takes the same arguments as <code>gaussiankernel</code> and output the kernel weights at given point. If no bandwidth selection function is provided, lscv with numeric integration will be used by default.</p>
<h2 dir="auto"><a id="user-content-functions" class="anchor" aria-hidden="true" href="#functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Functions</h2>
<p dir="auto">This package provides two major functions, <code>kerneldensity</code> for kernel density estimation and <code>npr</code> for nonparametric regression. For kernel density, you can simply use</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="xdata = randn(1000)
kerneldensity(xdata)"><pre class="notranslate"><code>xdata = randn(1000)
kerneldensity(xdata)
</code></pre></div>
<p dir="auto">or specify some options</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="xeval = linspace(-3, 3, 100)
bw = bwlscv(xdata, gaussiankernel)
kerneldensity(xdata, xeval=xeval, lb=-Inf, ub=Inf, kernel=gaussiankernel,h = bw)"><pre class="notranslate"><code>xeval = linspace(-3, 3, 100)
bw = bwlscv(xdata, gaussiankernel)
kerneldensity(xdata, xeval=xeval, lb=-Inf, ub=Inf, kernel=gaussiankernel,h = bw)
</code></pre></div>
<p dir="auto"><code>xeval</code> specifies the position you want to evaluate the density at. Default to be the same as <code>xdata</code>. <code>lb</code> and <code>ub</code> means lower bound and upper bound of the data. If you specify either of them to be some finite value, user choice of kernel function will be suppressed and <code>gammakernel</code> will be used with a warning. If you specify both, <code>betakernel</code> is used with a warning if user's choice is different.</p>
<p dir="auto">For kernel regression, you can use</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="x = rand(Beta(4,2), 500) * 10
y=2 .* x.^2 + x .* rand(Normal(0, 5), 500)
npr(x, y)"><pre class="notranslate"><code>x = rand(Beta(4,2), 500) * 10
y=2 .* x.^2 + x .* rand(Normal(0, 5), 500)
npr(x, y)
</code></pre></div>
<p dir="auto">or change the default by</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="npr(x, y, xeval=x, reg=locallinear, kernel=betakernel,lb=0.0, ub=10.0)"><pre class="notranslate"><code>npr(x, y, xeval=x, reg=locallinear, kernel=betakernel,lb=0.0, ub=10.0)
</code></pre></div>
<p dir="auto"><code>reg</code> specifies the order of local polynomial regression. You can choose <code>localconstant</code>, local constant regression or <code>locallinear</code>, local linear regression. <code>locallinear</code> has better theoretical properties in prediction <code>y</code> and is used by default but is more computing intensive.</p>
<p dir="auto">There is also a function computing the bootstrap confidence interval for regression.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="bootstrapCB(x, y; xeval=x, B=500, reg=locallinear, lb=-Inf, ub=Inf, kernel=gaussiankernel)"><pre class="notranslate"><code>bootstrapCB(x, y; xeval=x, B=500, reg=locallinear, lb=-Inf, ub=Inf, kernel=gaussiankernel)
</code></pre></div>
<p dir="auto"><code>B</code> specifies the number of bootstrap sampling.</p>
<p dir="auto">The following functions are also provided:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>localconstant(xdata::RealVector, ydata::RealVector; xeval::RealVector=xdata, kernel::Function=gaussiankernel, h::Real=bwlocalconstant(xdata,ydata,kernel))</code>, local constant regression (or Nadaraya-Watson)</p>
</li>
<li>
<p dir="auto"><code>locallinear(xdata::RealVector, ydata::RealVector; xeval::RealVector=xdata, kernel::Function=gaussiankernel, h::Real=bwlocalconstant(xdata,ydata,kernel))</code>,  local linear regression</p>
</li>
</ul>
<p dir="auto">and bandwidth selection functions:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>bwnormal(xdata::Vector)</code>, bandwidth selection for density estimate by referencing to normal distribution</p>
</li>
<li>
<p dir="auto"><code>bwlscv(xdata::RealVector, kernel::Function)</code>, bandwidth selection for density estimate by least square cross validation</p>
</li>
<li>
<p dir="auto"><code>bwlcv(xdata::RealVector, kernel::Function)</code>, bandwidth selection for density estimate by likelihood cross validation</p>
</li>
<li>
<p dir="auto"><code>bwlocalconstant(xdata, ydata::Vector, kernel)</code>, bandwidth selection for local constant regression using LSCV</p>
</li>
<li>
<p dir="auto"><code>bwlocallinear(xdata, ydata::Vector, kernel)</code>, bandwidth selection for local linear regression using corrected AIC. See reference [6]</p>
</li>
</ul>
<p dir="auto">The meaning of arguments:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>xeval</code> is the point(s) where the density or fitted value is calculated</p>
</li>
<li>
<p dir="auto"><code>xdata</code> is the input X</p>
</li>
<li>
<p dir="auto"><code>ydata</code> is the response vector y; should have same length with <code>xdata</code></p>
</li>
<li>
<p dir="auto"><code>reg</code> is the regression function, <code>localconstant</code> or <code>locallinear</code></p>
</li>
<li>
<p dir="auto"><code>kernel</code> defaults to be <code>gaussiankernel</code>; should be a function</p>
</li>
<li>
<p dir="auto"><code>h</code> is the bandwidth, should be a real scalar; If negative, the default bandwidth selection method will be used to find the bandwidth and replace it</p>
</li>
<li>
<p dir="auto"><code>lb</code>, <code>ub</code> are the boundary for x. Must provide if use Beta or Gamma kernel</p>
</li>
</ul>
<h2 dir="auto"><a id="user-content-demos" class="anchor" aria-hidden="true" href="#demos"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Demos</h2>
<ul dir="auto">
<li>
<p dir="auto">Kernel density estimate</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" using Distributions
 x=rand(Normal(), 500)
 xeval=linspace(minimum(x), maximum(x), 100)
 den=kerneldensity(x, xeval=xeval)"><pre class="notranslate"><code> using Distributions
 x=rand(Normal(), 500)
 xeval=linspace(minimum(x), maximum(x), 100)
 den=kerneldensity(x, xeval=xeval)
</code></pre></div>
</li>
<li>
<p dir="auto">Local regression</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" y=2 .* x.^2 + rand(Normal(), 500)
 yfit0=localconstant(x, y, xeval=xeval)
 yfit1=locallinear(x, y, xeval=xeval)
 yfit0=npr(x, y, xeval=xeval, reg=localconstant)
 yfit1=npr(x, y, xeval=xeval, reg=locallinear)"><pre class="notranslate"><code> y=2 .* x.^2 + rand(Normal(), 500)
 yfit0=localconstant(x, y, xeval=xeval)
 yfit1=locallinear(x, y, xeval=xeval)
 yfit0=npr(x, y, xeval=xeval, reg=localconstant)
 yfit1=npr(x, y, xeval=xeval, reg=locallinear)
</code></pre></div>
</li>
<li>
<p dir="auto">Confidence Band</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content=" cb=bootstrapCB(x, y, xeval=xeval)
 using Gadfly
 plot(layer(x=x, y=y, Geom.point), layer(x=xeval, y=yfit1, Geom.line, Theme(default_color=color(&quot;black&quot;))),
   layer(x = xeval, y = cb[1,:], Geom.line, Theme(default_color=color(&quot;red&quot;))),
   layer(x=xeval, y=cb[2,:], Geom.line, Theme(default_color=color(&quot;red&quot;))))"><pre class="notranslate"><code> cb=bootstrapCB(x, y, xeval=xeval)
 using Gadfly
 plot(layer(x=x, y=y, Geom.point), layer(x=xeval, y=yfit1, Geom.line, Theme(default_color=color("black"))),
   layer(x = xeval, y = cb[1,:], Geom.line, Theme(default_color=color("red"))),
   layer(x=xeval, y=cb[2,:], Geom.line, Theme(default_color=color("red"))))
</code></pre></div>
</li>
</ul>
<h2 dir="auto"><a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reference</h2>
<ul dir="auto">
<li>
<p dir="auto">[1] Lecture notes from Dr. Song Xi Chen</p>
</li>
<li>
<p dir="auto">[2] Chen, Song Xi. "Beta kernel estimators for density functions." <em>Computational Statistics &amp; Data Analysis</em> 31, no. 2 (1999): 131-145.</p>
</li>
<li>
<p dir="auto">[3] Chen, Song Xi. "Probability density function estimation using gamma kernels." <em>Annals of the Institute of Statistical Mathematics</em> 52, no. 3 (2000): 471-480.</p>
</li>
<li>
<p dir="auto">[4] W. Hardle and J. S. Marron (1991). Bootstrap Simultaneous Error Bars for Nonparametric Regression. <em>The Annals of Statistics</em>. Vol. 19, No. 2 (Jun., 1991), pp. 778-796</p>
</li>
<li>
<p dir="auto">[5] W.Hardle and E. Mammen (1993). Comparing Nonparametric Versus Parametric Regression Fits. <em>The Annals of Statistics</em>. Vol. 21, No. 4 (Dec., 1993), pp. 1926-1947</p>
</li>
<li>
<p dir="auto">[6] Clifford M. Hurvich, Jeffrey S. Simonoff and Chih-Ling Tsai (1998). Smoothing Parameter Selection in Nonparametric Regression Using an Improved Akaike Information Criterion. <em>Journal of the Royal Statistical Society. Series B (Statistical Methodology)</em>, Vol. 60, No. 2 (1998), pp. 271-293</p>
</li>
</ul>
</article></div>