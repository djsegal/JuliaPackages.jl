<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><div align="center">
<a href="https://en.wikipedia.org/wiki/Tangram" rel="nofollow"> <img src="./docs/logo/logo.gif" style="max-width:100%;"> </a>
<p> <a href="https://wiki.c2.com/?MakeItWorkMakeItRightMakeItFast" rel="nofollow">"Make It Work Make It Right Make It Fast"</a></p>
<p>― <a href="https://wiki.c2.com/?KentBeck" rel="nofollow">KentBeck</a></p>
</div>
<hr>
<p align="center">
  <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/actions?query=workflow%3ACI">
  <img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/workflows/CI/badge.svg" style="max-width:100%;">
  </a>
</p>
<p><g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> This package is moved into <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a> (2021-05-06)</p>
<p>This project aims to provide some implementations of the most typical reinforcement learning algorithms.</p>
<h1><a id="user-content-algorithms-implemented" class="anchor" aria-hidden="true" href="#algorithms-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Algorithms Implemented</h1>
<ul>
<li>VPG (Vanilla Policy Gradient, with a baseline)</li>
<li>DQN</li>
<li>Prioritized DQN</li>
<li>Rainbow</li>
<li>IQN</li>
<li>A2C/A2C with GAE/MAC</li>
<li>PPO</li>
<li>DDPG</li>
<li>TD3</li>
<li>SAC</li>
<li>CFR/OS-MCCFR/ES-MCCFR/DeepCFR</li>
<li>Minimax</li>
<li>Behavior Cloning</li>
</ul>
<p>If you are looking for tabular reinforcement learning algorithms, you may refer <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl">ReinforcementLearningAnIntroduction.jl</a>.</p>
<h1><a id="user-content-built-in-experiments" class="anchor" aria-hidden="true" href="#built-in-experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Built-in Experiments</h1>
<p>Some built-in experiments are exported to help new users to easily run benchmarks with one line. For experienced users, you are suggested to check <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/tree/master/src/experiments">the source code</a> of those experiments and make changes as needed.</p>
<h2><a id="user-content-list-of-built-in-experiments" class="anchor" aria-hidden="true" href="#list-of-built-in-experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>List of built-in experiments</h2>
<ul>
<li><code>E`JuliaRL_BasicDQN_CartPole` </code></li>
<li><code>E`JuliaRL_DQN_CartPole` </code></li>
<li><code>E`JuliaRL_PrioritizedDQN_CartPole` </code></li>
<li><code>E`JuliaRL_Rainbow_CartPole` </code></li>
<li><code>E`JuliaRL_IQN_CartPole` </code></li>
<li><code>E`JuliaRL_A2C_CartPole` </code></li>
<li><code>E`JuliaRL_A2CGAE_CartPole` </code> (Thanks to <a href="https://github.com/sriram13m">@sriram13m</a>)</li>
<li><code>E`JuliaRL_MAC_CartPole` </code> (Thanks to <a href="https://github.com/RajGhugare19">@RajGhugare19</a>)</li>
<li><code>E`JuliaRL_PPO_CartPole` </code></li>
<li><code>E`JuliaRL_VPG_CartPole` </code> (Thanks to <a href="https://github.com/norci">@norci</a>)</li>
<li><code>E`JuliaRL_VPG_Pendulum` </code> (continuous action space)</li>
<li><code>E`JuliaRL_VPG_PendulumD` </code> (discrete action space)</li>
<li><code>E`JuliaRL_DDPG_Pendulum` </code></li>
<li><code>E`JuliaRL_TD3_Pendulum` </code> (Thanks to <a href="https://github.com/rbange">@rbange</a>)</li>
<li><code>E`JuliaRL_SAC_Pendulum` </code> (Thanks to <a href="https://github.com/rbange">@rbange</a>)</li>
<li><code>E`JuliaRL_PPO_Pendulum` </code></li>
<li><code>E`JuliaRL_BasicDQN_MountainCar` </code> (Thanks to <a href="https://github.com/felixchalumeau">@felixchalumeau</a>)</li>
<li><code>E`JuliaRL_DQN_MountainCar` </code> (Thanks to <a href="https://github.com/felixchalumeau">@felixchalumeau</a>)</li>
<li><code>E`JuliaRL_Minimax_OpenSpiel(tic_tac_toe)` </code></li>
<li><code>E`JuliaRL_TabularCFR_OpenSpiel(kuhn_poker)` </code></li>
<li><code>E`JuliaRL_DeepCFR_OpenSpiel(leduc_poker)` </code></li>
<li><code>E`JuliaRL_DQN_SnakeGame` </code></li>
<li><code>E`JuliaRL_BC_CartPole` </code></li>
<li><code>E`JuliaRL_BasicDQN_EmptyRoom` </code></li>
<li><code>E`Dopamine_DQN_Atari(pong)` </code></li>
<li><code>E`Dopamine_Rainbow_Atari(pong)` </code></li>
<li><code>E`Dopamine_IQN_Atari(pong)` </code></li>
<li><code>E`rlpyt_A2C_Atari(pong)` </code></li>
<li><code>E`rlpyt_PPO_Atari(pong)` </code></li>
</ul>
<h2><a id="user-content-run-experiments" class="anchor" aria-hidden="true" href="#run-experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Run Experiments</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; ] add ReinforcementLearning

julia&gt; using ReinforcementLearning

julia&gt; run(E`JuliaRL_BasicDQN_CartPole`)

julia&gt; ] add ArcadeLearningEnvironment

julia&gt; using ArcadeLearningEnvironment

julia&gt; run(E`rlpyt_PPO_Atari(pong)`)  # the Atari environment is provided in ArcadeLearningEnvironment, so we need to install it first
"><pre>julia<span class="pl-k">&gt;</span> ] add ReinforcementLearning

julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ReinforcementLearning

julia<span class="pl-k">&gt;</span> <span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds"><span class="pl-c1">E</span>`</span>JuliaRL_BasicDQN_CartPole<span class="pl-pds">`</span></span>)

julia<span class="pl-k">&gt;</span> ] add ArcadeLearningEnvironment

julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ArcadeLearningEnvironment

julia<span class="pl-k">&gt;</span> <span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds"><span class="pl-c1">E</span>`</span>rlpyt_PPO_Atari(pong)<span class="pl-pds">`</span></span>)  <span class="pl-c"><span class="pl-c">#</span> the Atari environment is provided in ArcadeLearningEnvironment, so we need to install it first</span></pre></div>
<h3><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Notes:</h3>
<ul>
<li>Experiments on <code>CartPole</code> usually run faster with CPU only due to the overhead of sending data between CPU and GPU.</li>
<li>It shouldn't surprise you that our experiments on <code>CartPole</code> are much faster than those written in Python. The secret is that our environment is written in Julia!</li>
<li>Remember to set <code>JULIA_NUM_THREADS</code> to enable multi-threading when using algorithms like <code>A2C</code> and <code>PPO</code>.</li>
<li>Experiments on <code>Atari</code> (<code>OpenSpiel</code>, <code>SnakeGame</code>, <code>GridWorlds</code>) are only available after you have <code>ArcadeLearningEnvironment.jl</code> (<code>OpenSpiel.jl</code>, <code>SnakeGame.jl</code>, <code>GridWorlds.jl</code>) installed and <code>using ArcadeLearningEnvironment</code> (<code>using OpenSpiel</code>, <code>using SnakeGame</code>, <code>import GridWorlds</code>).</li>
</ul>
<h3><a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Speed</h3>
<ul>
<li>Different configurations might affect the performance a lot. According to our tests, our implementations are generally comparable to those written in PyTorch or TensorFlow with the same configuration (sometimes we are significantly faster).</li>
</ul>
<p>The following data are collected from experiments on <em>Intel(R) Xeon(R) W-2123 CPU @ 3.60GHz</em> with a GPU card of <em>RTX 2080ti</em>.</p>
<table>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="center">FPS</th>
<th align="right">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>E`Dopamine_DQN_Atari(pong)` </code></td>
<td align="center">~210</td>
<td align="right">Use the same config of <a href="https://github.com/google/dopamine/blob/master/dopamine/agents/dqn/configs/dqn.gin">dqn.gin in google/dopamine</a></td>
</tr>
<tr>
<td align="left"><code>E`Dopamine_Rainbow_Atari(pong)` </code></td>
<td align="center">~171</td>
<td align="right">Use the same config of <a href="https://github.com/google/dopamine/blob/master/dopamine/agents/implicit_quantile/configs/rainbow.gin">rainbow.gin in google/dopamine</a></td>
</tr>
<tr>
<td align="left"><code>E`Dopamine_IQN_Atari(pong)` </code></td>
<td align="center">~162</td>
<td align="right">Use the same config of <a href="https://github.com/google/dopamine/blob/master/dopamine/agents/implicit_quantile/configs/implicit_quantile.gin">implicit_quantile.gin in google/dopamine</a></td>
</tr>
<tr>
<td align="left"><code>E`rlpyt_A2C_Atari(pong)` </code></td>
<td align="center">~768</td>
<td align="right">Use the same default parameters of <a href="https://github.com/astooke/rlpyt/blob/master/rlpyt/algos/pg/a2c.py">A2C in rlpyt</a> with <strong>4 threads</strong></td>
</tr>
<tr>
<td align="left"><code>E`rlpyt_PPO_Atari(pong)` </code></td>
<td align="center">~711</td>
<td align="right">Use the same default parameters of <a href="https://github.com/astooke/rlpyt/blob/master/rlpyt/algos/pg/ppo.py">PPO in rlpyt</a> with <strong>4 threads</strong></td>
</tr>
</tbody>
</table>
</article></div>