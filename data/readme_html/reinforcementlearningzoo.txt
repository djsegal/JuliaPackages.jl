<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><div align="center"> 
<a href="https://en.wikipedia.org/wiki/Tangram" rel="nofollow"> <img src="https://camo.githubusercontent.com/fe90a0c742c4eefc16e9b2f36232026a817d32a3/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f372f37612f54616e6772616d2d6d616e2e737667" width="200" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/7/7a/Tangram-man.svg" style="max-width:100%;"> </a>
<p> <a href="https://wiki.c2.com/?MakeItWorkMakeItRightMakeItFast" rel="nofollow">"Make It Work Make It Right Make It Fast"</a></p>
<p>â€• <a href="https://wiki.c2.com/?KentBeck" rel="nofollow">KentBeck</a></p>
</div>
<hr>
<p align="center">
  <a href="https://travis-ci.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl" rel="nofollow">
  <img src="https://camo.githubusercontent.com/58ad3b6994305651453d7b0cdc3621d6fa56164d/68747470733a2f2f7472617669732d63692e636f6d2f4a756c69615265696e666f7263656d656e744c6561726e696e672f5265696e666f7263656d656e744c6561726e696e675a6f6f2e6a6c2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl.svg?branch=master" style="max-width:100%;">
  </a>
</p>
<p>This project aims to provide some implementations of the most typical reinforcement learning algorithms.</p>
<h1><a id="user-content-algorithms-implemented" class="anchor" aria-hidden="true" href="#algorithms-implemented"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Algorithms Implemented</h1>
<ul>
<li>DQN</li>
<li>Prioritized DQN</li>
<li>Rainbow</li>
<li>IQN</li>
<li>A2C</li>
<li>PPO</li>
<li>DDPG</li>
</ul>
<h1><a id="user-content-built-in-experiments" class="anchor" aria-hidden="true" href="#built-in-experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Built-in Experiments</h1>
<p>Some built-in experiments are exported to help new users to easily run benchmarks with one line (for example, <code>run(E`JuliaRL_BasicDQN_CartPole`)</code>). For experienced users, you are suggested to check <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/tree/master/src/experiments">the source code</a> of those experiments and make changes as needed.</p>
<h2><a id="user-content-list-of-built-in-experiments" class="anchor" aria-hidden="true" href="#list-of-built-in-experiments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>List of built-in experiments</h2>
<ul>
<li><code>E`JuliaRL_BasicDQN_CartPole` </code></li>
<li><code>E`JuliaRL_DQN_CartPole` </code></li>
<li><code>E`JuliaRL_PrioritizedDQN_CartPole` </code></li>
<li><code>E`JuliaRL_Rainbow_CartPole` </code></li>
<li><code>E`JuliaRL_IQN_CartPole` </code></li>
<li><code>E`JuliaRL_A2C_CartPole` </code></li>
<li><code>E`JuliaRL_A2CGAE_CartPole` </code></li>
<li><code>E`JuliaRL_PPO_CartPole` </code></li>
<li><code>E`JuliaRL_DDPG_Pendulum` </code></li>
<li><code>E`Dopamine_DQN_Atari(pong)` </code></li>
<li><code>E`Dopamine_Rainbow_Atari(pong)` </code></li>
<li><code>E`Dopamine_IQN_Atari(pong)` </code></li>
<li><code>E`JuliaRL_A2C_Atari(pong)` </code></li>
<li><code>E`JuliaRL_A2CGAE_Atari(pong)` </code></li>
<li><code>E`JuliaRL_PPO_Atari(pong)` </code></li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li>Experiments on <code>CartPole</code> usually run faster with CPU only due to the overhead of sending data between CPU and GPU.</li>
<li>It shouldn't surprise you that our experiments on <code>CartPole</code> are much faster than those written in Python. The secret is that our environment is written in Julia!</li>
<li>Remember to set <code>JULIA_NUM_THREADS</code> to enable multi-threading when using algorithms like <code>A2C</code> and <code>PPO</code>.</li>
<li>Experiments on <code>Atari</code> are only availabe when you have <code>ArcadeLearningEnvironment.jl</code> installed and <code>using ArcadeLearningEnvironment</code>.</li>
<li>Different configurations might affect the performance a lot. According to our tests, our implementations are generally comparable to those written in PyTorch or TensorFlow with the same configuration (sometimes we are significantly faster).</li>
</ul>
</article></div>