<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gengpt3jl" class="anchor" aria-hidden="true" href="#gengpt3jl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GenGPT3.jl</h1>
<p dir="auto"><a href="https://en.wikipedia.org/wiki/GPT-3" rel="nofollow">GPT-3</a> as a generative function in <a href="https://www.gen.dev/" rel="nofollow">Gen.jl</a>, implemented by wrapping the <a href="https://openai.com/api/" rel="nofollow">OpenAI API</a> in Gen's interface.</p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Install both Gen and this package via the Julia Pkg REPL:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="add Gen
add https://github.com/probcomp/GenGPT3.jl.git"><pre class="notranslate"><code>add Gen
add https://github.com/probcomp/GenGPT3.jl.git
</code></pre></div>
<p dir="auto">Add your OpenAI API key as an environment variable named <code>OPENAI_API_KEY</code>. You can follow <a href="https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety" rel="nofollow">this guide</a>, or set <code>ENV["OPENAI_API_KEY"]</code> to the value of your API key in the Julia REPL.</p>
<p dir="auto">Now you can construct GPT-3 as a generative function, and call GFI functions on it:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Gen, GenGPT3

# Construct GPT3GenerativeFunction
gpt3 = GPT3GF(model=&quot;text-davinci-002&quot;, max_tokens=256)

# Untraced execution 
prompt = &quot;What is the tallest mountain on Mars?&quot;
output = gpt3(prompt)

# Traced execution
trace = simulate(gpt3, (prompt,))

# Constrained generation
constraints = choicemap((:output, &quot;Olympus Mons.&quot;))
trace, weight = generate(gpt3, (prompt,), constraints)"><pre><span class="pl-k">using</span> Gen, GenGPT3

<span class="pl-c"><span class="pl-c">#</span> Construct GPT3GenerativeFunction</span>
gpt3 <span class="pl-k">=</span> <span class="pl-c1">GPT3GF</span>(model<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>text-davinci-002<span class="pl-pds">"</span></span>, max_tokens<span class="pl-k">=</span><span class="pl-c1">256</span>)

<span class="pl-c"><span class="pl-c">#</span> Untraced execution </span>
prompt <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>What is the tallest mountain on Mars?<span class="pl-pds">"</span></span>
output <span class="pl-k">=</span> <span class="pl-c1">gpt3</span>(prompt)

<span class="pl-c"><span class="pl-c">#</span> Traced execution</span>
trace <span class="pl-k">=</span> <span class="pl-c1">simulate</span>(gpt3, (prompt,))

<span class="pl-c"><span class="pl-c">#</span> Constrained generation</span>
constraints <span class="pl-k">=</span> <span class="pl-c1">choicemap</span>((<span class="pl-c1">:output</span>, <span class="pl-s"><span class="pl-pds">"</span>Olympus Mons.<span class="pl-pds">"</span></span>))
trace, weight <span class="pl-k">=</span> <span class="pl-c1">generate</span>(gpt3, (prompt,), constraints)</pre></div>
<h2 dir="auto"><a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Configuration</h2>
<p dir="auto">The constructor for a <code>GPT3GenerativeFunction</code> (or <code>GPT3GF</code> for short), can be used to configure a variety of options, documented below:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="GPT3GenerativeFunction(;
    model = &quot;text-davinci-002&quot;,
    temperature = 1.0,
    max_tokens = 1024,
    stop = nothing,
    api_key_lookup = () -&gt; ENV[&quot;OPENAI_API_KEY&quot;],
    organization_lookup = () -&gt; ENV[&quot;OPENAI_ORGANIZATION&quot;]
)"><pre><span class="pl-c1">GPT3GenerativeFunction</span>(;
    model <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>text-davinci-002<span class="pl-pds">"</span></span>,
    temperature <span class="pl-k">=</span> <span class="pl-c1">1.0</span>,
    max_tokens <span class="pl-k">=</span> <span class="pl-c1">1024</span>,
    stop <span class="pl-k">=</span> <span class="pl-c1">nothing</span>,
    api_key_lookup <span class="pl-k">=</span> () <span class="pl-k">-&gt;</span> <span class="pl-c1">ENV</span>[<span class="pl-s"><span class="pl-pds">"</span>OPENAI_API_KEY<span class="pl-pds">"</span></span>],
    organization_lookup <span class="pl-k">=</span> () <span class="pl-k">-&gt;</span> <span class="pl-c1">ENV</span>[<span class="pl-s"><span class="pl-pds">"</span>OPENAI_ORGANIZATION<span class="pl-pds">"</span></span>]
)</pre></div>
<p dir="auto">Constructs GPT-3 as a generative function, where sampling and scoring of completions are performed via calls to the OpenAI API.</p>
<p dir="auto">The generative function takes in a prompt as an (optional) argument, then samples and returns a completion. This represents a distribution over strings (up to <code>max_tokens</code> long) which end in a <code>stop</code> sequence. The completion is stored in the <code>:output</code> address of the resulting trace.</p>
<h3 dir="auto"><a id="user-content-arguments" class="anchor" aria-hidden="true" href="#arguments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Arguments</h3>
<ul dir="auto">
<li><code>model::String</code>:
The pretrained model to query. Defaults to <code>"text-davinci-002"</code>.</li>
<li><code>temperature::Float64 = 1.0</code>:
The softmax temperature. Values between <code>0.0</code> and <code>2.0</code> are allowed.
Higher temperatures increase randomness. Note that if this is not set
to <code>1.0</code>, then the resulting log probabilities will no longer be normalized.</li>
<li><code>max_tokens::Int = 1024</code>:
The maximum number of output tokens generated (including the stop sequence).</li>
<li><code>stop::Union{String,Nothing} = nothing</code>:
The stop sequence as a string. Defaults to the <code>&lt;|endoftext|&gt;</code> token if not
specified. If specified, then the model will be prevented from generating
any <code>&lt;|endoftext|&gt;</code> tokens (to avoid multiple termination possibilities).</li>
<li><code>api_key_lookup::Function</code>:
A zero-argument function that returns the OpenAI API key. Defaults to
looking up the <code>"OPENAI_API_KEY"</code> environment variable.</li>
<li><code>organization_lookup::Function</code>:
A zero-argument function that returns the OpenAI organization ID to use.
Defaults to the <code>"OPENAI_ORGANIZATION"</code> environment variable, if specified.</li>
</ul>
<h2 dir="auto"><a id="user-content-utilities" class="anchor" aria-hidden="true" href="#utilities"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Utilities</h2>
<p dir="auto">Utilities for converting between strings and tokens are also included as part of this package (using functionality provided by <a href="https://github.com/chengchingwen/BytePairEncoding.jl%60">BytePairEncoding.jl</a> and <a href="https://github.com/chengchingwen/TextEncodeBase.jl">TextEncodeBase.jl</a>):</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; tokens = GenGPT3.tokenize(&quot;What is the tallest mountain on Mars?&quot;)
[&quot;What&quot;, &quot;Ġis&quot;, &quot;Ġthe&quot;, &quot;Ġtallest&quot;, &quot;Ġmountain&quot;, &quot;Ġon&quot;, &quot;ĠMars&quot;, &quot;?&quot;]

julia&gt; ids = GenGPT3.encode(tokens)
[2061, 318, 262, 38760, 8598, 319, 8706, 30]

julia&gt; text = GenGPT3.id_detokenize(ids)
&quot;What is the tallest mountain on Mars?&quot;

julia&gt; ids = GenGPT3.id_tokenize(text)
[2061, 318, 262, 38760, 8598, 319, 8706, 30]

julia&gt; tokens = GenGPT3.decode(ids)
[&quot;What&quot;, &quot;Ġis&quot;, &quot;Ġthe&quot;, &quot;Ġtallest&quot;, &quot;Ġmountain&quot;, &quot;Ġon&quot;, &quot;ĠMars&quot;, &quot;?&quot;]"><pre lang="julia-repl" class="notranslate"><code>julia&gt; tokens = GenGPT3.tokenize("What is the tallest mountain on Mars?")
["What", "Ġis", "Ġthe", "Ġtallest", "Ġmountain", "Ġon", "ĠMars", "?"]

julia&gt; ids = GenGPT3.encode(tokens)
[2061, 318, 262, 38760, 8598, 319, 8706, 30]

julia&gt; text = GenGPT3.id_detokenize(ids)
"What is the tallest mountain on Mars?"

julia&gt; ids = GenGPT3.id_tokenize(text)
[2061, 318, 262, 38760, 8598, 319, 8706, 30]

julia&gt; tokens = GenGPT3.decode(ids)
["What", "Ġis", "Ġthe", "Ġtallest", "Ġmountain", "Ġon", "ĠMars", "?"]
</code></pre></div>
</article></div>