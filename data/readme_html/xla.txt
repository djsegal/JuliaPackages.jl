<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-xlajl---compiling-julia-to-xla" class="anchor" aria-hidden="true" href="#xlajl---compiling-julia-to-xla"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>XLA.jl - Compiling Julia to XLA</h1>
<p>NOTE: We're in the process of adding better instructions. Check back in a bit.</p>
<h1><a id="user-content-getting-started-on-tpus" class="anchor" aria-hidden="true" href="#getting-started-on-tpus"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting started on TPUs</h1>
<h2><a id="user-content-running-on-colab" class="anchor" aria-hidden="true" href="#running-on-colab"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Running on Colab</h2>
<p>Google currently offers free access to Cloud TPUs through its Colab notebook
service. Colab does not officially support julia at the moment, but it is
possible to install julia by manually installing it into the runtime (though
this has to be done every time the runtime gets reset). By this mechanism,
you can get access to TPUs through the notebook interface. Start with the installation
notebook in <code>docs/colab/InstallJuliaXLA.ipynb</code>:</p>
<p align="center">
<a href="https://colab.research.google.com/github/JuliaTPU/XLA.jl/blob/master/docs/colab/InstallJuliaXLA.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/dfbf50eed8dd2dea5f3e0beaaf2001eeca77f314/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f636f6c61625f6c6f676f5f333270782e706e67" alt="Open in Colab" data-canonical-src="https://www.tensorflow.org/images/colab_logo_32px.png" style="max-width:100%;"><br>Run in Google Colab</a>
</p>
<p>Afterwards, any JuliaTPU notebook should work if opened without resetting the runtime
in between.</p>
<h2><a id="user-content-running-on-gcp" class="anchor" aria-hidden="true" href="#running-on-gcp"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Running on GCP</h2>
<p>The process for setting up this repository to run against TPUs is much
the same as the process for setting up the repository locally. However, since
there is additional steps involved in launching the actual TPU, we are providing
a tutorial to walk you through all the steps. It is recommended for those new to
Julia and/or TPUs. If you're already familiar with both, you may skip the tutorial
and just use the setup guide below. The tutorial will open in Google Cloud Shell,
by clicking the button below:</p>
<p align="center">
<a href="https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2FJuliaTPU%2FXLA.jl&amp;cloudshell_tutorial=docs%2Ftpu_tutorial.md" rel="nofollow">
<img alt="Open in Cloud Shell" src="https://camo.githubusercontent.com/acf32864fdae185325f992b48fb2132badef1e9a/687474703a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e737667" data-canonical-src="http://gstatic.com/cloudssh/images/open-btn.svg" style="max-width:100%;"></a>
</p>
<h1><a id="user-content-getting-started-cpugpu-backend" class="anchor" aria-hidden="true" href="#getting-started-cpugpu-backend"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting started (CPU/GPU backend)</h1>
<ul>
<li>Grab julia on branch <a href="https://github.com/JuliaLang/julia/tree/kf/tpu3">kf/tpu3</a> (Prebuilt Linux x86_64 binaries with TPU support are available <a href="https://storage.googleapis.com/julia-tpu-binaries/julia.v1.1.0-kf.tpu3.x86_64-linux-gnu.tar.gz" rel="nofollow">here</a>)</li>
<li>Instantiate this repo</li>
<li><code>julia&gt; using TensorFlow</code></li>
<li>Get yourself an <code>xrt_server</code> (either running it locally via <code>run(`$(joinpath(dirname(pathof(TensorFlow)),"..","deps","downloads","bin","xrt_server"))`)</code> or by spinning up a Google Cloud TPU and starting an SSH tunnel to expose its port to the world) and connect it to localhost:8470</li>
<li>Run the script in <code>examples/vgg_forward.jl</code></li>
</ul>
</article></div>