<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://travis-ci.org/kzahedi/Shannon.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e6217b0b6fece81ea091058c4fa859252593b3b7f0f038a179e9591e7771d280/68747470733a2f2f7472617669732d63692e6f72672f6b7a61686564692f5368616e6e6f6e2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/kzahedi/Shannon.jl.svg?branch=master" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-shannonjl" class="anchor" aria-hidden="true" href="#shannonjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Shannon.jl</h1>
<p dir="auto">A collection of quantifications related to Shannon's information theory and methods to discretise data.</p>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using Shannon
xy = hcat([sin(x) + randn() * .1 for x=0:0.01:2pi], [cos(x) + randn() * .1 for x=0:0.01:2pi])
bxy = bin_matrix(xy, -1.0, 1.0, 10)
c=combine_binned_matrix(bxy)
c=relabel(c)
H = entropy(c)
I = MI(bxy)"><pre class="notranslate"><code>using Shannon
xy = hcat([sin(x) + randn() * .1 for x=0:0.01:2pi], [cos(x) + randn() * .1 for x=0:0.01:2pi])
bxy = bin_matrix(xy, -1.0, 1.0, 10)
c=combine_binned_matrix(bxy)
c=relabel(c)
H = entropy(c)
I = MI(bxy)
</code></pre></div>
<p dir="auto">A faster way is to call</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="unary_of_matrix(xy, -1.0, 1.0, 10)"><pre class="notranslate"><code>unary_of_matrix(xy, -1.0, 1.0, 10)
</code></pre></div>
<p dir="auto">which is a short cut for the lines below</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="bxy = bin_matrix(xy, -1.0, 1.0, 10)
c=combine_binned_matrix(bxy)
c=relabel(c)"><pre class="notranslate"><code>bxy = bin_matrix(xy, -1.0, 1.0, 10)
c=combine_binned_matrix(bxy)
c=relabel(c)
</code></pre></div>
<h2 dir="auto"><a id="user-content-entropy-estimators" class="anchor" aria-hidden="true" href="#entropy-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Entropy estimators</h2>
<p dir="auto">The estimators are implemented from the following list of publications:</p>
<p dir="auto">[1] A. Chao and T.-J. Shen. Nonparametric estimation of shannon’s index of diversity when there are unseen species in sample. Environmental and Ecological Statistics, 10(4):429–443, 2003.</p>
<p dir="auto">and the function call is</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entropy(data, base=2, mode=&quot;ML&quot;)"><pre class="notranslate"><code>entropy(data, base=2, mode="ML")
</code></pre></div>
<p dir="auto">where</p>
<table>
<tbody><tr> <td> **data** </td> <td> is the discrete data (*Vector{Int64}*)</td></tr>
<tr> <td valign="top"> **mode** </td> <td> determines which estimator should be used (see below). It is *not* case-sensitive </td> </tr>
<tr> <td> **base** </td>  <td> determines the base of the logarithm </td> </tr>
 </tbody></table>
<p dir="auto">###Maximum Likelihood Estimator
This is the default estimator.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entropy(data)
entropy(data, mode=&quot;ML&quot;)
entropy(data, mode=&quot;Maximum Likelihood&quot;)"><pre class="notranslate"><code>entropy(data)
entropy(data, mode="ML")
entropy(data, mode="Maximum Likelihood")
</code></pre></div>
<p dir="auto">###Maximum Likelihood Estimator with Bias Correction (implemented from [1])</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entropy(data, mode=&quot;MLBC&quot;)
entropy(data, mode=&quot;Maximum Likelihood with Bias Compensation&quot;)"><pre class="notranslate"><code>entropy(data, mode="MLBC")
entropy(data, mode="Maximum Likelihood with Bias Compensation")
</code></pre></div>
<p dir="auto">###Horovitz-Thompson Estimator (implemented from [1])</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entropy(data, mode=&quot;HT&quot;)
entropy(data, mode=&quot;Horovitz-Thompson&quot;)"><pre class="notranslate"><code>entropy(data, mode="HT")
entropy(data, mode="Horovitz-Thompson")
</code></pre></div>
<p dir="auto">###Chao-Shen Estimator (implemented from [1])</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entropy(data, mode=&quot;CS&quot;)
entropy(data, mode=&quot;Chao-Shen&quot;)
entropy(data, mode=&quot;ChaoShen&quot;)"><pre class="notranslate"><code>entropy(data, mode="CS")
entropy(data, mode="Chao-Shen")
entropy(data, mode="ChaoShen")
</code></pre></div>
<h4 dir="auto"><a id="user-content-setting-the-base" class="anchor" aria-hidden="true" href="#setting-the-base"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setting the base</h4>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="entropy(data, base=2) [ this is the default ]
entropy(data, mode=&quot;HT&quot;, base=10)"><pre class="notranslate"><code>entropy(data, base=2) [ this is the default ]
entropy(data, mode="HT", base=10)
</code></pre></div>
<h2 dir="auto"><a id="user-content-mutual-information-estimators" class="anchor" aria-hidden="true" href="#mutual-information-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Mutual Information estimators</h2>
<p dir="auto">Currently, only the <em>maximum likelihood estimator</em> is implemented. It can be used with different bases:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="MI(xy, base=2) [ this is the default ]
MI(xy, base=10)"><pre class="notranslate"><code>MI(xy, base=2) [ this is the default ]
MI(xy, base=10)
</code></pre></div>
<p dir="auto"><strong>xy</strong> is a two-dimensional matrix with <strong>n</strong> rows and two columns.</p>
<h2 dir="auto"><a id="user-content-predictive-information" class="anchor" aria-hidden="true" href="#predictive-information"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Predictive Information</h2>
<p dir="auto">This in an implementation of the one-step predictive information, which is given by the mutual information of consecutive data points. If x is the data vector, then:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="PI(x) = MI(hcat(x[1:end-1], x[2:end]))
PI(x,[base],[mode]) = MI(x[1:end-1], x[2:end], base, mode)"><pre class="notranslate"><code>PI(x) = MI(hcat(x[1:end-1], x[2:end]))
PI(x,[base],[mode]) = MI(x[1:end-1], x[2:end], base, mode)
</code></pre></div>
<h2 dir="auto"><a id="user-content-kullback-leibler-divergence" class="anchor" aria-hidden="true" href="#kullback-leibler-divergence"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Kullback-Leibler Divergence</h2>
<p dir="auto">This function calculates the KL-Divergence on two probability distributions, and is essentially given by:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="KL(p,q)=  sum([(p[i] != 0 &amp;&amp; q[i] != 0)? p[i] * log(base, p[i]/q[i]) : 0 for i=1:length(p)])"><pre class="notranslate"><code>KL(p,q)=  sum([(p[i] != 0 &amp;&amp; q[i] != 0)? p[i] * log(base, p[i]/q[i]) : 0 for i=1:length(p)])
</code></pre></div>
<p dir="auto"><strong>p</strong>,<strong>q</strong> must be valid probability distributions, i.e.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="x &gt;= 0 for x in p
y &gt;= 0 for y in q
sum(p) == sum(q) == 1.0"><pre class="notranslate"><code>x &gt;= 0 for x in p
y &gt;= 0 for y in q
sum(p) == sum(q) == 1.0
</code></pre></div>
<h2 dir="auto"><a id="user-content-quantifying-morphological-computation" class="anchor" aria-hidden="true" href="#quantifying-morphological-computation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quantifying Morphological Computation</h2>
<p dir="auto">Implementation of measures from</p>
<p dir="auto">Quantifying Morphological Computation, Zahedi &amp; Ay, Entropy, 2013: [<a href="http://www.mdpi.com/1099-4300/15/5/1887" rel="nofollow">pdf</a>]</p>
<p dir="auto">and</p>
<p dir="auto">Quantifying Morphological Computation based on an Information Decomposition of the Sensorimotor Loop, Ghazi-Zahedi &amp; Rauh, ECAL 2015: [<a href="https://mitpress.mit.edu/sites/default/files/titles/content/ecal2015/ch017.html" rel="nofollow">pdf</a>]</p>
</article></div>