<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-advancedvijl" class="anchor" aria-hidden="true" href="#advancedvijl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AdvancedVI.jl</h1>
<p>A library for variational Bayesian inference in Julia.</p>
<p>At the time of writing (05/02/2020), implementations of the variational inference (VI) interface and some algorithms are implemented in <a href="https://github.com/TuringLang/Turing.jl">Turing.jl</a>. The idea is to soon separate the VI functionality in Turing.jl out and into this package.</p>
<p>The purpose of this package will then be to provide a common interface together with implementations of standard algorithms and utilities with the goal of ease of use and the ability for other packages, e.g. Turing.jl, to write a light wrapper around AdvancedVI.jl for integration.</p>
<p>As an example, in Turing.jl we support automatic differentiation variational inference (ADVI) but really the only piece of code tied into the Turing.jl is the conversion of a <code>Turing.Model</code> to a <code>logjoint(z)</code> function which computes <code>z ‚Ü¶ log p(x, z)</code>, with <code>x</code> denoting the observations embedded in the <code>Turing.Model</code>. As long as this <code>logjoint(z)</code> method is compatible with some AD framework, e.g. <code>ForwardDiff.jl</code> or <code>Zygote.jl</code>, this is all we need from Turing.jl to be able to perform ADVI!</p>
<h2><a id="user-content-wip-interface" class="anchor" aria-hidden="true" href="#wip-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>[WIP] Interface</h2>
<ul>
<li><code>vi</code>: the main interface to the functionality in this package
<ul>
<li><code>vi(model, alg)</code>: only used when <code>alg</code> has a default variational posterior which it will provide.</li>
<li><code>vi(model, alg, q::VariationalPosterior, Œ∏)</code>: <code>q</code> represents the family of variational distributions and <code>Œ∏</code> is the initial parameters "indexing" the starting distribution. This assumes that there exists an implementation <code>Variational.update(q, Œ∏)</code> which returns the variational posterior corresponding to parameters <code>Œ∏</code>.</li>
<li><code>vi(model, alg, getq::Function, Œ∏)</code>: here <code>getq(Œ∏)</code> is a function returning a <code>VariationalPosterior</code> corresponding to <code>Œ∏</code>.</li>
</ul>
</li>
<li><code>optimize!(vo, alg::VariationalInference{AD}, q::VariationalPosterior, model::Model, Œ∏; optimizer = TruncatedADAGrad())</code></li>
<li><code>grad!(vo, alg::VariationalInference, q, model::Model, Œ∏, out, args...)</code>
<ul>
<li>Different combinations of variational objectives (<code>vo</code>), VI methods (<code>alg</code>), and variational posteriors (<code>q</code>) might use different gradient estimators. <code>grad!</code> allows us to specify these different behaviors.</li>
</ul>
</li>
</ul>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<h3><a id="user-content-variational-inference" class="anchor" aria-hidden="true" href="#variational-inference"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Variational Inference</h3>
<p>A very simple generative model is the following</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Œº ~ ùí©(0, 1)
x·µ¢ ‚àº ùí©(Œº, 1) , ‚àÄi = 1, ‚Ä¶, n
"><pre><code>Œº ~ ùí©(0, 1)
x·µ¢ ‚àº ùí©(Œº, 1) , ‚àÄi = 1, ‚Ä¶, n
</code></pre></div>
<p>where Œº and x·µ¢ are some ‚Ñù·µà vectors and ùí© denotes a d-dimensional multivariate Normal distribution.</p>
<p>Given a set of <code>n</code> observations <code>[x‚ÇÅ, ‚Ä¶, x‚Çô]</code> we're interested in finding the distribution <code>p(Œº‚à£x‚ÇÅ, ‚Ä¶, x‚Çô)</code> over the mean <code>Œº</code>. We can obtain (an approximation to) this distribution that using AdvancedVI.jl!</p>
<p>First we generate some observations and set up the problem:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using Distributions

julia&gt; d = 2; n = 100;

julia&gt; observations = randn((d, n)); # 100 observations from 2D ùí©(0, 1)

julia&gt; # Define generative model
       #    Œº ~ ùí©(0, 1)
       #    x·µ¢ ‚àº ùí©(Œº, 1) , ‚àÄi = 1, ‚Ä¶, n
       prior(Œº) = logpdf(MvNormal(ones(d)), Œº)
prior (generic function with 1 method)

julia&gt; likelihood(x, Œº) = sum(logpdf(MvNormal(Œº, ones(d)), x))
likelihood (generic function with 1 method)

julia&gt; logœÄ(Œº) = likelihood(observations, Œº) + prior(Œº)
logœÄ (generic function with 1 method)

julia&gt; logœÄ(randn(2))  # &lt;= just checking that it works
-311.74132761437653
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Distributions

julia<span class="pl-k">&gt;</span> d <span class="pl-k">=</span> <span class="pl-c1">2</span>; n <span class="pl-k">=</span> <span class="pl-c1">100</span>;

julia<span class="pl-k">&gt;</span> observations <span class="pl-k">=</span> <span class="pl-c1">randn</span>((d, n)); <span class="pl-c"><span class="pl-c">#</span> 100 observations from 2D ùí©(0, 1)</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Define generative model</span>
       <span class="pl-c"><span class="pl-c">#</span>    Œº ~ ùí©(0, 1)</span>
       <span class="pl-c"><span class="pl-c">#</span>    x·µ¢ ‚àº ùí©(Œº, 1) , ‚àÄi = 1, ‚Ä¶, n</span>
       <span class="pl-en">prior</span>(Œº) <span class="pl-k">=</span> <span class="pl-c1">logpdf</span>(<span class="pl-c1">MvNormal</span>(<span class="pl-c1">ones</span>(d)), Œº)
prior (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-en">likelihood</span>(x, Œº) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">logpdf</span>(<span class="pl-c1">MvNormal</span>(Œº, <span class="pl-c1">ones</span>(d)), x))
likelihood (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-en">logœÄ</span>(Œº) <span class="pl-k">=</span> <span class="pl-c1">likelihood</span>(observations, Œº) <span class="pl-k">+</span> <span class="pl-c1">prior</span>(Œº)
logœÄ (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">logœÄ</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">2</span>))  <span class="pl-c"><span class="pl-c">#</span> &lt;= just checking that it works</span>
<span class="pl-k">-</span><span class="pl-c1">311.74132761437653</span></pre></div>
<p>Now there are mainly two different ways of specifying the approximate posterior (and its family). The first is by providing a mapping from distribution parameters to the distribution <code>Œ∏ ‚Ü¶ q(‚ãÖ‚à£Œ∏)</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using DistributionsAD, AdvancedVI

julia&gt; # Using a function z ‚Ü¶ q(‚ãÖ‚à£z)
       getq(Œ∏) = TuringDiagMvNormal(Œ∏[1:d], exp.(Œ∏[d + 1:4]))
getq (generic function with 1 method)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> DistributionsAD, AdvancedVI

julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Using a function z ‚Ü¶ q(‚ãÖ‚à£z)</span>
       <span class="pl-en">getq</span>(Œ∏) <span class="pl-k">=</span> <span class="pl-c1">TuringDiagMvNormal</span>(Œ∏[<span class="pl-c1">1</span><span class="pl-k">:</span>d], <span class="pl-c1">exp</span>.(Œ∏[d <span class="pl-k">+</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>]))
getq (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)</pre></div>
<p>Then we make the choice of algorithm, a subtype of <code>VariationalInference</code>,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; # Perform VI
       advi = ADVI(10, 10_000)
ADVI{AdvancedVI.ForwardDiffAD{40}}(10, 10000)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Perform VI</span>
       advi <span class="pl-k">=</span> <span class="pl-c1">ADVI</span>(<span class="pl-c1">10</span>, <span class="pl-c1">10_000</span>)
<span class="pl-c1">ADVI</span><span class="pl-c1">{AdvancedVI.ForwardDiffAD{40}}</span>(<span class="pl-c1">10</span>, <span class="pl-c1">10000</span>)</pre></div>
<p>And finally we can perform VI! The usual inferface is to call <code>vi</code> which behind the scenes takes care of the optimization and returns the resulting variational posterior:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; q = vi(logœÄ, advi, getq, randn(4))
[ADVI] Optimizing...100% Time: 0:00:01
TuringDiagMvNormal{Array{Float64,1},Array{Float64,1}}(m=[0.16282745378074515, 0.15789310089462574], œÉ=[0.09519377533754399, 0.09273176907111745])
"><pre>julia<span class="pl-k">&gt;</span> q <span class="pl-k">=</span> <span class="pl-c1">vi</span>(logœÄ, advi, getq, <span class="pl-c1">randn</span>(<span class="pl-c1">4</span>))
[ADVI] Optimizing<span class="pl-k">...</span><span class="pl-c1">100</span><span class="pl-k">%</span> Time<span class="pl-k">:</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">00</span><span class="pl-k">:</span><span class="pl-c1">01</span>
<span class="pl-c1">TuringDiagMvNormal</span><span class="pl-c1">{Array{Float64,1},Array{Float64,1}}</span>(m<span class="pl-k">=</span>[<span class="pl-c1">0.16282745378074515</span>, <span class="pl-c1">0.15789310089462574</span>], œÉ<span class="pl-k">=</span>[<span class="pl-c1">0.09519377533754399</span>, <span class="pl-c1">0.09273176907111745</span>])</pre></div>
<p>Let's have a look at the resulting ELBO:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; AdvancedVI.elbo(advi, q, logœÄ, 1000)
-287.7866366886285
"><pre>julia<span class="pl-k">&gt;</span> AdvancedVI<span class="pl-k">.</span><span class="pl-c1">elbo</span>(advi, q, logœÄ, <span class="pl-c1">1000</span>)
<span class="pl-k">-</span><span class="pl-c1">287.7866366886285</span></pre></div>
<p>Unfortunately, the <em>final</em> value of the ELBO is not always a very good diagnostic, though the ELBO is an important metric to keep an eye on during training since an <em>increase</em> in the ELBO means we're going in the right direction. Luckily, this is such a simple problem that we can indeed obtain a closed form solution! Because we're lazy (at least I am), we'll let <a href="https://github.com/JuliaStats/ConjugatePriors.jl">ConjugatePriors.jl</a> do this for us:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; # True posterior
       using ConjugatePriors

julia&gt; pri = MvNormal(zeros(2), ones(2));

julia&gt; true_posterior = posterior((pri, pri.Œ£), MvNormal, observations)
DiagNormal(
dim: 2
Œº: [0.1746546592601148, 0.16457110079543008]
Œ£: [0.009900990099009901 0.0; 0.0 0.009900990099009901]
)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> True posterior</span>
       <span class="pl-k">using</span> ConjugatePriors

julia<span class="pl-k">&gt;</span> pri <span class="pl-k">=</span> <span class="pl-c1">MvNormal</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">ones</span>(<span class="pl-c1">2</span>));

julia<span class="pl-k">&gt;</span> true_posterior <span class="pl-k">=</span> <span class="pl-c1">posterior</span>((pri, pri<span class="pl-k">.</span>Œ£), MvNormal, observations)
<span class="pl-c1">DiagNormal</span>(
dim<span class="pl-k">:</span> <span class="pl-c1">2</span>
Œº: [<span class="pl-c1">0.1746546592601148</span>, <span class="pl-c1">0.16457110079543008</span>]
Œ£: [<span class="pl-c1">0.009900990099009901</span> <span class="pl-c1">0.0</span>; <span class="pl-c1">0.0</span> <span class="pl-c1">0.009900990099009901</span>]
)</pre></div>
<p>Comparing to our variational approximation, this looks pretty good! Worth noting that in this particular case the variational posterior seems to overestimate the variance.</p>
<p>To conclude, let's make a somewhat pretty picture:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using Plots

julia&gt; p_samples = rand(true_posterior, 10_000); q_samples = rand(q, 10_000);

julia&gt; p1 = histogram(p_samples[1, :], label=&quot;p&quot;); histogram!(q_samples[1, :], alpha=0.7, label=&quot;q&quot;)

julia&gt; title!(raw&quot;$\mu_1$&quot;)

julia&gt; p2 = histogram(p_samples[2, :], label=&quot;p&quot;); histogram!(q_samples[2, :], alpha=0.7, label=&quot;q&quot;)

julia&gt; title!(raw&quot;$\mu_2$&quot;)

julia&gt; plot(p1, p2)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Plots

julia<span class="pl-k">&gt;</span> p_samples <span class="pl-k">=</span> <span class="pl-c1">rand</span>(true_posterior, <span class="pl-c1">10_000</span>); q_samples <span class="pl-k">=</span> <span class="pl-c1">rand</span>(q, <span class="pl-c1">10_000</span>);

julia<span class="pl-k">&gt;</span> p1 <span class="pl-k">=</span> <span class="pl-c1">histogram</span>(p_samples[<span class="pl-c1">1</span>, :], label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>p<span class="pl-pds">"</span></span>); <span class="pl-c1">histogram!</span>(q_samples[<span class="pl-c1">1</span>, :], alpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>q<span class="pl-pds">"</span></span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">title!</span>(<span class="pl-s"><span class="pl-c1">raw</span><span class="pl-pds">"</span>$\mu_1$<span class="pl-pds">"</span></span>)

julia<span class="pl-k">&gt;</span> p2 <span class="pl-k">=</span> <span class="pl-c1">histogram</span>(p_samples[<span class="pl-c1">2</span>, :], label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>p<span class="pl-pds">"</span></span>); <span class="pl-c1">histogram!</span>(q_samples[<span class="pl-c1">2</span>, :], alpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>q<span class="pl-pds">"</span></span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">title!</span>(<span class="pl-s"><span class="pl-c1">raw</span><span class="pl-pds">"</span>$\mu_2$<span class="pl-pds">"</span></span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">plot</span>(p1, p2)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="hist.png?raw=true"><img src="hist.png?raw=true" alt="Histogram" style="max-width:100%;"></a></p>
<h3><a id="user-content-simple-example-using-advancedjl-to-directly-minimize-the-kl-divergence-between-two-distributions-pz-and-qz" class="anchor" aria-hidden="true" href="#simple-example-using-advancedjl-to-directly-minimize-the-kl-divergence-between-two-distributions-pz-and-qz"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Simple example: using Advanced.jl to directly minimize the KL-divergence between two distributions <code>p(z)</code> and <code>q(z)</code></h3>
<p>In VI we aim to approximate the true posterior <code>p(z ‚à£ x)</code> by some approximate variational posterior <code>q(z)</code> by maximizing the ELBO:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="ELBO(q) = ùîº_q[log p(x, z) - log q(z)]
"><pre><code>ELBO(q) = ùîº_q[log p(x, z) - log q(z)]
</code></pre></div>
<p>Observe that we can express the ELBO as the negative KL-divergence between <code>p(x, ‚ãÖ)</code> and <code>q(‚ãÖ)</code>:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="ELBO(q) = - ùîº_q[log (q(z) / p(x, z))]
        = - KL(q(‚ãÖ) || p(x, ‚ãÖ))
"><pre><code>ELBO(q) = - ùîº_q[log (q(z) / p(x, z))]
        = - KL(q(‚ãÖ) || p(x, ‚ãÖ))
</code></pre></div>
<p>So if we apply VI to something that isn't an actual posterior, i.e. there's no data involved and we write <code>p(z ‚à£ x) = p(z)</code>, we're really just minimizing the KL-divergence between the distributions.</p>
<p>Therefore, we can try out <code>AdvancedVI.jl</code> real quick by applying using the interface to minimize the KL-divergence between two distributions:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using Distributions, DistributionsAD, AdvancedVI

julia&gt; # Target distribution
       p = MvNormal(ones(2))
ZeroMeanDiagNormal(
dim: 2
Œº: [0.0, 0.0]
Œ£: [1.0 0.0; 0.0 1.0]
)

julia&gt; logœÄ(z) = logpdf(p, z)
logœÄ (generic function with 1 method)

julia&gt; # Make a choice of VI algorithm
       advi = ADVI(10, 1000)
ADVI{AdvancedVI.ForwardDiffAD{40}}(10, 1000)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Distributions, DistributionsAD, AdvancedVI

julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Target distribution</span>
       p <span class="pl-k">=</span> <span class="pl-c1">MvNormal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>))
<span class="pl-c1">ZeroMeanDiagNormal</span>(
dim<span class="pl-k">:</span> <span class="pl-c1">2</span>
Œº: [<span class="pl-c1">0.0</span>, <span class="pl-c1">0.0</span>]
Œ£: [<span class="pl-c1">1.0</span> <span class="pl-c1">0.0</span>; <span class="pl-c1">0.0</span> <span class="pl-c1">1.0</span>]
)

julia<span class="pl-k">&gt;</span> <span class="pl-en">logœÄ</span>(z) <span class="pl-k">=</span> <span class="pl-c1">logpdf</span>(p, z)
logœÄ (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Make a choice of VI algorithm</span>
       advi <span class="pl-k">=</span> <span class="pl-c1">ADVI</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1000</span>)
<span class="pl-c1">ADVI</span><span class="pl-c1">{AdvancedVI.ForwardDiffAD{40}}</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1000</span>)</pre></div>
<p>Now there are two different ways of specifying the approximate posterior (and its family); the first is by providing a mapping from parameters to distribution <code>Œ∏ ‚Ü¶ q(‚ãÖ‚à£Œ∏)</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; # Using a function z ‚Ü¶ q(‚ãÖ‚à£z)
       getq(Œ∏) = TuringDiagMvNormal(Œ∏[1:2], exp.(Œ∏[3:4]))
getq (generic function with 1 method)

julia&gt; # Perform VI
       q = vi(logœÄ, advi, getq, randn(4))
‚îå Info: [ADVI] Should only be seen once: optimizer created for Œ∏
‚îî   objectid(Œ∏) = 0x5ddb564423896704
[ADVI] Optimizing...100% Time: 0:00:01
TuringDiagMvNormal{Array{Float64,1},Array{Float64,1}}(m=[-0.012691337868985757, -0.0004442434543332919], œÉ=[1.0334797673569802, 0.9957355128767893])
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Using a function z ‚Ü¶ q(‚ãÖ‚à£z)</span>
       <span class="pl-en">getq</span>(Œ∏) <span class="pl-k">=</span> <span class="pl-c1">TuringDiagMvNormal</span>(Œ∏[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span>], <span class="pl-c1">exp</span>.(Œ∏[<span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">4</span>]))
getq (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span> Perform VI</span>
       q <span class="pl-k">=</span> <span class="pl-c1">vi</span>(logœÄ, advi, getq, <span class="pl-c1">randn</span>(<span class="pl-c1">4</span>))
‚îå Info<span class="pl-k">:</span> [ADVI] Should only be seen once<span class="pl-k">:</span> optimizer created <span class="pl-k">for</span> Œ∏
‚îî   <span class="pl-en">objectid</span>(Œ∏) <span class="pl-k">=</span> <span class="pl-c1">0x5ddb564423896704</span>
[ADVI] Optimizing<span class="pl-k">...</span><span class="pl-c1">100</span><span class="pl-k">%</span> Time<span class="pl-k">:</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">00</span><span class="pl-k">:</span><span class="pl-c1">01</span>
<span class="pl-c1">TuringDiagMvNormal</span><span class="pl-c1">{Array{Float64,1},Array{Float64,1}}</span>(m<span class="pl-k">=</span>[<span class="pl-k">-</span><span class="pl-c1">0.012691337868985757</span>, <span class="pl-k">-</span><span class="pl-c1">0.0004442434543332919</span>], œÉ<span class="pl-k">=</span>[<span class="pl-c1">1.0334797673569802</span>, <span class="pl-c1">0.9957355128767893</span>])</pre></div>
<p>Or we can check the ELBO (which in this case since, as mentioned, doesn't involve data, is the negative KL-divergence):</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; AdvancedVI.elbo(advi, q, logœÄ, 1000)  # empirical estimate
0.08031049170093245
"><pre>julia<span class="pl-k">&gt;</span> AdvancedVI<span class="pl-k">.</span><span class="pl-c1">elbo</span>(advi, q, logœÄ, <span class="pl-c1">1000</span>)  <span class="pl-c"><span class="pl-c">#</span> empirical estimate</span>
<span class="pl-c1">0.08031049170093245</span></pre></div>
<p>It's worth noting that the actual value of the ELBO doesn't really tell us too much about the quality of fit. In this particular case, because we're <em>directly</em> minimizing the KL-divergence, we can only say something useful if we reach 0, in which case we have obtained the true distribution.</p>
<p>Let's just quickly check the mean-squared error between the <code>log p(z)</code> and <code>log q(z)</code> for a random set of samples from the target <code>p</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; zs = rand(p, 100);

julia&gt; mean(abs2, logpdf(q, zs) - logpdf(p, zs))
0.0014889109427524852
"><pre>julia<span class="pl-k">&gt;</span> zs <span class="pl-k">=</span> <span class="pl-c1">rand</span>(p, <span class="pl-c1">100</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">mean</span>(abs2, <span class="pl-c1">logpdf</span>(q, zs) <span class="pl-k">-</span> <span class="pl-c1">logpdf</span>(p, zs))
<span class="pl-c1">0.0014889109427524852</span></pre></div>
<p>That doesn't look too bad!</p>
<h2><a id="user-content-implementing-your-own-training-loop" class="anchor" aria-hidden="true" href="#implementing-your-own-training-loop"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Implementing your own training loop</h2>
<p>Sometimes it might be convenient to roll your own training loop rather than using <code>vi(...)</code>. Here's some psuedo-code for how one would do that when used together with Turing.jl:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Turing, AdvancedVI, DiffResults
using Turing: Variational

using ProgressMeter

# Assuming you have an instance of a Turing model (`model`)

# 1. Create log-joint needed for ELBO evaluation
logœÄ = Variational.make_logjoint(model)

# 2. Define objective
variational_objective = Variational.ELBO()

# 3. Optimizer
optimizer = Variational.DecayedADAGrad()

# 4. VI-algorithm
alg = ADVI(10, 1000)

# 5. Variational distribution
function getq(Œ∏)
    # ...
end

# 6. [OPTIONAL] Implement convergence criterion
function hasconverged(args...)
    # ...
end

# 7. [OPTIONAL] Implement a callback for tracking stats
function callback(args...)
    # ...
end

# 8. Train
converged = false
step = 1

prog = ProgressMeter.Progress(num_steps, 1)

diff_results = DiffResults.GradientResult(Œ∏_init)

while (step ‚â§ num_steps) &amp;&amp; !converged
    # 1. Compute gradient and objective value; results are stored in `diff_results`
    AdvancedVI.grad!(variational_objective, alg, getq, model, diff_results)

    # 2. Extract gradient from `diff_result`
    ‚àá = DiffResults.gradient(diff_result)

    # 3. Apply optimizer, e.g. multiplying by step-size
    Œî = apply!(optimizer, Œ∏, ‚àá)

    # 4. Update parameters
    @. Œ∏ = Œ∏ - Œî

    # 5. Do whatever analysis you want
    callback(args...)

    # 6. Update
    converged = hasconverged(...) # or something user-defined
    step += 1

    ProgressMeter.next!(prog)
end
"><pre><span class="pl-k">using</span> Turing, AdvancedVI, DiffResults
<span class="pl-k">using</span> Turing<span class="pl-k">:</span> Variational

<span class="pl-k">using</span> ProgressMeter

<span class="pl-c"><span class="pl-c">#</span> Assuming you have an instance of a Turing model (`model`)</span>

<span class="pl-c"><span class="pl-c">#</span> 1. Create log-joint needed for ELBO evaluation</span>
logœÄ <span class="pl-k">=</span> Variational<span class="pl-k">.</span><span class="pl-c1">make_logjoint</span>(model)

<span class="pl-c"><span class="pl-c">#</span> 2. Define objective</span>
variational_objective <span class="pl-k">=</span> Variational<span class="pl-k">.</span><span class="pl-c1">ELBO</span>()

<span class="pl-c"><span class="pl-c">#</span> 3. Optimizer</span>
optimizer <span class="pl-k">=</span> Variational<span class="pl-k">.</span><span class="pl-c1">DecayedADAGrad</span>()

<span class="pl-c"><span class="pl-c">#</span> 4. VI-algorithm</span>
alg <span class="pl-k">=</span> <span class="pl-c1">ADVI</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1000</span>)

<span class="pl-c"><span class="pl-c">#</span> 5. Variational distribution</span>
<span class="pl-k">function</span> <span class="pl-en">getq</span>(Œ∏)
    <span class="pl-c"><span class="pl-c">#</span> ...</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> 6. [OPTIONAL] Implement convergence criterion</span>
<span class="pl-k">function</span> <span class="pl-en">hasconverged</span>(args<span class="pl-k">...</span>)
    <span class="pl-c"><span class="pl-c">#</span> ...</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> 7. [OPTIONAL] Implement a callback for tracking stats</span>
<span class="pl-k">function</span> <span class="pl-en">callback</span>(args<span class="pl-k">...</span>)
    <span class="pl-c"><span class="pl-c">#</span> ...</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> 8. Train</span>
converged <span class="pl-k">=</span> <span class="pl-c1">false</span>
step <span class="pl-k">=</span> <span class="pl-c1">1</span>

prog <span class="pl-k">=</span> ProgressMeter<span class="pl-k">.</span><span class="pl-c1">Progress</span>(num_steps, <span class="pl-c1">1</span>)

diff_results <span class="pl-k">=</span> DiffResults<span class="pl-k">.</span><span class="pl-c1">GradientResult</span>(Œ∏_init)

<span class="pl-k">while</span> (step <span class="pl-k">‚â§</span> num_steps) <span class="pl-k">&amp;&amp;</span> <span class="pl-k">!</span>converged
    <span class="pl-c"><span class="pl-c">#</span> 1. Compute gradient and objective value; results are stored in `diff_results`</span>
    AdvancedVI<span class="pl-k">.</span><span class="pl-c1">grad!</span>(variational_objective, alg, getq, model, diff_results)

    <span class="pl-c"><span class="pl-c">#</span> 2. Extract gradient from `diff_result`</span>
    ‚àá <span class="pl-k">=</span> DiffResults<span class="pl-k">.</span><span class="pl-c1">gradient</span>(diff_result)

    <span class="pl-c"><span class="pl-c">#</span> 3. Apply optimizer, e.g. multiplying by step-size</span>
    Œî <span class="pl-k">=</span> <span class="pl-c1">apply!</span>(optimizer, Œ∏, ‚àá)

    <span class="pl-c"><span class="pl-c">#</span> 4. Update parameters</span>
    <span class="pl-c1">@.</span> Œ∏ <span class="pl-k">=</span> Œ∏ <span class="pl-k">-</span> Œî

    <span class="pl-c"><span class="pl-c">#</span> 5. Do whatever analysis you want</span>
    <span class="pl-c1">callback</span>(args<span class="pl-k">...</span>)

    <span class="pl-c"><span class="pl-c">#</span> 6. Update</span>
    converged <span class="pl-k">=</span> <span class="pl-c1">hasconverged</span>(<span class="pl-k">...</span>) <span class="pl-c"><span class="pl-c">#</span> or something user-defined</span>
    step <span class="pl-k">+=</span> <span class="pl-c1">1</span>

    ProgressMeter<span class="pl-k">.</span><span class="pl-c1">next!</span>(prog)
<span class="pl-k">end</span></pre></div>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li>Jordan, Michael I., Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. "An introduction to variational methods for graphical models." Machine learning 37, no. 2 (1999): 183-233.</li>
<li>Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. "Variational inference: A review for statisticians." Journal of the American statistical Association 112, no. 518 (2017): 859-877.</li>
<li>Kucukelbir, Alp, Rajesh Ranganath, Andrew Gelman, and David Blei. "Automatic variational inference in Stan." In Advances in Neural Information Processing Systems, pp. 568-576. 2015.</li>
<li>Salimans, Tim, and David A. Knowles. "Fixed-form variational posterior approximation through stochastic linear regression." Bayesian Analysis 8, no. 4 (2013): 837-882.</li>
<li>Beal, Matthew James. Variational algorithms for approximate Bayesian inference. 2003.</li>
</ul>
</article></div>