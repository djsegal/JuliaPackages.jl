<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-robust-linear-models-in-julia" class="anchor" aria-hidden="true" href="#robust-linear-models-in-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Robust linear models in Julia</h1>
<table>
<thead>
<tr>
<th align="center">Documentation</th>
<th align="center">CI Status</th>
<th align="center">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://getzze.github.io/RobustModels.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a> <a href="https://getzze.github.io/RobustModels.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width: 100%;"></a></td>
<td align="center"><a href="https://github.com/getzze/RobustModels.jl/actions"><img src="https://github.com/getzze/RobustModels.jl/workflows/CI/badge.svg" alt="" style="max-width: 100%;"></a></td>
<td align="center"><a href="https://codecov.io/gh/getzze/RobustModels.jl/branch/master" rel="nofollow"><img src="https://camo.githubusercontent.com/675ff8f6d83dee320a0bffe5d6e6fd692fac7a75a06fc1246473553ec846cc76/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6765747a7a652f526f627573744d6f64656c732e6a6c3f6c6162656c3d436f6465636f76266c6f676f3d636f6465636f76" alt="" data-canonical-src="https://img.shields.io/codecov/c/github/getzze/RobustModels.jl?label=Codecov&amp;logo=codecov" style="max-width: 100%;"></a></td>
</tr>
</tbody>
</table>
<p dir="auto">This package defines robust linear models using the interfaces from
<a href="https://github.com/JuliaStats/StatsBase.jl">StatsBase.jl</a> and
<a href="https://github.com/JuliaStats/StatsModels.jl">StatsModels.jl</a>.
It defines an <code>AbstractRobustModel</code> type as a subtype of <code>RegressionModel</code> and
it defines the methods from the statistical model API like <code>fit</code>/<code>fit!</code>.</p>
<p dir="auto">A <em>robust model</em> is a regression model, meaning it finds a relationship between one or
several <em>covariates/independent variables</em> <code>X</code> and a <em>response/dependent</em> variable <code>y</code>.
Contrary to the ordinary least squares estimate, robust regression mitigates the influence
of <em>outliers</em> in the data. A standard view for <code>RobustLinearModel</code> is to consider that the residuals
are distributed according to a <em>contaminated</em> distribution, that is a <em>mixture model</em>
of the distribution of interest <code>F</code> and a contaminating distribution <code>Î”</code> with higher variance.
Therefore the residuals <code>r</code> follow the distribution <code>r ~ (1-Îµ) F + Îµ Î”</code>, with <code>0â‰¤Îµ&lt;1</code>.</p>
<p dir="auto">This package implements:</p>
<ul dir="auto">
<li>M-estimators</li>
<li>S-estimators</li>
<li>MM-estimators</li>
<li>Ï„-estimators</li>
<li>MQuantile regression (e.g. Expectile regression)</li>
<li>Robust Ridge regression (using any of the previous estimator)</li>
<li>Quantile regression using interior point method</li>
</ul>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt;] add RobustModels"><pre>julia<span class="pl-k">&gt;</span>] add RobustModels</pre></div>
<p dir="auto">To install the last development version:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt;] add RobustModels#main"><pre>julia<span class="pl-k">&gt;</span>] add RobustModels<span class="pl-c"><span class="pl-c">#</span>main</span></pre></div>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">The prefered way of performing robust regression is by calling the <code>rlm</code> function:</p>
<p dir="auto"><code>m = rlm(X, y, MEstimator{TukeyLoss}(); initial_scale=:mad)</code></p>
<p dir="auto">For quantile regression, use <code>quantreg</code>:</p>
<p dir="auto"><code>m = quantreg(X, y; quantile=0.5)</code></p>
<p dir="auto">For robust version of <code>mean</code>, <code>std</code>, <code>var</code> and <code>sem</code> statistics, specify the estimator as first argument.
Use the <code>dims</code> keyword for computing the statistics along specific dimensions.
The following functions are also implemented: <code>mean_and_std</code>, <code>mean_and_var</code> and <code>mean_and_sem</code>.</p>
<p dir="auto"><code>xm = mean(MEstimator{HuberLoss}(), x; dims=2)</code></p>
<p dir="auto"><code>(xm, sm) = mean_and_std(TauEstimator{YohaiZamarLoss}(), x)</code></p>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using RDatasets: dataset
using StatsModels
using RobustModels

data = dataset(&quot;robustbase&quot;, &quot;Animals2&quot;)
data.logBrain = log.(data.Brain)
data.logBody = log.(data.Body)
form = @formula(logBrain ~ 1 + logBody)

## M-estimator using Tukey estimator
m1 = rlm(form, data, MEstimator{TukeyLoss}(); method=:cg, initial_scale=:mad)

## MM-estimator using Tukey estimator
m2 = rlm(form, data, MMEstimator{TukeyLoss}(); method=:cg, initial_scale=:L1)

## M-estimator using Huber estimator and correcting for covariate outliers using leverage
m3 = rlm(form, data, MEstimator{HuberLoss}(); method=:cg, initial_scale=:L1, correct_leverage=true)

## M-estimator using Huber estimator, providing an initial scale estimate and using Cholesky method of solving.
m4 = rlm(form, data, MEstimator{HuberLoss}(); method=:chol, initial_scale=15.0)

## S-estimator using Tukey estimator
m5 = rlm(form, data, SEstimator{TukeyLoss}(); Ïƒ0=:mad)

## Ï„-estimator using Tukey estimator
m6 = rlm(form, data, TauEstimator{TukeyLoss}(); initial_scale=:L1)

## Ï„-estimator using YohaiZamar estimator and resampling to find the global minimum
opts = Dict(:Npoints=&gt;10, :Nsteps_Î²=&gt;3, :Nsteps_Ïƒ=&gt;3)
m7 = rlm(form, data, TauEstimator{YohaiZamarLoss}(); initial_scale=:L1, resample=true, resampling_options=opts)

## Expectile regression for Ï„=0.8
m8 = rlm(form, data, GeneralizedQuantileEstimator{L2Loss}(0.8))
#m8 = rlm(form, data, ExpectileEstimator(0.8))

## Refit with different parameters
refit!(m8; quantile=0.2)

## Robust ridge regression
m9 = rlm(form, data, MEstimator{TukeyLoss}(); initial_scale=:L1, ridgeÎ»=1.0)

## Quantile regression solved by linear programming interior point method
m10 = quantreg(form, data; quantile=0.2)
refit!(m10; quantile=0.8)
;

# output
"><pre><span class="pl-k">using</span> RDatasets<span class="pl-k">:</span> dataset
<span class="pl-k">using</span> StatsModels
<span class="pl-k">using</span> RobustModels

data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>robustbase<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Animals2<span class="pl-pds">"</span></span>)
data<span class="pl-k">.</span>logBrain <span class="pl-k">=</span> <span class="pl-c1">log</span>.(data<span class="pl-k">.</span>Brain)
data<span class="pl-k">.</span>logBody <span class="pl-k">=</span> <span class="pl-c1">log</span>.(data<span class="pl-k">.</span>Body)
form <span class="pl-k">=</span> <span class="pl-c1">@formula</span>(logBrain <span class="pl-k">~</span> <span class="pl-c1">1</span> <span class="pl-k">+</span> logBody)

<span class="pl-c"><span class="pl-c">#</span># M-estimator using Tukey estimator</span>
m1 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">MEstimator</span><span class="pl-c1">{TukeyLoss}</span>(); method<span class="pl-k">=</span><span class="pl-c1">:cg</span>, initial_scale<span class="pl-k">=</span><span class="pl-c1">:mad</span>)

<span class="pl-c"><span class="pl-c">#</span># MM-estimator using Tukey estimator</span>
m2 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">MMEstimator</span><span class="pl-c1">{TukeyLoss}</span>(); method<span class="pl-k">=</span><span class="pl-c1">:cg</span>, initial_scale<span class="pl-k">=</span><span class="pl-c1">:L1</span>)

<span class="pl-c"><span class="pl-c">#</span># M-estimator using Huber estimator and correcting for covariate outliers using leverage</span>
m3 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">MEstimator</span><span class="pl-c1">{HuberLoss}</span>(); method<span class="pl-k">=</span><span class="pl-c1">:cg</span>, initial_scale<span class="pl-k">=</span><span class="pl-c1">:L1</span>, correct_leverage<span class="pl-k">=</span><span class="pl-c1">true</span>)

<span class="pl-c"><span class="pl-c">#</span># M-estimator using Huber estimator, providing an initial scale estimate and using Cholesky method of solving.</span>
m4 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">MEstimator</span><span class="pl-c1">{HuberLoss}</span>(); method<span class="pl-k">=</span><span class="pl-c1">:chol</span>, initial_scale<span class="pl-k">=</span><span class="pl-c1">15.0</span>)

<span class="pl-c"><span class="pl-c">#</span># S-estimator using Tukey estimator</span>
m5 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">SEstimator</span><span class="pl-c1">{TukeyLoss}</span>(); Ïƒ0<span class="pl-k">=</span><span class="pl-c1">:mad</span>)

<span class="pl-c"><span class="pl-c">#</span># Ï„-estimator using Tukey estimator</span>
m6 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">TauEstimator</span><span class="pl-c1">{TukeyLoss}</span>(); initial_scale<span class="pl-k">=</span><span class="pl-c1">:L1</span>)

<span class="pl-c"><span class="pl-c">#</span># Ï„-estimator using YohaiZamar estimator and resampling to find the global minimum</span>
opts <span class="pl-k">=</span> <span class="pl-c1">Dict</span>(<span class="pl-c1">:Npoints</span><span class="pl-k">=&gt;</span><span class="pl-c1">10</span>, <span class="pl-c1">:Nsteps_Î²</span><span class="pl-k">=&gt;</span><span class="pl-c1">3</span>, <span class="pl-c1">:Nsteps_Ïƒ</span><span class="pl-k">=&gt;</span><span class="pl-c1">3</span>)
m7 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">TauEstimator</span><span class="pl-c1">{YohaiZamarLoss}</span>(); initial_scale<span class="pl-k">=</span><span class="pl-c1">:L1</span>, resample<span class="pl-k">=</span><span class="pl-c1">true</span>, resampling_options<span class="pl-k">=</span>opts)

<span class="pl-c"><span class="pl-c">#</span># Expectile regression for Ï„=0.8</span>
m8 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">GeneralizedQuantileEstimator</span><span class="pl-c1">{L2Loss}</span>(<span class="pl-c1">0.8</span>))
<span class="pl-c"><span class="pl-c">#</span>m8 = rlm(form, data, ExpectileEstimator(0.8))</span>

<span class="pl-c"><span class="pl-c">#</span># Refit with different parameters</span>
<span class="pl-c1">refit!</span>(m8; quantile<span class="pl-k">=</span><span class="pl-c1">0.2</span>)

<span class="pl-c"><span class="pl-c">#</span># Robust ridge regression</span>
m9 <span class="pl-k">=</span> <span class="pl-c1">rlm</span>(form, data, <span class="pl-c1">MEstimator</span><span class="pl-c1">{TukeyLoss}</span>(); initial_scale<span class="pl-k">=</span><span class="pl-c1">:L1</span>, ridgeÎ»<span class="pl-k">=</span><span class="pl-c1">1.0</span>)

<span class="pl-c"><span class="pl-c">#</span># Quantile regression solved by linear programming interior point method</span>
m10 <span class="pl-k">=</span> <span class="pl-c1">quantreg</span>(form, data; quantile<span class="pl-k">=</span><span class="pl-c1">0.2</span>)
<span class="pl-c1">refit!</span>(m10; quantile<span class="pl-k">=</span><span class="pl-c1">0.8</span>)
;

<span class="pl-c"><span class="pl-c">#</span> output</span>
</pre></div>
<h2 dir="auto"><a id="user-content-theory" class="anchor" aria-hidden="true" href="#theory"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Theory</h2>
<h3 dir="auto"><a id="user-content-m-estimators" class="anchor" aria-hidden="true" href="#m-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>M-estimators</h3>
<p dir="auto">With ordinary least square (OLS), the objective function is, from maximum likelihood estimation:</p>
<p dir="auto"><code>L = Â½ Î£áµ¢ (yáµ¢ - ğ’™áµ¢ ğœ·)Â² = Â½ Î£áµ¢ ráµ¢Â²</code></p>
<p dir="auto">where <code>yáµ¢</code> are the values of the response variable, <code>ğ’™áµ¢</code> are the covectors of individual covariates
(rows of the model matrix <code>X</code>), <code>ğœ·</code> is the vector of fitted coefficients and <code>ráµ¢</code> are the individual residuals.</p>
<p dir="auto">A <code>RobustLinearModel</code> solves instead for the following loss function: <code>L' = Î£áµ¢ Ï(ráµ¢)</code>
(more precisely <code>L' = Î£áµ¢ Ï(ráµ¢/Ïƒ)</code> where <code>Ïƒ</code> is an (robust) estimate of the standard deviation of the residual).
Several loss functions are implemented:</p>
<ul dir="auto">
<li><code>L2Loss</code>: <code>Ï(r) = Â½ rÂ²</code>, like ordinary OLS.</li>
<li><code>L1Loss</code>: <code>Ï(r) = |r|</code>, non-differentiable estimator also know as <em>Least absolute deviations</em>. Prefer the <code>QuantileRegression</code> solver.</li>
<li><code>HuberLoss</code>: <code>Ï(r) = if (r&lt;c); Â½(r/c)Â² else |r|/c - Â½ end</code>, convex estimator that behaves as <code>L2</code> cost for small residuals and <code>L1</code> for large esiduals and outliers.</li>
<li><code>L1L2Loss</code>: <code>Ï(r) = âˆš(1 + (r/c)Â²) - 1</code>, smooth version of <code>HuberLoss</code>.</li>
<li><code>FairLoss</code>: <code>Ï(r) = |r|/c - log(1 + |r|/c)</code>, smooth version of <code>HuberLoss</code>.</li>
<li><code>LogcoshLoss</code>: <code>Ï(r) = log(cosh(r/c))</code>, smooth version of <code>HuberLoss</code>.</li>
<li><code>ArctanLoss</code>: <code>Ï(r) = r/c * atan(r/c) - Â½ log(1+(r/c)Â²)</code>, smooth version of <code>HuberLoss</code>.</li>
<li><code>CauchyLoss</code>: <code>Ï(r) = log(1+(r/c)Â²)</code>, non-convex estimator, that also corresponds to a Student's-t distribution (with fixed degree of freedom). It suppresses outliers more strongly but it is not sure to converge.</li>
<li><code>GemanLoss</code>: <code>Ï(r) = Â½ (r/c)Â²/(1 + (r/c)Â²)</code>, non-convex and bounded estimator, it suppresses outliers more strongly.</li>
<li><code>WelschLoss</code>: <code>Ï(r) = Â½ (1 - exp(-(r/c)Â²))</code>, non-convex and bounded estimator, it suppresses outliers more strongly.</li>
<li><code>TukeyLoss</code>: <code>Ï(r) = if r&lt;c; â…™(1 - (1-(r/c)Â²)Â³) else â…™ end</code>, non-convex and bounded estimator, it suppresses outliers more strongly and it is the prefered estimator for most cases.</li>
<li><code>YohaiZamarLoss</code>: <code>Ï(r)</code> is quadratic for <code>r/c &lt; 2/3</code> and is bounded to 1; non-convex estimator, it is optimized to have the lowest bias for a given efficiency.</li>
</ul>
<p dir="auto">The value of the tuning constants <code>c</code> are optimized for each estimator so the M-estimators have a high efficiency of 0.95. However, these estimators have a low breakdown point.</p>
<h3 dir="auto"><a id="user-content-s-estimators" class="anchor" aria-hidden="true" href="#s-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>S-estimators</h3>
<p dir="auto">Instead of minimizing <code>Î£áµ¢ Ï(ráµ¢/Ïƒ)</code>, S-estimation minimizes the estimate of the squared scale <code>ÏƒÂ²</code> with the constraint that: <code>1/n Î£áµ¢ Ï(ráµ¢/Ïƒ) = 1/2</code>.
S-estimators are only defined for bounded estimators, like <code>TukeyLoss</code>.
These estimators have low efficiency but a high breakdown point of 1/2, by choosing the tuning constant <code>c</code> accordingly.</p>
<h3 dir="auto"><a id="user-content-mm-estimators" class="anchor" aria-hidden="true" href="#mm-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MM-estimators</h3>
<p dir="auto">It is a two-pass estimation, 1) Estimate <code>Ïƒ</code> using an S-estimator with high breakdown point and 2) estimate <code>ğœ·</code> using an M-estimator with high efficiency.
It results in an estimator with high efficiency and high breakdown point.</p>
<h3 dir="auto"><a id="user-content-Ï„-estimators" class="anchor" aria-hidden="true" href="#Ï„-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Ï„-estimators</h3>
<p dir="auto">Like MM-estimators, Ï„-estimators combine a high efficiency with a high breakdown point.
Similar to S-estimators, it minimizes a scale estimate:
<code>Ï„Â² = ÏƒÂ² (2/n Î£áµ¢Ïâ‚‚(ráµ¢/Ïƒ))</code> where <code>Ïƒ</code> is an M-scale estimate, solution of <code>1/n Î£áµ¢ Ïâ‚(ráµ¢/Ïƒ) = 1/2</code>.
Finding the minimum of a Ï„-estimator is similar to the procedure for an S-estimator with a special weight function
that combines both functions <code>Ïâ‚</code> and <code>Ïâ‚‚</code>. To ensure a high breakdown point and high efficiency,
the two loss functions should be the same but with different tuning constants.</p>
<h3 dir="auto"><a id="user-content-mquantile-estimators" class="anchor" aria-hidden="true" href="#mquantile-estimators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MQuantile-estimators</h3>
<p dir="auto">Using an asymmetric variant of the <code>L1Estimator</code>, quantile regression is performed
(although the <code>QuantileRegression</code> solver should be prefered because it gives an exact solution).
Identically, with an M-estimator using an asymetric version of the loss function,
a generalization of quantiles is obtained. For instance, using an asymetric <code>L2Loss</code> results in <em>Expectile Regression</em>.</p>
<h3 dir="auto"><a id="user-content-robust-ridge-regression" class="anchor" aria-hidden="true" href="#robust-ridge-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Robust Ridge regression</h3>
<p dir="auto">This is the robust version of the ridge regression, adding a penalty on the coefficients.
The objective function to solve is <code>L = Î£áµ¢ Ï(ráµ¢/Ïƒ) + Î» Î£â±¼|Î²â±¼|Â²</code>, where the sum of squares of
coefficients does not include the intersect <code>Î²â‚€</code>.
Robust ridge regression is implemented for all the estimators (not for <code>quantreg</code>).
By default, all the coefficients (except the intercept) have the same penalty, which assumes that
all the feature variables have the same scale. If it is not the case, use a robust estimate of scale
to normalize every column of the model matrix <code>X</code> before fitting the regression.</p>
<h3 dir="auto"><a id="user-content-quantile-regression" class="anchor" aria-hidden="true" href="#quantile-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quantile regression</h3>
<p dir="auto"><em>Quantile regression</em> results from minimizing the following objective function:
<code>L = Î£áµ¢ wáµ¢|yáµ¢ - ğ’™áµ¢ ğœ·| = Î£áµ¢ wáµ¢(ráµ¢) |ráµ¢|</code>,
where <code>wáµ¢ = ifelse(ráµ¢&gt;0, Ï„, 1-Ï„)</code> and <code>Ï„</code> is the quantile of interest. <code>Ï„=0.5</code> corresponds to <em>Least Absolute Deviations</em>.</p>
<p dir="auto">This problem is solved exactly using linear programming techniques,
specifically, interior point methods using the internal API of <a href="https://github.com/ds4dm/Tulip.jl">Tulip</a>.
The internal API is considered unstable, but it results in a much lighter dependency than
including the <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a> package with Tulip backend.</p>
<h2 dir="auto"><a id="user-content-credits" class="anchor" aria-hidden="true" href="#credits"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Credits</h2>
<p dir="auto">This package derives from the <a href="https://github.com/FugroRoames/RobustLeastSquares.jl">RobustLeastSquares</a>
package for the initial implementation, especially for the Conjugate Gradient
solver and the definition of the M-Estimator functions.</p>
<p dir="auto">Credits to the developpers of the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a>
and <a href="https://github.com/JuliaStats/MixedModels.jl">MixedModels</a> packages
for implementing the Iteratively Reweighted Least Square algorithm.</p>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<ul dir="auto">
<li>"Robust Statistics: Theory and Methods (with R)", 2nd Edition, 2019, R. Maronna, R. Martin, V. Yohai, M. SalibiÃ¡n-Barrera</li>
</ul>
</article></div>