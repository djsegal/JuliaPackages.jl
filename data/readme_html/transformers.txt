<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><div align="center"> <a target="_blank" rel="noopener noreferrer" href="images/transformerslogo.png"><img src="images/transformerslogo.png" alt="Transformers.jl" width="512" style="max-width:100%;"></a></div>
<p><a href="https://github.com/chengchingwen/Transformers.jl/actions"><img src="https://github.com/chengchingwen/Transformers.jl/workflows/CI/badge.svg" alt="Build status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/chengchingwen/Transformers.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/665032265cf6adbdf652b8a55192eaf3c529566e7b2fd2eb2c4274a5a3806a16/68747470733a2f2f636f6465636f762e696f2f67682f6368656e676368696e6777656e2f5472616e73666f726d6572732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/chengchingwen/Transformers.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a>
<a href="https://chengchingwen.github.io/Transformers.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></p>
<p>Julia implementation of <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">transformer</a>-based models, with <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>.</p>
<h1><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h1>
<p>In the Julia REPL:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="]add Transformers
"><pre><code>]add Transformers
</code></pre></div>
<p>For using GPU, install &amp; build:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="]add CUDA

]build 

julia&gt; using CUDA

julia&gt; using Transformers

#run the model below
.
.
.
"><pre><code>]add CUDA

]build 

julia&gt; using CUDA

julia&gt; using Transformers

#run the model below
.
.
.
</code></pre></div>
<h1><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h1>
<p>Using pretrained Bert with <code>Transformers.jl</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Transformers
using Transformers.Basic
using Transformers.Pretrain

ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = true

bert_model, wordpiece, tokenizer = pretrain&quot;bert-uncased_L-12_H-768_A-12&quot;
vocab = Vocabulary(wordpiece)

text1 = &quot;Peter Piper picked a peck of pickled peppers&quot; |&gt; tokenizer |&gt; wordpiece
text2 = &quot;Fuzzy Wuzzy was a bear&quot; |&gt; tokenizer |&gt; wordpiece

text = [&quot;[CLS]&quot;; text1; &quot;[SEP]&quot;; text2; &quot;[SEP]&quot;]
@assert text == [
    &quot;[CLS]&quot;, &quot;peter&quot;, &quot;piper&quot;, &quot;picked&quot;, &quot;a&quot;, &quot;peck&quot;, &quot;of&quot;, &quot;pick&quot;, &quot;##led&quot;, &quot;peppers&quot;, &quot;[SEP]&quot;, 
    &quot;fuzzy&quot;, &quot;wu&quot;, &quot;##zzy&quot;,  &quot;was&quot;, &quot;a&quot;, &quot;bear&quot;, &quot;[SEP]&quot;
]

token_indices = vocab(text)
segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]

sample = (tok = token_indices, segment = segment_indices)

bert_embedding = sample |&gt; bert_model.embed
feature_tensors = bert_embedding |&gt; bert_model.transformers
"><pre><span class="pl-k">using</span> Transformers
<span class="pl-k">using</span> Transformers<span class="pl-k">.</span>Basic
<span class="pl-k">using</span> Transformers<span class="pl-k">.</span>Pretrain

<span class="pl-c1">ENV</span>[<span class="pl-s"><span class="pl-pds">"</span>DATADEPS_ALWAYS_ACCEPT<span class="pl-pds">"</span></span>] <span class="pl-k">=</span> <span class="pl-c1">true</span>

bert_model, wordpiece, tokenizer <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds"><span class="pl-c1">pretrain</span>"</span>bert-uncased_L-12_H-768_A-12<span class="pl-pds">"</span></span>
vocab <span class="pl-k">=</span> <span class="pl-c1">Vocabulary</span>(wordpiece)

text1 <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Peter Piper picked a peck of pickled peppers<span class="pl-pds">"</span></span> <span class="pl-k">|&gt;</span> tokenizer <span class="pl-k">|&gt;</span> wordpiece
text2 <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Fuzzy Wuzzy was a bear<span class="pl-pds">"</span></span> <span class="pl-k">|&gt;</span> tokenizer <span class="pl-k">|&gt;</span> wordpiece

text <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>[CLS]<span class="pl-pds">"</span></span>; text1; <span class="pl-s"><span class="pl-pds">"</span>[SEP]<span class="pl-pds">"</span></span>; text2; <span class="pl-s"><span class="pl-pds">"</span>[SEP]<span class="pl-pds">"</span></span>]
<span class="pl-c1">@assert</span> text <span class="pl-k">==</span> [
    <span class="pl-s"><span class="pl-pds">"</span>[CLS]<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>peter<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>piper<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>picked<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>peck<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>of<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>pick<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>##led<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>peppers<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>[SEP]<span class="pl-pds">"</span></span>, 
    <span class="pl-s"><span class="pl-pds">"</span>fuzzy<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>wu<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>##zzy<span class="pl-pds">"</span></span>,  <span class="pl-s"><span class="pl-pds">"</span>was<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>bear<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>[SEP]<span class="pl-pds">"</span></span>
]

token_indices <span class="pl-k">=</span> <span class="pl-c1">vocab</span>(text)
segment_indices <span class="pl-k">=</span> [<span class="pl-c1">fill</span>(<span class="pl-c1">1</span>, <span class="pl-c1">length</span>(text1)<span class="pl-k">+</span><span class="pl-c1">2</span>); <span class="pl-c1">fill</span>(<span class="pl-c1">2</span>, <span class="pl-c1">length</span>(text2)<span class="pl-k">+</span><span class="pl-c1">1</span>)]

sample <span class="pl-k">=</span> (tok <span class="pl-k">=</span> token_indices, segment <span class="pl-k">=</span> segment_indices)

bert_embedding <span class="pl-k">=</span> sample <span class="pl-k">|&gt;</span> bert_model<span class="pl-k">.</span>embed
feature_tensors <span class="pl-k">=</span> bert_embedding <span class="pl-k">|&gt;</span> bert_model<span class="pl-k">.</span>transformers</pre></div>
<p>See <code>example</code> folder for the complete example.</p>
<h1><a id="user-content-huggingface" class="anchor" aria-hidden="true" href="#huggingface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Huggingface</h1>
<p>We have some support for the models from <a href="https://github.com/huggingface/transformers"><code>huggingface/transformers</code></a>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Transformers.HuggingFace

# loading a model from huggingface model hub
julia&gt; model = hgf&quot;bert-base-cased:forquestionanswering&quot;;
┌ Warning: Transformers.HuggingFace.HGFBertForQuestionAnswering doesn't have field cls.
└ @ Transformers.HuggingFace ~/peter/repo/gsoc2020/src/huggingface/models/models.jl:46
┌ Warning: Some fields of Transformers.HuggingFace.HGFBertForQuestionAnswering aren't initialized with loaded state: qa_outputs
└ @ Transformers.HuggingFace ~/peter/repo/gsoc2020/src/huggingface/models/models.jl:52

"><pre><span class="pl-k">using</span> Transformers<span class="pl-k">.</span>HuggingFace

<span class="pl-c"><span class="pl-c">#</span> loading a model from huggingface model hub</span>
julia<span class="pl-k">&gt;</span> model <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds"><span class="pl-c1">hgf</span>"</span>bert-base-cased:forquestionanswering<span class="pl-pds">"</span></span>;
┌ Warning<span class="pl-k">:</span> Transformers<span class="pl-k">.</span>HuggingFace<span class="pl-k">.</span>HGFBertForQuestionAnswering doesn<span class="pl-k">'</span>t have field cls.
└ @ Transformers<span class="pl-k">.</span>HuggingFace <span class="pl-k">~</span><span class="pl-k">/</span>peter<span class="pl-k">/</span>repo<span class="pl-k">/</span>gsoc2020<span class="pl-k">/</span>src<span class="pl-k">/</span>huggingface<span class="pl-k">/</span>models<span class="pl-k">/</span>models<span class="pl-k">.</span>jl<span class="pl-k">:</span><span class="pl-c1">46</span>
┌ Warning<span class="pl-k">:</span> Some fields of Transformers<span class="pl-k">.</span>HuggingFace<span class="pl-k">.</span>HGFBertForQuestionAnswering aren<span class="pl-k">'</span>t initialized with loaded state<span class="pl-k">:</span> qa_outputs
└ @ Transformers<span class="pl-k">.</span>HuggingFace <span class="pl-k">~</span><span class="pl-k">/</span>peter<span class="pl-k">/</span>repo<span class="pl-k">/</span>gsoc2020<span class="pl-k">/</span>src<span class="pl-k">/</span>huggingface<span class="pl-k">/</span>models<span class="pl-k">/</span>models<span class="pl-k">.</span>jl<span class="pl-k">:</span><span class="pl-c1">52</span>
</pre></div>
<p>Current we only support a few model and the tokenizer part is not finished yet.</p>
<h1><a id="user-content-for-more-information" class="anchor" aria-hidden="true" href="#for-more-information"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>For more information</h1>
<p>If you want to know more about this package, see the <a href="https://chengchingwen.github.io/Transformers.jl/dev/" rel="nofollow">document</a>
and the series of <a href="https://nextjournal.com/chengchingwen" rel="nofollow">blog posts</a> I wrote for JSoC and GSoC. You can also
tag me (@chengchingwen) on Julia's slack or discourse if you have any questions, or just create a new Issue on GitHub.</p>
<h1><a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Roadmap</h1>
<h2><a id="user-content-what-we-have-before-v02" class="anchor" aria-hidden="true" href="#what-we-have-before-v02"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What we have before v0.2</h2>
<ul>
<li><code>Transformer</code> and <code>TransformerDecoder</code> support for both 2d &amp; 3d data.</li>
<li><code>PositionEmbedding</code> implementation.</li>
<li><code>Positionwise</code> for handling 2d &amp; 3d input.</li>
<li>docstring for most of the functions.</li>
<li>runable examples (see <code>example</code> folder)</li>
<li><code>Transformers.HuggingFace</code> for handling pretrains from <code>huggingface/transformers</code></li>
</ul>
<h2><a id="user-content-what-we-will-have-in-v020" class="anchor" aria-hidden="true" href="#what-we-will-have-in-v020"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What we will have in v0.2.0</h2>
<ul>
<li>Complete tokenizer APIs</li>
<li>tutorials</li>
<li>benchmarks</li>
<li>more examples</li>
</ul>
</article></div>