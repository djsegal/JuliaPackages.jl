<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><div align="center" dir="auto"> <a target="_blank" rel="noopener noreferrer" href="images/transformerslogo.png"><img src="images/transformerslogo.png" alt="Transformers.jl" width="512" style="max-width: 100%;"></a></div>
<p dir="auto"><a href="https://github.com/chengchingwen/Transformers.jl/actions"><img src="https://github.com/chengchingwen/Transformers.jl/workflows/CI/badge.svg" alt="Build status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/chengchingwen/Transformers.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/665032265cf6adbdf652b8a55192eaf3c529566e7b2fd2eb2c4274a5a3806a16/68747470733a2f2f636f6465636f762e696f2f67682f6368656e676368696e6777656e2f5472616e73666f726d6572732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/chengchingwen/Transformers.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://chengchingwen.github.io/Transformers.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Julia implementation of <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">transformer</a>-based models, with <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>.</p>
<p dir="auto"><em>notice: The current version is almost complete different from the 0.1.x version. If you are using the old version, make sure to update the changes or stick to the old version.</em></p>
<h1 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h1>
<p dir="auto">In the Julia REPL:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="]add Transformers"><pre class="notranslate"><code>]add Transformers
</code></pre></div>
<h1 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h1>
<p dir="auto">Using pretrained Bert with <code>Transformers.jl</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Transformers
using Transformers.TextEncoders
using Transformers.HuggingFace

textencoder, bert_model = hgf&quot;bert-base-uncased&quot;

text1 = &quot;Peter Piper picked a peck of pickled peppers&quot;
text2 = &quot;Fuzzy Wuzzy was a bear&quot;

text = [[ text1, text2 ]] # 1 batch of contiguous sentences
sample = encode(textencoder, text) # tokenize + pre-process (add special tokens + truncate / padding + one-hot encode)

@assert reshape(decode(textencoder, sample.token), :) == [
    &quot;[CLS]&quot;, &quot;peter&quot;, &quot;piper&quot;, &quot;picked&quot;, &quot;a&quot;, &quot;peck&quot;, &quot;of&quot;, &quot;pick&quot;, &quot;##led&quot;, &quot;peppers&quot;, &quot;[SEP]&quot;,
    &quot;fuzzy&quot;, &quot;wu&quot;, &quot;##zzy&quot;,  &quot;was&quot;, &quot;a&quot;, &quot;bear&quot;, &quot;[SEP]&quot;
]

bert_features = bert_model(sample).hidden_state"><pre><span class="pl-k">using</span> Transformers
<span class="pl-k">using</span> Transformers<span class="pl-k">.</span>TextEncoders
<span class="pl-k">using</span> Transformers<span class="pl-k">.</span>HuggingFace

textencoder, bert_model <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds"><span class="pl-c1">hgf</span>"</span>bert-base-uncased<span class="pl-pds">"</span></span>

text1 <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Peter Piper picked a peck of pickled peppers<span class="pl-pds">"</span></span>
text2 <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Fuzzy Wuzzy was a bear<span class="pl-pds">"</span></span>

text <span class="pl-k">=</span> [[ text1, text2 ]] <span class="pl-c"><span class="pl-c">#</span> 1 batch of contiguous sentences</span>
sample <span class="pl-k">=</span> <span class="pl-c1">encode</span>(textencoder, text) <span class="pl-c"><span class="pl-c">#</span> tokenize + pre-process (add special tokens + truncate / padding + one-hot encode)</span>

<span class="pl-c1">@assert</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">decode</span>(textencoder, sample<span class="pl-k">.</span>token), :) <span class="pl-k">==</span> [
    <span class="pl-s"><span class="pl-pds">"</span>[CLS]<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>peter<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>piper<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>picked<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>peck<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>of<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>pick<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>##led<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>peppers<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>[SEP]<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>fuzzy<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>wu<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>##zzy<span class="pl-pds">"</span></span>,  <span class="pl-s"><span class="pl-pds">"</span>was<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>a<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>bear<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>[SEP]<span class="pl-pds">"</span></span>
]

bert_features <span class="pl-k">=</span> <span class="pl-c1">bert_model</span>(sample)<span class="pl-k">.</span>hidden_state</pre></div>
<p dir="auto">See <code>example</code> folder for the complete example.</p>
<h1 dir="auto"><a id="user-content-for-more-information" class="anchor" aria-hidden="true" href="#for-more-information"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For more information</h1>
<p dir="auto">If you want to know more about this package, see the <a href="https://chengchingwen.github.io/Transformers.jl/dev/" rel="nofollow">document</a>
and read code in the <code>example</code> folder. You can also tag me (@chengchingwen) on Julia's slack or discourse if
you have any questions, or just create a new Issue on GitHub.</p>
</article></div>