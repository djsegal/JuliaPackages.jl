<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-simplenn" class="anchor" aria-hidden="true" href="#simplenn"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>simpleNN</h1>
<h2><a id="user-content-manual-implementation-of-simple-neural-network-with-mnist-dataset" class="anchor" aria-hidden="true" href="#manual-implementation-of-simple-neural-network-with-mnist-dataset"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Manual implementation of simple neural network with MNIST dataset</h2>
<p>This is an updated version of neural network code from the original question by Anders, answered by Chris Rackauckas:
<a href="https://stackoverflow.com/questions/49719076/macos-python-with-numpy-faster-than-julia-in-training-neural-network/49724611" rel="nofollow">macOS Python with numpy faster than Julia in training neural network</a></p>
<p>There are four versions of this code:</p>
<ul>
<li>v1 - original version includes all recommendations from the answer, with operations changed to support Julia 1.1, and some minor changes to variable naming, styling, etc.</li>
<li>v2 - load data and batches format changed to matrix instead of array of tuples (loaddata, vectorize, etc.), first activation layer removed, no need to copy input data to it.</li>
<li>v3 - batch run converted into matrix operations, added batch_size preallocation for nn layers.</li>
<li>v4 - network structure rework: split into 3 different structures: <code>network_v4</code>, <code>batch_trainer</code> and <code>batch_tester</code> for preallocation, the whole run time is faster due to preallocation for evaluation.</li>
</ul>
<p>There is a performance increase by an order (!) of magnitude due to transition to matrix operations in batches.
On my machine:</p>
<ul>
<li>v1 and v2 take about 10 seconds per epoch</li>
<li>v3 and v4 take about 1.1 seconds per epoch</li>
</ul>
</article></div>