<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-simplenn" class="anchor" aria-hidden="true" href="#simplenn"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>simpleNN</h1>
<h2><a id="user-content-manual-implementation-of-simple-neural-network-with-mnist-dataset" class="anchor" aria-hidden="true" href="#manual-implementation-of-simple-neural-network-with-mnist-dataset"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Manual implementation of simple neural network with MNIST dataset</h2>
<p>This is an updated version of neural network code from the original question by Anders, answered by Chris Rackauckas:
<a href="https://stackoverflow.com/questions/49719076/macos-python-with-numpy-faster-than-julia-in-training-neural-network/49724611" rel="nofollow">macOS Python with numpy faster than Julia in training neural network</a></p>
<p>There are four versions of this code:</p>
<ul>
<li>v1 - original version includes all recommendations from the answer, with operations changed to support Julia 1.1, and some minor changes to variable naming, styling, etc.</li>
<li>v2 - load data and batches format changed to matrix instead of array of tuples (loaddata, vectorize, etc.), first activation layer removed, no need to copy input data to it.</li>
<li>v3 - batch run converted into matrix operations, added batch_size preallocation for nn layers.</li>
<li>v4 - network structure rework: split into 3 different structures: <code>network_v4</code>, <code>batch_trainer</code> and <code>batch_tester</code> for preallocation, the whole run time is faster due to preallocation for evaluation.</li>
</ul>
<p>There is a performance increase by an order (!) of magnitude due to transition to matrix operations in batches.
On my machine:</p>
<ul>
<li>v1 and v2 take about 10 seconds per epoch</li>
<li>v3 and v4 take about 1.1 seconds per epoch</li>
</ul>
</article></div>