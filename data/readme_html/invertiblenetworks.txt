<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-invertiblenetworksjl" class="anchor" aria-hidden="true" href="#invertiblenetworksjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>InvertibleNetworks.jl</h1>
<table>
<thead>
<tr>
<th align="center"><strong>Documentation</strong></th>
<th align="center"><strong>Build Status</strong></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://slimgroup.github.io/InvertibleNetworks.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a> <a href="https://slimgroup.github.io/InvertibleNetworks.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></td>
<td align="center"><a target="_blank" rel="noopener noreferrer" href="https://github.com/slimgroup/InvertibleNetworks.jl/workflows/CI/badge.svg"><img src="https://github.com/slimgroup/InvertibleNetworks.jl/workflows/CI/badge.svg" alt="CI" style="max-width:100%;"></a></td>
<td align="center"><a href="https://zenodo.org/badge/latestdoi/239018318" rel="nofollow"><img src="https://camo.githubusercontent.com/2b7c05c5376250e1178958e539f28fa6261f01936fc84894d19c5259a5a8807f/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3233393031383331382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/239018318.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>Building blocks for invertible neural networks in the <a href="https://julialang.org" rel="nofollow">Julia</a> programming language.</p>
<ul>
<li>Memory efficient building blocks for invertible neural networks</li>
<li>Hand-derived gradients, Jacobians $J$ , and $\log |J|$</li>
<li><a href="https://fluxml.ai" rel="nofollow">Flux</a> integration</li>
<li>Support for <a href="https://github.com/FluxML/Zygote.jl">Zygote</a> and <a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules</a></li>
<li>GPU support</li>
<li>Includes various examples of invertible neural networks, normalizing flows, variational inference, and uncertainty quantification</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="] dev https://github.com/slimgroup/InvertibleNetworks.jl
"><pre><code>] dev https://github.com/slimgroup/InvertibleNetworks.jl
</code></pre></div>
<h2><a id="user-content-papers" class="anchor" aria-hidden="true" href="#papers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h2>
<p>The following publications use <a href="https://github.com/slimgroup/InvertibleNetworks.jl">InvertibleNetworks.jl</a>:</p>
<ul>
<li>
<p><strong><a href="https://slim.gatech.edu/content/preconditioned-training-normalizing-flows-variational-inference-inverse-problems" rel="nofollow">“Preconditioned training of normalizing flows for variational inference in inverse problems”</a></strong></p>
<ul>
<li>paper: <a href="https://arxiv.org/abs/2101.03709" rel="nofollow">https://arxiv.org/abs/2101.03709</a></li>
<li><a href="https://slim.gatech.edu/Publications/Public/Conferences/AABI/2021/siahkoohi2021AABIpto/siahkoohi2021AABIpto_pres.pdf" rel="nofollow">presentation</a></li>
<li>code: <a href="https://github.com/slimgroup/Software.siahkoohi2021AABIpto">FastApproximateInference.jl</a></li>
</ul>
</li>
<li>
<p><strong><a href="https://slim.gatech.edu/content/parameterizing-uncertainty-deep-invertible-networks-application-reservoir-characterization" rel="nofollow">"Parameterizing uncertainty by deep invertible networks, an application to reservoir characterization"</a></strong></p>
<ul>
<li>paper: <a href="https://arxiv.org/abs/2004.07871" rel="nofollow">https://arxiv.org/abs/2004.07871</a></li>
<li><a href="https://slim.gatech.edu/Publications/Public/Conferences/SEG/2020/rizzuti2020SEGuqavp/rizzuti2020SEGuqavp_pres.pdf" rel="nofollow">presentation</a></li>
<li>code: <a href="https://github.com/slimgroup/Software.SEG2020">https://github.com/slimgroup/Software.SEG2020</a></li>
</ul>
</li>
<li>
<p><strong><a href="https://slim.gatech.edu/content/generalized-minkowski-sets-regularization-inverse-problems-1" rel="nofollow">"Generalized Minkowski sets for the regularization of inverse problems"</a></strong></p>
<ul>
<li>paper: <a href="http://arxiv.org/abs/1903.03942" rel="nofollow">http://arxiv.org/abs/1903.03942</a></li>
<li>code: <a href="https://github.com/slimgroup/SetIntersectionProjection.jl">SetIntersectionProjection.jl</a></li>
</ul>
</li>
</ul>
<h2><a id="user-content-building-blocks" class="anchor" aria-hidden="true" href="#building-blocks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Building blocks</h2>
<ul>
<li>
<p>1x1 Convolutions using Householder transformations (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_convolution_1x1.jl">example</a>)</p>
</li>
<li>
<p>Residual block (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_residual_block.jl">example</a>)</p>
</li>
<li>
<p>Invertible coupling layer from Dinh et al. (2017) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_coupling_glow.jl">example</a>)</p>
</li>
<li>
<p>Invertible hyperbolic layer from Lensink et al. (2019) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_coupling_hyperbolic.jl">example</a>)</p>
</li>
<li>
<p>Invertible coupling layer from Putzky and Welling (2019) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_coupling_irim.jl">example</a>)</p>
</li>
<li>
<p>Invertible recursive coupling layer HINT from Kruse et al. (2020) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_coupling_hint.jl">example</a>)</p>
</li>
<li>
<p>Activation normalization (Kingma and Dhariwal, 2018) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/layers/layer_actnorm.jl">example</a>)</p>
</li>
<li>
<p>Various activation functions (Sigmoid, ReLU, leaky ReLU, GaLU)</p>
</li>
<li>
<p>Objective and misfit functions (mean squared error, log-likelihood)</p>
</li>
<li>
<p>Dimensionality manipulation: squeeze/unsqueeze (column, patch, checkerboard), split/cat</p>
</li>
<li>
<p>Squeeze/unsqueeze using the wavelet transform</p>
</li>
</ul>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<ul>
<li>
<p>Invertible recurrent inference machines (Putzky and Welling, 2019) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/networks/network_irim.jl">generic example</a>)</p>
</li>
<li>
<p>Generative models with maximum likelihood via the change of variable formula (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/applications/application_glow_banana_dist.jl">example</a>)</p>
</li>
<li>
<p>Glow: Generative flow with invertible 1x1 convolutions (Kingma and Dhariwal, 2018) (<a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/examples/networks/network_glow.jl">generic example</a>, <a href="https://github.com/slimgroup/InvertibleNetworks.jl/tree/master/src/networks/invertible_network_glow.jl">source</a>)</p>
</li>
</ul>
<h2><a id="user-content-gpu-support" class="anchor" aria-hidden="true" href="#gpu-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPU support</h2>
<p>GPU support is supported via Flux/CuArray. To use the GPU, move the input and the network layer to GPU via <code>|&gt; gpu</code></p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using InvertibleNetworks, Flux

# Input
nx = 64
ny = 64
k = 10
batchsize = 4

# Input image: nx x ny x k x batchsize
X = randn(Float32, nx, ny, k, batchsize) |&gt; gpu

# Activation normalization
AN = ActNorm(k; logdet=true) |&gt; gpu

# Test invertibility
Y_, logdet = AN.forward(X)
"><pre><code>using InvertibleNetworks, Flux

# Input
nx = 64
ny = 64
k = 10
batchsize = 4

# Input image: nx x ny x k x batchsize
X = randn(Float32, nx, ny, k, batchsize) |&gt; gpu

# Activation normalization
AN = ActNorm(k; logdet=true) |&gt; gpu

# Test invertibility
Y_, logdet = AN.forward(X)
</code></pre></div>
<h2><a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgments</h2>
<p>This package uses functions from <a href="https://github.com/FluxML/NNlib.jl">NNlib.jl</a>, <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> and <a href="https://github.com/JuliaDSP/Wavelets.jl">Wavelets.jl</a></p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li>
<p>Yann Dauphin, Angela Fan, Michael Auli and David Grangier, "Language modeling with gated convolutional networks", Proceedings of the 34th International Conference on Machine Learning, 2017. <a href="https://arxiv.org/pdf/1612.08083.pdf" rel="nofollow">https://arxiv.org/pdf/1612.08083.pdf</a></p>
</li>
<li>
<p>Laurent Dinh, Jascha Sohl-Dickstein and Samy Bengio, "Density estimation using Real NVP",  International Conference on Learning Representations, 2017, <a href="https://arxiv.org/abs/1605.08803" rel="nofollow">https://arxiv.org/abs/1605.08803</a></p>
</li>
<li>
<p>Diederik P. Kingma and Prafulla Dhariwal, "Glow: Generative Flow with Invertible 1x1 Convolutions", Conference on Neural Information Processing Systems, 2018. <a href="https://arxiv.org/abs/1807.03039" rel="nofollow">https://arxiv.org/abs/1807.03039</a></p>
</li>
<li>
<p>Keegan Lensink, Eldad Haber and Bas Peters, "Fully Hyperbolic Convolutional Neural Networks", arXiv Computer Vision and Pattern Recognition, 2019. <a href="https://arxiv.org/abs/1905.10484" rel="nofollow">https://arxiv.org/abs/1905.10484</a></p>
</li>
<li>
<p>Patrick Putzky and Max Welling, "Invert to learn to invert", Advances in Neural Information Processing Systems, 2019. <a href="https://arxiv.org/abs/1911.10914" rel="nofollow">https://arxiv.org/abs/1911.10914</a></p>
</li>
<li>
<p>Jakob Kruse, Gianluca Detommaso, Robert Scheichl and Ullrich Köthe, "HINT: Hierarchical Invertible Neural Transport for Density Estimation and Bayesian Inference", arXiv Statistics and Machine Learning, 2020. <a href="https://arxiv.org/abs/1905.10687" rel="nofollow">https://arxiv.org/abs/1905.10687</a></p>
</li>
</ul>
<h2><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Authors</h2>
<ul>
<li>
<p>Philipp Witte, Microsoft Corporation (<a href="mailto:pwitte@microsoft.com">pwitte@microsoft.com</a>)</p>
</li>
<li>
<p>Gabrio Rizzuti, Utrecht University</p>
</li>
<li>
<p>Mathias Louboutin, Georgia Institute of Technology</p>
</li>
<li>
<p>Ali Siahkoohi, Georgia Institute of Technology</p>
</li>
</ul>
</article></div>