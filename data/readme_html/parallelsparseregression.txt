<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-parallelsparseregression" class="anchor" aria-hidden="true" href="#parallelsparseregression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ParallelSparseRegression</h1>
<p><a href="https://travis-ci.org/madeleineudell/ParallelSparseRegression.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f279023a980a46af91658ddb690718db74d59f5081c9749c43ec8a5de677a8f7/68747470733a2f2f7472617669732d63692e6f72672f6d6164656c65696e657564656c6c2f506172616c6c656c53706172736552656772657373696f6e2e6a6c2e706e67" alt="Build Status" data-canonical-src="https://travis-ci.org/madeleineudell/ParallelSparseRegression.jl.png" style="max-width:100%;"></a></p>
<p>A Julia library for parallel sparse regression using shared memory.
This library implements solvers for regression problems
including least squares, ridge regression, lasso, non-negative least squares,
and elastic net.
It also proposes to add fast methods to obtain regularization paths.</p>
<p>Using the <a href="http://www.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf" rel="nofollow">Alternating Direction Method of Multipliers</a>,
all of these problems can be reduced to computing the prox of each term in the objective.
We rely on the fact that the prox of each term in the objective
of these regression problems can be efficiently computed in parallel.</p>
<h1><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h1>
<p>To install, just open a Julia prompt and call</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Pkg.clone(&quot;git@github.com:madeleineudell/ParallelSparseRegression.jl.git&quot;)
"><pre><code>Pkg.clone("git@github.com:madeleineudell/ParallelSparseRegression.jl.git")
</code></pre></div>
<p>You'll also need to use a version of IterativeSolvers with support for caching temporary variables,</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Pkg.clone(&quot;git@github.com:madeleineudell/IterativeSolvers.jl.git&quot;)
"><pre><code>Pkg.clone("git@github.com:madeleineudell/IterativeSolvers.jl.git")
</code></pre></div>
<h1><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h1>
<p>Before you begin, initialize all the processes you want to participate in multiplying by your matrix.
You'll suffer decreased performance if you add more processes
than you have hyperthreads on your shared-memory computer.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="addprocs(3)
using ParallelSparseRegression
"><pre><code>addprocs(3)
using ParallelSparseRegression
</code></pre></div>
<p>We will solve a sparse non-negative least squares problem.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="m,n,p = 100,20,.1
A = sprand(m,n,p)
x0 = Base.shmem_randn(n)
b = A*x0
rho = 1
quiet = false
maxiters = 100

params = Params(rho,quiet,maxiters)
z = nnlsq(A,b; params=params)
"><pre><code>m,n,p = 100,20,.1
A = sprand(m,n,p)
x0 = Base.shmem_randn(n)
b = A*x0
rho = 1
quiet = false
maxiters = 100

params = Params(rho,quiet,maxiters)
z = nnlsq(A,b; params=params)
</code></pre></div>
<p>We can verify the solution obtained is better than merely thresholding
the entries of the least squares solution to be positive.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="println(&quot;Norm of Az-b is $(norm(A*z-b))&quot;)
xp = max(x0,0)
println(&quot;Norm of A(x)_+ -b is $(norm(A*xp-b))&quot;)
"><pre><code>println("Norm of Az-b is $(norm(A*z-b))")
xp = max(x0,0)
println("Norm of A(x)_+ -b is $(norm(A*xp-b))")
</code></pre></div>
</article></div>