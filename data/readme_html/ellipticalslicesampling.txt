<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-ellipticalslicesamplingjl" class="anchor" aria-hidden="true" href="#ellipticalslicesamplingjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>EllipticalSliceSampling.jl</h1>
<p>Julia implementation of elliptical slice sampling.</p>
<p><a href="https://www.repostatus.org/#active" rel="nofollow"><img src="https://camo.githubusercontent.com/2261082c77808ea734741b12e535d02d23c4101f6b8dfec807f4ddc5ef2eeec0/68747470733a2f2f7777772e7265706f7374617475732e6f72672f6261646765732f6c61746573742f6163746976652e737667" alt="Project Status: Active â€“ The project has reached a stable, usable state and is being actively developed." data-canonical-src="https://www.repostatus.org/badges/latest/active.svg" style="max-width:100%;"></a>
<a href="https://github.com/TuringLang/EllipticalSliceSampling.jl/actions?query=workflow%3ACI%20branch%3Amain"><img src="https://github.com/TuringLang/EllipticalSliceSampling.jl/workflows/CI/badge.svg?branch=main" alt="Build Status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/TuringLang/EllipticalSliceSampling.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/af84f658750aa91e1e6c4a7e17d86f1c188cdd8cd4c877244882e8a26d8b6d0e/68747470733a2f2f636f6465636f762e696f2f67682f547572696e674c616e672f456c6c6970746963616c536c69636553616d706c696e672e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/TuringLang/EllipticalSliceSampling.jl/branch/main/graph/badge.svg" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/TuringLang/EllipticalSliceSampling.jl?branch=main" rel="nofollow"><img src="https://camo.githubusercontent.com/17577cc4f1c49e6830b5a70e864d5b9baf6f298577b9ca2a02a6e6da2278e6d6/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f547572696e674c616e672f456c6c6970746963616c536c69636553616d706c696e672e6a6c2f62616467652e7376673f6272616e63683d6d61696e" alt="Coveralls" data-canonical-src="https://coveralls.io/repos/github/TuringLang/EllipticalSliceSampling.jl/badge.svg?branch=main" style="max-width:100%;"></a>
<a href="https://github.com/invenia/BlueStyle"><img src="https://camo.githubusercontent.com/c18fbaa52d94d16b90b19701fc90d289b8a5bb920c74c79bab200b14e75420a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c75652d3434393564312e737667" alt="Code Style: Blue" data-canonical-src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview</h2>
<p>This package implements elliptical slice sampling in the Julia language, as described in
<a href="http://proceedings.mlr.press/v9/murray10a/murray10a.pdf" rel="nofollow">Murray, Adams &amp; MacKay (2010)</a>.</p>
<p>Elliptical slice sampling is a "Markov chain Monte Carlo algorithm for performing
inference in models with multivariate Gaussian priors" (Murray, Adams &amp; MacKay (2010)).</p>
<p>Without loss of generality, the originally described algorithm assumes that the Gaussian
prior has zero mean. For convenience we allow the user to specify arbitrary Gaussian
priors with non-zero means and handle the change of variables internally.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p>Probably most users would like to generate a MC Markov chain of samples from
the posterior distribution by calling</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="sample([rng, ]ESSModel(prior, loglikelihood), ESS(), N[; kwargs...])
"><pre><span class="pl-c1">sample</span>([rng, ]<span class="pl-c1">ESSModel</span>(prior, loglikelihood), <span class="pl-c1">ESS</span>(), N[; kwargs<span class="pl-k">...</span>])</pre></div>
<p>which returns a vector of <code>N</code> samples for approximating the posterior of
a model with a Gaussian prior that allows sampling from the <code>prior</code> and
evaluation of the log likelihood <code>loglikelihood</code>.</p>
<p>You can sample multiple chains in parallel with multiple threads or processes
by running</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="sample([rng, ]ESSModel(prior, loglikelihood), ESS(), MCMCThreads(), N, nchains[; kwargs...])
"><pre><span class="pl-c1">sample</span>([rng, ]<span class="pl-c1">ESSModel</span>(prior, loglikelihood), <span class="pl-c1">ESS</span>(), <span class="pl-c1">MCMCThreads</span>(), N, nchains[; kwargs<span class="pl-k">...</span>])</pre></div>
<p>or</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="sample([rng, ]ESSModel(prior, loglikelihood), ESS(), MCMCDistributed(), N, nchains[; kwargs...])
"><pre><span class="pl-c1">sample</span>([rng, ]<span class="pl-c1">ESSModel</span>(prior, loglikelihood), <span class="pl-c1">ESS</span>(), <span class="pl-c1">MCMCDistributed</span>(), N, nchains[; kwargs<span class="pl-k">...</span>])</pre></div>
<p>If you want to have more control about the sampling procedure (e.g., if you
only want to save a subset of samples or want to use another stopping
criterion), the function</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="AbstractMCMC.steps(
    [rng,]
    ESSModel(prior, loglikelihood),
    ESS();
    kwargs...
)
"><pre>AbstractMCMC<span class="pl-k">.</span><span class="pl-c1">steps</span>(
    [rng,]
    <span class="pl-c1">ESSModel</span>(prior, loglikelihood),
    <span class="pl-c1">ESS</span>();
    kwargs<span class="pl-k">...</span>
)</pre></div>
<p>gives you access to an iterator from which you can generate an unlimited
number of samples.</p>
<p>For more details regarding <code>sample</code> and <code>steps</code> please check the documentation of
<a href="https://github.com/TuringLang/AbstractMCMC.jl">AbstractMCMC.jl</a>.</p>
<h3><a id="user-content-prior" class="anchor" aria-hidden="true" href="#prior"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Prior</h3>
<p>You may specify Gaussian priors with arbitrary means. EllipticalSliceSampling.jl
provides first-class support for the scalar and multivariate normal distributions
in <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>. For
instance, if the prior distribution is a standard normal distribution, you can
choose</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="prior = Normal()
"><pre>prior <span class="pl-k">=</span> <span class="pl-c1">Normal</span>()</pre></div>
<p>However, custom Gaussian priors are supported as well. For instance, if you want to
use a custom distribution type <code>GaussianPrior</code>, the following methods should be
implemented:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# state that the distribution is actually Gaussian
EllipticalSliceSampling.isgaussian(::Type{&lt;:GaussianPrior}) = true

# define the mean of the distribution
# alternatively implement `proposal(prior, ...)` and
# `proposal!(out, prior, ...)` (only if the samples are mutable)
Statistics.mean(dist::GaussianPrior) = ...

# define how to sample from the distribution
# only one of the following methods is needed:
# - if the samples are immutable (e.g., numbers or static arrays) only
#   `rand(rng, dist)` should be implemented
# - otherwise only `rand!(rng, dist, sample)` is required
Base.rand(rng::AbstractRNG, dist::GaussianPrior) = ...
Random.rand!(rng::AbstractRNG, dist::GaussianPrior, sample) = ...
"><pre><span class="pl-c"><span class="pl-c">#</span> state that the distribution is actually Gaussian</span>
EllipticalSliceSampling<span class="pl-k">.</span><span class="pl-en">isgaussian</span>(<span class="pl-k">::</span><span class="pl-c1">Type{&lt;:GaussianPrior}</span>) <span class="pl-k">=</span> <span class="pl-c1">true</span>

<span class="pl-c"><span class="pl-c">#</span> define the mean of the distribution</span>
<span class="pl-c"><span class="pl-c">#</span> alternatively implement `proposal(prior, ...)` and</span>
<span class="pl-c"><span class="pl-c">#</span> `proposal!(out, prior, ...)` (only if the samples are mutable)</span>
Statistics<span class="pl-k">.</span><span class="pl-en">mean</span>(dist<span class="pl-k">::</span><span class="pl-c1">GaussianPrior</span>) <span class="pl-k">=</span> <span class="pl-k">...</span>

<span class="pl-c"><span class="pl-c">#</span> define how to sample from the distribution</span>
<span class="pl-c"><span class="pl-c">#</span> only one of the following methods is needed:</span>
<span class="pl-c"><span class="pl-c">#</span> - if the samples are immutable (e.g., numbers or static arrays) only</span>
<span class="pl-c"><span class="pl-c">#</span>   `rand(rng, dist)` should be implemented</span>
<span class="pl-c"><span class="pl-c">#</span> - otherwise only `rand!(rng, dist, sample)` is required</span>
Base<span class="pl-k">.</span><span class="pl-en">rand</span>(rng<span class="pl-k">::</span><span class="pl-c1">AbstractRNG</span>, dist<span class="pl-k">::</span><span class="pl-c1">GaussianPrior</span>) <span class="pl-k">=</span> <span class="pl-k">...</span>
Random<span class="pl-k">.</span><span class="pl-en">rand!</span>(rng<span class="pl-k">::</span><span class="pl-c1">AbstractRNG</span>, dist<span class="pl-k">::</span><span class="pl-c1">GaussianPrior</span>, sample) <span class="pl-k">=</span> <span class="pl-k">...</span></pre></div>
<h3><a id="user-content-log-likelihood" class="anchor" aria-hidden="true" href="#log-likelihood"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Log likelihood</h3>
<p>In addition to the prior, you have to specify a Julia implementation of
the log likelihood function. Here the predefined log densities and log
likelihood functions in
<a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> might
be useful.</p>
<h3><a id="user-content-progress-monitor" class="anchor" aria-hidden="true" href="#progress-monitor"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Progress monitor</h3>
<p>If you use a package such as <a href="https://junolab.org/" rel="nofollow">Juno</a> or
<a href="https://github.com/c42f/TerminalLoggers.jl">TerminalLoggers.jl</a> that supports
progress logs created by the
<a href="https://github.com/JunoLab/ProgressLogging.jl">ProgressLogging.jl</a> API, then you can
monitor the progress of the sampling algorithm. If you do not specify a progress
logging frontend explicitly,
<a href="https://github.com/TuringLang/AbstractMCMC.jl">AbstractMCMC.jl</a> picks a frontend
for you automatically.</p>
<h2><a id="user-content-bibliography" class="anchor" aria-hidden="true" href="#bibliography"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bibliography</h2>
<p>Murray, I., Adams, R. &amp; MacKay, D.. (2010). Elliptical slice sampling. Proceedings of Machine Learning Research, 9:541-548.</p>
</article></div>