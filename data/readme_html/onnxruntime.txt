<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-onnxruntime" class="anchor" aria-hidden="true" href="#onnxruntime"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ONNXRunTime</h1>
<p dir="auto"><a href="https://jw3126.github.io/ONNXRunTime.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://jw3126.github.io/ONNXRunTime.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/jw3126/ONNXRunTime.jl/actions"><img src="https://github.com/jw3126/ONNXRunTime.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/jw3126/ONNXRunTime.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/3b171af0ca11a717188db838c6d18b77ab32d012b956e3de13228dafa1b12879/68747470733a2f2f636f6465636f762e696f2f67682f6a77333132362f4f4e4e5852756e54696d652e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/jw3126/ONNXRunTime.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><a href="https://github.com/jw3126/ONNXRunTime.jl">ONNXRunTime</a> provides inofficial <a href="https://github.com/JuliaLang/julia">julia</a> bindings for <a href="https://github.com/microsoft/onnxruntime">onnxruntime</a>.
It exposes both a low level interface, that mirrors the official <a href="https://github.com/microsoft/onnxruntime/blob/v1.8.1/include/onnxruntime/core/session/onnxruntime_c_api.h#L347">C-API</a>, as well as an high level interface.</p>
<p dir="auto">Contributions are welcome.</p>
<h1 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h1>
<p dir="auto">The high level API works as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="
julia&gt; import ONNXRunTime as OX

julia&gt; path = OX.testdatapath(&quot;increment2x3.onnx&quot;); # path to a toy model

julia&gt; model = OX.load_inference(path);

julia&gt; input = Dict(&quot;input&quot; =&gt; randn(Float32,2,3))
Dict{String, Matrix{Float32}} with 1 entry:
  &quot;input&quot; =&gt; [1.68127 1.18192 -0.474021; -1.13518 1.02199 2.75168]

julia&gt; model(input)
Dict{String, Matrix{Float32}} with 1 entry:
  &quot;output&quot; =&gt; [2.68127 2.18192 0.525979; -0.135185 2.02199 3.75168]"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">import</span> ONNXRunTime <span class="pl-k">as</span> OX

julia<span class="pl-k">&gt;</span> path <span class="pl-k">=</span> OX<span class="pl-k">.</span><span class="pl-c1">testdatapath</span>(<span class="pl-s"><span class="pl-pds">"</span>increment2x3.onnx<span class="pl-pds">"</span></span>); <span class="pl-c"><span class="pl-c">#</span> path to a toy model</span>

julia<span class="pl-k">&gt;</span> model <span class="pl-k">=</span> OX<span class="pl-k">.</span><span class="pl-c1">load_inference</span>(path);

julia<span class="pl-k">&gt;</span> input <span class="pl-k">=</span> <span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">randn</span>(Float32,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>))
Dict{String, Matrix{Float32}} with <span class="pl-c1">1</span> entry<span class="pl-k">:</span>
  <span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> [<span class="pl-c1">1.68127</span> <span class="pl-c1">1.18192</span> <span class="pl-k">-</span><span class="pl-c1">0.474021</span>; <span class="pl-k">-</span><span class="pl-c1">1.13518</span> <span class="pl-c1">1.02199</span> <span class="pl-c1">2.75168</span>]

julia<span class="pl-k">&gt;</span> <span class="pl-c1">model</span>(input)
Dict{String, Matrix{Float32}} with <span class="pl-c1">1</span> entry<span class="pl-k">:</span>
  <span class="pl-s"><span class="pl-pds">"</span>output<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> [<span class="pl-c1">2.68127</span> <span class="pl-c1">2.18192</span> <span class="pl-c1">0.525979</span>; <span class="pl-k">-</span><span class="pl-c1">0.135185</span> <span class="pl-c1">2.02199</span> <span class="pl-c1">3.75168</span>]</pre></div>
<p dir="auto">For GPU usage simply do:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pkg&gt; add CUDA

julia&gt; import CUDA

julia&gt; OX.load_inference(path, execution_provider=:cuda)"><pre>pkg<span class="pl-k">&gt;</span> add CUDA

julia<span class="pl-k">&gt;</span> <span class="pl-k">import</span> CUDA

julia<span class="pl-k">&gt;</span> OX<span class="pl-k">.</span><span class="pl-c1">load_inference</span>(path, execution_provider<span class="pl-k">=</span><span class="pl-c1">:cuda</span>)</pre></div>
<p dir="auto">The low level API mirrors the offical <a href="https://github.com/microsoft/onnxruntime/blob/v1.8.1/include/onnxruntime/core/session/onnxruntime_c_api.h#L347">C-API</a>. The above example looks like this:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ONNXRunTime.CAPI
using ONNXRunTime: testdatapath

api = GetApi();
env = CreateEnv(api, name=&quot;myenv&quot;);
so = CreateSessionOptions(api);
path = testdatapath(&quot;increment2x3.onnx&quot;);
session = CreateSession(api, env, path, so);
mem = CreateCpuMemoryInfo(api);
input_array = randn(Float32, 2,3)
input_tensor = CreateTensorWithDataAsOrtValue(api, mem, vec(input_array), size(input_array));
run_options = CreateRunOptions(api);
input_names = [&quot;input&quot;];
output_names = [&quot;output&quot;];
inputs = [input_tensor];
outputs = Run(api, session, run_options, input_names, inputs, output_names);
output_tensor = only(outputs);
output_array = GetTensorMutableData(api, output_tensor);"><pre><span class="pl-k">using</span> ONNXRunTime<span class="pl-k">.</span>CAPI
<span class="pl-k">using</span> ONNXRunTime<span class="pl-k">:</span> testdatapath

api <span class="pl-k">=</span> <span class="pl-c1">GetApi</span>();
env <span class="pl-k">=</span> <span class="pl-c1">CreateEnv</span>(api, name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>myenv<span class="pl-pds">"</span></span>);
so <span class="pl-k">=</span> <span class="pl-c1">CreateSessionOptions</span>(api);
path <span class="pl-k">=</span> <span class="pl-c1">testdatapath</span>(<span class="pl-s"><span class="pl-pds">"</span>increment2x3.onnx<span class="pl-pds">"</span></span>);
session <span class="pl-k">=</span> <span class="pl-c1">CreateSession</span>(api, env, path, so);
mem <span class="pl-k">=</span> <span class="pl-c1">CreateCpuMemoryInfo</span>(api);
input_array <span class="pl-k">=</span> <span class="pl-c1">randn</span>(Float32, <span class="pl-c1">2</span>,<span class="pl-c1">3</span>)
input_tensor <span class="pl-k">=</span> <span class="pl-c1">CreateTensorWithDataAsOrtValue</span>(api, mem, <span class="pl-c1">vec</span>(input_array), <span class="pl-c1">size</span>(input_array));
run_options <span class="pl-k">=</span> <span class="pl-c1">CreateRunOptions</span>(api);
input_names <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>input<span class="pl-pds">"</span></span>];
output_names <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>output<span class="pl-pds">"</span></span>];
inputs <span class="pl-k">=</span> [input_tensor];
outputs <span class="pl-k">=</span> <span class="pl-c1">Run</span>(api, session, run_options, input_names, inputs, output_names);
output_tensor <span class="pl-k">=</span> <span class="pl-c1">only</span>(outputs);
output_array <span class="pl-k">=</span> <span class="pl-c1">GetTensorMutableData</span>(api, output_tensor);</pre></div>
<h1 dir="auto"><a id="user-content-alternatives" class="anchor" aria-hidden="true" href="#alternatives"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Alternatives</h1>
<ul dir="auto">
<li>Use the onnxruntime python bindings via <a href="https://github.com/JuliaPy/PyCall.jl">PyCall.jl</a>.</li>
<li><a href="https://github.com/FluxML/ONNX.jl">ONNX.jl</a></li>
<li><a href="https://github.com/DrChainsaw/ONNXNaiveNASflux.jl">ONNXNaiveNASflux.jl</a></li>
</ul>
</article></div>