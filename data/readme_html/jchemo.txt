<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-jchemojl" class="anchor" aria-hidden="true" href="#jchemojl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Jchemo.jl</h1>
<h2 dir="auto"><a id="user-content-dimension-reduction-regression-and-discrimination-for-chemometrics" class="anchor" aria-hidden="true" href="#dimension-reduction-regression-and-discrimination-for-chemometrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dimension reduction, Regression and Discrimination for Chemometrics</h2>
<p dir="auto"><a href="https://mlesnoff.github.io/Jchemo.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://mlesnoff.github.io/Jchemo.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/mlesnoff/Jchemo.jl/actions"><img src="https://github.com/mlesnoff/Jchemo.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="http://www.repostatus.org/#active" rel="nofollow"><img src="https://camo.githubusercontent.com/ed51270ad41e2b842f9423b165977ac42f8498581be7c17c78818d1a45b834d5/687474703a2f2f7777772e7265706f7374617475732e6f72672f6261646765732f6c61746573742f6163746976652e737667" alt="Project Status: Active - The project has reached a stable, usable state and is being actively developed." data-canonical-src="http://www.repostatus.org/badges/latest/active.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><strong>Jchemo.jl</strong> is a package for <a href="https://mlesnoff.github.io/Jchemo.jl/dev/domains/" rel="nofollow"><strong>data exploration and prediction</strong></a> with focus on <strong>high dimensional data</strong>.</p>
<p dir="auto">The package was initially designed about <strong>partial least squares regression and discrimination models</strong> and variants, in particular locally weighted PLS models (<strong>LWPLS</strong>) (e.g. <a href="https://doi.org/10.1002/cem.3209" rel="nofollow">https://doi.org/10.1002/cem.3209</a>).
Then, it has been expanded to many other methods for
analyzing high dimensional data.</p>
<p dir="auto">The package was named <strong>Jchemo</strong> since it is orientated to chemometrics, but most of the provided methods are fully <strong>generic to other domains</strong>.</p>
<p dir="auto">Functions such as <strong>transform</strong>, <strong>predict</strong>, <strong>coef</strong> and <strong>summary</strong> are available.
<strong>Tuning the predictive models</strong> is facilitated by generic functions <strong>gridscore</strong> (validation dataset) and
<strong>gridcv</strong> (cross-validation). Faster versions of these functions are also available for models based on latent variables (LVs)
(<strong>gridscorelv</strong> and <strong>gridcvlv</strong>) and ridge regularization (<strong>gridscorelb</strong> and <strong>gridcvlb</strong>).</p>
<p dir="auto">Most of the functions of the package have a <strong>help page</strong> (providing an example), e.g.:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="?plskern"><pre>?plskern</pre></div>
<p dir="auto"><strong>Examples</strong> demonstrating <strong>Jchemo.jl</strong> are available in project <a href="https://github.com/mlesnoff/JchemoDemo"><strong>JchemoDemo</strong></a>, used for training only. <strong>The datasets</strong> of the examples are stored in package <a href="https://github.com/mlesnoff/JchemoData.jl"><strong>JchemoData.jl</strong></a>.</p>
<p dir="auto">Some of the functions of the package (in particular those using kNN selections) use <strong>multi-threading</strong>
to speed the computations. Taking advantage of this requires to specify a relevant number
of threads (e.g. from the 'Settings' menu of the VsCode Julia extension and the file 'settings.json').</p>
<p dir="auto"><strong>Jchemo.jl</strong> uses <strong>Makie.jl</strong> for plotting. To install and load one of the Makie's backends (e.g. <strong>CairoMakie.jl</strong>) is required to display the plots.</p>
<p dir="auto">Before to update the package, it is recommended to have a look on
<a href="https://github.com/mlesnoff/Jchemo.jl/tree/master/docs/src/news.md"><strong>What changed</strong></a> to avoid
problems due to eventual breaking changes.</p>
<h2 dir="auto"><a id="user-content--dependent-packages-" class="anchor" aria-hidden="true" href="#-dependent-packages-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><span> <strong>Dependent packages</strong> </span></h2>
<ul dir="auto">
<li><a href="https://github.com/mlesnoff/Jchemo.jl/blob/master/Project.toml"><strong>List of packages</strong></a></li>
</ul>
<h2 dir="auto"><a id="user-content--installation-" class="anchor" aria-hidden="true" href="#-installation-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><span> <strong>Installation</strong> </span></h2>
<p dir="auto">In order to install Jchemo, run in the Pkg REPL:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pkg&gt; add Jchemo"><pre>pkg<span class="pl-k">&gt;</span> add Jchemo</pre></div>
<p dir="auto">or for a specific version:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pkg&gt; add Jchemo@0.1.18"><pre>pkg<span class="pl-k">&gt;</span> add Jchemo@<span class="pl-c1">0.1</span>.<span class="pl-c1">18</span></pre></div>
<p dir="auto">or for the current developing version (not 100% stable):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pkg&gt; add https://github.com/mlesnoff/Jchemo.jl.git"><pre>pkg<span class="pl-k">&gt;</span> add https<span class="pl-k">:</span><span class="pl-k">//</span>github<span class="pl-k">.</span>com<span class="pl-k">/</span>mlesnoff<span class="pl-k">/</span>Jchemo<span class="pl-k">.</span>jl<span class="pl-k">.</span>git</pre></div>
<h2 dir="auto"><a id="user-content--benchmark---computation-time-for-a-pls-with-1e6-observations-" class="anchor" aria-hidden="true" href="#-benchmark---computation-time-for-a-pls-with-1e6-observations-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><span> <strong>Benchmark - Computation time for a PLS with 1e6 observations</strong> </span></h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; versioninfo()
Julia Version 1.8.5
Commit 17cfb8e65e (2023-01-08 06:45 UTC)
Platform Info:
  OS: Windows (x86_64-w64-mingw32)
  CPU: 16 × Intel(R) Core(TM) i9-10885H CPU @ 2.40GHz
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-13.0.1 (ORCJIT, skylake)
  Threads: 8 on 16 virtual cores
Environment:
  JULIA_EDITOR = code
  JULIA_NUM_THREADS = 8"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">versioninfo</span>()
Julia Version <span class="pl-c1">1.8</span>.<span class="pl-c1">5</span>
Commit <span class="pl-c1">17</span>cfb8e65e (<span class="pl-c1">2023</span><span class="pl-k">-</span><span class="pl-c1">01</span><span class="pl-k">-</span><span class="pl-c1">08</span> <span class="pl-c1">06</span><span class="pl-k">:</span><span class="pl-c1">45</span> UTC)
Platform Info<span class="pl-k">:</span>
  OS<span class="pl-k">:</span> Windows (x86_64<span class="pl-k">-</span>w64<span class="pl-k">-</span>mingw32)
  CPU<span class="pl-k">:</span> <span class="pl-c1">16</span> <span class="pl-k">×</span> <span class="pl-c1">Intel</span>(R) <span class="pl-c1">Core</span>(TM) i9<span class="pl-k">-</span><span class="pl-c1">10885</span>H CPU @ <span class="pl-c1">2.40</span>GHz
  WORD_SIZE<span class="pl-k">:</span> <span class="pl-c1">64</span>
  LIBM<span class="pl-k">:</span> libopenlibm
  LLVM<span class="pl-k">:</span> libLLVM<span class="pl-k">-</span><span class="pl-c1">13.0</span>.<span class="pl-c1">1</span> (ORCJIT, skylake)
  Threads<span class="pl-k">:</span> <span class="pl-c1">8</span> on <span class="pl-c1">16</span> virtual cores
Environment<span class="pl-k">:</span>
  JULIA_EDITOR <span class="pl-k">=</span> code
  JULIA_NUM_THREADS <span class="pl-k">=</span> <span class="pl-c1">8</span></pre></div>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Jchemo

## PLS2 with 1e6 observations
## (NB.: multi-threading is not used in plskern) 
n = 10^6  # nb. observations (samples)
p = 500   # nb. X-variables (features)
q = 10    # nb. Y-variables to predict
X = rand(n, p)
Y = rand(n, q)
nlv = 25  # nb. PLS latent variables

@time plskern(X, Y; nlv = nlv) ;
8.100469 seconds (299 allocations: 4.130 GiB, 6.58% gc time)

@time plskern!(X, Y; nlv = nlv) ;
7.232234 seconds (6.47 k allocations: 338.617 MiB, 7.39% gc time, 0.13% compilation time)"><pre><span class="pl-k">using</span> Jchemo

<span class="pl-c"><span class="pl-c">#</span># PLS2 with 1e6 observations</span>
<span class="pl-c"><span class="pl-c">#</span># (NB.: multi-threading is not used in plskern) </span>
n <span class="pl-k">=</span> <span class="pl-c1">10</span><span class="pl-k">^</span><span class="pl-c1">6</span>  <span class="pl-c"><span class="pl-c">#</span> nb. observations (samples)</span>
p <span class="pl-k">=</span> <span class="pl-c1">500</span>   <span class="pl-c"><span class="pl-c">#</span> nb. X-variables (features)</span>
q <span class="pl-k">=</span> <span class="pl-c1">10</span>    <span class="pl-c"><span class="pl-c">#</span> nb. Y-variables to predict</span>
X <span class="pl-k">=</span> <span class="pl-c1">rand</span>(n, p)
Y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(n, q)
nlv <span class="pl-k">=</span> <span class="pl-c1">25</span>  <span class="pl-c"><span class="pl-c">#</span> nb. PLS latent variables</span>

<span class="pl-c1">@time</span> <span class="pl-c1">plskern</span>(X, Y; nlv <span class="pl-k">=</span> nlv) ;
<span class="pl-c1">8.100469</span> seconds (<span class="pl-c1">299</span> allocations<span class="pl-k">:</span> <span class="pl-c1">4.130</span> GiB, <span class="pl-c1">6.58</span><span class="pl-k">%</span> gc time)

<span class="pl-c1">@time</span> <span class="pl-c1">plskern!</span>(X, Y; nlv <span class="pl-k">=</span> nlv) ;
<span class="pl-c1">7.232234</span> seconds (<span class="pl-c1">6.47</span> k allocations<span class="pl-k">:</span> <span class="pl-c1">338.617</span> MiB, <span class="pl-c1">7.39</span><span class="pl-k">%</span> gc time, <span class="pl-c1">0.13</span><span class="pl-k">%</span> compilation time)</pre></div>
<h2 dir="auto"><a id="user-content--examples-of-syntax-for-predictive-models-" class="anchor" aria-hidden="true" href="#-examples-of-syntax-for-predictive-models-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><span> <strong>Examples of syntax for predictive models</strong> </span></h2>
<h3 dir="auto"><a id="user-content-fitting-a-model" class="anchor" aria-hidden="true" href="#fitting-a-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><strong>Fitting a model</strong></h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Jchemo

n = 150 ; p = 200 ; q = 2 ; m = 50 
Xtrain = rand(n, p) ; Ytrain = rand(n, q) 
Xtest = rand(m, p) ; Ytest = rand(m, q) 

## Model fitting
nlv = 5 
fm = plskern(Xtrain, Ytrain; nlv = nlv) ;
pnames(fm) # print the names of objects contained in 'fm'

## Some summary
summary(fm, Xtrain)

## Computation of the PLS scores (LVs) for Xtest
Jchemo.transform(fm, Xtest)
Jchemo.transform(fm, Xtest; nlv = 1)

## PLS b-coefficients
Jchemo.coef(fm)
Jchemo.coef(fm; nlv = 2)

## Predictions and performance of the fitted model
res = Jchemo.predict(fm, Xtest) 
res.pred
rmsep(res.pred, Ytest)
mse(res.pred, Ytest)

Jchemo.predict(fm, Xtest).pred
Jchemo.predict(fm, Xtest; nlv = 0:3).pred "><pre><span class="pl-k">using</span> Jchemo

n <span class="pl-k">=</span> <span class="pl-c1">150</span> ; p <span class="pl-k">=</span> <span class="pl-c1">200</span> ; q <span class="pl-k">=</span> <span class="pl-c1">2</span> ; m <span class="pl-k">=</span> <span class="pl-c1">50</span> 
Xtrain <span class="pl-k">=</span> <span class="pl-c1">rand</span>(n, p) ; Ytrain <span class="pl-k">=</span> <span class="pl-c1">rand</span>(n, q) 
Xtest <span class="pl-k">=</span> <span class="pl-c1">rand</span>(m, p) ; Ytest <span class="pl-k">=</span> <span class="pl-c1">rand</span>(m, q) 

<span class="pl-c"><span class="pl-c">#</span># Model fitting</span>
nlv <span class="pl-k">=</span> <span class="pl-c1">5</span> 
fm <span class="pl-k">=</span> <span class="pl-c1">plskern</span>(Xtrain, Ytrain; nlv <span class="pl-k">=</span> nlv) ;
<span class="pl-c1">pnames</span>(fm) <span class="pl-c"><span class="pl-c">#</span> print the names of objects contained in 'fm'</span>

<span class="pl-c"><span class="pl-c">#</span># Some summary</span>
<span class="pl-c1">summary</span>(fm, Xtrain)

<span class="pl-c"><span class="pl-c">#</span># Computation of the PLS scores (LVs) for Xtest</span>
Jchemo<span class="pl-k">.</span><span class="pl-c1">transform</span>(fm, Xtest)
Jchemo<span class="pl-k">.</span><span class="pl-c1">transform</span>(fm, Xtest; nlv <span class="pl-k">=</span> <span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span># PLS b-coefficients</span>
Jchemo<span class="pl-k">.</span><span class="pl-c1">coef</span>(fm)
Jchemo<span class="pl-k">.</span><span class="pl-c1">coef</span>(fm; nlv <span class="pl-k">=</span> <span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span># Predictions and performance of the fitted model</span>
res <span class="pl-k">=</span> Jchemo<span class="pl-k">.</span><span class="pl-c1">predict</span>(fm, Xtest) 
res<span class="pl-k">.</span>pred
<span class="pl-c1">rmsep</span>(res<span class="pl-k">.</span>pred, Ytest)
<span class="pl-c1">mse</span>(res<span class="pl-k">.</span>pred, Ytest)

Jchemo<span class="pl-k">.</span><span class="pl-c1">predict</span>(fm, Xtest)<span class="pl-k">.</span>pred
Jchemo<span class="pl-k">.</span><span class="pl-c1">predict</span>(fm, Xtest; nlv <span class="pl-k">=</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">3</span>)<span class="pl-k">.</span>pred </pre></div>
<h3 dir="auto"><a id="user-content-tuning-a-model-by-grid-search" class="anchor" aria-hidden="true" href="#tuning-a-model-by-grid-search"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><strong>Tuning a model by grid-search</strong></h3>
<ul dir="auto">
<li>
<h3 dir="auto"><a id="user-content-with-gridscore" class="anchor" aria-hidden="true" href="#with-gridscore"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>With gridscore</h3>
</li>
</ul>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Jchemo, StatsBase, CairoMakie

ntrain = 150 ; p = 200
ntest = 80 
Xtrain = rand(ntrain, p) ; ytrain = rand(ntrain) 
Xtest = rand(ntest, p) ; ytest = rand(ntest)
## Train is splitted to Cal+Val to tune the model,
## and the generalization error is estimated on Test.
nval = 50
s = sample(1:ntrain, nval; replace = false) 
Xcal = rmrow(Xtrain, s)
ycal = rmrow(ytrain, s)
Xval = Xtrain[s, :]
yval = ytrain[s]

## Computation of the performance over the grid
## (the model is fitted on Cal, and the performance is 
## computed on Val)
nlv = 0:10 
res = gridscorelv(
    Xcal, ycal, Xval, yval;
    score = rmsep, fun = plskern, nlv = nlv) 

## Plot the results
plotgrid(res.nlv, res.y1,
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

## Predictions and performance of the best model
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
fm = plskern(Xtrain, ytrain; nlv = res.nlv[u]) ;
res = Jchemo.predict(fm, Xtest) 
rmsep(res.pred, ytest)

## *Note*: For PLSR models, using gridscorelv is much faster
## than using the generic function gridscore.
## In the same manner, for ridge regression models,
## gridscorelb is much faster than gridscore.

## Syntax for the generic gridscore
pars = mpar(nlv = nlv)
res = gridscore(
    Xcal, ycal, Xval, yval;
    score = rmsep, fun = plskern, pars = pars) "><pre><span class="pl-k">using</span> Jchemo, StatsBase, CairoMakie

ntrain <span class="pl-k">=</span> <span class="pl-c1">150</span> ; p <span class="pl-k">=</span> <span class="pl-c1">200</span>
ntest <span class="pl-k">=</span> <span class="pl-c1">80</span> 
Xtrain <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntrain, p) ; ytrain <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntrain) 
Xtest <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntest, p) ; ytest <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntest)
<span class="pl-c"><span class="pl-c">#</span># Train is splitted to Cal+Val to tune the model,</span>
<span class="pl-c"><span class="pl-c">#</span># and the generalization error is estimated on Test.</span>
nval <span class="pl-k">=</span> <span class="pl-c1">50</span>
s <span class="pl-k">=</span> <span class="pl-c1">sample</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>ntrain, nval; replace <span class="pl-k">=</span> <span class="pl-c1">false</span>) 
Xcal <span class="pl-k">=</span> <span class="pl-c1">rmrow</span>(Xtrain, s)
ycal <span class="pl-k">=</span> <span class="pl-c1">rmrow</span>(ytrain, s)
Xval <span class="pl-k">=</span> Xtrain[s, :]
yval <span class="pl-k">=</span> ytrain[s]

<span class="pl-c"><span class="pl-c">#</span># Computation of the performance over the grid</span>
<span class="pl-c"><span class="pl-c">#</span># (the model is fitted on Cal, and the performance is </span>
<span class="pl-c"><span class="pl-c">#</span># computed on Val)</span>
nlv <span class="pl-k">=</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">10</span> 
res <span class="pl-k">=</span> <span class="pl-c1">gridscorelv</span>(
    Xcal, ycal, Xval, yval;
    score <span class="pl-k">=</span> rmsep, fun <span class="pl-k">=</span> plskern, nlv <span class="pl-k">=</span> nlv) 

<span class="pl-c"><span class="pl-c">#</span># Plot the results</span>
<span class="pl-c1">plotgrid</span>(res<span class="pl-k">.</span>nlv, res<span class="pl-k">.</span>y1,
    xlabel <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Nb. LVs<span class="pl-pds">"</span></span>, ylabel <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>RMSEP<span class="pl-pds">"</span></span>)<span class="pl-k">.</span>f

<span class="pl-c"><span class="pl-c">#</span># Predictions and performance of the best model</span>
u <span class="pl-k">=</span> <span class="pl-c1">findall</span>(res<span class="pl-k">.</span>y1 <span class="pl-k">.==</span> <span class="pl-c1">minimum</span>(res<span class="pl-k">.</span>y1))[<span class="pl-c1">1</span>] 
res[u, :]
fm <span class="pl-k">=</span> <span class="pl-c1">plskern</span>(Xtrain, ytrain; nlv <span class="pl-k">=</span> res<span class="pl-k">.</span>nlv[u]) ;
res <span class="pl-k">=</span> Jchemo<span class="pl-k">.</span><span class="pl-c1">predict</span>(fm, Xtest) 
<span class="pl-c1">rmsep</span>(res<span class="pl-k">.</span>pred, ytest)

<span class="pl-c"><span class="pl-c">#</span># *Note*: For PLSR models, using gridscorelv is much faster</span>
<span class="pl-c"><span class="pl-c">#</span># than using the generic function gridscore.</span>
<span class="pl-c"><span class="pl-c">#</span># In the same manner, for ridge regression models,</span>
<span class="pl-c"><span class="pl-c">#</span># gridscorelb is much faster than gridscore.</span>

<span class="pl-c"><span class="pl-c">#</span># Syntax for the generic gridscore</span>
pars <span class="pl-k">=</span> <span class="pl-c1">mpar</span>(nlv <span class="pl-k">=</span> nlv)
res <span class="pl-k">=</span> <span class="pl-c1">gridscore</span>(
    Xcal, ycal, Xval, yval;
    score <span class="pl-k">=</span> rmsep, fun <span class="pl-k">=</span> plskern, pars <span class="pl-k">=</span> pars) </pre></div>
<ul dir="auto">
<li>
<h3 dir="auto"><a id="user-content-with-gridcv" class="anchor" aria-hidden="true" href="#with-gridcv"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>With gridcv</h3>
</li>
</ul>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Jchemo, StatsBase, CairoMakie

ntrain = 150 ; p = 200
ntest = 80 
Xtrain = rand(ntrain, p) ; ytrain = rand(ntrain) 
Xtest = rand(ntest, p) ; ytest = rand(ntest)
## Train is used to tune the model,
## and the generalization error is estimated on Test.

## Build the cross-validation (CV) segments
## Replicated K-Fold CV
K = 5      # Nb. folds
rep = 10   # Nb. replications (rep = 1 ==&gt; no replication)
segm = segmkf(ntrain, K; rep = rep)

## Or replicated test-set CV
m = 30     # Size of the test-set
rep = 10   # Nb. replications (rep = 1 ==&gt; no replication)
segm = segmts(ntrain, m; rep = rep) 

## Computation of the performances over the grid
nlv = 0:10 
rescv = gridcvlv(
    Xtrain, ytrain; segm = segm,
    score = rmsep, fun = plskern, nlv = nlv) ;
pnames(rescv)
res = rescv.res

## Plot the results
plotgrid(res.nlv, res.y1,
    xlabel = &quot;Nb. LVs&quot;, ylabel = &quot;RMSEP&quot;).f

## Predictions and performance of the best model 
u = findall(res.y1 .== minimum(res.y1))[1] 
res[u, :]
fm = plskern(Xtrain, ytrain; nlv = res.nlv[u]) ;
res = Jchemo.predict(fm, Xtest) 
rmsep(res.pred, ytest)

## *Note*: For PLSR models, using gridcvlv is much faster
## than using the generic function gridcv.
## In the same manner, for ridge regression models,
## gridcvlb is much faster than gridcv.

## Using the generic function gridcv:
pars = mpar(nlv = nlv)
rescv = gridcv(
    Xtrain, ytrain; segm = segm,
    score = rmsep, fun = plskern, pars = pars) ;
pnames(rescv)
res = rescv.res"><pre><span class="pl-k">using</span> Jchemo, StatsBase, CairoMakie

ntrain <span class="pl-k">=</span> <span class="pl-c1">150</span> ; p <span class="pl-k">=</span> <span class="pl-c1">200</span>
ntest <span class="pl-k">=</span> <span class="pl-c1">80</span> 
Xtrain <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntrain, p) ; ytrain <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntrain) 
Xtest <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntest, p) ; ytest <span class="pl-k">=</span> <span class="pl-c1">rand</span>(ntest)
<span class="pl-c"><span class="pl-c">#</span># Train is used to tune the model,</span>
<span class="pl-c"><span class="pl-c">#</span># and the generalization error is estimated on Test.</span>

<span class="pl-c"><span class="pl-c">#</span># Build the cross-validation (CV) segments</span>
<span class="pl-c"><span class="pl-c">#</span># Replicated K-Fold CV</span>
K <span class="pl-k">=</span> <span class="pl-c1">5</span>      <span class="pl-c"><span class="pl-c">#</span> Nb. folds</span>
rep <span class="pl-k">=</span> <span class="pl-c1">10</span>   <span class="pl-c"><span class="pl-c">#</span> Nb. replications (rep = 1 ==&gt; no replication)</span>
segm <span class="pl-k">=</span> <span class="pl-c1">segmkf</span>(ntrain, K; rep <span class="pl-k">=</span> rep)

<span class="pl-c"><span class="pl-c">#</span># Or replicated test-set CV</span>
m <span class="pl-k">=</span> <span class="pl-c1">30</span>     <span class="pl-c"><span class="pl-c">#</span> Size of the test-set</span>
rep <span class="pl-k">=</span> <span class="pl-c1">10</span>   <span class="pl-c"><span class="pl-c">#</span> Nb. replications (rep = 1 ==&gt; no replication)</span>
segm <span class="pl-k">=</span> <span class="pl-c1">segmts</span>(ntrain, m; rep <span class="pl-k">=</span> rep) 

<span class="pl-c"><span class="pl-c">#</span># Computation of the performances over the grid</span>
nlv <span class="pl-k">=</span> <span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">10</span> 
rescv <span class="pl-k">=</span> <span class="pl-c1">gridcvlv</span>(
    Xtrain, ytrain; segm <span class="pl-k">=</span> segm,
    score <span class="pl-k">=</span> rmsep, fun <span class="pl-k">=</span> plskern, nlv <span class="pl-k">=</span> nlv) ;
<span class="pl-c1">pnames</span>(rescv)
res <span class="pl-k">=</span> rescv<span class="pl-k">.</span>res

<span class="pl-c"><span class="pl-c">#</span># Plot the results</span>
<span class="pl-c1">plotgrid</span>(res<span class="pl-k">.</span>nlv, res<span class="pl-k">.</span>y1,
    xlabel <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Nb. LVs<span class="pl-pds">"</span></span>, ylabel <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>RMSEP<span class="pl-pds">"</span></span>)<span class="pl-k">.</span>f

<span class="pl-c"><span class="pl-c">#</span># Predictions and performance of the best model </span>
u <span class="pl-k">=</span> <span class="pl-c1">findall</span>(res<span class="pl-k">.</span>y1 <span class="pl-k">.==</span> <span class="pl-c1">minimum</span>(res<span class="pl-k">.</span>y1))[<span class="pl-c1">1</span>] 
res[u, :]
fm <span class="pl-k">=</span> <span class="pl-c1">plskern</span>(Xtrain, ytrain; nlv <span class="pl-k">=</span> res<span class="pl-k">.</span>nlv[u]) ;
res <span class="pl-k">=</span> Jchemo<span class="pl-k">.</span><span class="pl-c1">predict</span>(fm, Xtest) 
<span class="pl-c1">rmsep</span>(res<span class="pl-k">.</span>pred, ytest)

<span class="pl-c"><span class="pl-c">#</span># *Note*: For PLSR models, using gridcvlv is much faster</span>
<span class="pl-c"><span class="pl-c">#</span># than using the generic function gridcv.</span>
<span class="pl-c"><span class="pl-c">#</span># In the same manner, for ridge regression models,</span>
<span class="pl-c"><span class="pl-c">#</span># gridcvlb is much faster than gridcv.</span>

<span class="pl-c"><span class="pl-c">#</span># Using the generic function gridcv:</span>
pars <span class="pl-k">=</span> <span class="pl-c1">mpar</span>(nlv <span class="pl-k">=</span> nlv)
rescv <span class="pl-k">=</span> <span class="pl-c1">gridcv</span>(
    Xtrain, ytrain; segm <span class="pl-k">=</span> segm,
    score <span class="pl-k">=</span> rmsep, fun <span class="pl-k">=</span> plskern, pars <span class="pl-k">=</span> pars) ;
<span class="pl-c1">pnames</span>(rescv)
res <span class="pl-k">=</span> rescv<span class="pl-k">.</span>res</pre></div>
<h2 dir="auto"><a id="user-content--author-" class="anchor" aria-hidden="true" href="#-author-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><span> <strong>Author</strong> </span></h2>
<p dir="auto"><strong>Matthieu Lesnoff</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Cirad, <a href="https://umr-selmet.cirad.fr/en" rel="nofollow"><strong>UMR Selmet</strong></a>, Montpellier, France</p>
</li>
<li>
<p dir="auto"><a href="https://www.chemproject.org/ChemHouse" rel="nofollow"><strong>ChemHouse</strong></a>, Montpellier</p>
</li>
</ul>
<p dir="auto"><strong><a href="mailto:matthieu.lesnoff@cirad.fr">matthieu.lesnoff@cirad.fr</a></strong></p>
<h2 dir="auto"><a id="user-content-how-to-cite" class="anchor" aria-hidden="true" href="#how-to-cite"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to cite</h2>
<p dir="auto">Lesnoff, M. 2021. Jchemo: a Julia package for dimension reduction, regression and discrimination for
chemometrics. <a href="https://github.com/mlesnoff/Jchemo">https://github.com/mlesnoff/Jchemo</a>. CIRAD, UMR SELMET, Montpellier, France</p>
<h2 dir="auto"><a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgments</h2>
<ul dir="auto">
<li>G. Cornu (Cirad) <a href="https://ur-forets-societes.cirad.fr/en/l-unite/l-equipe" rel="nofollow">https://ur-forets-societes.cirad.fr/en/l-unite/l-equipe</a></li>
<li>L. Plagne, F. Févotte (Triscale.innov) <a href="https://www.triscale-innov.com" rel="nofollow">https://www.triscale-innov.com</a></li>
<li>R. Vezy (Cirad) <a href="https://www.youtube.com/channel/UCxArXLI-gxlTmWGGgec5D7w" rel="nofollow">https://www.youtube.com/channel/UCxArXLI-gxlTmWGGgec5D7w</a></li>
</ul>
</article></div>