<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-dynamicdiscretemodelsjl" class="anchor" aria-hidden="true" href="#dynamicdiscretemodelsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DynamicDiscreteModels.jl</h1>
<p>Back-end support for doing statistical inference with any model that can described as a "partially observed" (discrete) Markov chain, such as a Hidden Markov Model.</p>
<p>Model construction and data simulation:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>use a constructor defined in a front-end package:</span>
julia<span class="pl-k">&gt;</span> model<span class="pl-k">=</span><span class="pl-c1">myawesomemodel</span>()
julia<span class="pl-k">&gt;</span> <span class="pl-c1">coef!</span>(model,trueparameter)
julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>or simply:</span>
julia<span class="pl-k">&gt;</span> model<span class="pl-k">=</span><span class="pl-c1">myawesomemodel</span>(trueparameter)
julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>generate data from the model:</span>
julia<span class="pl-k">&gt;</span> data<span class="pl-k">=</span><span class="pl-c1">rand</span>(model,<span class="pl-c1">1000</span>);         <span class="pl-c"><span class="pl-c">#</span>1 time series with 1000 observations</span>
julia<span class="pl-k">&gt;</span> data<span class="pl-k">=</span><span class="pl-c1">rand</span>(model,<span class="pl-c1">100</span>,<span class="pl-c1">10</span>);       <span class="pl-c"><span class="pl-c">#</span>10 time series with 100 observations each</span>
julia<span class="pl-k">&gt;</span> data<span class="pl-k">=</span><span class="pl-c1">rand</span>(model,[<span class="pl-c1">100</span>,<span class="pl-c1">120</span>,<span class="pl-c1">115</span>]);    <span class="pl-c"><span class="pl-c">#</span>3 time series with 100, 120 and 115 obs.</span></pre></div>
<p>Parameter estimation:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>plain maximum likelihood:</span>
julia<span class="pl-k">&gt;</span> thetahat<span class="pl-k">=</span><span class="pl-c1">mle</span>(model,data)
julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>maximum likelihood via EM algorithm:</span>
julia<span class="pl-k">&gt;</span> thetahat<span class="pl-k">=</span><span class="pl-c1">em</span>(model,data)</pre></div>
<p>Hidden state inference (Viterbi filtering):</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>pretend thetahat is the "true" parameter:</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">coef!</span>(model,thetahat)
julia<span class="pl-k">&gt;</span> <span class="pl-c"><span class="pl-c">#</span>most likely path of latent states:</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">viterbi</span>(model,data)</pre></div>
<h2><a id="user-content-scope-of-the-package-and-role-of-front-end-packages" class="anchor" aria-hidden="true" href="#scope-of-the-package-and-role-of-front-end-packages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scope of the package and role of front-end packages</h2>
<p>The scope of the package includes:</p>
<ul>
<li>filter/smoother algorithms (aka. forward/backward)</li>
<li>parameter estimation: maximum likelihood and EM algorithm</li>
<li>hidden state inference: Viterbi filtering</li>
<li>support for deep modelling of the transition matrices</li>
<li>support for sparse transition matrices</li>
<li>support for Jacobians in all of the above</li>
</ul>
<p>In practice <a href="https://github.com/BenConnault/DynamicDiscreteModels.jl">DynamicDiscreteModels.jl</a> implements:</p>
<ul>
<li>some core methods: <code>rand()</code>, <code>loglikelihood()</code>, <code>estep()</code> and <code>viterbi()</code>.</li>
<li>convenience methods: <code>em()</code> wraps <code>estep()</code> with generic M-step numerical optimizations. <a href="https://github.com/BenConnault/DynamicDiscreteModels.jl">DynamicDiscreteModels.jl</a> also inherits <code>mle()</code> from <code>StatisticalModels</code>, which wraps <code>loglikelihood()</code> with generic numerical optimization.</li>
</ul>
<p>The job of a front-end package <a href="http://imgc.allpostersimages.com/images/P-473-488-90/56/5632/N32MG00Z/posters/h-armstrong-roberts-mad-scientist-in-laboratory-mixing-chemicals.jpg" rel="nofollow">MyModel.jl</a> is simply to specify <code>coef!(mymodel,myparameter)</code> which maps <code>myparameter</code> to <a href="https://github.com/BenConnault/DynamicDiscreteModels.jl">DynamicDiscreteModels.jl</a>'s canonical representation. All of the above methods will then be available. Optionally, a front-end package may also extend <code>em()</code> and/or <code>mle()</code> with specialized optimization wrapping <a href="https://github.com/BenConnault/DynamicDiscreteModels.jl">DynamicDiscreteModels.jl</a>'s <code>estep()</code> and <code>loglikelihood()</code>.</p>
<p>See <strong>Usage</strong> below for a simple example and see <a href="https://github.com/BenConnault/HiddenMarkovModels.jl">HiddenMarkovModels.jl</a> for an actual front-end package built on top of <a href="https://github.com/BenConnault/DynamicDiscreteModels.jl">DynamicDiscreteModels.jl</a>.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>DynamicDiscreteModels<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<p>For the purpose of this package, a "dynamic discrete model" is a discrete Markov chain z=(x,y), where x=1...dx is latent and y=1...dy is observed. The package defines an abstract type <code>DynamicDiscreteModel</code> with two fields:</p>
<ul>
<li><code>m</code> is a (dx,dy,dx,dy) array that holds the Markov probabilities m[x,y,x',y'] of moving from (x,y) today to (x',y') tomorrow.</li>
<li><code>mu</code> is a (dx,dy) array that holds the joint initial distribution of the chain.</li>
</ul>
<p>Any front-end implementation of a <code>DynamicDiscreteModel</code> will specify a mapping from a statistical parameter θ to the transition matrix <code>m</code> in a <code>coef!(model,parameter)</code> function. Being a back-end package, <code>DynamicDiscreteModels.jl</code> is agnostic as to the specific mapping, but <a href="examples/toymodel.jl">examples/toymodel.jl</a> provides a simple example. In this example (x,y) is a Hidden Markov model where x moves from today to tomorrow according to the Markov transition matrix A(θ) and y is drawn conditional on x according to the emission/transition matrix B(θ):</p>
<pre><code>.
       [ θ₁     1-θ₁ ]           [θ₂          (1-θ₂)/2   (1-θ₂)/2  ]
A(θ) = [ 1-θ₁   θ₁   ]    B(θ) = [(1-θ₂)/2    (1-θ₂)/2   θ₂        ]
</code></pre>
<p>The statistical parameter θ is of dimension 2, x can take values 1, 2 and y can take values 1, 2, 3. Here is the code defining a toy model:</p>
<div class="highlight highlight-source-julia"><pre>importall DynamicDiscreteModels

type ToyModel <span class="pl-k">&lt;:</span> <span class="pl-c1">DynamicDiscreteModel</span>

	<span class="pl-c"><span class="pl-c">#</span>DynamicDiscreteModel fields</span>
	m<span class="pl-k">::</span><span class="pl-c1">Array{Float64,4}</span>			  	<span class="pl-c"><span class="pl-c">#</span>the transition matrix given as m[x,y,x',y']</span>
	mu<span class="pl-k">::</span><span class="pl-c1">Array{Float64,2}</span>  			<span class="pl-c"><span class="pl-c">#</span>initial distribution (dx,dy)</span>

	<span class="pl-c"><span class="pl-c">#</span>DynamicDiscreteModel's technical fields</span>
	rho<span class="pl-k">::</span><span class="pl-c1">Array{Float64,1}</span>
	phi<span class="pl-k">::</span><span class="pl-c1">Array{Float64,1}</span>
	psi<span class="pl-k">::</span><span class="pl-c1">Array{Float64,1}</span>
<span class="pl-k">end</span>

dx,dy<span class="pl-k">=</span><span class="pl-c1">2</span>,<span class="pl-c1">3</span>

<span class="pl-en">toymodel</span>()<span class="pl-k">=</span><span class="pl-c1">ToyModel</span>(
  <span class="pl-c1">Array</span>(Float64,dx,dy,dx,dy),
  <span class="pl-c1">fill</span>(<span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">6</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>),
  <span class="pl-c1">Array</span>(Float64,<span class="pl-c1">1</span>),
  <span class="pl-c1">Array</span>(Float64,dx),
  <span class="pl-c1">Array</span>(Float64,dx))

<span class="pl-k">function</span> <span class="pl-en">coef!</span>(model<span class="pl-k">::</span><span class="pl-c1">ToyModel</span>,theta<span class="pl-k">::</span><span class="pl-c1">Tuple</span>)
	p1,p2<span class="pl-k">=</span>theta[<span class="pl-c1">1</span>],theta[<span class="pl-c1">2</span>]
	a<span class="pl-k">=</span>[p1 <span class="pl-c1">1</span><span class="pl-k">-</span>p1;<span class="pl-c1">1</span><span class="pl-k">-</span>p1 p1]
	p3<span class="pl-k">=</span>(<span class="pl-c1">1</span><span class="pl-k">-</span>p2)<span class="pl-k">/</span><span class="pl-c1">2</span>
	b<span class="pl-k">=</span>[p2 p3 p3; p3 p3 p2]
  <span class="pl-c"><span class="pl-c">#</span>a convenience function is provided for calibrating models with hidden Markov structure</span>
	<span class="pl-c1">hmm2ddm!</span>(model,a,b)         
<span class="pl-k">end</span></pre></div>
<p>We can initiate a toy model, calibrate it to a true parameter value θ*=(.65,.5), and simulate 100 time series of length 100:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> thetastar<span class="pl-k">=</span>(<span class="pl-c1">.65</span>, <span class="pl-c1">.5</span>);
julia<span class="pl-k">&gt;</span> model<span class="pl-k">=</span><span class="pl-c1">toymodel</span>();
julia<span class="pl-k">&gt;</span> <span class="pl-c1">coef!</span>(model,thetastar);
julia<span class="pl-k">&gt;</span> data<span class="pl-k">=</span><span class="pl-c1">simulate</span>(model,<span class="pl-c1">100</span>,<span class="pl-c1">100</span>);
julia<span class="pl-k">&gt;</span> data[<span class="pl-c1">1</span>]
<span class="pl-c1">100</span><span class="pl-k">-</span>element Array{Int64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">1</span>
 <span class="pl-c1">3</span>
 <span class="pl-c1">1</span>
 <span class="pl-c1">1</span>
 <span class="pl-c1">2</span>
 <span class="pl-c1">3</span>
 <span class="pl-k">...</span></pre></div>
<p>With the model and some data, we can find the most likely parameter value behind the data, according to the model, by computing the maximum likelihood estimator. Two methods are available: direct numerical optimization of the log-likelihood via <code>mle()</code> and the EM algorithm via <code>em()</code>. To help the optimizer, it is a good idea to first reparametrize the model on R rather than on [0,1]:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">theta2eta</span>(theta<span class="pl-k">::</span><span class="pl-c1">Tuple</span>)<span class="pl-k">=</span>[<span class="pl-c1">log</span>(theta[<span class="pl-c1">1</span>]<span class="pl-k">/</span>(<span class="pl-c1">1</span><span class="pl-k">-</span>theta[<span class="pl-c1">1</span>])),<span class="pl-c1">log</span>(theta[<span class="pl-c1">2</span>]<span class="pl-k">/</span>(<span class="pl-c1">1</span><span class="pl-k">-</span>theta[<span class="pl-c1">2</span>]))]
<span class="pl-en">eta2theta</span>(eta<span class="pl-k">::</span><span class="pl-c1">Array</span>)<span class="pl-k">=</span>(<span class="pl-c1">exp</span>(eta[<span class="pl-c1">1</span>])<span class="pl-k">/</span>(<span class="pl-c1">1</span><span class="pl-k">+</span><span class="pl-c1">exp</span>(eta[<span class="pl-c1">1</span>])),<span class="pl-c1">exp</span>(eta[<span class="pl-c1">2</span>])<span class="pl-k">/</span>(<span class="pl-c1">1</span><span class="pl-k">+</span><span class="pl-c1">exp</span>(eta[<span class="pl-c1">2</span>])))
<span class="pl-en">coef!</span>(model<span class="pl-k">::</span><span class="pl-c1">ToyModel</span>,eta<span class="pl-k">::</span><span class="pl-c1">Array</span>)<span class="pl-k">=</span><span class="pl-c1">calibrate!</span>(model,<span class="pl-c1">eta2theta</span>(eta))</pre></div>
<p>Notice how <code>coef!()</code> will use multiple dispatch to choose between the <code>eta</code> and <code>theta</code> parametrizations. <code>mle()</code> and <code>em()</code> will dispatch on <code>parameter::Array</code> by default so keep this signature for a parametrization which plays nicely with numerical optimization.</p>
<p>We then simply call:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> thetahat<span class="pl-k">=</span><span class="pl-c1">eta2theta</span>(<span class="pl-c1">mle</span>(model,data))
(<span class="pl-c1">0.6762467624375702</span>,<span class="pl-c1">0.5026000995341361</span>)
julia<span class="pl-k">&gt;</span> thetahat2<span class="pl-k">=</span><span class="pl-c1">eta2theta</span>(<span class="pl-c1">em</span>(model,data))
(<span class="pl-c1">0.6763156818261421</span>,<span class="pl-c1">0.5025930701695173</span>)</pre></div>
<p>Good news, up to some numerical precision error, <code>mle()</code> and <code>em()</code> agree on the maximum of the likelihood function. It also turns out that with this sample size, we can pinpoint the true parameter value (0.65,0.5) rather well. This is not the case with a smaller sample size:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> thetahat3<span class="pl-k">=</span><span class="pl-c1">eta2theta</span>(<span class="pl-c1">mle</span>(model,data[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10</span>]))
(<span class="pl-c1">0.8420953672648955</span>,<span class="pl-c1">0.4391327611992014</span>)</pre></div>
<p>Here is a plot of the likelihood surface using 10 and 100 time series, along with their maximums <code>thetahat</code> and <code>thetahat3</code> computed above (in blue) and the true parameter value (in green):</p>
<p><a target="_blank" rel="noopener noreferrer" href="pic/readme.png"><img src="pic/readme.png" alt="" style="max-width:100%;"></a></p>
<p>(See <a href="examples/toymodel.jl">examples/toymodel.jl</a> for the code used generate the picture.)</p>
</article></div>