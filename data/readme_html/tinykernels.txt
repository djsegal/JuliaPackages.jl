<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content---tinykernelsjl-" class="anchor" aria-hidden="true" href="#--tinykernelsjl-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a> <a target="_blank" rel="noopener noreferrer" href="docs/logo/logo_TinyKernels.png"><img src="docs/logo/logo_TinyKernels.png" alt="TinyKernels.jl" width="50" style="max-width: 100%;"></a> TinyKernels.jl </h1>
<p dir="auto"><a href="https://github.com/utkinis/TinyKernels.jl/actions/workflows/CI.yml"><img src="https://github.com/utkinis/TinyKernels.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a></p>
<p dir="auto"><strong>TinyKernels.jl</strong> provides a tiny abstraction for GPU (and CPU) kernels, with full support for CUDA (Nvidia) and ROCm (AMD) backends, limited support for Metal (GPU programming on MacOS ARM) backend, and allowing for multi-threaded CPU execution.</p>
<p dir="auto">TinyKernels.jl is mostly a heavily stripped-down version of <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a> supporting the bare minimum of the features. This package provides a sandbox for Julia GPU tooling and to measure the performance of kernels in a GPU-agnostic way. While the API of KernelAbstractions.jl is in a "transient" state, this package will provide the thin abstraction layer on top the <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>, <a href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a> and <a href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a> packages.</p>
<p dir="auto">TinyKernels.jl allows to explicitly launch GPU kernels asynchronously on different streams or queues with given priority. This feature facilitates the overlap between computations and memory transfers in distributed configurations.</p>
<p dir="auto">TinyKernels.jl supports automatic differentiation with <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> overloading the <code>Enzyme.autodiff</code> function to enable reverse mode AD of GPU (and CPU) kernels.</p>
<p dir="auto">Preliminary benchmarks can be found in <a href="https://github.com/luraess/TinyBenchmarks.jl">TinyBenchmarks.jl</a> and Metal playground in <a href="https://github.com/luraess/MetalGPU">MetalGPU</a>.</p>
<p dir="auto">Stay tuned <g-emoji class="g-emoji" alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">üöÄ</g-emoji></p>
<h3 dir="auto"><a id="user-content-compat" class="anchor" aria-hidden="true" href="#compat"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Compat</h3>
<ul dir="auto">
<li>AMDGPU ‚â• v0.4.8</li>
<li>CUDA ‚â• 3.13</li>
<li>Metal ‚â• v0.3.0</li>
</ul>
<h3 dir="auto"><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notes</h3>
<p dir="auto"><g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">‚ö†Ô∏è</g-emoji> <strong>Metal backend:</strong></p>
<ul dir="auto">
<li>Only <code>Float32</code> is being supported. For <code>Float64</code>, one could try using a construct from <a href="https://github.com/JuliaMath/DoubleFloats.jl/blob/ef689ccbab37d84943e2533309d34c6665229cab/src/Double.jl#L30">DoubleFloats.jl</a> <em>which may impact performance</em>.</li>
<li>Automatic differentiation (AD) capabilities (Enzyme.jl) are currently not working on ARM GPU (Metal) and giving erroneous results on ARM CPU.</li>
</ul>
</article></div>