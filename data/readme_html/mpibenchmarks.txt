<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-mpibenchmarks" class="anchor" aria-hidden="true" href="#mpibenchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MPIBenchmarks</h1>


<p dir="auto"><a href="https://github.com/JuliaParallel/MPIBenchmarks.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/JuliaParallel/MPIBenchmarks.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaParallel/MPIBenchmarks.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/2a5fb8e0a5dca91ac216e0a6a70c13ab149acef734fbb58f1d0743255f1852ad/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961506172616c6c656c2f4d504942656e63686d61726b732e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/JuliaParallel/MPIBenchmarks.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><code>MPIBenchmarks.jl</code> is a collection of benchmarks for
<a href="https://github.com/JuliaParallel/MPI.jl"><code>MPI.jl</code></a>, the
<a href="https://julialang.org/" rel="nofollow">Julia</a> wrapper for the Message Passing Interface
(<a href="https://www.mpi-forum.org/" rel="nofollow">MPI</a>).</p>
<p dir="auto">The goal is to have a suite of benchmarks which will allow <code>MPI.jl</code> users to</p>
<ul dir="auto">
<li>compare performance of different MPI libraries used with <code>MPI.jl</code>;</li>
<li>compare performance of <code>MPI.jl</code> with analogue benchmarks written in other languages, like
C/C++.</li>
</ul>
<p dir="auto">For this purpose we have benchmarks inspired by <a href="https://www.intel.com/content/www/us/en/develop/documentation/imb-user-guide/top.html" rel="nofollow">Intel(R) MPI
Benchmarks</a>
(IMB) and <a href="http://mvapich.cse.ohio-state.edu/benchmarks/" rel="nofollow">OSU Micro-Benchmarks</a>, to make it
easier to compare results with established MPI benchmarks suites.</p>
<p dir="auto"><em><strong>NOTE: <code>MPIBenchmarks.jl</code> is a work in progress.  Contributions are very much welcome!</strong></em></p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">To install <code>MPIBenchmarks.jl</code>, open a Julia REPL, type <code>]</code> to enter the Pkg REPL mode and
run the command</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="add https://github.com/JuliaParallel/MPIBenchmarks.jl"><pre class="notranslate"><code>add https://github.com/JuliaParallel/MPIBenchmarks.jl
</code></pre></div>
<p dir="auto"><code>MPIBenchmarks.jl</code> currently requires Julia v1.6 and <code>MPI.jl</code> v0.20.</p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto"><code>MPIBenchmarks.jl</code> currently provides the following benchmark types</p>
<ul dir="auto">
<li>collective
<ul dir="auto">
<li><code>IMBAllreduce()</code>: inspired by IMB Allreduce</li>
<li><code>IMBAlltoall()</code>: inspired by IMB Alltoall</li>
<li><code>IMBGatherv()</code>: inspired by IMB Gatherv</li>
<li><code>IMBReduce()</code>: inspired by IMB Reduce</li>
<li><code>OSUAllreduce()</code>: inspired by OSU Allreduce</li>
<li><code>OSUAlltoall()</code>: inspired by OSU Alltoall</li>
<li><code>OSUReduce()</code>: inspired by OSU Reduce</li>
</ul>
</li>
<li>point-to-point:
<ul dir="auto">
<li><code>IMBPingPong()</code>: inspired by IMB PingPong</li>
<li><code>IMBPingPing()</code>: inspired by IMB PingPing</li>
<li><code>OSULatency()</code>: inspired by OSU Latency</li>
</ul>
</li>
</ul>
<p dir="auto">After loading the package</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MPIBenchmarks"><pre><span class="pl-k">using</span> MPIBenchmarks</pre></div>
<p dir="auto">to run a benchmark use the function <code>benchmark(&lt;BENCHMARK TYPE&gt;)</code>, replacing <code>&lt;BENCHMARK TYPE&gt;</code>
with the name of the benchmark you want to run, from the list above.  The benchmarking types
take the following arguments:</p>
<ul dir="auto">
<li>optional positional argument:
<ul dir="auto">
<li><code>T::Type</code>: data type to use for the communication.  It must be a bits type, with size in
bytes which is a power of 2.  Default is <code>UInt8</code></li>
</ul>
</li>
<li>keyword arguments:
<ul dir="auto">
<li><code>verbose::Bool</code>: whether to print to <code>stdout</code> some information.  Default is <code>true</code>.</li>
<li><code>max_size::Int</code>: maximum size of the data to transmit, in bytes.  It must be
a power of 2 and larger than the size of the datatype <code>T</code>.</li>
<li><code>filename::Union{String,Nothing}</code>: name of the output CSV file where to save the results
of the benchmark.  If <code>nothing</code>, the file is not written.  Default is a string with the
name of the given benchmark (e.g., <code>"julia_imb_pingpong.csv"</code> for <code>IMBPingPong</code>).</li>
</ul>
</li>
</ul>
<p dir="auto"><em><strong>NOTE: kernels of the benchmarks in the IMB and OSU suites are usually very
similar, if not identical.  After all, they benchmark the same MPI functions.
However, there are usually subtle differences, for example with regards to the
number of iterations, datatypes used, how buffers are initialized, etc, which
can slightly affect the results.  <code>MPIBenchmarks.jl</code> tries to match what the
original benchmarks do, but there is no guarantee about this and there may still
be unwanted differences.  If you spot any, please open an issue or submit a pull
request.  As a rule of thumb, OSU benchmarks tend be easier to follow than the
IMB's, so our implementations of their benchmarks should generally be more
faithful than compared to the IMB ones.</strong></em></p>
<h3 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h3>
<p dir="auto">Write a script like the following:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MPIBenchmarks

# Collective benchmarks
benchmark(IMBAllreduce())
benchmark(IMBAlltoall())
benchmark(IMBGatherv())
benchmark(IMBReduce())
benchmark(OSUAllreduce())
benchmark(OSUAlltoall())
benchmark(OSUReduce())

# Point-to-point benchmarks.
# NOTE: they require exactly two MPI processes.
benchmark(IMBPingPong())
benchmark(IMBPingPing())
benchmark(OSULatency())"><pre><span class="pl-k">using</span> MPIBenchmarks

<span class="pl-c"><span class="pl-c">#</span> Collective benchmarks</span>
<span class="pl-c1">benchmark</span>(<span class="pl-c1">IMBAllreduce</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">IMBAlltoall</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">IMBGatherv</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">IMBReduce</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">OSUAllreduce</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">OSUAlltoall</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">OSUReduce</span>())

<span class="pl-c"><span class="pl-c">#</span> Point-to-point benchmarks.</span>
<span class="pl-c"><span class="pl-c">#</span> NOTE: they require exactly two MPI processes.</span>
<span class="pl-c1">benchmark</span>(<span class="pl-c1">IMBPingPong</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">IMBPingPing</span>())
<span class="pl-c1">benchmark</span>(<span class="pl-c1">OSULatency</span>())</pre></div>
<p dir="auto">Then execute it with the following command</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="mpiexecjl -np 2 julia --project mpi_benchmarks.jl"><pre class="notranslate"><code>mpiexecjl -np 2 julia --project mpi_benchmarks.jl
</code></pre></div>
<p dir="auto">where</p>
<ul dir="auto">
<li><code>mpiexecjl</code> is <a href="https://juliaparallel.org/MPI.jl/dev/usage/#Julia-wrapper-for-mpiexec" rel="nofollow"><code>MPI.jl</code>'s wrapper for
<code>mpiexec</code></a>,</li>
<li><code>2</code> is the number of MPI process to launch.  Use any other suitable number for
your benchmarks, typically at least 2.  Note that point-to-point benchmarks
require exactly 2 processes, so if you want to use more processes for the
collective operations you will have to run them in a separate script,</li>
<li><code>mpi_benchmarks.jl</code> is the name of the script you created.</li>
</ul>
<h2 dir="auto"><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">The <code>MPIBenchmarks.jl</code> package is licensed under the MIT "Expat" License.  The original
author is Mosè Giordano.</p>
</article></div>