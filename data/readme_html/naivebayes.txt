<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-naivebayesjl" class="anchor" aria-hidden="true" href="#naivebayesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NaiveBayes.jl</h1>
<p><a href="https://travis-ci.org/dfdx/NaiveBayes.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/fc2ca36a6bd82748561c131293cfd84f46b36aa6167df33fa6bffed7ab55d4f6/68747470733a2f2f7472617669732d63692e6f72672f646664782f4e6169766542617965732e6a6c2e737667" alt="Build Status" data-canonical-src="https://travis-ci.org/dfdx/NaiveBayes.jl.svg" style="max-width:100%;"></a>
<a href="http://codecov.io/github/dfdx/NaiveBayes.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/26856ddf92f171d2cc86e9c80c11b43bb41b65e13d0069dd65f51771fad4df5f/687474703a2f2f636f6465636f762e696f2f6769746875622f646664782f4e6169766542617965732e6a6c2f636f7665726167652e737667" alt="codecov.io" data-canonical-src="http://codecov.io/github/dfdx/NaiveBayes.jl/coverage.svg" style="max-width:100%;"></a></p>
<p>Naive Bayes classifier. Currently 3 types of NB are supported:</p>
<ul>
<li><strong>MultinomialNB</strong> - Assumes variables have a multinomial distribution. Good for text classification. See <code>examples/nums.jl</code> for usage.</li>
<li><strong>GaussianNB</strong> - Assumes variables have a multivariate normal distribution. Good for real-valued data. See <code>examples/iris.jl</code> for usage.</li>
<li><strong>HybridNB</strong> - A hybrid empirical naive Bayes model for a mixture of continuous and discrete features. The continuous features are estimated using Kernel Density Estimation.
<em>Note</em>: fit/predict methods take <code>Dict{Symbol/AstractString, Vector}</code> rather than a <code>Matrix</code>. Also, discrete features must be integers while continuous features must be floats. If all features are continuous <code>Matrix</code> input is supported.</li>
</ul>
<p>Since <code>GaussianNB</code> models multivariate distribution, it's not really a "naive" classifier (i.e. no independence assumption is made), so the name may change in the future.</p>
<p>As a subproduct, this package also provides a <code>DataStats</code> type that may be used for incremental calculation of common data statistics such as mean and covariance matrix. See <code>test/datastatstest.jl</code> for a usage example.</p>
<h3><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples:</h3>
<ol>
<li>
<p>Continuous and discrete features as <code>Dict{Symbol, Vector}}</code></p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="f_c1 = randn(10)
f_c2 = randn(10)
f_d1 = rand(1:5, 10)
f_d2 = randn(3:7, 10)
training_features_continuous = Dict{Symbol, Vector{Float64}}(:c1=&gt;f_c1, :c2=&gt;f_c2)
training_features_discrete   = Dict{Symbol, Vector{Int}}(:d1=&gt;f_d1, :d2=&gt;f_d2) #discrete features as Int64

hybrid_model = HybridNB(labels)

# train the model
fit(hybrid_model, training_features_continuous, training_features_discrete, labels)
# predict the classification for new events (points): features_c, features_d
y = predict(hybrid_model, features_c, features_d)
"><pre>f_c1 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">10</span>)
f_c2 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">10</span>)
f_d1 <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5</span>, <span class="pl-c1">10</span>)
f_d2 <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span><span class="pl-k">:</span><span class="pl-c1">7</span>, <span class="pl-c1">10</span>)
training_features_continuous <span class="pl-k">=</span> <span class="pl-c1">Dict</span><span class="pl-c1">{Symbol, Vector{Float64}}</span>(<span class="pl-c1">:c1</span><span class="pl-k">=&gt;</span>f_c1, <span class="pl-c1">:c2</span><span class="pl-k">=&gt;</span>f_c2)
training_features_discrete   <span class="pl-k">=</span> <span class="pl-c1">Dict</span><span class="pl-c1">{Symbol, Vector{Int}}</span>(<span class="pl-c1">:d1</span><span class="pl-k">=&gt;</span>f_d1, <span class="pl-c1">:d2</span><span class="pl-k">=&gt;</span>f_d2) <span class="pl-c"><span class="pl-c">#</span>discrete features as Int64</span>

hybrid_model <span class="pl-k">=</span> <span class="pl-c1">HybridNB</span>(labels)

<span class="pl-c"><span class="pl-c">#</span> train the model</span>
<span class="pl-c1">fit</span>(hybrid_model, training_features_continuous, training_features_discrete, labels)
<span class="pl-c"><span class="pl-c">#</span> predict the classification for new events (points): features_c, features_d</span>
y <span class="pl-k">=</span> <span class="pl-c1">predict</span>(hybrid_model, features_c, features_d)</pre></div>
<p>Alternatively one can skip declaring the model and train it directly:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model = train(HybridNB, training_features_continuous, training_features_discrete, labels)
y = predict(hybrid_model, features_c, features_d)
"><pre>model <span class="pl-k">=</span> <span class="pl-c1">train</span>(HybridNB, training_features_continuous, training_features_discrete, labels)
y <span class="pl-k">=</span> <span class="pl-c1">predict</span>(hybrid_model, features_c, features_d)</pre></div>
</li>
<li>
<p>Continuous features only as a <code>Matrix</code></p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="X_train = randn(3,400);
X_classify = randn(3,10)

hybrid_model = HybridNB(labels) # the number of discrete features is 0 so it's not needed
fit(hybrid_model, X_train, labels)
y = predict(hybrid_model, X_classify)
"><pre>X_train <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>,<span class="pl-c1">400</span>);
X_classify <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>,<span class="pl-c1">10</span>)

hybrid_model <span class="pl-k">=</span> <span class="pl-c1">HybridNB</span>(labels) <span class="pl-c"><span class="pl-c">#</span> the number of discrete features is 0 so it's not needed</span>
<span class="pl-c1">fit</span>(hybrid_model, X_train, labels)
y <span class="pl-k">=</span> <span class="pl-c1">predict</span>(hybrid_model, X_classify)</pre></div>
</li>
<li>
<p>Continuous and discrete features as a <code>Matrix{Float}</code></p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="#X is a matrix of features
# the first 3 rows are continuous
training_features_continuous = restructure_matrix(X[1:3, :])
# the last 2 rows are discrete and must be integers
training_features_discrete = map(Int, restructure_matrix(X[4:5, :]))
# train the model
hybrid_model = train(HybridNB, training_features_continuous, training_features_discrete, labels)

# predict the classification for new events (points): features_c, features_d
y = predict(hybrid_model, features_c, features_d)
"><pre><span class="pl-c"><span class="pl-c">#</span>X is a matrix of features</span>
<span class="pl-c"><span class="pl-c">#</span> the first 3 rows are continuous</span>
training_features_continuous <span class="pl-k">=</span> <span class="pl-c1">restructure_matrix</span>(X[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>, :])
<span class="pl-c"><span class="pl-c">#</span> the last 2 rows are discrete and must be integers</span>
training_features_discrete <span class="pl-k">=</span> <span class="pl-c1">map</span>(Int, <span class="pl-c1">restructure_matrix</span>(X[<span class="pl-c1">4</span><span class="pl-k">:</span><span class="pl-c1">5</span>, :]))
<span class="pl-c"><span class="pl-c">#</span> train the model</span>
hybrid_model <span class="pl-k">=</span> <span class="pl-c1">train</span>(HybridNB, training_features_continuous, training_features_discrete, labels)

<span class="pl-c"><span class="pl-c">#</span> predict the classification for new events (points): features_c, features_d</span>
y <span class="pl-k">=</span> <span class="pl-c1">predict</span>(hybrid_model, features_c, features_d)</pre></div>
</li>
</ol>
<h3><a id="user-content-writeload-models-to-files" class="anchor" aria-hidden="true" href="#writeload-models-to-files"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Write/Load models to files</h3>
<p>It is useful to train a model once and then use it for prediction many times later. For example, train your classifier on a local machine and then use it on a cluster to classify points in parallel.</p>
<p>There is support for writing <code>HybridNB</code> models to HDF5 files via the methods <code>write_model</code> and <code>load_model</code>. This is useful for interacting with other programs/languages. If the model file is going to be read only in Julia it is easier to use <strong>JLD.jl</strong> for saving and loading the file.</p>
</article></div>