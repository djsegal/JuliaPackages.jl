<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-oilmmsjl-orthogonal-instantaneous-linear-mixing-models-in-julia" class="anchor" aria-hidden="true" href="#oilmmsjl-orthogonal-instantaneous-linear-mixing-models-in-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OILMMs.jl: Orthogonal Instantaneous Linear Mixing Models in Julia</h1>

<p><a href="https://travis-ci.com/willtebbutt/OILMMs.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/1568cbf1b4a84015c37d30d8a9efc5e3398fa26351982d57b678552607cf83c5/68747470733a2f2f7472617669732d63692e636f6d2f77696c6c746562627574742f4f494c4d4d732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/willtebbutt/OILMMs.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/willtebbutt/OILMMs.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/13c19d11d0d2f860cf827e8e625fc4b42e7b0bf693fd8c048b9a14c9ddf26e06/68747470733a2f2f636f6465636f762e696f2f67682f77696c6c746562627574742f4f494c4d4d732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/willtebbutt/OILMMs.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a>
<a href="https://github.com/SciML/ColPrac"><img src="https://camo.githubusercontent.com/a6c1efcb19a957860ecb25966a730260b03d6e05380d0c27992ee7f9e3b1feb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6c507261632d436f6e7472696275746f72277325323047756964652d626c756576696f6c6574" alt="ColPrac: Contributor's Guide on Collaborative Practices for Community Packages" data-canonical-src="https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet" style="max-width:100%;"></a></p>
<p>An implementation of the Orthogonal Instantaneous Linear Mixing Model (OILMM).</p>
<p>The Python implementation can be found <a href="https://github.com/wesselb/oilmm">here</a>.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<p>Please refer to the examples directory for basic usage, or below for a very quick intro.</p>
<h2><a id="user-content-api" class="anchor" aria-hidden="true" href="#api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>API</h2>
<p>The API broadly follows <a href="https://github.com/willtebbutt/Stheno.jl/">Stheno.jl</a>'s.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="f = OILMM(...)
"><pre>f <span class="pl-k">=</span> <span class="pl-c1">OILMM</span>(<span class="pl-k">...</span>)</pre></div>
<p>constructs an Orthogonal Instantaneous Linear Mixing Model. This object represents a distribution over vector-valued functions -- see the docstring for more info.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="f(x)
"><pre><span class="pl-c1">f</span>(x)</pre></div>
<p>represents <code>f</code> at the input locations <code>x</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="logpdf(f(x), y) # compute the log marginal probability of `y` under the model.
rand(rng, f(x)) # sample from `f` at `x`, for random number generator `rng`.
marginals(f(x)) # compute the marginal statistics of `f` at `x`.
"><pre><span class="pl-c1">logpdf</span>(<span class="pl-c1">f</span>(x), y) <span class="pl-c"><span class="pl-c">#</span> compute the log marginal probability of `y` under the model.</span>
<span class="pl-c1">rand</span>(rng, <span class="pl-c1">f</span>(x)) <span class="pl-c"><span class="pl-c">#</span> sample from `f` at `x`, for random number generator `rng`.</span>
<span class="pl-c1">marginals</span>(<span class="pl-c1">f</span>(x)) <span class="pl-c"><span class="pl-c">#</span> compute the marginal statistics of `f` at `x`.</span></pre></div>
<p><code>y</code> should be an <code>AbstractVector{&lt;:Real}</code> of the same length as <code>x</code>.</p>
<p>To perform inference, simply invoke the <code>posterior</code> function:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="f_post = posterior(f(x), y)
"><pre><code>f_post = posterior(f(x), y)
</code></pre></div>
<p><code>f_post</code> is then another <code>OILMM</code> that is the posterior distribution. That this works is one of the very convenient properties of the OILMM.</p>
<p>All public functions should have docstrings. If you encounter something that is unclear, please raise an issue so that it can be fixed.</p>
<h2><a id="user-content-worked-example" class="anchor" aria-hidden="true" href="#worked-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Worked Example</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using AbstractGPs
using LinearAlgebra
using OILMMs
using Random

# Specify and construct an OILMM.
p = 10
m = 3
U, s, _ = svd(randn(p, m))
σ² = 0.1

f = OILMM(
    [GP(SEKernel()) for _ in 1:m],
    U,
    Diagonal(s),
    Diagonal(rand(m) .+ 0.1),
);

# Sample from the model.
x = MOInput(randn(10), p);
fx = f(x, σ²);

rng = MersenneTwister(123456);
y = rand(rng, fx);

# Compute the logpdf of the data under the model.
logpdf(fx, y)

# Perform posterior inference. This produces another OILMM.
f_post = posterior(fx, y)

# Compute the posterior marginals. We can also use `rand` and `logpdf` as before.
post_marginals = marginals(f_post(x));
"><pre><span class="pl-k">using</span> AbstractGPs
<span class="pl-k">using</span> LinearAlgebra
<span class="pl-k">using</span> OILMMs
<span class="pl-k">using</span> Random

<span class="pl-c"><span class="pl-c">#</span> Specify and construct an OILMM.</span>
p <span class="pl-k">=</span> <span class="pl-c1">10</span>
m <span class="pl-k">=</span> <span class="pl-c1">3</span>
U, s, _ <span class="pl-k">=</span> <span class="pl-c1">svd</span>(<span class="pl-c1">randn</span>(p, m))
σ² <span class="pl-k">=</span> <span class="pl-c1">0.1</span>

f <span class="pl-k">=</span> <span class="pl-c1">OILMM</span>(
    [<span class="pl-c1">GP</span>(<span class="pl-c1">SEKernel</span>()) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>m],
    U,
    <span class="pl-c1">Diagonal</span>(s),
    <span class="pl-c1">Diagonal</span>(<span class="pl-c1">rand</span>(m) <span class="pl-k">.+</span> <span class="pl-c1">0.1</span>),
);

<span class="pl-c"><span class="pl-c">#</span> Sample from the model.</span>
x <span class="pl-k">=</span> <span class="pl-c1">MOInput</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">10</span>), p);
fx <span class="pl-k">=</span> <span class="pl-c1">f</span>(x, σ²);

rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>);
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, fx);

<span class="pl-c"><span class="pl-c">#</span> Compute the logpdf of the data under the model.</span>
<span class="pl-c1">logpdf</span>(fx, y)

<span class="pl-c"><span class="pl-c">#</span> Perform posterior inference. This produces another OILMM.</span>
f_post <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(fx, y)

<span class="pl-c"><span class="pl-c">#</span> Compute the posterior marginals. We can also use `rand` and `logpdf` as before.</span>
post_marginals <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f_post</span>(x));</pre></div>
<h2><a id="user-content-worked-example-using-temporalgpsjl" class="anchor" aria-hidden="true" href="#worked-example-using-temporalgpsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Worked Example using TemporalGPs.jl.</h2>
<p><a href="https://github.com/willtebbutt/TemporalGPs.jl/">TemporalGPs.jl</a> makes inference and learning in GPs for time series much more efficient than performing exact inference.
It plays nicely with this package, and can be used to accelerate inference in an OILMM
simply by wrapping each of the base processes using <code>to_sde</code>. See the TemporalGPs.jl docs
for more info on this.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using AbstractGPs
using LinearAlgebra
using OILMMs
using Random
using TemporalGPs

# Specify and construct an OILMM.
p = 10
m = 3
U, s, _ = svd(randn(p, m))
σ² = 0.1

f = OILMM(
    [to_sde(GP(Matern52Kernel()), SArrayStorage(Float64)) for _ in 1:m],
    U,
    Diagonal(s),
    Diagonal(rand(m) .+ 0.1),
);

# Sample from the model. LARGE DATA SET!
x = MOInput(RegularSpacing(0.0, 1.0, 1_000_000), p);
fx = f(x, σ²);
rng = MersenneTwister(123456);
y = rand(rng, fx);

# Compute the logpdf of the data under the model.
logpdf(fx, y)

# Perform posterior inference. This produces another OILMM.
f_post = posterior(fx, y)

# Compute the posterior marginals. We can also use `rand` and `logpdf` as before.
post_marginals = marginals(f_post(x));
"><pre><span class="pl-k">using</span> AbstractGPs
<span class="pl-k">using</span> LinearAlgebra
<span class="pl-k">using</span> OILMMs
<span class="pl-k">using</span> Random
<span class="pl-k">using</span> TemporalGPs

<span class="pl-c"><span class="pl-c">#</span> Specify and construct an OILMM.</span>
p <span class="pl-k">=</span> <span class="pl-c1">10</span>
m <span class="pl-k">=</span> <span class="pl-c1">3</span>
U, s, _ <span class="pl-k">=</span> <span class="pl-c1">svd</span>(<span class="pl-c1">randn</span>(p, m))
σ² <span class="pl-k">=</span> <span class="pl-c1">0.1</span>

f <span class="pl-k">=</span> <span class="pl-c1">OILMM</span>(
    [<span class="pl-c1">to_sde</span>(<span class="pl-c1">GP</span>(<span class="pl-c1">Matern52Kernel</span>()), <span class="pl-c1">SArrayStorage</span>(Float64)) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>m],
    U,
    <span class="pl-c1">Diagonal</span>(s),
    <span class="pl-c1">Diagonal</span>(<span class="pl-c1">rand</span>(m) <span class="pl-k">.+</span> <span class="pl-c1">0.1</span>),
);

<span class="pl-c"><span class="pl-c">#</span> Sample from the model. LARGE DATA SET!</span>
x <span class="pl-k">=</span> <span class="pl-c1">MOInput</span>(<span class="pl-c1">RegularSpacing</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">1.0</span>, <span class="pl-c1">1_000_000</span>), p);
fx <span class="pl-k">=</span> <span class="pl-c1">f</span>(x, σ²);
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>);
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, fx);

<span class="pl-c"><span class="pl-c">#</span> Compute the logpdf of the data under the model.</span>
<span class="pl-c1">logpdf</span>(fx, y)

<span class="pl-c"><span class="pl-c">#</span> Perform posterior inference. This produces another OILMM.</span>
f_post <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(fx, y)

<span class="pl-c"><span class="pl-c">#</span> Compute the posterior marginals. We can also use `rand` and `logpdf` as before.</span>
post_marginals <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f_post</span>(x));</pre></div>
<p>Notice that this example is nearly identical to the one above, because all GP-related
packages used utilise the
<a href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl">AbstractGPs.jl</a> APIs.</p>
<h2><a id="user-content-worked-example-learning-parameters" class="anchor" aria-hidden="true" href="#worked-example-learning-parameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Worked Example Learning Parameters</h2>
<p>We don't provide a fit+predict interface, instead we rely on external packages to provide
something similar that is more flexible.
Specifically, we recommend using a mixture of
<a href="https://github.com/invenia/ParameterHandling.jl/">ParameterHandling.jl</a>,
<a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a>
or some other general non-linear optimisation package), and
<a href="https://github.com/FluxML/Zygote.jl/">Zygote.jl</a>. Below we provide a small example:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Load GP-related packages.
using AbstractGPs
using OILMMs
using TemporalGPs

# Load standard packages from the Julia ecosystem
using LinearAlgebra
using Optim # Standard optimisation algorithms.
using ParameterHandling # Helper functionality for dealing with model parameters.
using Random
using Zygote # Algorithmic Differentiation

# Specify OILMM parameters as a NamedTuple.
# Utilise orthogonal, and positive from ParameterHandling.jl to constrain appropriately.
p = 2
m = 1
θ_init = (
    U = orthogonal(randn(p, m)),
    s = positive.(rand(m) .+ 0.1),
    σ² = positive(0.1),
)

# Define a function which builds an OILMM, given a NamedTuple of parameters.
function build_oilmm(θ::NamedTuple)
    return OILMM(
        [to_sde(GP(Matern52Kernel()), SArrayStorage(Float64)) for _ in 1:m],
        θ.U,
        Diagonal(θ.s),
        Diagonal(zeros(m)),
    )
end

# Generate some synthetic data to train on.
f = build_oilmm(ParameterHandling.value(θ_init));
const x = MOInput(RegularSpacing(0.0, 0.01, 1_000_000), p);
fx = f(x, 0.1);
rng = MersenneTwister(123456);
const y = rand(rng, fx);

# Define a function which computes the NLML given the parameters.
function objective(θ::NamedTuple)
    f = build_oilmm(θ)
    return -logpdf(f(x, θ.σ²), y)
end

# Build a version of the objective function which can be used with Optim.jl.
θ_init_flat, unflatten = flatten(θ_init);
unpack(θ::Vector{&lt;:Real}) = ParameterHandling.value(unflatten(θ))
objective(θ::Vector{&lt;:Real}) = objective(unpack(θ))

# Utilise Optim.jl + Zygote.jl to optimise the model parameters.
training_results = Optim.optimize(
    objective,
    θ -&gt; only(Zygote.gradient(objective, θ)),
    θ_init_flat + randn(length(θ_init_flat)), # Add some noise to make learning non-trivial
    BFGS(
        alphaguess = Optim.LineSearches.InitialStatic(scaled=true),
        linesearch = Optim.LineSearches.BackTracking(),
    ),
    Optim.Options(show_trace = true);
    inplace=false,
)

# Compute posterior marginals at optimal solution.
θ_opt = unpack(training_results.minimizer)
f = build_oilmm(θ_opt)
f_post = posterior(f(x, θ_opt.σ²), y)
fx = marginals(f_post(x))
"><pre><span class="pl-c"><span class="pl-c">#</span> Load GP-related packages.</span>
<span class="pl-k">using</span> AbstractGPs
<span class="pl-k">using</span> OILMMs
<span class="pl-k">using</span> TemporalGPs

<span class="pl-c"><span class="pl-c">#</span> Load standard packages from the Julia ecosystem</span>
<span class="pl-k">using</span> LinearAlgebra
<span class="pl-k">using</span> Optim <span class="pl-c"><span class="pl-c">#</span> Standard optimisation algorithms.</span>
<span class="pl-k">using</span> ParameterHandling <span class="pl-c"><span class="pl-c">#</span> Helper functionality for dealing with model parameters.</span>
<span class="pl-k">using</span> Random
<span class="pl-k">using</span> Zygote <span class="pl-c"><span class="pl-c">#</span> Algorithmic Differentiation</span>

<span class="pl-c"><span class="pl-c">#</span> Specify OILMM parameters as a NamedTuple.</span>
<span class="pl-c"><span class="pl-c">#</span> Utilise orthogonal, and positive from ParameterHandling.jl to constrain appropriately.</span>
p <span class="pl-k">=</span> <span class="pl-c1">2</span>
m <span class="pl-k">=</span> <span class="pl-c1">1</span>
θ_init <span class="pl-k">=</span> (
    U <span class="pl-k">=</span> <span class="pl-c1">orthogonal</span>(<span class="pl-c1">randn</span>(p, m)),
    s <span class="pl-k">=</span> <span class="pl-c1">positive</span>.(<span class="pl-c1">rand</span>(m) <span class="pl-k">.+</span> <span class="pl-c1">0.1</span>),
    σ² <span class="pl-k">=</span> <span class="pl-c1">positive</span>(<span class="pl-c1">0.1</span>),
)

<span class="pl-c"><span class="pl-c">#</span> Define a function which builds an OILMM, given a NamedTuple of parameters.</span>
<span class="pl-k">function</span> <span class="pl-en">build_oilmm</span>(θ<span class="pl-k">::</span><span class="pl-c1">NamedTuple</span>)
    <span class="pl-k">return</span> <span class="pl-c1">OILMM</span>(
        [<span class="pl-c1">to_sde</span>(<span class="pl-c1">GP</span>(<span class="pl-c1">Matern52Kernel</span>()), <span class="pl-c1">SArrayStorage</span>(Float64)) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>m],
        θ<span class="pl-k">.</span>U,
        <span class="pl-c1">Diagonal</span>(θ<span class="pl-k">.</span>s),
        <span class="pl-c1">Diagonal</span>(<span class="pl-c1">zeros</span>(m)),
    )
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Generate some synthetic data to train on.</span>
f <span class="pl-k">=</span> <span class="pl-c1">build_oilmm</span>(ParameterHandling<span class="pl-k">.</span><span class="pl-c1">value</span>(θ_init));
<span class="pl-k">const</span> x <span class="pl-k">=</span> <span class="pl-c1">MOInput</span>(<span class="pl-c1">RegularSpacing</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">0.01</span>, <span class="pl-c1">1_000_000</span>), p);
fx <span class="pl-k">=</span> <span class="pl-c1">f</span>(x, <span class="pl-c1">0.1</span>);
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>);
<span class="pl-k">const</span> y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, fx);

<span class="pl-c"><span class="pl-c">#</span> Define a function which computes the NLML given the parameters.</span>
<span class="pl-k">function</span> <span class="pl-en">objective</span>(θ<span class="pl-k">::</span><span class="pl-c1">NamedTuple</span>)
    f <span class="pl-k">=</span> <span class="pl-c1">build_oilmm</span>(θ)
    <span class="pl-k">return</span> <span class="pl-k">-</span><span class="pl-c1">logpdf</span>(<span class="pl-c1">f</span>(x, θ<span class="pl-k">.</span>σ²), y)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Build a version of the objective function which can be used with Optim.jl.</span>
θ_init_flat, unflatten <span class="pl-k">=</span> <span class="pl-c1">flatten</span>(θ_init);
<span class="pl-en">unpack</span>(θ<span class="pl-k">::</span><span class="pl-c1">Vector{&lt;:Real}</span>) <span class="pl-k">=</span> ParameterHandling<span class="pl-k">.</span><span class="pl-c1">value</span>(<span class="pl-c1">unflatten</span>(θ))
<span class="pl-en">objective</span>(θ<span class="pl-k">::</span><span class="pl-c1">Vector{&lt;:Real}</span>) <span class="pl-k">=</span> <span class="pl-c1">objective</span>(<span class="pl-c1">unpack</span>(θ))

<span class="pl-c"><span class="pl-c">#</span> Utilise Optim.jl + Zygote.jl to optimise the model parameters.</span>
training_results <span class="pl-k">=</span> Optim<span class="pl-k">.</span><span class="pl-c1">optimize</span>(
    objective,
    θ <span class="pl-k">-&gt;</span> <span class="pl-c1">only</span>(Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(objective, θ)),
    θ_init_flat <span class="pl-k">+</span> <span class="pl-c1">randn</span>(<span class="pl-c1">length</span>(θ_init_flat)), <span class="pl-c"><span class="pl-c">#</span> Add some noise to make learning non-trivial</span>
    <span class="pl-c1">BFGS</span>(
        alphaguess <span class="pl-k">=</span> Optim<span class="pl-k">.</span>LineSearches<span class="pl-k">.</span><span class="pl-c1">InitialStatic</span>(scaled<span class="pl-k">=</span><span class="pl-c1">true</span>),
        linesearch <span class="pl-k">=</span> Optim<span class="pl-k">.</span>LineSearches<span class="pl-k">.</span><span class="pl-c1">BackTracking</span>(),
    ),
    Optim<span class="pl-k">.</span><span class="pl-c1">Options</span>(show_trace <span class="pl-k">=</span> <span class="pl-c1">true</span>);
    inplace<span class="pl-k">=</span><span class="pl-c1">false</span>,
)

<span class="pl-c"><span class="pl-c">#</span> Compute posterior marginals at optimal solution.</span>
θ_opt <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(training_results<span class="pl-k">.</span>minimizer)
f <span class="pl-k">=</span> <span class="pl-c1">build_oilmm</span>(θ_opt)
f_post <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(<span class="pl-c1">f</span>(x, θ_opt<span class="pl-k">.</span>σ²), y)
fx <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f_post</span>(x))</pre></div>
<h2><a id="user-content-bib-info" class="anchor" aria-hidden="true" href="#bib-info"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bib Info</h2>
<p>Please refer to the CITATION.bib file.</p>
</article></div>