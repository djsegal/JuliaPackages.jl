<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gpars" class="anchor" aria-hidden="true" href="#gpars"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GPARs</h1>
<p dir="auto"><a href="https://github.com/willtebbutt/GPARs.jl/actions"><img src="https://github.com/willtebbutt/GPARs.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/willtebbutt/GPARs.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c619939bfbb5e81550cc1d52dae4472f3751ae44491cda37f2bb9f7c3b88b128/68747470733a2f2f636f6465636f762e696f2f67682f77696c6c746562627574742f47504152732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/willtebbutt/GPARs.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://github.com/invenia/BlueStyle"><img src="https://camo.githubusercontent.com/c18fbaa52d94d16b90b19701fc90d289b8a5bb920c74c79bab200b14e75420a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c75652d3434393564312e737667" alt="Code Style: Blue" data-canonical-src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" style="max-width: 100%;"></a>
<a href="https://github.com/SciML/ColPrac"><img src="https://camo.githubusercontent.com/a6c1efcb19a957860ecb25966a730260b03d6e05380d0c27992ee7f9e3b1feb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6c507261632d436f6e7472696275746f72277325323047756964652d626c756576696f6c6574" alt="ColPrac: Contributor's Guide on Collaborative Practices for Community Packages" data-canonical-src="https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet" style="max-width: 100%;"></a></p>
<p dir="auto">GPARs.jl is a rudimentary implementation of the Gaussian Process Autoregressive Regressor (GPAR), introduced in <a href="http://proceedings.mlr.press/v89/requeima19a.html" rel="nofollow">our AISTATS paper</a>. See CITATION.bib for an appropriate bibtex citation.</p>
<p dir="auto">We also maintain a <a href="https://github.com/wesselb/gpar">Python version of this package</a> -- this is much more fully featured, and we recommend that you use this implementation if you require the full collection of techniques introduced in that paper.</p>
<h2 dir="auto"><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basic Usage</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using AbstractGPs
using GPARs
using Random

# Build a GPAR from a collection of GPs. For more info on how to specify particular
# kernels and their parameters, please see [AbstractGPs.jl](https://github.com/willtebbutt/AbstractGPs.jl) or
# [Stheno.jl](https://github.com/willtebbutt/Stheno.jl)
# You should think of this as a vector-valued regressor.
f = GPAR([GP(SEKernel()) for _ in 1:3])

# Specify inputs. `ColVecs` says &quot;interpret this matrix as a vector of column-vecrors&quot;.
# Inputs are 2 dimensional, and there are 10 of them. This means that the pth GP in f
# will receive (2 + (p-1))-dimensional inputs, of which the first 2 dimensions comprise
# x, and the remaining the outputs of the first p-1 GPs in f.
x = ColVecs(randn(2, 10))

# Specify noise variance for each output.
Σs = rand(3) .+ 0.1

# Generate samples from the regressor at inputs `x` under observation noise `Σs`.
# You'll see that these are `ColVecs` of length `N`, each element of which is a length 3
# vector.
y = rand(MersenneTwister(123456), f(x, Σs))
y.X # this produces the matrix underlying the observations.

# Compute the log marginal likelihood of the observations under the model.
logpdf(f(x, Σs), y)

# Generate a new GPAR that is conditioned on these observations. This is just another
# GPAR object (in the simplest case, GPARs are closed under conditioning).
f_post = posterior(f(x, Σs), y)

# Since `f_post` is just another GPAR, we can use it to generate posterior samples
# and to compute log posterior predictive probabilities in the same way as the prior.
x_post = ColVecs(randn(2, 15))
rng = MersenneTwister(123456)
y_post = rand(rng, f_post(x, Σs))
logpdf(f_post(x, Σs), y_post)"><pre><span class="pl-k">using</span> AbstractGPs
<span class="pl-k">using</span> GPARs
<span class="pl-k">using</span> Random

<span class="pl-c"><span class="pl-c">#</span> Build a GPAR from a collection of GPs. For more info on how to specify particular</span>
<span class="pl-c"><span class="pl-c">#</span> kernels and their parameters, please see [AbstractGPs.jl](https://github.com/willtebbutt/AbstractGPs.jl) or</span>
<span class="pl-c"><span class="pl-c">#</span> [Stheno.jl](https://github.com/willtebbutt/Stheno.jl)</span>
<span class="pl-c"><span class="pl-c">#</span> You should think of this as a vector-valued regressor.</span>
f <span class="pl-k">=</span> <span class="pl-c1">GPAR</span>([<span class="pl-c1">GP</span>(<span class="pl-c1">SEKernel</span>()) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>])

<span class="pl-c"><span class="pl-c">#</span> Specify inputs. `ColVecs` says "interpret this matrix as a vector of column-vecrors".</span>
<span class="pl-c"><span class="pl-c">#</span> Inputs are 2 dimensional, and there are 10 of them. This means that the pth GP in f</span>
<span class="pl-c"><span class="pl-c">#</span> will receive (2 + (p-1))-dimensional inputs, of which the first 2 dimensions comprise</span>
<span class="pl-c"><span class="pl-c">#</span> x, and the remaining the outputs of the first p-1 GPs in f.</span>
x <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">2</span>, <span class="pl-c1">10</span>))

<span class="pl-c"><span class="pl-c">#</span> Specify noise variance for each output.</span>
Σs <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>) <span class="pl-k">.+</span> <span class="pl-c1">0.1</span>

<span class="pl-c"><span class="pl-c">#</span> Generate samples from the regressor at inputs `x` under observation noise `Σs`.</span>
<span class="pl-c"><span class="pl-c">#</span> You'll see that these are `ColVecs` of length `N`, each element of which is a length 3</span>
<span class="pl-c"><span class="pl-c">#</span> vector.</span>
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>), <span class="pl-c1">f</span>(x, Σs))
y<span class="pl-k">.</span>X <span class="pl-c"><span class="pl-c">#</span> this produces the matrix underlying the observations.</span>

<span class="pl-c"><span class="pl-c">#</span> Compute the log marginal likelihood of the observations under the model.</span>
<span class="pl-c1">logpdf</span>(<span class="pl-c1">f</span>(x, Σs), y)

<span class="pl-c"><span class="pl-c">#</span> Generate a new GPAR that is conditioned on these observations. This is just another</span>
<span class="pl-c"><span class="pl-c">#</span> GPAR object (in the simplest case, GPARs are closed under conditioning).</span>
f_post <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(<span class="pl-c1">f</span>(x, Σs), y)

<span class="pl-c"><span class="pl-c">#</span> Since `f_post` is just another GPAR, we can use it to generate posterior samples</span>
<span class="pl-c"><span class="pl-c">#</span> and to compute log posterior predictive probabilities in the same way as the prior.</span>
x_post <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">2</span>, <span class="pl-c1">15</span>))
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>)
y_post <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, <span class="pl-c1">f_post</span>(x, Σs))
<span class="pl-c1">logpdf</span>(<span class="pl-c1">f_post</span>(x, Σs), y_post)</pre></div>
<p dir="auto">Using this functionality, you have everything you need to do learning using standard
off-the-shelf functionality (<a href="https://github.com/FluxML/Zygote.jl/">Zygote.jl</a> to get gradients, <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> to get optimisers such as (L-)BFGS, and <a href="https://github.com/invenia/ParameterHandling.jl/">ParameterHandling.jl</a> to make dealing with large numbers of model parameters more straightforward.
See the examples in <a href="https://github.com/willtebbutt/Stheno.jl">Stheno.jl</a>'s docs for inspiration.</p>
</article></div>