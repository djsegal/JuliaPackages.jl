<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-jitrench" class="anchor" aria-hidden="true" href="#jitrench"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>JITrench</h1>
<h1 align="center" dir="auto"><a id="user-content---" class="anchor" aria-hidden="true" href="#--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/222e47fa987211600bb18cdfbf8df65a5bd2ead46d263c47e3caf289c7fb0bd5/68747470733a2f2f63646e2e646973636f72646170702e636f6d2f6174746163686d656e74732f3831303437383333313739303439313638312f3835353736383135333931333432353933302f756e6b6e6f776e2e706e67"><img src="https://camo.githubusercontent.com/222e47fa987211600bb18cdfbf8df65a5bd2ead46d263c47e3caf289c7fb0bd5/68747470733a2f2f63646e2e646973636f72646170702e636f6d2f6174746163686d656e74732f3831303437383333313739303439313638312f3835353736383135333931333432353933302f756e6b6e6f776e2e706e67" width="450" data-canonical-src="https://cdn.discordapp.com/attachments/810478331790491681/855768153913425930/unknown.png" style="max-width: 100%;"></a><br>
</h1>
<p align="center" dir="auto">Let's dive into the deep trenches of the loss function <br>with JITrench.jl.</p>
<p dir="auto"><a href="https://abap34.github.io/JITrench.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://abap34.github.io/JITrench.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://travis-ci.com/abap34/JITrench.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/54b98c445c2fb7c7efdc70352f523b85f585a29dcdbdc411832075c6b06fee44/68747470733a2f2f7472617669732d63692e636f6d2f6162617033342f4a495472656e63682e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/abap34/JITrench.jl.svg?branch=master" style="max-width: 100%;"></a></p>
<hr>
<h2 dir="auto"><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install</h2>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="]add https://github.com/abap34/JITrench.jl"><pre class="notranslate"><code>]add https://github.com/abap34/JITrench.jl
</code></pre></div>
<h2 dir="auto"><a id="user-content-automatic-gradient-calculation" class="anchor" aria-hidden="true" href="#automatic-gradient-calculation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Automatic gradient calculation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using JITrench

julia&gt; f(x) = sin(x) + 1
f (generic function with 1 method)

julia&gt; JITrench.@diff! f(x)
f′ (generic function with 1 method)

julia&gt; f′(π)
-1.0"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> JITrench

julia<span class="pl-k">&gt;</span> <span class="pl-en">f</span>(x) <span class="pl-k">=</span> <span class="pl-c1">sin</span>(x) <span class="pl-k">+</span> <span class="pl-c1">1</span>
f (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> JITrench<span class="pl-k">.</span><span class="pl-c1">@diff!</span> <span class="pl-c1">f</span>(x)
f′ (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">f′</span>(π)
<span class="pl-k">-</span><span class="pl-c1">1.0</span></pre></div>
<h2 dir="auto"><a id="user-content-compute-gradients-for-deep-functions-visualize-computational-graphs" class="anchor" aria-hidden="true" href="#compute-gradients-for-deep-functions-visualize-computational-graphs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Compute gradients for "Deep" functions, visualize computational graphs</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; x = Scalar(2.5)
Scalar{Float64}(2.5) 


julia&gt; y = Scalar(3.5)
Scalar{Float64}(3.5) 

julia&gt; goldstain(x, y) = (1 + (x + y + 1)^2 * (19 - 14x + 3x^2 - 14y + 6x*y + 3y^2)) *  (30 + (2x - 3y)^2 * (18 - 32x + 12x^2 + 48y - 36x*y + 27*y^2))
goldstain (generic function with 1 method)

julia&gt; z = goldstain(x, y)
Scalar{Float64}(1.260939725e7) 

julia&gt; backward!(z)

julia&gt; x.grad
-5.324409e6

julia&gt; y.grad
3.3109701e7

julia&gt; JITrench.plot_graph(z, to_file=&quot;example/visualize/goldstain.png&quot;)"><pre>julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">Scalar</span>(<span class="pl-c1">2.5</span>)
<span class="pl-c1">Scalar</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">2.5</span>) 


julia<span class="pl-k">&gt;</span> y <span class="pl-k">=</span> <span class="pl-c1">Scalar</span>(<span class="pl-c1">3.5</span>)
<span class="pl-c1">Scalar</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">3.5</span>) 

julia<span class="pl-k">&gt;</span> <span class="pl-en">goldstain</span>(x, y) <span class="pl-k">=</span> (<span class="pl-c1">1</span> <span class="pl-k">+</span> (x <span class="pl-k">+</span> y <span class="pl-k">+</span> <span class="pl-c1">1</span>)<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> (<span class="pl-c1">19</span> <span class="pl-k">-</span> <span class="pl-c1">14</span>x <span class="pl-k">+</span> <span class="pl-c1">3</span>x<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">-</span> <span class="pl-c1">14</span>y <span class="pl-k">+</span> <span class="pl-c1">6</span>x<span class="pl-k">*</span>y <span class="pl-k">+</span> <span class="pl-c1">3</span>y<span class="pl-k">^</span><span class="pl-c1">2</span>)) <span class="pl-k">*</span>  (<span class="pl-c1">30</span> <span class="pl-k">+</span> (<span class="pl-c1">2</span>x <span class="pl-k">-</span> <span class="pl-c1">3</span>y)<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> (<span class="pl-c1">18</span> <span class="pl-k">-</span> <span class="pl-c1">32</span>x <span class="pl-k">+</span> <span class="pl-c1">12</span>x<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">+</span> <span class="pl-c1">48</span>y <span class="pl-k">-</span> <span class="pl-c1">36</span>x<span class="pl-k">*</span>y <span class="pl-k">+</span> <span class="pl-c1">27</span><span class="pl-k">*</span>y<span class="pl-k">^</span><span class="pl-c1">2</span>))
goldstain (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> z <span class="pl-k">=</span> <span class="pl-c1">goldstain</span>(x, y)
<span class="pl-c1">Scalar</span><span class="pl-c1">{Float64}</span>(<span class="pl-c1">1.260939725e7</span>) 

julia<span class="pl-k">&gt;</span> <span class="pl-c1">backward!</span>(z)

julia<span class="pl-k">&gt;</span> x<span class="pl-k">.</span>grad
<span class="pl-k">-</span><span class="pl-c1">5.324409e6</span>

julia<span class="pl-k">&gt;</span> y<span class="pl-k">.</span>grad
<span class="pl-c1">3.3109701e7</span>

julia<span class="pl-k">&gt;</span> JITrench<span class="pl-k">.</span><span class="pl-c1">plot_graph</span>(z, to_file<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>example/visualize/goldstain.png<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="example/visualize/goldstain.png"><img src="example/visualize/goldstain.png" alt="" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-compute-gradients-for-operations-on-multidimensional-arrays" class="anchor" aria-hidden="true" href="#compute-gradients-for-operations-on-multidimensional-arrays"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Compute gradients for operations on multidimensional arrays</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; A = AutoDiff.Tensor([1 2; 3 4; 5 6])
3×2 Tensor{Matrix{Int64}}: 
 1  2
 3  4
 5  6 
 

julia&gt; B = reshape(A, (2, 3))
2×3 Tensor{Matrix{Int64}}: 
 1  5  4
 3  2  6 
 

julia&gt; C = B[1, :]
3×1 Tensor{Vector{Int64}}: 
 1
 5
 4 
 

julia&gt; y = sum(C)
Scalar{Int64}(10) 


julia&gt; backward!(y)

julia&gt; A.grad
3×2 Matrix{Float64}:
 1.0  0.0
 0.0  1.0
 1.0  0.0"><pre>julia<span class="pl-k">&gt;</span> A <span class="pl-k">=</span> AutoDiff<span class="pl-k">.</span><span class="pl-c1">Tensor</span>([<span class="pl-c1">1</span> <span class="pl-c1">2</span>; <span class="pl-c1">3</span> <span class="pl-c1">4</span>; <span class="pl-c1">5</span> <span class="pl-c1">6</span>])
<span class="pl-c1">3</span><span class="pl-k">×</span><span class="pl-c1">2</span> Tensor{Matrix{Int64}}<span class="pl-k">:</span> 
 <span class="pl-c1">1</span>  <span class="pl-c1">2</span>
 <span class="pl-c1">3</span>  <span class="pl-c1">4</span>
 <span class="pl-c1">5</span>  <span class="pl-c1">6</span> 
 

julia<span class="pl-k">&gt;</span> B <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(A, (<span class="pl-c1">2</span>, <span class="pl-c1">3</span>))
<span class="pl-c1">2</span><span class="pl-k">×</span><span class="pl-c1">3</span> Tensor{Matrix{Int64}}<span class="pl-k">:</span> 
 <span class="pl-c1">1</span>  <span class="pl-c1">5</span>  <span class="pl-c1">4</span>
 <span class="pl-c1">3</span>  <span class="pl-c1">2</span>  <span class="pl-c1">6</span> 
 

julia<span class="pl-k">&gt;</span> C <span class="pl-k">=</span> B[<span class="pl-c1">1</span>, :]
<span class="pl-c1">3</span><span class="pl-k">×</span><span class="pl-c1">1</span> Tensor{Vector{Int64}}<span class="pl-k">:</span> 
 <span class="pl-c1">1</span>
 <span class="pl-c1">5</span>
 <span class="pl-c1">4</span> 
 

julia<span class="pl-k">&gt;</span> y <span class="pl-k">=</span> <span class="pl-c1">sum</span>(C)
<span class="pl-c1">Scalar</span><span class="pl-c1">{Int64}</span>(<span class="pl-c1">10</span>) 


julia<span class="pl-k">&gt;</span> <span class="pl-c1">backward!</span>(y)

julia<span class="pl-k">&gt;</span> A<span class="pl-k">.</span>grad
<span class="pl-c1">3</span><span class="pl-k">×</span><span class="pl-c1">2</span> Matrix{Float64}<span class="pl-k">:</span>
 <span class="pl-c1">1.0</span>  <span class="pl-c1">0.0</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">1.0</span>
 <span class="pl-c1">1.0</span>  <span class="pl-c1">0.0</span></pre></div>
<h1 dir="auto"><a id="user-content-gpu-support-cutensor" class="anchor" aria-hidden="true" href="#gpu-support-cutensor"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GPU Support: CuTensor</h1>
<p dir="auto">With the <code>CuTensor</code> type, you can perform calculations on the GPU as you would with <code>Tensor</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using JITrench

julia&gt; using BenchmarkTools

julia&gt; x = Tensor(rand(512, 512));

julia&gt; W = Tensor(rand(512, 512));

julia&gt; x_gpu = CuTensor(rand(512, 512));

julia&gt; W_gpu = CuTensor(rand(512, 512));

julia&gt; @benchmark x * W
BenchmarkTools.Trial: 7490 samples with 1 evaluation.
 Range (min … max):  616.548 μs …  1.238 ms  ┊ GC (min … max): 0.00% … 47.29%
 Time  (median):     649.301 μs              ┊ GC (median):    0.00%
 Time  (mean ± σ):   665.530 μs ± 90.051 μs  ┊ GC (mean ± σ):  2.36% ±  7.64%

    ▆█▆▂                                                  ▁    ▁
  ▄▇████▇▆▃▃▁▁▁▄▃▁▁▁▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆██▇▇ █
  617 μs        Histogram: log(frequency) by time      1.19 ms &lt;

 Memory estimate: 2.00 MiB, allocs estimate: 5.

julia&gt; @benchmark x_gpu * W_gpu
BenchmarkTools.Trial: 10000 samples with 3 evaluations.
 Range (min … max):   8.317 μs …  12.454 ms  ┊ GC (min … max): 0.00% … 10.13%
 Time  (median):     38.716 μs               ┊ GC (median):    0.00%
 Time  (mean ± σ):   38.481 μs ± 124.332 μs  ┊ GC (mean ± σ):  0.33% ±  0.10%

  ▄                                            ▃▁▂    ▁  ▄█▆▃▄ ▂
  █▅▄▅▁▁▃▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄███▅▄▁▄█▇▆█████ █
  8.32 μs       Histogram: log(frequency) by time      40.6 μs &lt;

 Memory estimate: 704 bytes, allocs estimate: 31."><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> JITrench

julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BenchmarkTools

julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">Tensor</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">512</span>, <span class="pl-c1">512</span>));

julia<span class="pl-k">&gt;</span> W <span class="pl-k">=</span> <span class="pl-c1">Tensor</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">512</span>, <span class="pl-c1">512</span>));

julia<span class="pl-k">&gt;</span> x_gpu <span class="pl-k">=</span> <span class="pl-c1">CuTensor</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">512</span>, <span class="pl-c1">512</span>));

julia<span class="pl-k">&gt;</span> W_gpu <span class="pl-k">=</span> <span class="pl-c1">CuTensor</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">512</span>, <span class="pl-c1">512</span>));

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> x <span class="pl-k">*</span> W
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span> <span class="pl-c1">7490</span> samples with <span class="pl-c1">1</span> evaluation.
 Range (min … max)<span class="pl-k">:</span>  <span class="pl-c1">616.548</span> μs …  <span class="pl-c1">1.238</span> ms  ┊ GC (min … max)<span class="pl-k">:</span> <span class="pl-c1">0.00</span><span class="pl-k">%</span> … <span class="pl-c1">47.29</span><span class="pl-k">%</span>
 Time  (median)<span class="pl-k">:</span>     <span class="pl-c1">649.301</span> μs              ┊ GC (median)<span class="pl-k">:</span>    <span class="pl-c1">0.00</span><span class="pl-k">%</span>
 Time  (mean ± σ)<span class="pl-k">:</span>   <span class="pl-c1">665.530</span> μs ± <span class="pl-c1">90.051</span> μs  ┊ GC (mean ± σ)<span class="pl-k">:</span>  <span class="pl-c1">2.36</span><span class="pl-k">%</span> ±  <span class="pl-c1">7.64</span><span class="pl-k">%</span>

    ▆█▆▂                                                  ▁    ▁
  ▄▇████▇▆▃▃▁▁▁▄▃▁▁▁▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆██▇▇ █
  <span class="pl-c1">617</span> μs        Histogram<span class="pl-k">:</span> <span class="pl-c1">log</span>(frequency) by time      <span class="pl-c1">1.19</span> ms <span class="pl-k">&lt;</span>

 Memory estimate<span class="pl-k">:</span> <span class="pl-c1">2.00</span> MiB, allocs estimate<span class="pl-k">:</span> <span class="pl-c1">5.</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> x_gpu <span class="pl-k">*</span> W_gpu
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span> <span class="pl-c1">10000</span> samples with <span class="pl-c1">3</span> evaluations.
 Range (min … max)<span class="pl-k">:</span>   <span class="pl-c1">8.317</span> μs …  <span class="pl-c1">12.454</span> ms  ┊ GC (min … max)<span class="pl-k">:</span> <span class="pl-c1">0.00</span><span class="pl-k">%</span> … <span class="pl-c1">10.13</span><span class="pl-k">%</span>
 Time  (median)<span class="pl-k">:</span>     <span class="pl-c1">38.716</span> μs               ┊ GC (median)<span class="pl-k">:</span>    <span class="pl-c1">0.00</span><span class="pl-k">%</span>
 Time  (mean ± σ)<span class="pl-k">:</span>   <span class="pl-c1">38.481</span> μs ± <span class="pl-c1">124.332</span> μs  ┊ GC (mean ± σ)<span class="pl-k">:</span>  <span class="pl-c1">0.33</span><span class="pl-k">%</span> ±  <span class="pl-c1">0.10</span><span class="pl-k">%</span>

  ▄                                            ▃▁▂    ▁  ▄█▆▃▄ ▂
  █▅▄▅▁▁▃▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄███▅▄▁▄█▇▆█████ █
  <span class="pl-c1">8.32</span> μs       Histogram<span class="pl-k">:</span> <span class="pl-c1">log</span>(frequency) by time      <span class="pl-c1">40.6</span> μs <span class="pl-k">&lt;</span>

 Memory estimate<span class="pl-k">:</span> <span class="pl-c1">704</span> bytes, allocs estimate<span class="pl-k">:</span> <span class="pl-c1">31.</span></pre></div>
<h1 dir="auto"><a id="user-content-example-gradient-descent" class="anchor" aria-hidden="true" href="#example-gradient-descent"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example: Gradient Descent</h1>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="rosenbrock(x₀, x₁) = 100 * (x₁ - x₀^2)^2 + (x₀ - 1)^2

x₀ = Scalar(0.0)
x₁ = Scalar(5.0)
lr = 1e-3
iters = 10000

for i in 1:iters    
    y = rosenbrock(x₀, x₁)
    JITrench.AutoDiff.cleargrad!(x₀)
    JITrench.AutoDiff.cleargrad!(x₁)
    backward!(y)
    x₀.values -= lr * x₀.grad
    x₁.values -= lr * x₁.grad
end"><pre><span class="pl-en">rosenbrock</span>(x₀, x₁) <span class="pl-k">=</span> <span class="pl-c1">100</span> <span class="pl-k">*</span> (x₁ <span class="pl-k">-</span> x₀<span class="pl-k">^</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">+</span> (x₀ <span class="pl-k">-</span> <span class="pl-c1">1</span>)<span class="pl-k">^</span><span class="pl-c1">2</span>

x₀ <span class="pl-k">=</span> <span class="pl-c1">Scalar</span>(<span class="pl-c1">0.0</span>)
x₁ <span class="pl-k">=</span> <span class="pl-c1">Scalar</span>(<span class="pl-c1">5.0</span>)
lr <span class="pl-k">=</span> <span class="pl-c1">1e-3</span>
iters <span class="pl-k">=</span> <span class="pl-c1">10000</span>

<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>iters    
    y <span class="pl-k">=</span> <span class="pl-c1">rosenbrock</span>(x₀, x₁)
    JITrench<span class="pl-k">.</span>AutoDiff<span class="pl-k">.</span><span class="pl-c1">cleargrad!</span>(x₀)
    JITrench<span class="pl-k">.</span>AutoDiff<span class="pl-k">.</span><span class="pl-c1">cleargrad!</span>(x₁)
    <span class="pl-c1">backward!</span>(y)
    x₀<span class="pl-k">.</span>values <span class="pl-k">-=</span> lr <span class="pl-k">*</span> x₀<span class="pl-k">.</span>grad
    x₁<span class="pl-k">.</span>values <span class="pl-k">-=</span> lr <span class="pl-k">*</span> x₁<span class="pl-k">.</span>grad
<span class="pl-k">end</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="example/visualize/gradient_decent.gif"><img src="example/visualize/gradient_decent.gif" alt="" data-animated-image="" style="max-width: 100%;"></a></p>
<p dir="auto">See example/optimization/gradient_descent.jl for details.</p>
<h1 dir="auto"><a id="user-content-example-newton-method" class="anchor" aria-hidden="true" href="#example-newton-method"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example: Newton Method</h1>
<p dir="auto">JITrench.jl can also compute higher-order derivatives!</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using JITrench

f(x) = x^4 - 2x^2

x = Scalar(2.0)
iters = 10

for i in 1:iters    
    y = f(x)
    JITrench.AutoDiff.cleargrad!(x)
    backward!(y, create_graph=true)
    gx = x.grad
    JITrench.AutoDiff.cleargrad!(gx)
    backward!(gx)
    gx2 = x.grad
    x.values -= gx.values / gx2.values
end"><pre><span class="pl-k">using</span> JITrench

<span class="pl-en">f</span>(x) <span class="pl-k">=</span> x<span class="pl-k">^</span><span class="pl-c1">4</span> <span class="pl-k">-</span> <span class="pl-c1">2</span>x<span class="pl-k">^</span><span class="pl-c1">2</span>

x <span class="pl-k">=</span> <span class="pl-c1">Scalar</span>(<span class="pl-c1">2.0</span>)
iters <span class="pl-k">=</span> <span class="pl-c1">10</span>

<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>iters    
    y <span class="pl-k">=</span> <span class="pl-c1">f</span>(x)
    JITrench<span class="pl-k">.</span>AutoDiff<span class="pl-k">.</span><span class="pl-c1">cleargrad!</span>(x)
    <span class="pl-c1">backward!</span>(y, create_graph<span class="pl-k">=</span><span class="pl-c1">true</span>)
    gx <span class="pl-k">=</span> x<span class="pl-k">.</span>grad
    JITrench<span class="pl-k">.</span>AutoDiff<span class="pl-k">.</span><span class="pl-c1">cleargrad!</span>(gx)
    <span class="pl-c1">backward!</span>(gx)
    gx2 <span class="pl-k">=</span> x<span class="pl-k">.</span>grad
    x<span class="pl-k">.</span>values <span class="pl-k">-=</span> gx<span class="pl-k">.</span>values <span class="pl-k">/</span> gx2<span class="pl-k">.</span>values
<span class="pl-k">end</span></pre></div>
<p dir="auto">See example/optimization/newton_method.jl for details.</p>
<h1 dir="auto"><a id="user-content-example-train-neural-network" class="anchor" aria-hidden="true" href="#example-train-neural-network"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example: Train Neural Network</h1>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using JITrench
using JITrench.NN 
using Printf


N = 100
p = 1
n_iter = 20000

x = rand(N, p)
y = sin.(2π .* x) + (rand(N, p) / 1)

function model(x)
    x = NN.Linear(out_dim=10)(x)
    x = NN.functions.sigmoid.(x)
    x = NN.Linear(out_dim=1)(x)
    return NN.result(x)
end

params = NN.init(model, NN.Initializer((nothing, 1)))
optimizer = NN.SGD(params, 1e-1)

x = Tensor(x)
y = Tensor(y)

for iter in 1:n_iter
    pred = NN.apply(model, x, params)
    loss = NN.functions.mean_squared_error(y, pred)
    NN.cleargrads!(params)
    backward!(loss)
    NN.optimize!(optimizer)
    if (iter % 500 == 0)
        @printf &quot;[iters] %4i [loss] %.4f\n&quot; iter loss.values 
    end
end


NN.save_weight(params, &quot;weight&quot;)"><pre><span class="pl-k">using</span> JITrench
<span class="pl-k">using</span> JITrench<span class="pl-k">.</span>NN 
<span class="pl-k">using</span> Printf


N <span class="pl-k">=</span> <span class="pl-c1">100</span>
p <span class="pl-k">=</span> <span class="pl-c1">1</span>
n_iter <span class="pl-k">=</span> <span class="pl-c1">20000</span>

x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(N, p)
y <span class="pl-k">=</span> <span class="pl-c1">sin</span>.(<span class="pl-c1">2</span>π <span class="pl-k">.*</span> x) <span class="pl-k">+</span> (<span class="pl-c1">rand</span>(N, p) <span class="pl-k">/</span> <span class="pl-c1">1</span>)

<span class="pl-k">function</span> <span class="pl-en">model</span>(x)
    x <span class="pl-k">=</span> NN<span class="pl-k">.</span><span class="pl-c1">Linear</span>(out_dim<span class="pl-k">=</span><span class="pl-c1">10</span>)(x)
    x <span class="pl-k">=</span> NN<span class="pl-k">.</span>functions<span class="pl-k">.</span><span class="pl-c1">sigmoid</span>.(x)
    x <span class="pl-k">=</span> NN<span class="pl-k">.</span><span class="pl-c1">Linear</span>(out_dim<span class="pl-k">=</span><span class="pl-c1">1</span>)(x)
    <span class="pl-k">return</span> NN<span class="pl-k">.</span><span class="pl-c1">result</span>(x)
<span class="pl-k">end</span>

params <span class="pl-k">=</span> NN<span class="pl-k">.</span><span class="pl-c1">init</span>(model, NN<span class="pl-k">.</span><span class="pl-c1">Initializer</span>((<span class="pl-c1">nothing</span>, <span class="pl-c1">1</span>)))
optimizer <span class="pl-k">=</span> NN<span class="pl-k">.</span><span class="pl-c1">SGD</span>(params, <span class="pl-c1">1e-1</span>)

x <span class="pl-k">=</span> <span class="pl-c1">Tensor</span>(x)
y <span class="pl-k">=</span> <span class="pl-c1">Tensor</span>(y)

<span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n_iter
    pred <span class="pl-k">=</span> NN<span class="pl-k">.</span><span class="pl-c1">apply</span>(model, x, params)
    loss <span class="pl-k">=</span> NN<span class="pl-k">.</span>functions<span class="pl-k">.</span><span class="pl-c1">mean_squared_error</span>(y, pred)
    NN<span class="pl-k">.</span><span class="pl-c1">cleargrads!</span>(params)
    <span class="pl-c1">backward!</span>(loss)
    NN<span class="pl-k">.</span><span class="pl-c1">optimize!</span>(optimizer)
    <span class="pl-k">if</span> (iter <span class="pl-k">%</span> <span class="pl-c1">500</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>)
        <span class="pl-c1">@printf</span> <span class="pl-s"><span class="pl-pds">"</span>[iters] %4i [loss] %.4f<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> iter loss<span class="pl-k">.</span>values 
    <span class="pl-k">end</span>
<span class="pl-k">end</span>


NN<span class="pl-k">.</span><span class="pl-c1">save_weight</span>(params, <span class="pl-s"><span class="pl-pds">"</span>weight<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">This is a simple example of training a sine curve on an MLP and saving the weights as 'weights.jtw'</p>
<p dir="auto">Progress of the training:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="example/NN/learning.gif"><img src="example/NN/learning.gif" alt="" data-animated-image="" style="max-width: 100%;"></a></p>
<p dir="auto">The visualization code is available in 'example/NN/MLP.jl'.</p>
</article></div>