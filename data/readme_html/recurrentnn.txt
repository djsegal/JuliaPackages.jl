<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-recurrentnnjl" class="anchor" aria-hidden="true" href="#recurrentnnjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>RecurrentNN.jl</h1>
<p>RecurrentNN.jl is a Julia language package originally based on Andrej Karpathy's excellent <a href="http://cs.stanford.edu/people/karpathy/recurrentjs" rel="nofollow">RecurrentJS</a> library in javascript.
It implements:</p>
<ul>
<li>Deep <strong>Recurrent Neural Networks</strong> (RNN)</li>
<li><strong>Long Short-Term Memory networks</strong> (LSTM)</li>
<li><strong>Gated Recurrent Neural Networks</strong> (GRU)</li>
<li><strong>Gated Feedback Recurrent Neural Networks</strong> (GF-RNN)</li>
<li><strong>Gated Feedback Long Short-Term Memory networks</strong> (GF-LSTM)</li>
<li>In fact, the library is more general because it has functionality to construct arbitrary <strong>expression graphs</strong> over which the library can perform <strong>automatic differentiation</strong> similar to what you may find in Theano for Python, or in Torch etc. Currently, the code uses this very general functionality to implement RNN/LSTM/GRU, but one can build arbitrary Neural Networks and do automatic backprop.</li>
<li>For information an the <strong>Gated Feedback</strong> variants see <a href="http://arxiv.org/abs/1502.02367" rel="nofollow">Gated Feedback Recurrent Neural Networks</a></li>
</ul>
<h2><a id="user-content-online-demo-of-original-library-in-javascript" class="anchor" aria-hidden="true" href="#online-demo-of-original-library-in-javascript"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Online demo of original library in javascript</h2>
<p>An online demo that memorizes character sequences can be found below. Sentences are input data and the networks are trained to predict the next character in a sentence. Thus, they learn English from scratch character by character and eventually after some training generate entirely new sentences that sometimes make some sense :)</p>
<p><a href="http://cs.stanford.edu/people/karpathy/recurrentjs" rel="nofollow">Character Sequence Memorization Demo</a></p>
<p><em>The same demo as above implemented in Julia can be found in</em> <a href="https://github.com/Andy-P/RecurrentNN.jl/blob/master/example/example.jl">example/example.jl</a></p>
<h2><a id="user-content-example-code" class="anchor" aria-hidden="true" href="#example-code"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example code</h2>
<p>To construct and train an LSTM for example, you would proceed as follows:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> RecurrentNN

<span class="pl-c"><span class="pl-c">#</span> takes as input Mat of 10x1, contains 2 hidden layers of</span>
<span class="pl-c"><span class="pl-c">#</span> 20 neurons each, and outputs a Mat of size 2x1</span>
hiddensizes <span class="pl-k">=</span> [<span class="pl-c1">20</span>, <span class="pl-c1">20</span>]
outputsize <span class="pl-k">=</span> <span class="pl-c1">2</span>
cost <span class="pl-k">=</span> <span class="pl-c1">0.</span>
lstm <span class="pl-k">=</span> <span class="pl-c1">LSTM</span>(<span class="pl-c1">10</span>, hiddensizes, outputsize)
x1 <span class="pl-k">=</span> <span class="pl-c1">randNNMat</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> example input #1</span>
x2 <span class="pl-k">=</span> <span class="pl-c1">randNNMat</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> example input #2</span>
x3 <span class="pl-k">=</span> <span class="pl-c1">randNNMat</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> example input #3</span>

<span class="pl-c"><span class="pl-c">#</span> pass 3 examples through the LSTM</span>
G <span class="pl-k">=</span> <span class="pl-c1">Graph</span>()
<span class="pl-c"><span class="pl-c">#</span> build container to hold output after each time step</span>
prevhd   <span class="pl-k">=</span> <span class="pl-c1">Array</span>(NNMatrix,<span class="pl-c1">0</span>) <span class="pl-c"><span class="pl-c">#</span> holds final hidden layer of the recurrent model</span>
prevcell <span class="pl-k">=</span> <span class="pl-c1">Array</span>(NNMatrix,<span class="pl-c1">0</span>) <span class="pl-c"><span class="pl-c">#</span>  holds final cell output of the LSTM model</span>
out  <span class="pl-k">=</span> <span class="pl-c1">NNMatrix</span>(outputsize,<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> output of the recurrent model</span>
prev <span class="pl-k">=</span> (prevhd, prevcell, out)

out1 <span class="pl-k">=</span> <span class="pl-c1">forwardprop</span>(G, lstm, x1, prev)
out2 <span class="pl-k">=</span> <span class="pl-c1">forwardprop</span>(G, lstm, x2, out1);
out3 <span class="pl-k">=</span> <span class="pl-c1">forwardprop</span>(G, lstm, x3, out2);

<span class="pl-c"><span class="pl-c">#</span> the last part of the tuple contains the outputs:</span>
outMat <span class="pl-k">=</span>  prev[<span class="pl-c1">end</span>]

<span class="pl-c"><span class="pl-c">#</span> for example lets assume we have binary classification problem</span>
<span class="pl-c"><span class="pl-c">#</span> so the output of the LSTM are the log probabilities of the</span>
<span class="pl-c"><span class="pl-c">#</span> two classes. Lets first get the probabilities:</span>
probs <span class="pl-k">=</span> <span class="pl-c1">softmax</span>(outMat)
ix_target <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-c"><span class="pl-c">#</span> suppose first input has target class</span>

<span class="pl-c"><span class="pl-c">#</span> cross-entropy loss for softmax is simply the probabilities:</span>
outMat<span class="pl-k">.</span>dw <span class="pl-k">=</span> probs<span class="pl-k">.</span>w
<span class="pl-c"><span class="pl-c">#</span> but the correct class gets an extra -1:</span>
outMat<span class="pl-k">.</span>dw[ix_target] <span class="pl-k">-=</span> <span class="pl-c1">1</span>;

<span class="pl-c"><span class="pl-c">#</span> in real application you'd probably have a desired class</span>
<span class="pl-c"><span class="pl-c">#</span> for every input, so you'd iteratively se the .dw loss on each</span>
<span class="pl-c"><span class="pl-c">#</span> one. In the example provided demo we are, for example,</span>
<span class="pl-c"><span class="pl-c">#</span> predicting the index of the next letter in an input sentence.</span>

<span class="pl-c"><span class="pl-c">#</span> update the LSTM parameters</span>
<span class="pl-c1">backprop</span>(G)
s <span class="pl-k">=</span> <span class="pl-c1">Solver</span>() <span class="pl-c"><span class="pl-c">#</span> RMSProp optimizer</span>

<span class="pl-c"><span class="pl-c">#</span> perform RMSprop update with</span>
<span class="pl-c"><span class="pl-c">#</span> step size of 0.01</span>
<span class="pl-c"><span class="pl-c">#</span> L2 regularization of 0.00001</span>
<span class="pl-c"><span class="pl-c">#</span> and clipping the gradients at 5.0 elementwise</span>
<span class="pl-c1">step</span>(s, lstm, <span class="pl-c1">0.01</span>, <span class="pl-c1">0.00001</span>, <span class="pl-c1">5.0</span>);</pre></div>
<p>A much more detailed example can be found in the example folder.</p>
<p>##Credits
This library draws on the work of <a href="https://github.com/karpathy">Andrej Karpathy</a>. Speed enhancements were added by <a href="https://github.com/IainNZ">Iain Dunning</a>. The Gated Recurrent Neural Network implementation and Gated Feedback variants were added by <a href="https://github.com/paulheideman">Paul Heideman</a>.</p>
<h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2>
<p>MIT</p>
<h2><a id="user-content-note-this-library-is-no-longer-supported" class="anchor" aria-hidden="true" href="#note-this-library-is-no-longer-supported"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Note: This library is no longer supported!</h2>
<p>This library is no longer supported and has only been tested up to 3.x. I suggest using <a href="http://dmlc.ml/MXNet.jl/" rel="nofollow">MXNet.jl</a>.
There is a good <a href="http://dmlc.ml/MXNet.jl/latest/tutorial/char-lstm/" rel="nofollow">example of how to implement an LSTM is MXNet here.</a></p>
</article></div>