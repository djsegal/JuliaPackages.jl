<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-adversarialpredictionjl" class="anchor" aria-hidden="true" href="#adversarialpredictionjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>AdversarialPrediction.jl</h1>
<p dir="auto">This package provides a way to easily optimize generic performance metrics in supervised learning settings using the <a href="https://arxiv.org/abs/1812.07526" rel="nofollow">Adversarial Prediction</a> framework.
The method can be integrated easily into differentiable learning pipelines.
The package is a Julia implementation of an AISTATS 2020 paper, <a href="https://arxiv.org/abs/1912.00965" rel="nofollow">"AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning"</a>, by <a href="http://rizal.fathony.com" rel="nofollow">Rizal Fathony</a> and <a href="http://zicokolter.com" rel="nofollow">Zico Kolter</a>.
For a Python implementation of the framework, please check  <a href="https://github.com/rizalzaf/ap_perf">ap_perf</a>.</p>
<h2 dir="auto"><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<p dir="auto">AdversarialPrediction.jl enables easy integration of generic performance metrics (including non-decomposable metrics) into our differentiable learning pipeline. It currently supports performance metrics that are defined over binary classification problems.
Below is a code example for incorporating the F-2 score metric into a convolutional neural network training pipeline of <a href="https://github.com/FluxML/Flux.jl">FluxML</a>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Flux
using AdversarialPrediction
import AdversarialPrediction: define, constraint

model = Chain(
  Conv((5, 5), 1=&gt;20, relu), MaxPool((2,2)),
  Conv((5, 5), 20=&gt;50, relu), MaxPool((2,2)),
  x -&gt; reshape(x, :, size(x, 4)),
  Dense(4*4*50, 500), Dense(500, 1), vec
)      

@metric FBeta beta
function define(::Type{FBeta}, C::ConfusionMatrix, beta)
    return ((1 + beta^2) * C.tp) / (beta^2 * C.ap + C.pp)  
end   
f2_score = FBeta(2)
special_case_positive!(f2_score)

objective(x, y) = ap_objective(model(x), y, f2_score)
Flux.train!(objective, params(model), train_set, ADAM(1e-3))"><pre><span class="pl-k">using</span> Flux
<span class="pl-k">using</span> AdversarialPrediction
<span class="pl-k">import</span> AdversarialPrediction<span class="pl-k">:</span> define, constraint

model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
  <span class="pl-c1">Conv</span>((<span class="pl-c1">5</span>, <span class="pl-c1">5</span>), <span class="pl-c1">1</span><span class="pl-k">=&gt;</span><span class="pl-c1">20</span>, relu), <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)),
  <span class="pl-c1">Conv</span>((<span class="pl-c1">5</span>, <span class="pl-c1">5</span>), <span class="pl-c1">20</span><span class="pl-k">=&gt;</span><span class="pl-c1">50</span>, relu), <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)),
  x <span class="pl-k">-&gt;</span> <span class="pl-c1">reshape</span>(x, :, <span class="pl-c1">size</span>(x, <span class="pl-c1">4</span>)),
  <span class="pl-c1">Dense</span>(<span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">50</span>, <span class="pl-c1">500</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">500</span>, <span class="pl-c1">1</span>), vec
)      

<span class="pl-c1">@metric</span> FBeta beta
<span class="pl-k">function</span> <span class="pl-en">define</span>(<span class="pl-k">::</span><span class="pl-c1">Type{FBeta}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>, beta)
    <span class="pl-k">return</span> ((<span class="pl-c1">1</span> <span class="pl-k">+</span> beta<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">*</span> C<span class="pl-k">.</span>tp) <span class="pl-k">/</span> (beta<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> C<span class="pl-k">.</span>ap <span class="pl-k">+</span> C<span class="pl-k">.</span>pp)  
<span class="pl-k">end</span>   
f2_score <span class="pl-k">=</span> <span class="pl-c1">FBeta</span>(<span class="pl-c1">2</span>)
<span class="pl-c1">special_case_positive!</span>(f2_score)

<span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">ap_objective</span>(<span class="pl-c1">model</span>(x), y, f2_score)
Flux<span class="pl-k">.</span><span class="pl-c1">train!</span>(objective, <span class="pl-c1">params</span>(model), train_set, <span class="pl-c1">ADAM</span>(<span class="pl-c1">1e-3</span>))</pre></div>
<p dir="auto">As we can see from the code above, we can just write a function that calculates the F-2 score from the entities in the confusion matrix, and incorporate it into our learning pipeline using <code>ap_objective</code> function.
This is a straightforward modification from the standard cross entropy training by using the <code>ap_objective</code> function to replace the <code>logitbinarycrossentropy</code> objective.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Flux

model = Chain(
  Conv((5, 5), 1=&gt;20, relu), MaxPool((2,2)),
  Conv((5, 5), 20=&gt;50, relu), MaxPool((2,2)),
  x -&gt; reshape(x, :, size(x, 4)),
  Dense(4*4*50, 500), Dense(500, 1), vec
)     

objective(x, y) = mean(logitbinarycrossentropy(model(x), y))
Flux.train!(objective, params(model), train_set, ADAM(1e-3))"><pre><span class="pl-k">using</span> Flux

model <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(
  <span class="pl-c1">Conv</span>((<span class="pl-c1">5</span>, <span class="pl-c1">5</span>), <span class="pl-c1">1</span><span class="pl-k">=&gt;</span><span class="pl-c1">20</span>, relu), <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)),
  <span class="pl-c1">Conv</span>((<span class="pl-c1">5</span>, <span class="pl-c1">5</span>), <span class="pl-c1">20</span><span class="pl-k">=&gt;</span><span class="pl-c1">50</span>, relu), <span class="pl-c1">MaxPool</span>((<span class="pl-c1">2</span>,<span class="pl-c1">2</span>)),
  x <span class="pl-k">-&gt;</span> <span class="pl-c1">reshape</span>(x, :, <span class="pl-c1">size</span>(x, <span class="pl-c1">4</span>)),
  <span class="pl-c1">Dense</span>(<span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">4</span><span class="pl-k">*</span><span class="pl-c1">50</span>, <span class="pl-c1">500</span>), <span class="pl-c1">Dense</span>(<span class="pl-c1">500</span>, <span class="pl-c1">1</span>), vec
)     

<span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">mean</span>(<span class="pl-c1">logitbinarycrossentropy</span>(<span class="pl-c1">model</span>(x), y))
Flux<span class="pl-k">.</span><span class="pl-c1">train!</span>(objective, <span class="pl-c1">params</span>(model), train_set, <span class="pl-c1">ADAM</span>(<span class="pl-c1">1e-3</span>))</pre></div>
<p dir="auto">Note that the equation for F-beta in general is:</p>
<div dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/fbeta.gif"><img src="assets/fbeta.gif" data-animated-image="" style="max-width: 100%;"></a></div>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">AdversarialPrediction.jl can be installed from a Julia terminal:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="]add AdversarialPrediction"><pre class="notranslate"><code>]add AdversarialPrediction
</code></pre></div>
<p dir="auto">Some pre-requisite packages will be installed automatically: <code>Zygote</code> (Flux's automatic differential engine), <code>Requires</code>, and <code>LBFGSB</code>. Please also install <code>Flux</code> separately. For GPU training, <code>CuArrays</code> package needs to be installed.</p>
<h2 dir="auto"><a id="user-content-performance-metrics" class="anchor" aria-hidden="true" href="#performance-metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance Metrics</h2>
<p dir="auto">Different tasks in machine learning  require different metrics that align  well with the tasks. For binary classification problems, many of the commonly used performance metrics are derived from the confusion matrix.
A confusion matrix is a table that reports the values that relate the prediction of a classifier with the ground truth labels. The table below shows the anatomy of a confusion matrix.</p>
<div dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/confusion_matrix.png"><img src="assets/confusion_matrix.png" width="470" style="max-width: 100%;"></a></div>
<p dir="auto">Some of the metrics are decomposable, which means that it can be broken down to an independent sum of another metric that depends only on a single sample. However, most of the interesting performance metrics are non-decomposable, where we need to consider all samples at once. There are a wide variety of non-decomposable performance metrics, for example:</p>
<div dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/metrics.png"><img src="assets/metrics.png" width="500" style="max-width: 100%;"></a></div>
<p dir="auto">AdversarialPrediction.jl supports a family of performance metrics that can be expressed as a sum of fractions:</p>
<div dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/metric_construction.gif"><img src="assets/metric_construction.gif" data-animated-image="" style="max-width: 100%;"></a></div>
<p dir="auto">where a<sub>j</sub> and b<sub>j</sub> are constants, whereas f<sub>j</sub> and g<sub>j</sub> are functions over PP and AP.
Hence, the numerator is a linear function over true positive (TP) and true negative (TN) which may also depends on sum statistics, i.e., predicted and actual positive (PP and AP) as well as their negative counterparts (predicted and actual negative (PN and AN)) and all data (ALL). Note that PN, AN, and ALL can be derived form PP and AP since ALL is just a constant, PN = ALL - PP, and AN = ALL - AP.
The denominator depends only on the sum statistics (PP, AP, PN, AN, and ALL). This construction of performance metrics covers a vast range of commonly used metrics, including all metrics in the table above.</p>
<h2 dir="auto"><a id="user-content-defining-performance-metrics" class="anchor" aria-hidden="true" href="#defining-performance-metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Defining Performance Metrics</h2>
<p dir="auto">A performance metric can be defined in AdversarialPrediction.jl using the macro <code>@metric MetricName</code>.  We also need to write the definition of the metric by implementing a function that depends on the type of the metric and the confusion matrix: <code>define(::Type{MetricName}, C::ConfusionMatrix)</code>. Below is an example of the F-1 score metric definition.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@metric F1Score
function define(::Type{F1Score}, C::ConfusionMatrix)
    return (2 * C.tp) / (C.ap + C.pp)  
end "><pre><span class="pl-c1">@metric</span> F1Score
<span class="pl-k">function</span> <span class="pl-en">define</span>(<span class="pl-k">::</span><span class="pl-c1">Type{F1Score}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>)
    <span class="pl-k">return</span> (<span class="pl-c1">2</span> <span class="pl-k">*</span> C<span class="pl-k">.</span>tp) <span class="pl-k">/</span> (C<span class="pl-k">.</span>ap <span class="pl-k">+</span> C<span class="pl-k">.</span>pp)  
<span class="pl-k">end</span> </pre></div>
<p dir="auto">Some performance metrics (e.g., precision, recall, F-score, sensitivity, and specificity) enforce special cases to avoid division by zero. For the metrics that contain true positive, the special case is usually defined when the prediction or the true label for every sample are all zero. In this case, the metric is usually defined as 1 if both the prediction and the true label are all zero; otherwise, the metric is 0. For the metrics that contain true negative, similar cases occur, but with the prediction or the true label for every sample are all one.
Therefore, when instantiating a metric, we need to take into account these special cases, for example, in the case of F-1 score:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="f1_score = F1Score()
special_case_positive!(f1_score)"><pre>f1_score <span class="pl-k">=</span> <span class="pl-c1">F1Score</span>()
<span class="pl-c1">special_case_positive!</span>(f1_score)</pre></div>
<p dir="auto">For some performance metrics, we may want to define a parametric metric. For example, the F-beta score, which depends on the value of beta. In this case, we can write a macro with arguments, for example, <code>@metric MetricName arg1 arg2</code>. We also need to adjust the function definition to: <code>define(::Type{MetricName}, C::ConfusionMatrix, arg1, arg2)</code>. For the case of the F-beta score metric, the code is:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@metric FBeta beta
function define(::Type{FBeta}, C::ConfusionMatrix, beta)
    return ((1 + beta^2) * C.tp) / (beta^2 * C.ap + C.pp)  
end   

f1_score = FBeta(1)
special_case_positive!(f1_score)

f2_score = FBeta(2)
special_case_positive!(f2_score)"><pre><span class="pl-c1">@metric</span> FBeta beta
<span class="pl-k">function</span> <span class="pl-en">define</span>(<span class="pl-k">::</span><span class="pl-c1">Type{FBeta}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>, beta)
    <span class="pl-k">return</span> ((<span class="pl-c1">1</span> <span class="pl-k">+</span> beta<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">*</span> C<span class="pl-k">.</span>tp) <span class="pl-k">/</span> (beta<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">*</span> C<span class="pl-k">.</span>ap <span class="pl-k">+</span> C<span class="pl-k">.</span>pp)  
<span class="pl-k">end</span>   

f1_score <span class="pl-k">=</span> <span class="pl-c1">FBeta</span>(<span class="pl-c1">1</span>)
<span class="pl-c1">special_case_positive!</span>(f1_score)

f2_score <span class="pl-k">=</span> <span class="pl-c1">FBeta</span>(<span class="pl-c1">2</span>)
<span class="pl-c1">special_case_positive!</span>(f2_score)</pre></div>
<p dir="auto">We can define arbitrary complex performance metrics inside the <code>define</code> function, so long as it follows the construction of metrics that the package support. We can also use intermediate variables to store partial expression of the metric. Below is a code example for Cohen's kappa score.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@metric Kappa
function define(::Type{Kappa}, C::ConfusionMatrix)
    pe = (C.ap * C.pp + C.an * C.pn) / C.all^2
    num = (C.tp + C.tn) / C.all - pe
    den = 1 - pe
    return num / den
end  

kappa = Kappa()
special_case_positive!(kappa)
special_case_negative!(kappa)"><pre><span class="pl-c1">@metric</span> Kappa
<span class="pl-k">function</span> <span class="pl-en">define</span>(<span class="pl-k">::</span><span class="pl-c1">Type{Kappa}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>)
    pe <span class="pl-k">=</span> (C<span class="pl-k">.</span>ap <span class="pl-k">*</span> C<span class="pl-k">.</span>pp <span class="pl-k">+</span> C<span class="pl-k">.</span>an <span class="pl-k">*</span> C<span class="pl-k">.</span>pn) <span class="pl-k">/</span> C<span class="pl-k">.</span>all<span class="pl-k">^</span><span class="pl-c1">2</span>
    num <span class="pl-k">=</span> (C<span class="pl-k">.</span>tp <span class="pl-k">+</span> C<span class="pl-k">.</span>tn) <span class="pl-k">/</span> C<span class="pl-k">.</span>all <span class="pl-k">-</span> pe
    den <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-k">-</span> pe
    <span class="pl-k">return</span> num <span class="pl-k">/</span> den
<span class="pl-k">end</span>  

kappa <span class="pl-k">=</span> <span class="pl-c1">Kappa</span>()
<span class="pl-c1">special_case_positive!</span>(kappa)
<span class="pl-c1">special_case_negative!</span>(kappa)</pre></div>
<h2 dir="auto"><a id="user-content-performance-metric-with-constraints" class="anchor" aria-hidden="true" href="#performance-metric-with-constraints"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance Metric with Constraints</h2>
<p dir="auto">In some machine learning settings, we may want to optimize a performance metric subject to constraints on other metrics. This occurs in the case where there are trade-offs between different performance metrics. For example, a machine learning system may want to optimize the precision of the prediction; subject to its recall is greater than some threshold. We can define the constraints in the metric by implementing the function: <code>constraint(::Type{MetricName}, C::ConfusionMatrix)</code>. The code format for the constraints is <code>metric &gt;= th</code>, where <code>th</code> is a real-valued threshold. Below is an example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Precision given recall metric
@metric PrecisionGvRecall th
function define(::Type{PrecisionGvRecall}, C::ConfusionMatrix, th)
    return C.tp / C.pp
end   
function constraint(::Type{PrecisionGvRecall}, C::ConfusionMatrix, th)
    return C.tp / C.ap &gt;= th
end   

precision_gv_recall_80 = PrecisionGvRecall(0.8)
special_case_positive!(precision_gv_recall_80)
cs_special_case_positive!(precision_gv_recall_80, true)

precision_gv_recall_60 = PrecisionGvRecall(0.6)
special_case_positive!(precision_gv_recall_60)
cs_special_case_positive!(precision_gv_recall_60, true)"><pre><span class="pl-c"><span class="pl-c">#</span> Precision given recall metric</span>
<span class="pl-c1">@metric</span> PrecisionGvRecall th
<span class="pl-k">function</span> <span class="pl-en">define</span>(<span class="pl-k">::</span><span class="pl-c1">Type{PrecisionGvRecall}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>, th)
    <span class="pl-k">return</span> C<span class="pl-k">.</span>tp <span class="pl-k">/</span> C<span class="pl-k">.</span>pp
<span class="pl-k">end</span>   
<span class="pl-k">function</span> <span class="pl-en">constraint</span>(<span class="pl-k">::</span><span class="pl-c1">Type{PrecisionGvRecall}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>, th)
    <span class="pl-k">return</span> C<span class="pl-k">.</span>tp <span class="pl-k">/</span> C<span class="pl-k">.</span>ap <span class="pl-k">&gt;=</span> th
<span class="pl-k">end</span>   

precision_gv_recall_80 <span class="pl-k">=</span> <span class="pl-c1">PrecisionGvRecall</span>(<span class="pl-c1">0.8</span>)
<span class="pl-c1">special_case_positive!</span>(precision_gv_recall_80)
<span class="pl-c1">cs_special_case_positive!</span>(precision_gv_recall_80, <span class="pl-c1">true</span>)

precision_gv_recall_60 <span class="pl-k">=</span> <span class="pl-c1">PrecisionGvRecall</span>(<span class="pl-c1">0.6</span>)
<span class="pl-c1">special_case_positive!</span>(precision_gv_recall_60)
<span class="pl-c1">cs_special_case_positive!</span>(precision_gv_recall_60, <span class="pl-c1">true</span>)</pre></div>
<p dir="auto">Note that the function <code>special_case_positive!</code> enforces special cases for the precision metric, whereas <code>cs_special_case_positive!</code> enforces special cases for the metric in the constraint (recall metric).</p>
<p dir="auto">We can also have two or more metrics in the constraints, for example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Precision given recall &gt;= th1 and specificity &gt;= th2
@metric PrecisionGvRecallSpecificity th1 th2        
function define(::Type{PrecisionGvRecallSpecificity}, C::ConfusionMatrix, th1, th2)
    return C.tp / C.pp
end   
function constraint(::Type{PrecisionGvRecallSpecificity}, C::ConfusionMatrix, th1, th2)
    return [C.tp / C.ap &gt;= th1,
            C.tn / C.an &gt;= th2]
end   

precision_gv_recall_spec = PrecisionGvRecallSpecificity(0.8, 0.8)
special_case_positive!(precision_gv_recall_spec)
cs_special_case_positive!(precision_gv_recall_spec, [true, false])
cs_special_case_negative!(precision_gv_recall_spec, [false, true])"><pre><span class="pl-c"><span class="pl-c">#</span> Precision given recall &gt;= th1 and specificity &gt;= th2</span>
<span class="pl-c1">@metric</span> PrecisionGvRecallSpecificity th1 th2        
<span class="pl-k">function</span> <span class="pl-en">define</span>(<span class="pl-k">::</span><span class="pl-c1">Type{PrecisionGvRecallSpecificity}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>, th1, th2)
    <span class="pl-k">return</span> C<span class="pl-k">.</span>tp <span class="pl-k">/</span> C<span class="pl-k">.</span>pp
<span class="pl-k">end</span>   
<span class="pl-k">function</span> <span class="pl-en">constraint</span>(<span class="pl-k">::</span><span class="pl-c1">Type{PrecisionGvRecallSpecificity}</span>, C<span class="pl-k">::</span><span class="pl-c1">ConfusionMatrix</span>, th1, th2)
    <span class="pl-k">return</span> [C<span class="pl-k">.</span>tp <span class="pl-k">/</span> C<span class="pl-k">.</span>ap <span class="pl-k">&gt;=</span> th1,
            C<span class="pl-k">.</span>tn <span class="pl-k">/</span> C<span class="pl-k">.</span>an <span class="pl-k">&gt;=</span> th2]
<span class="pl-k">end</span>   

precision_gv_recall_spec <span class="pl-k">=</span> <span class="pl-c1">PrecisionGvRecallSpecificity</span>(<span class="pl-c1">0.8</span>, <span class="pl-c1">0.8</span>)
<span class="pl-c1">special_case_positive!</span>(precision_gv_recall_spec)
<span class="pl-c1">cs_special_case_positive!</span>(precision_gv_recall_spec, [<span class="pl-c1">true</span>, <span class="pl-c1">false</span>])
<span class="pl-c1">cs_special_case_negative!</span>(precision_gv_recall_spec, [<span class="pl-c1">false</span>, <span class="pl-c1">true</span>])</pre></div>
<p dir="auto">Here, we need to provide an array of boolean for the function <code>cs_special_case_positive!</code> and <code>cs_special_case_negative!</code>.</p>
<h2 dir="auto"><a id="user-content-computing-the-values-of-the-metric" class="anchor" aria-hidden="true" href="#computing-the-values-of-the-metric"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Computing the Values of the Metric</h2>
<p dir="auto">Given we have a prediction for each sample <code>yhat</code> and the true label <code>y</code>, we can call the function <code>compute_metric</code> to compute the value of the metric. Both <code>yhat</code> and <code>y</code> are vectors containing 0 or 1.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; compute_metric(f1_score, yhat, y)
0.8f0"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">compute_metric</span>(f1_score, yhat, y)
<span class="pl-c1">0.8f0</span></pre></div>
<p dir="auto">For a metric with constraints, we can call the function <code>compute_constraints</code> to compute the value of every  metric in the constraints. For example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; compute_constraints(precision_gv_recall_spec, yhat, y)
2-element Array{Float32,1}:
 0.6
 0.6"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">compute_constraints</span>(precision_gv_recall_spec, yhat, y)
<span class="pl-c1">2</span><span class="pl-k">-</span>element Array{Float32,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.6</span>
 <span class="pl-c1">0.6</span></pre></div>
<h2 dir="auto"><a id="user-content-incorporating-the-metric-into-differentiable-learning-pipeline" class="anchor" aria-hidden="true" href="#incorporating-the-metric-into-differentiable-learning-pipeline"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Incorporating the Metric into Differentiable Learning Pipeline</h2>
<p dir="auto">As we can see from the first code example, we can use the function <code>ap_objective</code> to incorporate the metrics we define into differentiable learning pipeline.
This function provides objective and gradient information from the adversarial prediction formulation, which then be propagated to the previous layers.
This serves as a replacement to the standard loss function like the binary cross-entropy loss, i.e.:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="objective(x, y) = mean(logitbinarycrossentropy(model(x), y))
Flux.train!(objective, params(model), train_set, ADAM(1e-3))"><pre><span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">mean</span>(<span class="pl-c1">logitbinarycrossentropy</span>(<span class="pl-c1">model</span>(x), y))
Flux<span class="pl-k">.</span><span class="pl-c1">train!</span>(objective, <span class="pl-c1">params</span>(model), train_set, <span class="pl-c1">ADAM</span>(<span class="pl-c1">1e-3</span>))</pre></div>
<p dir="auto">We can easily replace the existing codes that use binary cross-entropy by simply change the objective to <code>ap_objective</code>, i.e.:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="objective(x, y) = ap_objective(model(x), y, f1_score)
Flux.train!(objective, params(model), train_set, ADAM(1e-3))"><pre><span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">ap_objective</span>(<span class="pl-c1">model</span>(x), y, f1_score)
Flux<span class="pl-k">.</span><span class="pl-c1">train!</span>(objective, <span class="pl-c1">params</span>(model), train_set, <span class="pl-c1">ADAM</span>(<span class="pl-c1">1e-3</span>))</pre></div>
<h2 dir="auto"><a id="user-content-customizing-inner-optimization-solver" class="anchor" aria-hidden="true" href="#customizing-inner-optimization-solver"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Customizing Inner Optimization Solver</h2>
<p dir="auto">For solving the inner optimization problem, AdversarialPrediction.jl uses an ADMM based formulation. In the default setting, it will run 100 iterations of the ADMM optimization. We can also manually set the number of iteration using <code>max_iter</code> argument in the <code>ap_objective</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="objective(x, y) = ap_objective(model(x), y, f1_score)            # 100 iterations
objective(x, y) = ap_objective(model(x), y, max_iter = 50)       # 50 iterations
objective(x, y) = ap_objective(model(x), y, max_iter = 200)      # 200 iterations"><pre><span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">ap_objective</span>(<span class="pl-c1">model</span>(x), y, f1_score)            <span class="pl-c"><span class="pl-c">#</span> 100 iterations</span>
<span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">ap_objective</span>(<span class="pl-c1">model</span>(x), y, max_iter <span class="pl-k">=</span> <span class="pl-c1">50</span>)       <span class="pl-c"><span class="pl-c">#</span> 50 iterations</span>
<span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">ap_objective</span>(<span class="pl-c1">model</span>(x), y, max_iter <span class="pl-k">=</span> <span class="pl-c1">200</span>)      <span class="pl-c"><span class="pl-c">#</span> 200 iterations</span></pre></div>
<h2 dir="auto"><a id="user-content-running-time-and-batch-size" class="anchor" aria-hidden="true" href="#running-time-and-batch-size"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Running Time and Batch Size</h2>
<p dir="auto">The adversarial prediction formulation inside the function <code>ap_objective</code> needs to solve a maximin problem with a quadratic size of variables using the ADMM solver. The complexity of solving the problem is O(m^3), where m is the number of samples in a minibatch.
In practice, for a batch size of 25, the ADMM solver takes around 20 - 30 milliseconds to solve on a PC with an Intel Core i7 processor. If we reduce the ADMM iterations to 50 iterations, it will take around 10 - 15 milliseconds.</p>
<h2 dir="auto"><a id="user-content-commonly-used-metrics" class="anchor" aria-hidden="true" href="#commonly-used-metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Commonly Used Metrics</h2>
<p dir="auto">The package provides definitions of commonly use metrics including: <code>f1_score</code>, <code>f2_score</code>, <code>gpr</code>, <code>mcc</code>, <code>kappa</code>, etc. To load the metrics to the current Julia environment, please use <code>AdversarialPrediction.CommonMetrics</code>. Please check the detailed definition of the metrics in <code>src/common_metrics/common_metrics.jl</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using AdversarialPrediction
using AdversarialPrediction.CommonMetrics: f1_score, kappa

objective(x, y) = ap_objective(model(x), y, kappa)"><pre><span class="pl-k">using</span> AdversarialPrediction
<span class="pl-k">using</span> AdversarialPrediction<span class="pl-k">.</span>CommonMetrics<span class="pl-k">:</span> f1_score, kappa

<span class="pl-en">objective</span>(x, y) <span class="pl-k">=</span> <span class="pl-c1">ap_objective</span>(<span class="pl-c1">model</span>(x), y, kappa)</pre></div>
<h2 dir="auto"><a id="user-content-code-examples" class="anchor" aria-hidden="true" href="#code-examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Code Examples</h2>
<p dir="auto">For working examples, please visit <a href="https://github.com/rizalzaf/AP-examples">AP-examples</a> repository. The project contains examples of using AdversarialPrediction.jl for classification with tabular datasets, as well as for image classification with MNIST and FashionMNIST datasets.</p>
<h2 dir="auto"><a id="user-content-python-implementation-and-interface" class="anchor" aria-hidden="true" href="#python-implementation-and-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Python Implementation and Interface</h2>
<p dir="auto">We also provides a Python implementation of the framework: <a href="https://github.com/rizalzaf/ap_perf">ap_perf</a>. This enables easy integration with Python codes as well as PyTorch deep learning framework.</p>
<p dir="auto">We also provide a python interface to AdversarialPrediction.jl via <a href="https://pyjulia.readthedocs.io/en/stable/" rel="nofollow">PyJulia</a> library in <a href="https://github.com/rizalzaf/ap_perf_py">ap_perf_py</a>.</p>
<h2 dir="auto"><a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<p dir="auto">Please cite the following paper if you use the AdversarialPrediction.jl for your research.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="@article{ap-perf,
  title={AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning},
  author={Fathony, Rizal and Kolter, Zico},
  journal={arXiv preprint arXiv:1912.00965},
  year={2019}
}"><pre class="notranslate"><code>@article{ap-perf,
  title={AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning},
  author={Fathony, Rizal and Kolter, Zico},
  journal={arXiv preprint arXiv:1912.00965},
  year={2019}
}
</code></pre></div>
<h2 dir="auto"><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">This project is supported by a grant from the <a href="https://www.bosch-ai.com/" rel="nofollow">Bosch Center for Artificial Intelligence</a>.</p>
<p dir="auto">This project is not possible without previous foundational research in Adversarial Prediction by <a href="https://www.cs.uic.edu/Ziebart" rel="nofollow">Prof. Brian Ziebart's</a> and <a href="https://www.cs.uic.edu/~zhangx/" rel="nofollow">Prof. Xinhua Zhang's</a> research groups at the <a href="https://www.cs.uic.edu" rel="nofollow">University of Illinois at Chicago</a>.</p>
</article></div>