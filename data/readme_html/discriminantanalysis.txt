<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-discriminant-analysis" class="anchor" aria-hidden="true" href="#discriminant-analysis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Discriminant Analysis</h1>
<p><a href="https://travis-ci.org/trthatcher/DiscriminantAnalysis.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/8748238707f3635c51fce8e165650a9ed085cebc6f2c91d7468c3e1d6580a063/68747470733a2f2f7472617669732d63692e6f72672f747274686174636865722f4469736372696d696e616e74416e616c797369732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/trthatcher/DiscriminantAnalysis.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/trthatcher/DiscriminantAnalysis.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/dc16940ef86f075e080a83a66deff0ca74037c305528ef6b351421e3f7ac35cf/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f747274686174636865722f4469736372696d696e616e74416e616c797369732e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/trthatcher/DiscriminantAnalysis.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a></p>
<p><strong>DiscriminantAnalysis.jl</strong> is a Julia package for multiple linear and quadratic
regularized discriminant analysis (LDA &amp; QDA respectively). LDA and QDA are
distribution-based classifiers with the underlying assumption that data follows
a multivariate normal distribution. LDA differs from QDA in the assumption about
the class variability; LDA assumes that all classes share the same within-class
covariance matrix whereas QDA relaxes that constraint and allows for distinct
within-class covariance matrices. This results in LDA being a linear classifier
and QDA being a quadratic classifier.</p>
<p>The package is currently a work in progress work in progress - see <a href="https://github.com/trthatcher/DiscriminantAnalysis.jl/issues/12">issue #12</a> for the package status.</p>
<h2><a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Getting Started</h2>
<p>A bare-bones implementation of LDA is currently available but is not exported. Calls to the solver must be prefixed with <code>DiscriminantAnalysis</code> after running <code>using DiscriminantAnalysis</code>. Below is a brief overview of the API:</p>
<ul>
<li><code>lda(X, y; kwargs...)</code>: construct a Linear Discriminant Analysis model.
<ul>
<li><code>X</code>: the matrix of predictors (design matrix). Data may be per-column or per-row; this is specified by the <code>dims</code> keyword argument.</li>
<li><code>y</code>: the vector of class indices. For c classes, the values must range from 1 to c.</li>
<li><code>dims=1</code>: the dimension along which observations are stored. Use 1 for row-per-observation and 2 for column-per-observation.</li>
<li><code>canonical=false</code>: compute the canonical coordinates if true. For c classes, the data is mapped to a c-1 dimensional space for prediction.</li>
<li><code>compute_covariance=false</code>: compute the full class covariance matrix if true. Data is whitened prior to compute discriminant values, so generally the covariance is not computed unless specified.</li>
<li><code>centroids=nothing</code>: matrix of pre-computed class centroids. This can be used if the class centroids are known a priori. Otherwise, the centroids are estimated from the data. The centroid matrix must have the same orientation as specified by the <code>dims</code> argument.</li>
<li><code>priors=nothing</code>: vector of pre-computed class prior probabilities. This can be used if the class prior probabilities are known a priori. Otherwise, the priors are estimated from the class frequencies.</li>
<li><code>gamma=nothing</code>: real value between 0 and 1. Gamma is a regularization parameter that is used to shrink the covariance matrix towards an identity matrix scaled by the average eigenvalue of the covariance matrix. A value of <code>0.2</code> retains 80% of the original covariance matrix.</li>
</ul>
</li>
<li><code>posteriors(LDA, Z)</code>: compute the class posterior probabilities on a new matrix of predictors <code>Z</code>. This matrix must have the same <code>dims</code> orientation as the original design matrix <code>X</code>.</li>
<li><code>classify(LDA, Z)</code>: compute the class label predictions on a new matrix of predictors <code>Z</code>. This matrix must have the same <code>dims</code> orientation as the original design matrix <code>X</code>.</li>
</ul>
<p>The script below demonstrates how to fit an LDA model to some synthetic data using the interface described above:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using DiscriminantAnalysis
using Random

const DA = DiscriminantAnalysis

# Generate two sets of 100 samples of a 5-dimensional random normal 
# variable offset by +1/-1
X = [randn(250,5) .- 1;
     randn(250,5) .+ 1];

# Generate class labels for the two samples
#   NOTE: classes must be indexed by integers from 1 to the number of 
#         classes (2 in this case)
y = repeat(1:2, inner=250);

# Construct the LDA model
model = DA.lda(X, y; dims=1, canonical=true, priors=[0.5; 0.5])

# Generate some new data
Z = rand(10,5) .- 0.5

# Get the posterior probabilities for new data
Z_prob = DA.posteriors(model, Z)

# Get the class predictions
Z_class = DA.classify(model, Z)
"><pre><span class="pl-k">using</span> DiscriminantAnalysis
<span class="pl-k">using</span> Random

<span class="pl-k">const</span> DA <span class="pl-k">=</span> DiscriminantAnalysis

<span class="pl-c"><span class="pl-c">#</span> Generate two sets of 100 samples of a 5-dimensional random normal </span>
<span class="pl-c"><span class="pl-c">#</span> variable offset by +1/-1</span>
X <span class="pl-k">=</span> [<span class="pl-c1">randn</span>(<span class="pl-c1">250</span>,<span class="pl-c1">5</span>) <span class="pl-k">.-</span> <span class="pl-c1">1</span>;
     <span class="pl-c1">randn</span>(<span class="pl-c1">250</span>,<span class="pl-c1">5</span>) <span class="pl-k">.+</span> <span class="pl-c1">1</span>];

<span class="pl-c"><span class="pl-c">#</span> Generate class labels for the two samples</span>
<span class="pl-c"><span class="pl-c">#</span>   NOTE: classes must be indexed by integers from 1 to the number of </span>
<span class="pl-c"><span class="pl-c">#</span>         classes (2 in this case)</span>
y <span class="pl-k">=</span> <span class="pl-c1">repeat</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span>, inner<span class="pl-k">=</span><span class="pl-c1">250</span>);

<span class="pl-c"><span class="pl-c">#</span> Construct the LDA model</span>
model <span class="pl-k">=</span> DA<span class="pl-k">.</span><span class="pl-c1">lda</span>(X, y; dims<span class="pl-k">=</span><span class="pl-c1">1</span>, canonical<span class="pl-k">=</span><span class="pl-c1">true</span>, priors<span class="pl-k">=</span>[<span class="pl-c1">0.5</span>; <span class="pl-c1">0.5</span>])

<span class="pl-c"><span class="pl-c">#</span> Generate some new data</span>
Z <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">10</span>,<span class="pl-c1">5</span>) <span class="pl-k">.-</span> <span class="pl-c1">0.5</span>

<span class="pl-c"><span class="pl-c">#</span> Get the posterior probabilities for new data</span>
Z_prob <span class="pl-k">=</span> DA<span class="pl-k">.</span><span class="pl-c1">posteriors</span>(model, Z)

<span class="pl-c"><span class="pl-c">#</span> Get the class predictions</span>
Z_class <span class="pl-k">=</span> DA<span class="pl-k">.</span><span class="pl-c1">classify</span>(model, Z)</pre></div>
</article></div>