<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aicenter/ConditionalDists.jl/workflows/Run%20tests/badge.svg"><img src="https://github.com/aicenter/ConditionalDists.jl/workflows/Run%20tests/badge.svg" alt="Run tests" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/aicenter/ConditionalDists.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/83eb91dd1fd8b7146e7e7c096c86a774ac9c2481de60b25b3bab38dc74b360f9/68747470733a2f2f636f6465636f762e696f2f67682f616963656e7465722f436f6e646974696f6e616c44697374732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/aicenter/ConditionalDists.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-conditionaldistsjl" class="anchor" aria-hidden="true" href="#conditionaldistsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ConditionalDists.jl</h1>
<p dir="auto">Conditional probability distributions powered by Flux.jl and DistributionsAD.jl.</p>
<p dir="auto">The conditional PDFs that are defined in this package can be used in
conjunction with Flux models to provide trainable mappings. As an example,
assume you want to learn the mapping from a conditional to an MvNormal.  The
mapping <code>m</code> takes a vector <code>x</code> and maps it to a mean <code>μ</code> and a variance <code>σ</code>,
which can be achieved by using a <code>ConditionalDists.SplitLayer</code> as the last
layer in the network.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; m = Chain(Dense(2,2,σ), SplitLayer(2, [3,4]))
julia&gt; m(rand(2))
(Float32[0.07946974, 0.13797458, 0.03939067], Float32[0.7006321, 0.37641272, 0.3586885, 0.82230335])"><pre>julia<span class="pl-k">&gt;</span> m <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>,σ), <span class="pl-c1">SplitLayer</span>(<span class="pl-c1">2</span>, [<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]))
julia<span class="pl-k">&gt;</span> <span class="pl-c1">m</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">2</span>))
(Float32[<span class="pl-c1">0.07946974</span>, <span class="pl-c1">0.13797458</span>, <span class="pl-c1">0.03939067</span>], Float32[<span class="pl-c1">0.7006321</span>, <span class="pl-c1">0.37641272</span>, <span class="pl-c1">0.3586885</span>, <span class="pl-c1">0.82230335</span>])</pre></div>
<p dir="auto">With the mapping <code>m</code> we can create a conditional distribution with trainable
mapping parameters:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using ConditionalDists, Distributions
julia&gt; using ConditionalDists: SplitLayer

julia&gt; xlength = 3
julia&gt; zlength = 2
julia&gt; batchsize = 10
julia&gt; m = SplitLayer(zlength, [xlength,xlength])
julia&gt; p = ConditionalMvNormal(m)

julia&gt; res = condition(p, rand(zlength))  # this also works for batches!
julia&gt; μ = mean(res)
julia&gt; σ2 = var(res)
julia&gt; @assert res isa DistributionsAD.TuringDiagMvNormal

julia&gt; x = rand(xlength, batchsize)
julia&gt; z = rand(zlength, batchsize)
julia&gt; logpdf(p,x,z)
julia&gt; rand(p, randn(zlength, 10))"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ConditionalDists, Distributions
julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ConditionalDists<span class="pl-k">:</span> SplitLayer

julia<span class="pl-k">&gt;</span> xlength <span class="pl-k">=</span> <span class="pl-c1">3</span>
julia<span class="pl-k">&gt;</span> zlength <span class="pl-k">=</span> <span class="pl-c1">2</span>
julia<span class="pl-k">&gt;</span> batchsize <span class="pl-k">=</span> <span class="pl-c1">10</span>
julia<span class="pl-k">&gt;</span> m <span class="pl-k">=</span> <span class="pl-c1">SplitLayer</span>(zlength, [xlength,xlength])
julia<span class="pl-k">&gt;</span> p <span class="pl-k">=</span> <span class="pl-c1">ConditionalMvNormal</span>(m)

julia<span class="pl-k">&gt;</span> res <span class="pl-k">=</span> <span class="pl-c1">condition</span>(p, <span class="pl-c1">rand</span>(zlength))  <span class="pl-c"><span class="pl-c">#</span> this also works for batches!</span>
julia<span class="pl-k">&gt;</span> μ <span class="pl-k">=</span> <span class="pl-c1">mean</span>(res)
julia<span class="pl-k">&gt;</span> σ2 <span class="pl-k">=</span> <span class="pl-c1">var</span>(res)
julia<span class="pl-k">&gt;</span> <span class="pl-c1">@assert</span> res <span class="pl-k">isa</span> DistributionsAD<span class="pl-k">.</span>TuringDiagMvNormal

julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(xlength, batchsize)
julia<span class="pl-k">&gt;</span> z <span class="pl-k">=</span> <span class="pl-c1">rand</span>(zlength, batchsize)
julia<span class="pl-k">&gt;</span> <span class="pl-c1">logpdf</span>(p,x,z)
julia<span class="pl-k">&gt;</span> <span class="pl-c1">rand</span>(p, <span class="pl-c1">randn</span>(zlength, <span class="pl-c1">10</span>))</pre></div>
<p dir="auto">The trainable parameters (of the <code>SplitLayer</code>) are accessible as usual through
<code>Flux.params</code>. For different variance configurations (i.e. fixed/unit variance,
etc) check the doc strings with <code>julia&gt;? ConditionalMvNormal</code>/<code>julia&gt;? SplitLayer</code>.</p>
<p dir="auto">The next few lines show how to optimize <code>p</code> to match a
given Gaussian by using the <code>kl_divergence</code> defined in
<a href="https://github.com/aicenter/IPMeasures.jl">IPMeasures.jl</a>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using IPMeasures

julia&gt; d = MvNormal(zeros(xlength), ones(xlength))
julia&gt; loss(x) = sum(kl_divergence(p, d, x))
julia&gt; opt = ADAM(0.1)
julia&gt; data = [(randn(zlength),) for i in 1:2000]
julia&gt; Flux.train!(loss, Flux.params(p), data, opt)"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> IPMeasures

julia<span class="pl-k">&gt;</span> d <span class="pl-k">=</span> <span class="pl-c1">MvNormal</span>(<span class="pl-c1">zeros</span>(xlength), <span class="pl-c1">ones</span>(xlength))
julia<span class="pl-k">&gt;</span> <span class="pl-en">loss</span>(x) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">kl_divergence</span>(p, d, x))
julia<span class="pl-k">&gt;</span> opt <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>(<span class="pl-c1">0.1</span>)
julia<span class="pl-k">&gt;</span> data <span class="pl-k">=</span> [(<span class="pl-c1">randn</span>(zlength),) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2000</span>]
julia<span class="pl-k">&gt;</span> Flux<span class="pl-k">.</span><span class="pl-c1">train!</span>(loss, Flux<span class="pl-k">.</span><span class="pl-c1">params</span>(p), data, opt)</pre></div>
<p dir="auto">The learnt mean and variance are close to a standard normal:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; @assert condition(p, randn(zlength)) isa DistributionsAD.TuringDiagMvNormal
julia&gt; mean(p,randn(zlength))
3-element Array{Float32,1}:
  0.003194877
  1.7912015f-32
 -1.6101733f-6

julia&gt; var(p,randn(zlength))
3-element Array{Float32,1}:
 1.000585
 1.0021493
 1.0000007"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">@assert</span> <span class="pl-c1">condition</span>(p, <span class="pl-c1">randn</span>(zlength)) <span class="pl-k">isa</span> DistributionsAD<span class="pl-k">.</span>TuringDiagMvNormal
julia<span class="pl-k">&gt;</span> <span class="pl-c1">mean</span>(p,<span class="pl-c1">randn</span>(zlength))
<span class="pl-c1">3</span><span class="pl-k">-</span>element Array{Float32,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
  <span class="pl-c1">0.003194877</span>
  <span class="pl-c1">1.7912015f-32</span>
 <span class="pl-k">-</span><span class="pl-c1">1.6101733f-6</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">var</span>(p,<span class="pl-c1">randn</span>(zlength))
<span class="pl-c1">3</span><span class="pl-k">-</span>element Array{Float32,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">1.000585</span>
 <span class="pl-c1">1.0021493</span>
 <span class="pl-c1">1.0000007</span></pre></div>
</article></div>