<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-neuroflow" class="anchor" aria-hidden="true" href="#neuroflow"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>NeuroFlow</h1>
<p dir="auto"><a href="https://github.com/Algebra-FUN/NeuroFlow.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/Algebra-FUN/%60NeuroFlow.jl%60/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status" style="max-width: 100%;"></a></p>
<p dir="auto"><code>NeuroFlow</code> is an experimental deep learning framework written in Julia.</p>
<p dir="auto">It implements <code>auto-gradient</code> functionality with an atomic level dynamic computational graph and provides api in <code>Pytorch</code> style.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Pkg
Pkg.add(&quot;NeuroFlow&quot;)"><pre><span class="pl-k">import</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>NeuroFlow<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto">We start with a simple linear example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using NeuroFlow
import Distributions: Uniform, Normal, mean
using Plots

# generate some fake data obeyed the linear model
N = 1000
x = rand(Uniform(-10, 10), N) |&gt; sort
ϵ = rand(Normal(0, 1), N)
# parameters setting
a, b = 2.5, 1.5
y = a .* x .+ b .+ ϵ

# declare parameters which needs to be optimize
â,b̂ = Param(1.), Param(1.)
# define the linear model with parameters
lm(x) = â * x + b̂
# use SGD optimizer
optimizer = SGD([â;b̂]; η=1e-2)

loss_records = []

# train for 100 epochs
for epoch in 1:100
    ŷ = lm.(x)
    loss = mean((y.-ŷ).^2)

    # this three steps are just like pytorch
    zero_grad!(optimizer)
    backward!(loss)
    step!(optimizer)

    push!(loss_records, loss.val)
    if epoch % 5 == 0
        println(&quot;epoch=$epoch,loss=$(loss.val)&quot;)
    end
end"><pre><span class="pl-k">using</span> NeuroFlow
<span class="pl-k">import</span> Distributions<span class="pl-k">:</span> Uniform, Normal, mean
<span class="pl-k">using</span> Plots

<span class="pl-c"><span class="pl-c">#</span> generate some fake data obeyed the linear model</span>
N <span class="pl-k">=</span> <span class="pl-c1">1000</span>
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">Uniform</span>(<span class="pl-k">-</span><span class="pl-c1">10</span>, <span class="pl-c1">10</span>), N) <span class="pl-k">|&gt;</span> sort
ϵ <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">Normal</span>(<span class="pl-c1">0</span>, <span class="pl-c1">1</span>), N)
<span class="pl-c"><span class="pl-c">#</span> parameters setting</span>
a, b <span class="pl-k">=</span> <span class="pl-c1">2.5</span>, <span class="pl-c1">1.5</span>
y <span class="pl-k">=</span> a <span class="pl-k">.*</span> x <span class="pl-k">.+</span> b <span class="pl-k">.+</span> ϵ

<span class="pl-c"><span class="pl-c">#</span> declare parameters which needs to be optimize</span>
â,b̂ <span class="pl-k">=</span> <span class="pl-c1">Param</span>(<span class="pl-c1">1.</span>), <span class="pl-c1">Param</span>(<span class="pl-c1">1.</span>)
<span class="pl-c"><span class="pl-c">#</span> define the linear model with parameters</span>
<span class="pl-en">lm</span>(x) <span class="pl-k">=</span> â <span class="pl-k">*</span> x <span class="pl-k">+</span> b̂
<span class="pl-c"><span class="pl-c">#</span> use SGD optimizer</span>
optimizer <span class="pl-k">=</span> <span class="pl-c1">SGD</span>([â;b̂]; η<span class="pl-k">=</span><span class="pl-c1">1e-2</span>)

loss_records <span class="pl-k">=</span> []

<span class="pl-c"><span class="pl-c">#</span> train for 100 epochs</span>
<span class="pl-k">for</span> epoch <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>
    ŷ <span class="pl-k">=</span> <span class="pl-c1">lm</span>.(x)
    loss <span class="pl-k">=</span> <span class="pl-c1">mean</span>((y<span class="pl-k">.-</span>ŷ)<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>)

    <span class="pl-c"><span class="pl-c">#</span> this three steps are just like pytorch</span>
    <span class="pl-c1">zero_grad!</span>(optimizer)
    <span class="pl-c1">backward!</span>(loss)
    <span class="pl-c1">step!</span>(optimizer)

    <span class="pl-c1">push!</span>(loss_records, loss<span class="pl-k">.</span>val)
    <span class="pl-k">if</span> epoch <span class="pl-k">%</span> <span class="pl-c1">5</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>
        <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>epoch=<span class="pl-v">$epoch</span>,loss=<span class="pl-v">$(loss<span class="pl-k">.</span>val)</span><span class="pl-pds">"</span></span>)
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<blockquote>
<p dir="auto">More detail about this example can be seen in <a href="examples/LinearRegression.jl">examples/LinearRegression.jl</a></p>
</blockquote>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">More examples can be found in <a href="examples/"><code>examples</code></a></p>
</article></div>