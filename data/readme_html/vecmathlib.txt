<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-vecmathlibjl" class="anchor" aria-hidden="true" href="#vecmathlibjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vecmathlib.jl</h1>
<p>Vectorizable elemental math functions for Julia</p>
<p>Vecmathlib provides efficient, accurate, tunable, and most importantly
vectorizable math functions such as sqrt, sin, or atan.</p>
<h1><a id="user-content-current-state" class="anchor" aria-hidden="true" href="#current-state"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Current State</h1>
<p>This repository contains currently only a proof-of-concept
implementation, with code for the <code>exp</code> and <code>log</code> functions and a
benchmarking harness. Most ideas are taken from
<a href="https://bitbucket.org/eschnett/vecmathlib" rel="nofollow">https://bitbucket.org/eschnett/vecmathlib</a>, which is a C++
implementation.</p>
<h1><a id="user-content-benchmark-results" class="anchor" aria-hidden="true" href="#benchmark-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmark results</h1>
<p><strong>TL;DR:</strong> The <code>exp</code> function in this library is about twice as
fast as Julia's standard implementation for SIMD-vectorized 64-bit
floating-point operations.</p>
<p>Below are benchmark results from a MacBook Pro 2.7 GHz Intel Core i7
(with AVX instructions). The benchmarks ran on a single core, which
thus likely ran at a higher frequency than 2.7 GHz. The system was
otherwise only lightly used.</p>
<p>Benchmarking paramters were <code>ni=1000</code>, i.e. 1000 iterations for an
inner SIMD-parallelized loop, and <code>nj=1000*1000</code>, i.e. 1e6 iterations
of this loop. These numbers ensure that all benchmarking data live in
the level 1 data cache. The benchmarking harness performs additional
operations to ensure that these iterations are not optimized away (see
the source code).</p>
<p>All times are in ns (nanoseconds, 1e-9 seconds, smaller is better),
per single amortized function call. That is, this benchmark does not
measure how fast a single call is -- it measures how fast it is to
make many calls in a tight for loop.</p>
<table>
<thead>
<tr>
<th align="left">Operation</th>
<th align="left">Float32 [ns]</th>
<th align="left">Float64 [ns]</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">no-op</td>
<td align="left">0.15</td>
<td align="left">0.27</td>
</tr>
<tr>
<td align="left">add</td>
<td align="left">0.17</td>
<td align="left">0.31</td>
</tr>
<tr>
<td align="left">mul</td>
<td align="left">0.17</td>
<td align="left">0.31</td>
</tr>
</tbody>
</table>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="          |              |
"><pre><code>          |              |
</code></pre></div>
<p><code>exp2</code>        | 6.83         | 7.93
<code>vexp2</code>       | 1.08         | 3.39
(yeppp <code>exp2</code> | ?            | *2.07)
|              |
<code>log2</code>        | 12.37        | 16.29
<code>vlog2</code>       | 6.42         | 10.90</p>
<p>"no-op" performs no operation and measures the overhead of the
benchmarking overhead. "add" and "mul" perform a floating-point
addition and multiplication, respectively. "exp2" is the standard
Julia <code>exp2</code> function, "vexp2" is the vectorizable implementation
provided by this library.</p>
<p>The "yeppp" value is an estimate for the performance of the Yeppp
library <a href="http://www.yeppp.info/" rel="nofollow">http://www.yeppp.info/</a> according to its documentation, which
lists 5.6 cycles per call for this CPU architecture
<a href="http://www.yeppp.info/home/yeppp-performance-numbers/" rel="nofollow">http://www.yeppp.info/home/yeppp-performance-numbers/</a>. The main
difference in implementation seems to be that Yeppp aggressively
unrolls the SIMD loop, something that Julia/LLVM doesn't do here. (Is
there an <code>@unroll</code> macro for Julia?)</p>
<p>As a sanity check, we can compare these numbers to the theoretical
peak performance of this CPU. With AVX instructions, it should execute
4 add and 4 multiply 64-bit operations per cycle, i.e. a single add or
multiply should take 0.09 us plus benchmarking overhead. This is
approximately what we see here. In fact, we measure even a higher
performance, likely because the benchmark payload can be executed in
parallel (superscalar) with the benchmarking harness. That is an
unavoidable measurement error, unless we were to add significantly
more complexity.</p>
<h1><a id="user-content-plans" class="anchor" aria-hidden="true" href="#plans"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Plans</h1>
<p>Things that could/should be done:</p>
<ul>
<li>Calculate coefficients with Julia</li>
<li>Offer (compile-time) options to choose accuracy, inf/nan/subnormal
handling, etc.</li>
</ul>
</article></div>