<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-glmnet" class="anchor" aria-hidden="true" href="#glmnet"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GLMNet</h1>
<p><a href="https://travis-ci.org/JuliaStats/GLMNet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9e0c61a3baeae194538789d7769e22b62180a281/68747470733a2f2f7472617669732d63692e6f72672f4a756c696153746174732f474c4d4e65742e6a6c2e7376673f6272616e63683d76302e302e34" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaStats/GLMNet.jl.svg?branch=v0.0.4" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/JuliaStats/GLMNet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f04499beb8e2c32b2e42b5cde2fb92e02a276c6a/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4a756c696153746174732f474c4d4e65742e6a6c2f62616467652e737667" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/JuliaStats/GLMNet.jl/badge.svg" style="max-width:100%;"></a></p>
<p><a href="http://www.jstatsoft.org/v33/i01/" rel="nofollow">glmnet</a> is an R package by Jerome Friedman, Trevor Hastie, Rob Tibshirani that fits entire Lasso or ElasticNet regularization paths for linear, logistic, multinomial, and Cox models using cyclic coordinate descent. This Julia package wraps the Fortran code from glmnet.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick start</h2>
<p>To fit a basic model:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> GLMNet

julia<span class="pl-k">&gt;</span> y <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>) <span class="pl-k">+</span> <span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">10</span>;

julia<span class="pl-k">&gt;</span> X <span class="pl-k">=</span> [<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span> (<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)<span class="pl-k">+</span><span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">5</span> (<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)<span class="pl-k">+</span><span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">10</span> (<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)<span class="pl-k">+</span><span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">20</span>];

julia<span class="pl-k">&gt;</span> path <span class="pl-k">=</span> <span class="pl-c1">glmnet</span>(X, y)
Least Squares GLMNet Solution Path (<span class="pl-c1">55</span> solutions <span class="pl-k">for</span> <span class="pl-c1">4</span> predictors <span class="pl-k">in</span> <span class="pl-c1">163</span> passes)<span class="pl-k">:</span>
<span class="pl-c1">55</span>x3 DataFrame<span class="pl-k">:</span>
         df  pct_dev        λ
[<span class="pl-c1">1</span>,]      <span class="pl-c1">0</span>      <span class="pl-c1">0.0</span>  <span class="pl-c1">27.1988</span>
[<span class="pl-c1">2</span>,]      <span class="pl-c1">1</span> <span class="pl-c1">0.154843</span>  <span class="pl-c1">24.7825</span>
[<span class="pl-c1">3</span>,]      <span class="pl-c1">1</span> <span class="pl-c1">0.283396</span>  <span class="pl-c1">22.5809</span>
  :
[<span class="pl-c1">53</span>,]     <span class="pl-c1">2</span> <span class="pl-c1">0.911956</span> <span class="pl-c1">0.215546</span>
[<span class="pl-c1">54</span>,]     <span class="pl-c1">2</span> <span class="pl-c1">0.911966</span> <span class="pl-c1">0.196397</span>
[<span class="pl-c1">55</span>,]     <span class="pl-c1">2</span> <span class="pl-c1">0.911974</span>  <span class="pl-c1">0.17895</span></pre></div>
<p><code>path</code> represents the Lasso or ElasticNet fits for varying values of λ. The value of the intercept for each λ value are in <code>path.a0</code>. The coefficients for each fit are stored in compressed form in <code>path.betas</code>.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> path<span class="pl-k">.</span>betas
<span class="pl-c1">4</span>x55 CompressedPredictorMatrix<span class="pl-k">:</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.083706</span>  <span class="pl-c1">0.159976</span>  <span class="pl-c1">0.22947</span>  …  <span class="pl-c1">0.929157</span>    <span class="pl-c1">0.929315</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>         <span class="pl-c1">0.00655753</span>  <span class="pl-c1">0.00700862</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>         <span class="pl-c1">0.0</span>         <span class="pl-c1">0.0</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>         <span class="pl-c1">0.0</span>         <span class="pl-c1">0.0</span></pre></div>
<p>This CompressedPredictorMatrix can be indexed as any other AbstractMatrix, or converted to a Matrix using <code>convert(Matrix, path.betas)</code>.</p>
<p>To predict the output for each model along the path for a given set of predictors, use <code>predict</code>:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">predict</span>(path, [<span class="pl-c1">22</span> <span class="pl-c1">22</span><span class="pl-k">+</span><span class="pl-c1">randn</span>()<span class="pl-k">*</span><span class="pl-c1">5</span> <span class="pl-c1">22</span><span class="pl-k">+</span><span class="pl-c1">randn</span>()<span class="pl-k">*</span><span class="pl-c1">10</span> <span class="pl-c1">22</span><span class="pl-k">+</span><span class="pl-c1">randn</span>()<span class="pl-k">*</span><span class="pl-c1">20</span>])
<span class="pl-c1">1</span>x55 Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-c1">51.7098</span>  <span class="pl-c1">49.3242</span>  <span class="pl-c1">47.1505</span>  <span class="pl-c1">45.1699</span>  …  <span class="pl-c1">25.1036</span>  <span class="pl-c1">25.0878</span>  <span class="pl-c1">25.0736</span></pre></div>
<p>To find the best value of λ by cross-validation, use <code>glmnetcv</code>:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> cv <span class="pl-k">=</span> <span class="pl-c1">glmnetcv</span>(X, y)
Least Squares GLMNet Cross Validation
<span class="pl-c1">55</span> models <span class="pl-k">for</span> <span class="pl-c1">4</span> predictors <span class="pl-k">in</span> <span class="pl-c1">10</span> folds
Best λ <span class="pl-c1">0.343</span> (mean loss <span class="pl-c1">76.946</span>, std <span class="pl-c1">12.546</span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">argmin</span>(cv<span class="pl-k">.</span>meanloss)
<span class="pl-c1">48</span>

julia<span class="pl-k">&gt;</span> cv<span class="pl-k">.</span>path<span class="pl-k">.</span>betas[:, <span class="pl-c1">48</span>]
<span class="pl-c1">4</span><span class="pl-k">-</span>element Array{Float64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.926911</span>
 <span class="pl-c1">0.00366805</span>
 <span class="pl-c1">0.0</span>
 <span class="pl-c1">0.0</span></pre></div>
<h2><a id="user-content-fitting-models" class="anchor" aria-hidden="true" href="#fitting-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fitting models</h2>
<p><code>glmnet</code> has two required parameters: the m x n predictor matrix <code>X</code> and the dependent variable <code>y</code>. It additionally accepts an optional third argument, <code>family</code>, which can be used to specify a generalized linear model. Currently, only <code>Normal()</code> (least squares, default), <code>Binomial()</code> (logistic), and <code>Poisson()</code> are supported, although the glmnet Fortran code also implements a Cox model. For logistic models, <code>y</code> is a m x 2 matrix, where the first column is the count of negative responses for each row in <code>X</code> and the second column is the count of positive responses. For all other models, <code>y</code> is a vector.</p>
<p><code>glmnet</code> also accepts many optional parameters, described below:</p>
<ul>
<li><code>weights</code>: A vector of weights for each sample of the same size as <code>y</code>.</li>
<li><code>alpha</code>: The tradeoff between lasso and ridge regression. This defaults to <code>1.0</code>, which specifies a lasso model.</li>
<li><code>penalty_factor</code>: A vector of length n of penalties for each predictor in <code>X</code>. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.</li>
<li><code>constraints</code>: An n x 2 matrix specifying lower bounds (first column) and upper bounds (second column) on each predictor. By default, this is <code>[-Inf Inf]</code> for each predictor in <code>X</code>.</li>
<li><code>dfmax</code>: The maximum number of predictors in the largest model.</li>
<li><code>pmax</code>: The maximum number of predictors in any model.</li>
<li><code>nlambda</code>: The number of values of λ along the path to consider.</li>
<li><code>lambda_min_ratio</code>: The smallest λ value to consider, as a ratio of the value of λ that gives the null model (i.e., the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to <code>0.0001</code>, otherwise <code>0.01</code>.</li>
<li><code>lambda</code>: The λ values to consider. By default, this is determined from <code>nlambda</code> and <code>lambda_min_ratio</code>.</li>
<li><code>tol</code>: Convergence criterion. Defaults to <code>1e-7</code>.</li>
<li><code>standardize</code>: Whether to standardize predictors so that they are in the same units. Defaults to <code>true</code>. Beta values are always presented on the original scale.</li>
<li><code>intercept</code>: Whether to fit an intercept term. The intercept is always unpenalized. Defaults to <code>true</code>.</li>
<li><code>maxit</code>: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.</li>
</ul>
<h2><a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>See also</h2>
<ul>
<li><a href="https://github.com/simonster/Lasso.jl">Lasso.jl</a>, a pure Julia implementation of the glmnet coordinate descent algorithm that often achieves better performance.</li>
<li><a href="https://github.com/simonster/LARS.jl">LARS.jl</a>, an implementation
of least angle regression for fitting entire linear (but not
generalized linear) Lasso and Elastic Net coordinate paths.</li>
</ul>
</article></div>