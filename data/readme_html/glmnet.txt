<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text">
<h1><a id="user-content-glmnet" class="anchor" aria-hidden="true" href="#glmnet"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GLMNet</h1>
<p><a href="https://travis-ci.org/JuliaStats/GLMNet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a9c08844ad8630def0fd7ae0dbcd968325354916c6c937f58965d09f9e444f71/68747470733a2f2f7472617669732d63692e6f72672f4a756c696153746174732f474c4d4e65742e6a6c2e7376673f6272616e63683d76302e302e34" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaStats/GLMNet.jl.svg?branch=v0.0.4" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/JuliaStats/GLMNet.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/2f02a89eafeb99bd0556cced867fbf5239da6af6f9f29c1914a10cb10ea9a097/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4a756c696153746174732f474c4d4e65742e6a6c2f62616467652e737667" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/JuliaStats/GLMNet.jl/badge.svg" style="max-width:100%;"></a></p>
<p><a href="http://www.jstatsoft.org/v33/i01/" rel="nofollow">glmnet</a> is an R package by Jerome Friedman, Trevor Hastie, Rob Tibshirani that fits entire Lasso or ElasticNet regularization paths for linear, logistic, multinomial, and Cox models using cyclic coordinate descent. This Julia package wraps the Fortran code from glmnet.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick start</h2>
<p>To fit a basic regression model:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using GLMNet

julia&gt; y = collect(1:100) + randn(100)*10;

julia&gt; X = [1:100 (1:100)+randn(100)*5 (1:100)+randn(100)*10 (1:100)+randn(100)*20];

julia&gt; path = glmnet(X, y)
Least Squares GLMNet Solution Path (86 solutions for 4 predictors in 930 passes):
──────────────────────────────
      df   pct_dev           λ
──────────────────────────────
 [1]   0  0.0       30.0573
 [2]   1  0.152922  27.3871
 [3]   1  0.279881  24.9541
 : 
[84]   4  0.90719    0.0133172
[85]   4  0.9072     0.0121342
[86]   4  0.907209   0.0110562
──────────────────────────────
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> GLMNet

julia<span class="pl-k">&gt;</span> y <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>) <span class="pl-k">+</span> <span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">10</span>;

julia<span class="pl-k">&gt;</span> X <span class="pl-k">=</span> [<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span> (<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)<span class="pl-k">+</span><span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">5</span> (<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)<span class="pl-k">+</span><span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">10</span> (<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100</span>)<span class="pl-k">+</span><span class="pl-c1">randn</span>(<span class="pl-c1">100</span>)<span class="pl-k">*</span><span class="pl-c1">20</span>];

julia<span class="pl-k">&gt;</span> path <span class="pl-k">=</span> <span class="pl-c1">glmnet</span>(X, y)
Least Squares GLMNet Solution Path (<span class="pl-c1">86</span> solutions <span class="pl-k">for</span> <span class="pl-c1">4</span> predictors <span class="pl-k">in</span> <span class="pl-c1">930</span> passes)<span class="pl-k">:</span>
──────────────────────────────
      df   pct_dev           λ
──────────────────────────────
 [<span class="pl-c1">1</span>]   <span class="pl-c1">0</span>  <span class="pl-c1">0.0</span>       <span class="pl-c1">30.0573</span>
 [<span class="pl-c1">2</span>]   <span class="pl-c1">1</span>  <span class="pl-c1">0.152922</span>  <span class="pl-c1">27.3871</span>
 [<span class="pl-c1">3</span>]   <span class="pl-c1">1</span>  <span class="pl-c1">0.279881</span>  <span class="pl-c1">24.9541</span>
 <span class="pl-k">:</span> 
[<span class="pl-c1">84</span>]   <span class="pl-c1">4</span>  <span class="pl-c1">0.90719</span>    <span class="pl-c1">0.0133172</span>
[<span class="pl-c1">85</span>]   <span class="pl-c1">4</span>  <span class="pl-c1">0.9072</span>     <span class="pl-c1">0.0121342</span>
[<span class="pl-c1">86</span>]   <span class="pl-c1">4</span>  <span class="pl-c1">0.907209</span>   <span class="pl-c1">0.0110562</span>
──────────────────────────────</pre></div>
<p><code>path</code> represents the Lasso or ElasticNet fits for varying values of λ. The value of the intercept for each λ value are in <code>path.a0</code>. The coefficients for each fit are stored in compressed form in <code>path.betas</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; path.betas
4×86 CompressedPredictorMatrix:
 0.0  0.0925032  0.176789  0.253587  0.323562  0.387321  0.445416  0.498349  0.546581  0.590527  0.63057  0.667055  0.700299  …   1.33905      1.34855     1.35822     1.36768     1.37563     1.3829      1.39005     1.39641     1.40204     1.40702     1.41195
 0.0  0.0        0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0      0.0       0.0          -0.165771    -0.17235    -0.178991   -0.185479   -0.190945   -0.195942   -0.200851   -0.20521    -0.209079   -0.212501   -0.215883
 0.0  0.0        0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0      0.0       0.0          -0.00968611  -0.0117121  -0.0135919  -0.0154413  -0.0169859  -0.0183965  -0.0197951  -0.0210362  -0.0221345  -0.0231023  -0.0240649
 0.0  0.0        0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0       0.0      0.0       0.0          -0.110093    -0.110505   -0.111078   -0.11163    -0.112102   -0.112533   -0.112951   -0.113324   -0.113656   -0.113953   -0.11424
"><pre>julia<span class="pl-k">&gt;</span> path<span class="pl-k">.</span>betas
<span class="pl-c1">4</span><span class="pl-k">×</span><span class="pl-c1">86</span> CompressedPredictorMatrix<span class="pl-k">:</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0925032</span>  <span class="pl-c1">0.176789</span>  <span class="pl-c1">0.253587</span>  <span class="pl-c1">0.323562</span>  <span class="pl-c1">0.387321</span>  <span class="pl-c1">0.445416</span>  <span class="pl-c1">0.498349</span>  <span class="pl-c1">0.546581</span>  <span class="pl-c1">0.590527</span>  <span class="pl-c1">0.63057</span>  <span class="pl-c1">0.667055</span>  <span class="pl-c1">0.700299</span>  …   <span class="pl-c1">1.33905</span>      <span class="pl-c1">1.34855</span>     <span class="pl-c1">1.35822</span>     <span class="pl-c1">1.36768</span>     <span class="pl-c1">1.37563</span>     <span class="pl-c1">1.3829</span>      <span class="pl-c1">1.39005</span>     <span class="pl-c1">1.39641</span>     <span class="pl-c1">1.40204</span>     <span class="pl-c1">1.40702</span>     <span class="pl-c1">1.41195</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0</span>        <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>      <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>          <span class="pl-k">-</span><span class="pl-c1">0.165771</span>    <span class="pl-k">-</span><span class="pl-c1">0.17235</span>    <span class="pl-k">-</span><span class="pl-c1">0.178991</span>   <span class="pl-k">-</span><span class="pl-c1">0.185479</span>   <span class="pl-k">-</span><span class="pl-c1">0.190945</span>   <span class="pl-k">-</span><span class="pl-c1">0.195942</span>   <span class="pl-k">-</span><span class="pl-c1">0.200851</span>   <span class="pl-k">-</span><span class="pl-c1">0.20521</span>    <span class="pl-k">-</span><span class="pl-c1">0.209079</span>   <span class="pl-k">-</span><span class="pl-c1">0.212501</span>   <span class="pl-k">-</span><span class="pl-c1">0.215883</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0</span>        <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>      <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>          <span class="pl-k">-</span><span class="pl-c1">0.00968611</span>  <span class="pl-k">-</span><span class="pl-c1">0.0117121</span>  <span class="pl-k">-</span><span class="pl-c1">0.0135919</span>  <span class="pl-k">-</span><span class="pl-c1">0.0154413</span>  <span class="pl-k">-</span><span class="pl-c1">0.0169859</span>  <span class="pl-k">-</span><span class="pl-c1">0.0183965</span>  <span class="pl-k">-</span><span class="pl-c1">0.0197951</span>  <span class="pl-k">-</span><span class="pl-c1">0.0210362</span>  <span class="pl-k">-</span><span class="pl-c1">0.0221345</span>  <span class="pl-k">-</span><span class="pl-c1">0.0231023</span>  <span class="pl-k">-</span><span class="pl-c1">0.0240649</span>
 <span class="pl-c1">0.0</span>  <span class="pl-c1">0.0</span>        <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>      <span class="pl-c1">0.0</span>       <span class="pl-c1">0.0</span>          <span class="pl-k">-</span><span class="pl-c1">0.110093</span>    <span class="pl-k">-</span><span class="pl-c1">0.110505</span>   <span class="pl-k">-</span><span class="pl-c1">0.111078</span>   <span class="pl-k">-</span><span class="pl-c1">0.11163</span>    <span class="pl-k">-</span><span class="pl-c1">0.112102</span>   <span class="pl-k">-</span><span class="pl-c1">0.112533</span>   <span class="pl-k">-</span><span class="pl-c1">0.112951</span>   <span class="pl-k">-</span><span class="pl-c1">0.113324</span>   <span class="pl-k">-</span><span class="pl-c1">0.113656</span>   <span class="pl-k">-</span><span class="pl-c1">0.113953</span>   <span class="pl-k">-</span><span class="pl-c1">0.11424</span></pre></div>
<p>This CompressedPredictorMatrix can be indexed as any other AbstractMatrix, or converted to a Matrix using <code>convert(Matrix, path.betas)</code>.</p>
<p>One can visualize the path by</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using Plots, LinearAlgebra, LaTeXStrings

julia&gt; betaNorm = [norm(x, 1) for x in eachslice(path.betas,dims=2)];

julia&gt; extraOptions = (xlabel=L&quot;\| \beta \|_1&quot;,ylabel=L&quot;\beta_i&quot;, legend=:topleft,legendtitle=&quot;Variable&quot;, labels=[1 2 3 4]);

julia&gt; plot(betaNorm, path.betas'; extraOptions...)
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Plots, LinearAlgebra, LaTeXStrings

julia<span class="pl-k">&gt;</span> betaNorm <span class="pl-k">=</span> [<span class="pl-c1">norm</span>(x, <span class="pl-c1">1</span>) <span class="pl-k">for</span> x <span class="pl-k">in</span> <span class="pl-c1">eachslice</span>(path<span class="pl-k">.</span>betas,dims<span class="pl-k">=</span><span class="pl-c1">2</span>)];

julia<span class="pl-k">&gt;</span> extraOptions <span class="pl-k">=</span> (xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds"><span class="pl-c1">L</span>"</span><span class="pl-cce">\|</span> <span class="pl-cce">\b</span>eta <span class="pl-cce">\|</span>_1<span class="pl-pds">"</span></span>,ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds"><span class="pl-c1">L</span>"</span><span class="pl-cce">\b</span>eta_i<span class="pl-pds">"</span></span>, legend<span class="pl-k">=</span><span class="pl-c1">:topleft</span>,legendtitle<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Variable<span class="pl-pds">"</span></span>, labels<span class="pl-k">=</span>[<span class="pl-c1">1</span> <span class="pl-c1">2</span> <span class="pl-c1">3</span> <span class="pl-c1">4</span>]);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">plot</span>(betaNorm, path<span class="pl-k">.</span>betas<span class="pl-k">'</span>; extraOptions<span class="pl-k">...</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="regressionLassoPath.svg"><img src="regressionLassoPath.svg" alt="regression-lasso-path" style="max-width:100%;"></a></p>
<p>To predict the output for each model along the path for a given set of predictors, use <code>predict</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; predict(path, [22 22+randn()*5 22+randn()*10 22+randn()*20])

1×86 Array{Float64,2}:
 50.3295  47.6932  45.291  43.1023  41.108  39.2909  37.6352  36.1265  34.7519  33.4995  32.3583  31.3184  30.371  29.5077  28.7211  28.0044  …  21.3966  21.3129  21.2472  21.1746  21.1191  21.0655  21.0127  20.9687  20.9284  20.8885  20.8531  20.8218  20.7942  20.7667
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">predict</span>(path, [<span class="pl-c1">22</span> <span class="pl-c1">22</span><span class="pl-k">+</span><span class="pl-c1">randn</span>()<span class="pl-k">*</span><span class="pl-c1">5</span> <span class="pl-c1">22</span><span class="pl-k">+</span><span class="pl-c1">randn</span>()<span class="pl-k">*</span><span class="pl-c1">10</span> <span class="pl-c1">22</span><span class="pl-k">+</span><span class="pl-c1">randn</span>()<span class="pl-k">*</span><span class="pl-c1">20</span>])

<span class="pl-c1">1</span><span class="pl-k">×</span><span class="pl-c1">86</span> Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-c1">50.3295</span>  <span class="pl-c1">47.6932</span>  <span class="pl-c1">45.291</span>  <span class="pl-c1">43.1023</span>  <span class="pl-c1">41.108</span>  <span class="pl-c1">39.2909</span>  <span class="pl-c1">37.6352</span>  <span class="pl-c1">36.1265</span>  <span class="pl-c1">34.7519</span>  <span class="pl-c1">33.4995</span>  <span class="pl-c1">32.3583</span>  <span class="pl-c1">31.3184</span>  <span class="pl-c1">30.371</span>  <span class="pl-c1">29.5077</span>  <span class="pl-c1">28.7211</span>  <span class="pl-c1">28.0044</span>  …  <span class="pl-c1">21.3966</span>  <span class="pl-c1">21.3129</span>  <span class="pl-c1">21.2472</span>  <span class="pl-c1">21.1746</span>  <span class="pl-c1">21.1191</span>  <span class="pl-c1">21.0655</span>  <span class="pl-c1">21.0127</span>  <span class="pl-c1">20.9687</span>  <span class="pl-c1">20.9284</span>  <span class="pl-c1">20.8885</span>  <span class="pl-c1">20.8531</span>  <span class="pl-c1">20.8218</span>  <span class="pl-c1">20.7942</span>  <span class="pl-c1">20.7667</span></pre></div>
<p>To find the best value of λ by cross-validation, use <code>glmnetcv</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; cv = glmnetcv(X, y) 
Least Squares GLMNet Cross Validation
86 models for 4 predictors in 10 folds
Best λ 0.136 (mean loss 101.530, std 10.940)

julia&gt; argmin(cv.meanloss)
59

julia&gt; coef(cv) # equivalent to cv.path.betas[:, 59]
4-element Array{Float64,1}:
  1.1277676556880305
  0.0
  0.0
 -0.08747434292954445
"><pre>julia<span class="pl-k">&gt;</span> cv <span class="pl-k">=</span> <span class="pl-c1">glmnetcv</span>(X, y) 
Least Squares GLMNet Cross Validation
<span class="pl-c1">86</span> models <span class="pl-k">for</span> <span class="pl-c1">4</span> predictors <span class="pl-k">in</span> <span class="pl-c1">10</span> folds
Best λ <span class="pl-c1">0.136</span> (mean loss <span class="pl-c1">101.530</span>, std <span class="pl-c1">10.940</span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">argmin</span>(cv<span class="pl-k">.</span>meanloss)
<span class="pl-c1">59</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">coef</span>(cv) <span class="pl-c"><span class="pl-c">#</span> equivalent to cv.path.betas[:, 59]</span>
<span class="pl-c1">4</span><span class="pl-k">-</span>element Array{Float64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
  <span class="pl-c1">1.1277676556880305</span>
  <span class="pl-c1">0.0</span>
  <span class="pl-c1">0.0</span>
 <span class="pl-k">-</span><span class="pl-c1">0.08747434292954445</span></pre></div>
<h3><a id="user-content-a-classification-example" class="anchor" aria-hidden="true" href="#a-classification-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>A classification Example</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using RDatasets

julia&gt; iris = dataset(&quot;datasets&quot;, &quot;iris&quot;);

julia&gt; X = convert(Matrix, iris[:, 1:4]);

julia&gt; y = convert(Vector, iris[:Species]);

julia&gt; iTrain = sample(1:size(X,1), 100, replace = false);

julia&gt; iTest = setdiff(1:size(X,1), iTrain);

julia&gt; iris_cv = glmnetcv(X[iTrain, :], y[iTrain])
Multinomial GLMNet Cross Validation
100 models for 4 predictors in 10 folds
Best λ 0.001 (mean loss 0.130, std 0.054)

julia&gt; yht = round.(predict(iris_cv, X[iTest, :], outtype = :prob), digits=3);

julia&gt; DataFrame(target=y[iTest], set=yht[:,1], ver=yht[:,2], vir=yht[:,3])[5:5:50,:]
10×4 DataFrame
│ Row │ target     │ set     │ ver     │ vir     │
│     │ Cat…       │ Float64 │ Float64 │ Float64 │
├─────┼────────────┼─────────┼─────────┼─────────┤
│ 1   │ setosa     │ 0.997   │ 0.003   │ 0.0     │
│ 2   │ setosa     │ 0.995   │ 0.005   │ 0.0     │
│ 3   │ setosa     │ 0.999   │ 0.001   │ 0.0     │
│ 4   │ versicolor │ 0.0     │ 0.997   │ 0.003   │
│ 5   │ versicolor │ 0.0     │ 0.36    │ 0.64    │
│ 6   │ versicolor │ 0.0     │ 0.05    │ 0.95    │
│ 7   │ virginica  │ 0.0     │ 0.002   │ 0.998   │
│ 8   │ virginica  │ 0.0     │ 0.001   │ 0.999   │
│ 9   │ virginica  │ 0.0     │ 0.0     │ 1.0     │
│ 10  │ virginica  │ 0.0     │ 0.001   │ 0.999   │


julia&gt; irisLabels = reshape(names(iris)[1:4],(1,4));
julia&gt; βs =iris_cv.path.betas;
julia&gt; λs= iris_cv.lambda;
julia&gt; sharedOpts =(legend=false,  xlabel=L&quot;\lambda&quot;, xscale=:log10) 
julia&gt; p1 = plot(λs,βs[:,1,:]',ylabel=L&quot;\beta_i&quot;;sharedOpts...);
julia&gt; p2 = plot(λs,βs[:,2,:]',title=&quot;Across Cross Validation runs&quot;;sharedOpts...);
julia&gt; p3 = plot(λs,βs[:,3,:]', legend=:topright,legendtitle=&quot;Variable&quot;, labels=irisLabels,xlabel=L&quot;\lambda&quot;,xscale=:log10);
julia&gt; plot(p1,p2,p3,layout=(1,3))
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> RDatasets

julia<span class="pl-k">&gt;</span> iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>);

julia<span class="pl-k">&gt;</span> X <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Matrix, iris[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>]);

julia<span class="pl-k">&gt;</span> y <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Vector, iris[<span class="pl-c1">:Species</span>]);

julia<span class="pl-k">&gt;</span> iTrain <span class="pl-k">=</span> <span class="pl-c1">sample</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(X,<span class="pl-c1">1</span>), <span class="pl-c1">100</span>, replace <span class="pl-k">=</span> <span class="pl-c1">false</span>);

julia<span class="pl-k">&gt;</span> iTest <span class="pl-k">=</span> <span class="pl-c1">setdiff</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(X,<span class="pl-c1">1</span>), iTrain);

julia<span class="pl-k">&gt;</span> iris_cv <span class="pl-k">=</span> <span class="pl-c1">glmnetcv</span>(X[iTrain, :], y[iTrain])
Multinomial GLMNet Cross Validation
<span class="pl-c1">100</span> models <span class="pl-k">for</span> <span class="pl-c1">4</span> predictors <span class="pl-k">in</span> <span class="pl-c1">10</span> folds
Best λ <span class="pl-c1">0.001</span> (mean loss <span class="pl-c1">0.130</span>, std <span class="pl-c1">0.054</span>)

julia<span class="pl-k">&gt;</span> yht <span class="pl-k">=</span> <span class="pl-c1">round</span>.(<span class="pl-c1">predict</span>(iris_cv, X[iTest, :], outtype <span class="pl-k">=</span> <span class="pl-c1">:prob</span>), digits<span class="pl-k">=</span><span class="pl-c1">3</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">DataFrame</span>(target<span class="pl-k">=</span>y[iTest], set<span class="pl-k">=</span>yht[:,<span class="pl-c1">1</span>], ver<span class="pl-k">=</span>yht[:,<span class="pl-c1">2</span>], vir<span class="pl-k">=</span>yht[:,<span class="pl-c1">3</span>])[<span class="pl-c1">5</span><span class="pl-k">:</span><span class="pl-c1">5</span><span class="pl-k">:</span><span class="pl-c1">50</span>,:]
<span class="pl-c1">10</span><span class="pl-k">×</span><span class="pl-c1">4</span> DataFrame
│ Row │ target     │ set     │ ver     │ vir     │
│     │ Cat…       │ Float64 │ Float64 │ Float64 │
├─────┼────────────┼─────────┼─────────┼─────────┤
│ <span class="pl-c1">1</span>   │ setosa     │ <span class="pl-c1">0.997</span>   │ <span class="pl-c1">0.003</span>   │ <span class="pl-c1">0.0</span>     │
│ <span class="pl-c1">2</span>   │ setosa     │ <span class="pl-c1">0.995</span>   │ <span class="pl-c1">0.005</span>   │ <span class="pl-c1">0.0</span>     │
│ <span class="pl-c1">3</span>   │ setosa     │ <span class="pl-c1">0.999</span>   │ <span class="pl-c1">0.001</span>   │ <span class="pl-c1">0.0</span>     │
│ <span class="pl-c1">4</span>   │ versicolor │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.997</span>   │ <span class="pl-c1">0.003</span>   │
│ <span class="pl-c1">5</span>   │ versicolor │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.36</span>    │ <span class="pl-c1">0.64</span>    │
│ <span class="pl-c1">6</span>   │ versicolor │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.05</span>    │ <span class="pl-c1">0.95</span>    │
│ <span class="pl-c1">7</span>   │ virginica  │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.002</span>   │ <span class="pl-c1">0.998</span>   │
│ <span class="pl-c1">8</span>   │ virginica  │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.001</span>   │ <span class="pl-c1">0.999</span>   │
│ <span class="pl-c1">9</span>   │ virginica  │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">1.0</span>     │
│ <span class="pl-c1">10</span>  │ virginica  │ <span class="pl-c1">0.0</span>     │ <span class="pl-c1">0.001</span>   │ <span class="pl-c1">0.999</span>   │


julia<span class="pl-k">&gt;</span> irisLabels <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(<span class="pl-c1">names</span>(iris)[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>],(<span class="pl-c1">1</span>,<span class="pl-c1">4</span>));
julia<span class="pl-k">&gt;</span> βs <span class="pl-k">=</span>iris_cv<span class="pl-k">.</span>path<span class="pl-k">.</span>betas;
julia<span class="pl-k">&gt;</span> λs<span class="pl-k">=</span> iris_cv<span class="pl-k">.</span>lambda;
julia<span class="pl-k">&gt;</span> sharedOpts <span class="pl-k">=</span>(legend<span class="pl-k">=</span><span class="pl-c1">false</span>,  xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds"><span class="pl-c1">L</span>"</span><span class="pl-cce">\l</span>ambda<span class="pl-pds">"</span></span>, xscale<span class="pl-k">=</span><span class="pl-c1">:log10</span>) 
julia<span class="pl-k">&gt;</span> p1 <span class="pl-k">=</span> <span class="pl-c1">plot</span>(λs,βs[:,<span class="pl-c1">1</span>,:]',ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds"><span class="pl-c1">L</span>"</span><span class="pl-cce">\b</span>eta_i<span class="pl-pds">"</span></span>;sharedOpts<span class="pl-k">...</span>);
julia<span class="pl-k">&gt;</span> p2 <span class="pl-k">=</span> <span class="pl-c1">plot</span>(λs,βs[:,<span class="pl-c1">2</span>,:]',title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Across Cross Validation runs<span class="pl-pds">"</span></span>;sharedOpts<span class="pl-k">...</span>);
julia<span class="pl-k">&gt;</span> p3 <span class="pl-k">=</span> <span class="pl-c1">plot</span>(λs,βs[:,<span class="pl-c1">3</span>,:]', legend<span class="pl-k">=</span><span class="pl-c1">:topright</span>,legendtitle<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Variable<span class="pl-pds">"</span></span>, labels<span class="pl-k">=</span>irisLabels,xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds"><span class="pl-c1">L</span>"</span><span class="pl-cce">\l</span>ambda<span class="pl-pds">"</span></span>,xscale<span class="pl-k">=</span><span class="pl-c1">:log10</span>);
julia<span class="pl-k">&gt;</span> <span class="pl-c1">plot</span>(p1,p2,p3,layout<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">3</span>))</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="iris_path.svg"><img src="iris_path.svg" alt="iris-lasso-path" style="max-width:100%;"></a></p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; plot(iris_cv.lambda, iris_cv.meanloss, xscale=:log10, legend=false, yerror=iris_cv.stdloss,xlabel=L&quot;\lambda&quot;,ylabel=&quot;loss&quot;)
julia&gt; vline!([lambdamin(iris_cv)])
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">plot</span>(iris_cv<span class="pl-k">.</span>lambda, iris_cv<span class="pl-k">.</span>meanloss, xscale<span class="pl-k">=</span><span class="pl-c1">:log10</span>, legend<span class="pl-k">=</span><span class="pl-c1">false</span>, yerror<span class="pl-k">=</span>iris_cv<span class="pl-k">.</span>stdloss,xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds"><span class="pl-c1">L</span>"</span><span class="pl-cce">\l</span>ambda<span class="pl-pds">"</span></span>,ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>loss<span class="pl-pds">"</span></span>)
julia<span class="pl-k">&gt;</span> <span class="pl-c1">vline!</span>([<span class="pl-c1">lambdamin</span>(iris_cv)])</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="lambda_plot.svg"><img src="lambda_plot.svg" alt="iris-cv" style="max-width:100%;"></a></p>
<h2><a id="user-content-fitting-models" class="anchor" aria-hidden="true" href="#fitting-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fitting models</h2>
<p><code>glmnet</code> has two required parameters: the m x n predictor matrix <code>X</code> and the dependent variable <code>y</code>. It additionally accepts an optional third argument, <code>family</code>, which can be used to specify a generalized linear model. Currently, <code>Normal()</code> (least squares, default), <code>Binomial()</code> (logistic), <code>Poisson()</code> , <code>Multinomial()</code>, <code>CoxPH()</code> (Cox model) are supported.</p>
<ul>
<li>For linear and Poisson models, <code>y</code> is a numerical vector.</li>
<li>For logistic models, <code>y</code> is either a string vector or a m x 2 matrix, where the first column is the count of negative responses for each row in <code>X</code> and the second column is the count of positive responses.</li>
<li>For multinomial models, <code>y</code> is etiher a string vector (with at least 3 unique values) or a m x k matrix, where k is number of unique values (classes).</li>
<li>For Cox models, <code>y</code> is a 2-column matrix, where the first column is survival time and second column is (right) censoring status. Indeed, For survival data, <code>glmnet</code> has another method <code>glmnet(X::Matrix, time::Vector, status::Vector)</code>. Same for <code>glmnetcv</code>.</li>
</ul>
<p><code>glmnet</code> also accepts many optional keyword parameters, described below:</p>
<ul>
<li><code>weights</code>: A vector of weights for each sample of the same size as <code>y</code>.</li>
<li><code>alpha</code>: The tradeoff between lasso and ridge regression. This defaults to <code>1.0</code>, which specifies a lasso model.</li>
<li><code>penalty_factor</code>: A vector of length n of penalties for each predictor in <code>X</code>. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.</li>
<li><code>constraints</code>: An 2 x n matrix specifying lower bounds (first line) and upper bounds (second line) on each predictor. By default, this is <code>[-Inf; Inf]</code> for each predictor in <code>X</code>.</li>
<li><code>dfmax</code>: The maximum number of predictors in the largest model.</li>
<li><code>pmax</code>: The maximum number of predictors in any model.</li>
<li><code>nlambda</code>: The number of values of λ along the path to consider.</li>
<li><code>lambda_min_ratio</code>: The smallest λ value to consider, as a ratio of the value of λ that gives the null model (i.e., the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to <code>0.0001</code>, otherwise <code>0.01</code>.</li>
<li><code>lambda</code>: The λ values to consider. By default, this is determined from <code>nlambda</code> and <code>lambda_min_ratio</code>.</li>
<li><code>tol</code>: Convergence criterion. Defaults to <code>1e-7</code>.</li>
<li><code>standardize</code>: Whether to standardize predictors so that they are in the same units. Defaults to <code>true</code>. Beta values are always presented on the original scale.</li>
<li><code>intercept</code>: Whether to fit an intercept term. The intercept is always unpenalized. Defaults to <code>true</code>.</li>
<li><code>maxit</code>: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.</li>
</ul>
<h2><a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>See also</h2>
<ul>
<li><a href="https://github.com/simonster/Lasso.jl">Lasso.jl</a>, a pure Julia implementation of the glmnet coordinate descent algorithm that often achieves better performance.</li>
<li><a href="https://github.com/simonster/LARS.jl">LARS.jl</a>, an implementation
of least angle regression for fitting entire linear (but not
generalized linear) Lasso and Elastic Net coordinate paths.</li>
</ul>
</article></div>