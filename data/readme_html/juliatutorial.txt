<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-juliatutorial" class="anchor" aria-hidden="true" href="#juliatutorial"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>JuliaTutorial</h1>
<p><a href="https://travis-ci.org/abhi123link/JuliaTutorial.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/65008b9a17de78e85faf7312db3d7ee4c42a77dfff8d1e969e0189ab58a8d2af/68747470733a2f2f7472617669732d63692e6f72672f616268696a69746863682f4a756c69615475746f7269616c2e6a6c2e706e67" alt="Build Status" data-canonical-src="https://travis-ci.org/abhijithch/JuliaTutorial.jl.png" style="max-width:100%;"></a></p>
<p>This is a Julia based tutorial covering the following topics:</p>
<ul>
<li>Introduction to Linear Algebra</li>
<li>Applications of Matrix Factorizations</li>
<li>Introduction to Text Mining</li>
<li>Introduction to Recommender Systems</li>
</ul>
<p>This package has each of the above mentioned topics as sub-modules. However as of now only Text Mining tutorial is available, and the rest are under construction.</p>
<h3><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h3>
<p>This is an unregistered package, and can be installed in either of the following two ways:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Pkg.clone(&quot;https://github.com/abhijithch/JuliaTutorial.jl.git&quot;)
"><pre><code>Pkg.clone("https://github.com/abhijithch/JuliaTutorial.jl.git")
</code></pre></div>
<p>alternatively, this also could be directly cloned from github as follows,</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="git clone https://github.com/abhijithch/JuliaTutorial.jl.git
"><pre><code>git clone https://github.com/abhijithch/JuliaTutorial.jl.git
</code></pre></div>
<p>in which case the dependent packages will have to be installed. If installed through the package manager, <code>Pkg.clone()</code> the dependent packages would be automatically installed.</p>
<h3><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h3>
<p>To start using the package, first do <code>using JuliaTutorial</code>. Then according to the options given, include the sub-modules by <code>using JuliaTutorial.TextMining</code> to enable all the functions of Text Mining tutorial.</p>
<h2><a id="user-content-text-mining" class="anchor" aria-hidden="true" href="#text-mining"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Text Mining</h2>
<p>Please refer to docs/Julia_TextMining.pdf for the theoretical concepts. This Text Mining module depends on <code>TextAnalysis.jl</code>, for most of the preprocessing and preparation of the Term Document Matrix.</p>
<h3><a id="user-content-preparation" class="anchor" aria-hidden="true" href="#preparation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Preparation</h3>
<p>The first thing to do is generate a corpus from collection of textual data. In this module we work with documents as the source of textual data. These documents could be collection of research articles, HTML files etc, and the function <code>PrepDocCorpus(dirname::String,DocType::Type)</code> prepares a corpus, i.e., collection of all the documents under one entity. It also standardizes all the documents to a singly type, specified by <code>DocType</code>. The types could be any of <code>StringDocument</code>, <code>TokenDocument</code> or <code>NGramDocument</code>.</p>
<p>The query corpus are to be obtained using the function, <code>PrepQueriesCorpus(NoQueries::Int,QueryFile::String)</code>. The <code>NoQueries</code> number of queries are stored in a single text file, <code>QueryFile</code>. Each queries are delimited by 2 blank lines.</p>
<p>The <code>PreProcess!(crps::Corpus)</code> function does all the preprocessing like removal of articles, pronouns, prepositions and stop words.</p>
<p>The functions <code>dtm</code> or the <code>tdm</code> from the <code>TextAnalysis</code> package are used to generate the TDM(Term Document Matrix). All the models end up factoring this TDM.</p>
<h3><a id="user-content-query-matching" class="anchor" aria-hidden="true" href="#query-matching"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Query Matching</h3>
<p>The proximity measure used is the cosine measure, the function <code>CosTheta(q::Array{Float64,1},d::Array{Float64,1})</code>, returns the cosine of the angle between the query vector <code>q</code> and the document vector <code>d</code>.</p>
<h3><a id="user-content-performance-modeling" class="anchor" aria-hidden="true" href="#performance-modeling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Performance Modeling</h3>
<p>Like in any information retrieval tasks, <em>Recall</em>, <code>R</code> and <em>Precision</em>, <code>P</code> model the performance. <code>R=Dr/Nr</code>, where <code>Dr</code> is the number of relevant documents retrieved and <code>Nr</code> is the total number of relevant documents in the database, <code>P=Dr/Dt</code> where <code>Dt</code> is the total number of documents retrieved. The function <code>PrepTest()</code> prepares the test matrix, which is human verified list of the relevant documents for the correspoding queries.</p>
<h3><a id="user-content-vsm---vector-space-model" class="anchor" aria-hidden="true" href="#vsm---vector-space-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>VSM - Vector Space Model</h3>
<p>This is the basic model in which the column vecotrs of the TDM constitute the Document space, of dimension equal to number of terms(keywords). A new query will also be another vector in the same space, and in the VSM model we just find the cosine similarity between the query and all the documents. A tolerance value decides the number of documents which will be returned. The performance analysis is done for various tolerance levels.</p>
<p>The VSM can be tested using the function <code>VSMModel()</code> with constrained parmeters types. The method <code>VSMModel(A::Array{Float64,2},nq::Int64)</code> gives the <em>Precision</em> and <em>Recall</em> for <code>nq</code> queries which form the first <code>nq</code> columns of the <code>A</code> matrix. The Documents are the remaining column vectors of <code>A</code>.</p>
<p>The method <code>VSMModel(Q_C::Corpus,D_C::Corpus)</code> forms the TDM from the Query and Document corpus, and gives the average <em>Recall</em> and <em>Precision</em>.</p>
<p>The Method <code>VSMModel(QueryNum::Int64,A::Array{Float64,2},nq::Int64)</code> give the <em>Recall</em> and <em>Precision</em> for a single query identified by <code>QueryNum</code>.</p>
<h3><a id="user-content-latent-semantic-indexing-model" class="anchor" aria-hidden="true" href="#latent-semantic-indexing-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Latent Semantic Indexing Model</h3>
<p>The LSI model finds the SVD of the Term Document Matrix, and decomposes the same into <em>Document Space</em> and <em>Query Space</em>. The method <code>SVDModel(A::Array{Float64,2},nq::Int64,rank::Int64)</code> uses the reduced rank approximation, and returns the average <em>Recall</em> and <em>Precision</em>. The methods <code>SVDModel(Q_C::Corpus,D_C::Corpus,rank::Int64)</code> does the same for the Query and Document corpus. The methods <code>SVDModel(QueryNum::Int64,A::Array{Float64,2},NumQueries::Int64,rank::Int64)</code> gives the <em>Recall</em>
and <em>Precision</em> for the single query <code>QueryNum</code>.</p>
<h3><a id="user-content-k-means-model" class="anchor" aria-hidden="true" href="#k-means-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>K-Means Model</h3>
<p>Considering the Documents to be points in <code>m</code> dimensional space, documents with similar content tend to be closer to each other. Hence by clustering the documents into <code>K</code> clusters, with the <em>centroid</em> of each each clusters representing them. Hence all these <code>k</code> centroid vectors as a mtrix <code>C</code> represent the entire Document Space. But to obtain an orthonormal basis of this space, we do QR-Factorization of <code>C</code>, represented by <code>G</code>. Then by projecting the Document vectors and query vectors onto this space <code>G</code>, we find the cosine measure between the query and all of the douments.</p>
<p>The method <code>KMeansModel(A::Array{Float64,2},NumQueries::Int64,NumClusters::Int64)</code> gives the average <em>Recall</em> and <em>Precision</em> by using <code>NumClusters</code>. The method <code>KMeansModel(QueryNum::Int64,A::Array{Float64,2},NumQueries::Int64,Clusters::Int64)</code> does the same for single query <code>QueryNum</code>. The method <code>KMeansModel(Q_C::Corpus,D_C::Corpus,Clusters::Int64)</code> finds the <em>Recall</em> and <em>Precision</em> for the Query and Document Corpus <code>Q_C</code> and <code>D_C</code>.</p>
<h3><a id="user-content-plotting-results" class="anchor" aria-hidden="true" href="#plotting-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Plotting Results</h3>
<p>The function <code>plot_DrDtNr(Dr::Array{Float64,1},Dt::Array{Float64,1},Nr::Array{Float64,1},qNum::Int64,tol::Array{Float64,1})</code> can be used to plot the <code>Dr</code>, <code>Dt</code> and <code>Nr</code> for a single query <code>qNum</code> against the tolerance levels specified by <code>tol</code>.</p>
<p>The function <code>plotNew_RecPrec(Rec::Array{Float64,1},Prec::Array{Float64,1},strMethod::String)</code> must be used to plot the recall and precision. The <code>strMethod</code> specifies the model used, e.x, VSM or LSI etc. This function generates a new figure().</p>
<p>By using the function <code>plotAdd_RecPrec(Rec::Array{Float64,1},Prec::Array{Float64,1},strMethod::String)</code> a plot can be added to an already existing figure object. The plots automatically chooses different colors and corresponding legends are created. It supports upto 7 plots of the following colors, <code>Colors=["red","blue","green","black","cyan","magenta","yellow"]</code>. In the <code>PlotResults.jl</code>, new colors can be added to enable more plots.</p>
</article></div>