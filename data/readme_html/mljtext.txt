<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-mljtextjl" class="anchor" aria-hidden="true" href="#mljtextjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MLJText.jl</h1>
<p dir="auto">An <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="nofollow">MLJ</a>
extension providing tools and models for text analysis.</p>
<table>
<thead>
<tr>
<th align="left">Linux</th>
<th align="left">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://github.com/JuliaAI/MLJText.jl/actions"><img src="https://github.com/JuliaAI/MLJText.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a></td>
<td align="left"><a href="https://codecov.io/github/JuliaAI/MLJText.jl?branch=dev" rel="nofollow"><img src="https://camo.githubusercontent.com/af2914742e73be3225a0b80e99e2da5634d3d0f05d3ec6fdd8ddd25d0a83ff80/68747470733a2f2f636f6465636f762e696f2f67682f4a756c696141492f4d4c4a546578742e6a6c2f6272616e63682f6465762f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/JuliaAI/MLJText.jl/branch/dev/graph/badge.svg" style="max-width: 100%;"></a></td>
</tr>
</tbody>
</table>
<p dir="auto">The goal of this package is to provide an interface to various Natural Language Processing (NLP) resources for <code>MLJ</code> via such existing packages like <a href="https://github.com/JuliaText/TextAnalysis.jl">TextAnalysis</a></p>
<p dir="auto">Currently, we have a TF-IDF Transformer which converts a collection of raw documents into a TF-IDF matrix. We also have a similar way of representing documents using the Okapi Best Match 25 algorithm - this works in a similar fashion to TF-IDF but introduces the probability that a term is relevant in a particular document.  See <a href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="nofollow">Okapi BM25</a>. Finally, there is also a simple Bag-of-Word representation available.</p>
<h2 dir="auto"><a id="user-content-tf-idf-transformer" class="anchor" aria-hidden="true" href="#tf-idf-transformer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TF-IDF Transformer</h2>
<p dir="auto">"TF" means term-frequency while "TF-IDF" means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.</p>
<p dir="auto">The goal of using TF-IDF instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</p>
<h3 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h3>
<p dir="auto">The TF-IDF Transformer accepts a variety of inputs for the raw documents that one wishes to convert into a TF-IDF matrix.</p>
<p dir="auto">Raw documents can simply be provided as tokenized documents.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ, MLJText, TextAnalysis

docs = [&quot;Hi my name is Sam.&quot;, &quot;How are you today?&quot;]
tfidf_transformer = TfidfTransformer()
mach = machine(tfidf_transformer, tokenize.(docs))
MLJ.fit!(mach)

tfidf_mat = transform(mach, tokenize.(docs))"><pre><span class="pl-k">using</span> MLJ, MLJText, TextAnalysis

docs <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>Hi my name is Sam.<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>How are you today?<span class="pl-pds">"</span></span>]
tfidf_transformer <span class="pl-k">=</span> <span class="pl-c1">TfidfTransformer</span>()
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(tfidf_transformer, <span class="pl-c1">tokenize</span>.(docs))
MLJ<span class="pl-k">.</span><span class="pl-c1">fit!</span>(mach)

tfidf_mat <span class="pl-k">=</span> <span class="pl-c1">transform</span>(mach, <span class="pl-c1">tokenize</span>.(docs))</pre></div>
<p dir="auto">The resulting matrix looks like:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="2×11 adjoint(::SparseArrays.SparseMatrixCSC{Float64, Int64}) with eltype Float64:
 0.234244  0.0       0.234244  0.0       0.234244  0.0       0.234244  0.234244  0.234244  0.0       0.0
 0.0       0.281093  0.0       0.281093  0.0       0.281093  0.0       0.0       0.0       0.281093  0.281093"><pre class="notranslate"><code>2×11 adjoint(::SparseArrays.SparseMatrixCSC{Float64, Int64}) with eltype Float64:
 0.234244  0.0       0.234244  0.0       0.234244  0.0       0.234244  0.234244  0.234244  0.0       0.0
 0.0       0.281093  0.0       0.281093  0.0       0.281093  0.0       0.0       0.0       0.281093  0.281093
</code></pre></div>
<p dir="auto">Functionality similar to Scikit-Learn's implementation with N-Grams can easily be implemented using features from <code>TextAnalysis</code>. Then the N-Grams themselves (either as a dictionary of Strings or dictionary of Tuples) can be passed into the transformer. We will likely introduce an additional transformer to handle these types of conversions in a future update to <code>MLJText</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="
# this will create unigrams and bigrams
corpus = Corpus(NGramDocument.(docs, 1, 2))

ngram_docs = ngrams.(corpus)
tfidf_transformer = TfidfTransformer()
mach = machine(tfidf_transformer, ngram_docs)
MLJ.fit!(mach)

tfidf_mat = transform(mach, ngram_docs)"><pre><span class="pl-c"><span class="pl-c">#</span> this will create unigrams and bigrams</span>
corpus <span class="pl-k">=</span> <span class="pl-c1">Corpus</span>(<span class="pl-c1">NGramDocument</span>.(docs, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>))

ngram_docs <span class="pl-k">=</span> <span class="pl-c1">ngrams</span>.(corpus)
tfidf_transformer <span class="pl-k">=</span> <span class="pl-c1">TfidfTransformer</span>()
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(tfidf_transformer, ngram_docs)
MLJ<span class="pl-k">.</span><span class="pl-c1">fit!</span>(mach)

tfidf_mat <span class="pl-k">=</span> <span class="pl-c1">transform</span>(mach, ngram_docs)</pre></div>
<h2 dir="auto"><a id="user-content-bm25-transformer" class="anchor" aria-hidden="true" href="#bm25-transformer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BM25 Transformer</h2>
<p dir="auto">BM25 is an approach similar to that of TF-IDF in terms of representing documents in a vector space. The BM25 scoring function uses both term frequency (TF) and inverse document frequency (IDF) so that, for each term in a document, its relative concentration in the document is scored (like TF-IDF). However, BM25 improves upon TF-IDF by incorporating probability - particularly, the probability that a user will consider a search result relevant based on the terms in the search query and those in each document.</p>
<h3 dir="auto"><a id="user-content-usage-1" class="anchor" aria-hidden="true" href="#usage-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h3>
<p dir="auto">This transformer is used in much the same way as the <code>TfidfTransformer</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ, MLJText, TextAnalysis

docs = [&quot;Hi my name is Sam.&quot;, &quot;How are you today?&quot;]
bm25_transformer = BM25Transformer()
mach = machine(bm25_transformer, tokenize.(docs))
MLJ.fit!(mach)

bm25_mat = transform(mach, tokenize.(docs))"><pre><span class="pl-k">using</span> MLJ, MLJText, TextAnalysis

docs <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>Hi my name is Sam.<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>How are you today?<span class="pl-pds">"</span></span>]
bm25_transformer <span class="pl-k">=</span> <span class="pl-c1">BM25Transformer</span>()
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(bm25_transformer, <span class="pl-c1">tokenize</span>.(docs))
MLJ<span class="pl-k">.</span><span class="pl-c1">fit!</span>(mach)

bm25_mat <span class="pl-k">=</span> <span class="pl-c1">transform</span>(mach, <span class="pl-c1">tokenize</span>.(docs))</pre></div>
<p dir="auto">The resulting matrix looks like:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="2×11 adjoint(::SparseArrays.SparseMatrixCSC{Float64, Int64}) with eltype Float64:
 0.676463  0.0      0.676463  0.0      0.676463  0.0      0.676463  0.676463  0.676463  0.0      0.0
 0.0       0.81599  0.0       0.81599  0.0       0.81599  0.0       0.0       0.0       0.81599  0.81599"><pre class="notranslate"><code>2×11 adjoint(::SparseArrays.SparseMatrixCSC{Float64, Int64}) with eltype Float64:
 0.676463  0.0      0.676463  0.0      0.676463  0.0      0.676463  0.676463  0.676463  0.0      0.0
 0.0       0.81599  0.0       0.81599  0.0       0.81599  0.0       0.0       0.0       0.81599  0.81599
</code></pre></div>
<p dir="auto">You will note that this transformer has some additional parameters compared to the <code>TfidfTransformer</code>:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="BM25Transformer(
    max_doc_freq = 1.0,
    min_doc_freq = 0.0,
    κ = 2,
    β = 0.75,
    smooth_idf = true)"><pre class="notranslate"><code>BM25Transformer(
    max_doc_freq = 1.0,
    min_doc_freq = 0.0,
    κ = 2,
    β = 0.75,
    smooth_idf = true)
</code></pre></div>
<p dir="auto">Please see <a href="http://ethen8181.github.io/machine-learning/search/bm25_intro.html" rel="nofollow">http://ethen8181.github.io/machine-learning/search/bm25_intro.html</a> for more details about how these parameters affect the matrix that is generated.</p>
<h2 dir="auto"><a id="user-content-count-transformer" class="anchor" aria-hidden="true" href="#count-transformer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Count Transformer</h2>
<p dir="auto">The <code>MLJText</code> package also offers a way to represent documents using the simpler bag-of-words representation. This returns a document-term matrix (as you would get in <code>TextAnalysis</code>) that consists of the count for every word in the corpus for each document in the corpus.</p>
<h3 dir="auto"><a id="user-content-usage-2" class="anchor" aria-hidden="true" href="#usage-2"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ, MLJText, TextAnalysis

docs = [&quot;Hi my name is Sam.&quot;, &quot;How are you today?&quot;]
count_transformer = CountTransformer()
mach = machine(count_transformer, tokenize.(docs))
MLJ.fit!(mach)

count_mat = transform(mach, tokenize.(docs))"><pre><span class="pl-k">using</span> MLJ, MLJText, TextAnalysis

docs <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>Hi my name is Sam.<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>How are you today?<span class="pl-pds">"</span></span>]
count_transformer <span class="pl-k">=</span> <span class="pl-c1">CountTransformer</span>()
mach <span class="pl-k">=</span> <span class="pl-c1">machine</span>(count_transformer, <span class="pl-c1">tokenize</span>.(docs))
MLJ<span class="pl-k">.</span><span class="pl-c1">fit!</span>(mach)

count_mat <span class="pl-k">=</span> <span class="pl-c1">transform</span>(mach, <span class="pl-c1">tokenize</span>.(docs))</pre></div>
<p dir="auto">The resulting matrix looks like:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="2×11 adjoint(::SparseArrays.SparseMatrixCSC{Int64, Int64}) with eltype Int64:
 1  0  1  0  1  0  1  1  1  0  0
 0  1  0  1  0  1  0  0  0  1  1"><pre class="notranslate"><code>2×11 adjoint(::SparseArrays.SparseMatrixCSC{Int64, Int64}) with eltype Int64:
 1  0  1  0  1  0  1  1  1  0  0
 0  1  0  1  0  1  0  0  0  1  1
</code></pre></div>
</article></div>