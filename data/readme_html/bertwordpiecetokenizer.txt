<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-bertwordpiecetokenizer" class="anchor" aria-hidden="true" href="#bertwordpiecetokenizer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BertWordPieceTokenizer</h1>
<p dir="auto">Load BERT WordPiece Tokenizer</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using Pkg; Pkg.add(&quot;BertWordPieceTokenizer&quot;)"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>BertWordPieceTokenizer<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<h3 dir="auto"><a id="user-content-initialize" class="anchor" aria-hidden="true" href="#initialize"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Initialize</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import BertWordPieceTokenizer as BWP

# initialize tokenizer from local file
BWP.init(&quot;/path/to/vocab.txt&quot;)
BWP.init(&quot;/path/to/vocab.txt&quot;, do_lowercase=true)
BWP.init(&quot;/path/to/vocab.txt&quot;, do_lowercase=false)

# initialize tokenizer from HTTP URL
BWP.init(&quot;https://huggingface.co/bert-base-uncased/raw/main/vocab.txt&quot;, do_lowercase=true)

# initialize tokenizer from HTTP URL, and cache vocabulary to a local file
BWP.init(&quot;https://huggingface.co/bert-base-uncased/raw/main/vocab.txt&quot;, cache_path=&quot;/path/to/vocab.txt&quot;, do_lowercase=true)"><pre><span class="pl-k">import</span> BertWordPieceTokenizer <span class="pl-k">as</span> BWP

<span class="pl-c"><span class="pl-c">#</span> initialize tokenizer from local file</span>
BWP<span class="pl-k">.</span><span class="pl-c1">init</span>(<span class="pl-s"><span class="pl-pds">"</span>/path/to/vocab.txt<span class="pl-pds">"</span></span>)
BWP<span class="pl-k">.</span><span class="pl-c1">init</span>(<span class="pl-s"><span class="pl-pds">"</span>/path/to/vocab.txt<span class="pl-pds">"</span></span>, do_lowercase<span class="pl-k">=</span><span class="pl-c1">true</span>)
BWP<span class="pl-k">.</span><span class="pl-c1">init</span>(<span class="pl-s"><span class="pl-pds">"</span>/path/to/vocab.txt<span class="pl-pds">"</span></span>, do_lowercase<span class="pl-k">=</span><span class="pl-c1">false</span>)

<span class="pl-c"><span class="pl-c">#</span> initialize tokenizer from HTTP URL</span>
BWP<span class="pl-k">.</span><span class="pl-c1">init</span>(<span class="pl-s"><span class="pl-pds">"</span>https://huggingface.co/bert-base-uncased/raw/main/vocab.txt<span class="pl-pds">"</span></span>, do_lowercase<span class="pl-k">=</span><span class="pl-c1">true</span>)

<span class="pl-c"><span class="pl-c">#</span> initialize tokenizer from HTTP URL, and cache vocabulary to a local file</span>
BWP<span class="pl-k">.</span><span class="pl-c1">init</span>(<span class="pl-s"><span class="pl-pds">"</span>https://huggingface.co/bert-base-uncased/raw/main/vocab.txt<span class="pl-pds">"</span></span>, cache_path<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>/path/to/vocab.txt<span class="pl-pds">"</span></span>, do_lowercase<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<h3 dir="auto"><a id="user-content-tokenize" class="anchor" aria-hidden="true" href="#tokenize"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tokenize</h3>
<p dir="auto">After initializing, you can tokenize text using <code>BWP.tokenize</code>, and encode text using <code>BWP.encode</code></p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# tokenize text
tokens = BWP.tokenize(&quot;i like apples&quot;)

# encode text
token_ids, segment_ids = BWP.encode(&quot;i like apples&quot;)"><pre><span class="pl-c"><span class="pl-c">#</span> tokenize text</span>
tokens <span class="pl-k">=</span> BWP<span class="pl-k">.</span><span class="pl-c1">tokenize</span>(<span class="pl-s"><span class="pl-pds">"</span>i like apples<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> encode text</span>
token_ids, segment_ids <span class="pl-k">=</span> BWP<span class="pl-k">.</span><span class="pl-c1">encode</span>(<span class="pl-s"><span class="pl-pds">"</span>i like apples<span class="pl-pds">"</span></span>)</pre></div>
<h3 dir="auto"><a id="user-content-truncation" class="anchor" aria-hidden="true" href="#truncation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Truncation</h3>
<p dir="auto">You can specify max length and truncation strategy using <code>BWP.enable_truncation</code></p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="BWP.enable_truncation(512, truncation=&quot;post&quot;)
BWP.enable_truncation(512, truncation=&quot;pre&quot;)"><pre>BWP<span class="pl-k">.</span><span class="pl-c1">enable_truncation</span>(<span class="pl-c1">512</span>, truncation<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>post<span class="pl-pds">"</span></span>)
BWP<span class="pl-k">.</span><span class="pl-c1">enable_truncation</span>(<span class="pl-c1">512</span>, truncation<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>pre<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-benchmark" class="anchor" aria-hidden="true" href="#benchmark"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Benchmark</h2>
<ol dir="auto">
<li>Julia</li>
</ol>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using TimeIt

julia&gt; import BertWordPieceTokenizer as BWP

julia&gt; BWP.init(&quot;https://huggingface.co/bert-base-uncased/raw/main/vocab.txt&quot;, cache_path=&quot;bert_uncased_vocab.txt&quot;, do_lowercase=true)

julia&gt; BWP.encode(&quot;I like apples&quot;)
([101, 1045, 2066, 18108, 102], [0, 0, 0, 0, 0])

julia&gt; @timeit BWP.encode(&quot;I like apples&quot;)
100000 loops, best of 3: 6.77 µs per loop"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> TimeIt

julia<span class="pl-k">&gt;</span> <span class="pl-k">import</span> BertWordPieceTokenizer <span class="pl-k">as</span> BWP

julia<span class="pl-k">&gt;</span> BWP<span class="pl-k">.</span><span class="pl-c1">init</span>(<span class="pl-s"><span class="pl-pds">"</span>https://huggingface.co/bert-base-uncased/raw/main/vocab.txt<span class="pl-pds">"</span></span>, cache_path<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>bert_uncased_vocab.txt<span class="pl-pds">"</span></span>, do_lowercase<span class="pl-k">=</span><span class="pl-c1">true</span>)

julia<span class="pl-k">&gt;</span> BWP<span class="pl-k">.</span><span class="pl-c1">encode</span>(<span class="pl-s"><span class="pl-pds">"</span>I like apples<span class="pl-pds">"</span></span>)
([<span class="pl-c1">101</span>, <span class="pl-c1">1045</span>, <span class="pl-c1">2066</span>, <span class="pl-c1">18108</span>, <span class="pl-c1">102</span>], [<span class="pl-c1">0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>])

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@timeit</span> BWP<span class="pl-k">.</span><span class="pl-c1">encode</span>(<span class="pl-s"><span class="pl-pds">"</span>I like apples<span class="pl-pds">"</span></span>)
<span class="pl-c1">100000</span> loops, best of <span class="pl-c1">3</span><span class="pl-k">:</span> <span class="pl-c1">6.77</span> µs per loop</pre></div>
<ol start="2" dir="auto">
<li>Pure Python</li>
</ol>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="In [1]: from bert4keras.tokenizers import Tokenizer

In [2]: tokenizer = Tokenizer(&quot;bert_uncased_vocab.txt&quot;, do_lower_case=True)

In [3]: tokenizer.encode(&quot;I like apples&quot;)
Out[3]: ([101, 1045, 2066, 18108, 102], [0, 0, 0, 0, 0])

In [4]: %timeit tokenizer.encode(&quot;I like apples&quot;)
48.7 µs ± 1.93 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)"><pre><span class="pl-v">In</span> [<span class="pl-c1">1</span>]: <span class="pl-s1">from</span> <span class="pl-s1">bert4keras</span>.<span class="pl-s1">tokenizers</span> <span class="pl-s1">import</span> <span class="pl-v">Tokenizer</span>

<span class="pl-v">In</span> [<span class="pl-c1">2</span>]: <span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">Tokenizer</span>(<span class="pl-s">"bert_uncased_vocab.txt"</span>, <span class="pl-s1">do_lower_case</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)

<span class="pl-v">In</span> [<span class="pl-c1">3</span>]: <span class="pl-s1">tokenizer</span>.<span class="pl-en">encode</span>(<span class="pl-s">"I like apples"</span>)
<span class="pl-v">Out</span>[<span class="pl-c1">3</span>]: ([<span class="pl-c1">101</span>, <span class="pl-c1">1045</span>, <span class="pl-c1">2066</span>, <span class="pl-c1">18108</span>, <span class="pl-c1">102</span>], [<span class="pl-c1">0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>, <span class="pl-c1">0</span>])

<span class="pl-v">In</span> [<span class="pl-c1">4</span>]: <span class="pl-c1">%</span><span class="pl-s1">timeit</span> <span class="pl-s1">tokenizer</span>.<span class="pl-en">encode</span>(<span class="pl-s">"I like apples"</span>)
<span class="pl-c1">48.7</span> <span class="pl-s1">µs</span> ± <span class="pl-c1">1.93</span> <span class="pl-s1">µs</span> <span class="pl-s1">per</span> <span class="pl-en">loop</span> (<span class="pl-s1">mean</span> ± <span class="pl-s1">std</span>. <span class="pl-s1">dev</span>. <span class="pl-s1">of</span> <span class="pl-c1">7</span> <span class="pl-s1">runs</span>, <span class="pl-c1">10000</span> <span class="pl-s1">loops</span> <span class="pl-s1">each</span>)</pre></div>
<ol start="3" dir="auto">
<li>Rust binding for Python</li>
</ol>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="In [1]: from tokenizers import BertWordPieceTokenizer

In [2]: tokenizer = BertWordPieceTokenizer(&quot;bert_uncased_vocab.txt&quot;, lowercase=True)

In [3]: tokenizer.encode(&quot;I like apples&quot;)
Out[3]: Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])

In [4]: %timeit tokenizer.encode(&quot;I like apples&quot;)
23.8 µs ± 274 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)"><pre><span class="pl-v">In</span> [<span class="pl-c1">1</span>]: <span class="pl-s1">from</span> <span class="pl-s1">tokenizers</span> <span class="pl-k">import</span> <span class="pl-v">BertWordPieceTokenizer</span>

<span class="pl-v">In</span> [<span class="pl-c1">2</span>]: <span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">BertWordPieceTokenizer</span>(<span class="pl-s">"bert_uncased_vocab.txt"</span>, <span class="pl-s1">lowercase</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)

<span class="pl-v">In</span> [<span class="pl-c1">3</span>]: <span class="pl-s1">tokenizer</span>.<span class="pl-en">encode</span>(<span class="pl-s">"I like apples"</span>)
<span class="pl-v">Out</span>[<span class="pl-c1">3</span>]: <span class="pl-v">Encoding</span>(<span class="pl-s1">num_tokens</span><span class="pl-c1">=</span><span class="pl-c1">5</span>, <span class="pl-s1">attributes</span><span class="pl-c1">=</span>[<span class="pl-s1">ids</span>, <span class="pl-s1">type_ids</span>, <span class="pl-s1">tokens</span>, <span class="pl-s1">offsets</span>, <span class="pl-s1">attention_mask</span>, <span class="pl-s1">special_tokens_mask</span>, <span class="pl-s1">overflowing</span>])

<span class="pl-v">In</span> [<span class="pl-c1">4</span>]: <span class="pl-c1">%</span><span class="pl-s1">timeit</span> <span class="pl-s1">tokenizer</span>.<span class="pl-en">encode</span>(<span class="pl-s">"I like apples"</span>)
<span class="pl-c1">23.8</span> <span class="pl-s1">µs</span> ± <span class="pl-c1">274</span> <span class="pl-s1">ns</span> <span class="pl-s1">per</span> <span class="pl-en">loop</span> (<span class="pl-s1">mean</span> ± <span class="pl-s1">std</span>. <span class="pl-s1">dev</span>. <span class="pl-s1">of</span> <span class="pl-c1">7</span> <span class="pl-s1">runs</span>, <span class="pl-c1">10000</span> <span class="pl-s1">loops</span> <span class="pl-s1">each</span>)</pre></div>
</article></div>