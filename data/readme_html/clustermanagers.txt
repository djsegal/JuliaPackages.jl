<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-clustermanagers" class="anchor" aria-hidden="true" href="#clustermanagers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ClusterManagers</h1>
<p>Support for different job queue systems commonly used on compute clusters.</p>
<h2><a id="user-content-currently-supported-job-queue-systems" class="anchor" aria-hidden="true" href="#currently-supported-job-queue-systems"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Currently supported job queue systems</h2>
<table>
<thead>
<tr>
<th>Job queue system</th>
<th>Command to add processors</th>
</tr>
</thead>
<tbody>
<tr>
<td>Load Sharing Facility (LSF)</td>
<td><code>addprocs_lsf(np::Integer, flags=``)</code> or <code>addprocs(LSFManager(np, flags))</code></td>
</tr>
<tr>
<td>Sun Grid Engine</td>
<td><code>addprocs_sge(np::Integer, queue="")</code> or <code>addprocs(SGEManager(np, queue))</code></td>
</tr>
<tr>
<td>SGE via qrsh</td>
<td><code>addprocs_qrsh(np::Integer, queue="")</code> or <code>addprocs(QRSHManager(np, queue))</code></td>
</tr>
<tr>
<td>PBS</td>
<td><code>addprocs_pbs(np::Integer, queue="")</code> or <code>addprocs(PBSManager(np, queue))</code></td>
</tr>
<tr>
<td>Scyld</td>
<td><code>addprocs_scyld(np::Integer)</code> or <code>addprocs(ScyldManager(np))</code></td>
</tr>
<tr>
<td>HTCondor</td>
<td><code>addprocs_htc(np::Integer)</code> or <code>addprocs(HTCManager(np))</code></td>
</tr>
<tr>
<td>Slurm</td>
<td><code>addprocs_slurm(np::Integer; kwargs...)</code> or <code>addprocs(SlurmManager(np); kwargs...)</code></td>
</tr>
<tr>
<td>Local manager with CPU affinity setting</td>
<td><code>addprocs(LocalAffinityManager(;np=CPU_CORES, mode::AffinityMode=BALANCED, affinities=[]); kwargs...)</code></td>
</tr>
</tbody>
</table>
<p>You can also write your own custom cluster manager; see the instructions in the <a href="https://docs.julialang.org/en/latest/manual/parallel-computing/#ClusterManagers-1" rel="nofollow">Julia manual</a></p>
<h3><a id="user-content-slurm-a-simple-example" class="anchor" aria-hidden="true" href="#slurm-a-simple-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Slurm: a simple example</h3>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ClusterManagers

<span class="pl-c"><span class="pl-c">#</span> Arguments to the Slurm srun(1) command can be given as keyword</span>
<span class="pl-c"><span class="pl-c">#</span> arguments to addprocs.  The argument name and value is translated to</span>
<span class="pl-c"><span class="pl-c">#</span> a srun(1) command line argument as follows:</span>
<span class="pl-c"><span class="pl-c">#</span> 1) If the length of the argument is 1 =&gt; "-arg value",</span>
<span class="pl-c"><span class="pl-c">#</span>    e.g. t="0:1:0" =&gt; "-t 0:1:0"</span>
<span class="pl-c"><span class="pl-c">#</span> 2) If the length of the argument is &gt; 1 =&gt; "--arg=value"</span>
<span class="pl-c"><span class="pl-c">#</span>    e.g. time="0:1:0" =&gt; "--time=0:1:0"</span>
<span class="pl-c"><span class="pl-c">#</span> 3) If the value is the empty string, it becomes a flag value,</span>
<span class="pl-c"><span class="pl-c">#</span>    e.g. exclusive="" =&gt; "--exclusive"</span>
<span class="pl-c"><span class="pl-c">#</span> 4) If the argument contains "_", they are replaced with "-",</span>
<span class="pl-c"><span class="pl-c">#</span>    e.g. mem_per_cpu=100 =&gt; "--mem-per-cpu=100"</span>
<span class="pl-c1">addprocs</span>(<span class="pl-c1">SlurmManager</span>(<span class="pl-c1">2</span>), partition<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>debug<span class="pl-pds">"</span></span>, t<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>00:5:00<span class="pl-pds">"</span></span>)

hosts <span class="pl-k">=</span> []
pids <span class="pl-k">=</span> []
<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">workers</span>()
	host, pid <span class="pl-k">=</span> <span class="pl-c1">fetch</span>(<span class="pl-c1">@spawnat</span> i (<span class="pl-c1">gethostname</span>(), <span class="pl-c1">getpid</span>()))
	<span class="pl-c1">push!</span>(hosts, host)
	<span class="pl-c1">push!</span>(pids, pid)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> The Slurm resource allocation is released when all the workers have</span>
<span class="pl-c"><span class="pl-c">#</span> exited</span>
<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">workers</span>()
	<span class="pl-c1">rmprocs</span>(i)
<span class="pl-k">end</span></pre></div>
<h3><a id="user-content-sge---a-simple-interactive-example" class="anchor" aria-hidden="true" href="#sge---a-simple-interactive-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SGE - a simple interactive example</h3>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ClusterManagers

julia<span class="pl-k">&gt;</span> ClusterManagers<span class="pl-k">.</span><span class="pl-c1">addprocs_sge</span>(<span class="pl-c1">5</span>)
job id is <span class="pl-c1">961</span>, waiting <span class="pl-k">for</span> job to start .
<span class="pl-c1">5</span><span class="pl-k">-</span>element Array{Any,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
<span class="pl-c1">2</span>
<span class="pl-c1">3</span>
<span class="pl-c1">4</span>
<span class="pl-c1">5</span>
<span class="pl-c1">6</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@parallel</span> <span class="pl-k">for</span> i<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5</span>
       <span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds">`</span>hostname<span class="pl-pds">`</span></span>)
       <span class="pl-k">end</span>

julia<span class="pl-k">&gt;</span>  From worker <span class="pl-c1">2</span><span class="pl-k">:</span>  compute<span class="pl-k">-</span><span class="pl-c1">6</span>
        From worker <span class="pl-c1">4</span><span class="pl-k">:</span>  compute<span class="pl-k">-</span><span class="pl-c1">6</span>
        From worker <span class="pl-c1">5</span><span class="pl-k">:</span>  compute<span class="pl-k">-</span><span class="pl-c1">6</span>
        From worker <span class="pl-c1">6</span><span class="pl-k">:</span>  compute<span class="pl-k">-</span><span class="pl-c1">6</span>
        From worker <span class="pl-c1">3</span><span class="pl-k">:</span>  compute<span class="pl-k">-</span><span class="pl-c1">6</span></pre></div>
<h3><a id="user-content-sge---an-example-with-resource-list" class="anchor" aria-hidden="true" href="#sge---an-example-with-resource-list"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SGE - an example with resource list</h3>
<p>Some clusters require the user to specify a list of required resources. For example, it may be necessary to specify how much memory will be needed by the job - see this <a href="https://github.com/JuliaLang/julia/issues/10390">issue</a>.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> ClusterManagers

julia<span class="pl-k">&gt;</span> <span class="pl-c1">addprocs_sge</span>(<span class="pl-c1">5</span>,res_list<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>h_vmem=4G,tmem=4G<span class="pl-pds">"</span></span>)
job id is <span class="pl-c1">9827051</span>, waiting <span class="pl-k">for</span> job to start <span class="pl-k">........</span>
<span class="pl-c1">5</span><span class="pl-k">-</span>element Array{Int64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">22</span>
 <span class="pl-c1">23</span>
 <span class="pl-c1">24</span>
 <span class="pl-c1">25</span>
 <span class="pl-c1">26</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">pmap</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds">`</span>hostname<span class="pl-pds">`</span></span>),<span class="pl-c1">workers</span>());

julia<span class="pl-k">&gt;</span>  From worker <span class="pl-c1">26</span><span class="pl-k">:</span> lum<span class="pl-k">-</span><span class="pl-c1">7</span><span class="pl-k">-</span><span class="pl-c1">2.</span><span class="pl-k">local</span>
        From worker <span class="pl-c1">23</span><span class="pl-k">:</span> pace<span class="pl-k">-</span><span class="pl-c1">6</span><span class="pl-k">-</span><span class="pl-c1">10.</span><span class="pl-k">local</span>
        From worker <span class="pl-c1">22</span><span class="pl-k">:</span> chong<span class="pl-k">-</span><span class="pl-c1">207</span><span class="pl-k">-</span><span class="pl-c1">10.</span><span class="pl-k">local</span>
        From worker <span class="pl-c1">24</span><span class="pl-k">:</span> pace<span class="pl-k">-</span><span class="pl-c1">6</span><span class="pl-k">-</span><span class="pl-c1">11.</span><span class="pl-k">local</span>
        From worker <span class="pl-c1">25</span><span class="pl-k">:</span> cheech<span class="pl-k">-</span><span class="pl-c1">207</span><span class="pl-k">-</span><span class="pl-c1">16.</span><span class="pl-k">local</span></pre></div>
<h3><a id="user-content-sge-via-qrsh" class="anchor" aria-hidden="true" href="#sge-via-qrsh"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SGE via qrsh</h3>
<p><code>SGEManager</code> uses SGE's <code>qsub</code> command to launch workers, which communicate the
TCP/IP host:port info back to the master via the filesystem.  On filesystems
that are tuned to make heavy use of caching to increase throughput, launching
Julia workers can frequently timeout waiting for the standard output files to appear.
In this case, it's better to use the <code>QRSHManager</code>, which uses SGE's <code>qrsh</code>
command to bypass the filesystem and captures STDOUT directly.</p>
<h3><a id="user-content-load-sharing-facility-lsf" class="anchor" aria-hidden="true" href="#load-sharing-facility-lsf"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load Sharing Facility (LSF)</h3>
<p><code>LSFManager</code> supports IBM's scheduler.  Similar to <code>QRSHManager</code> in that it
uses the <code>-I</code> (i.e. interactive) flag to <code>bsub</code>.</p>
<h3><a id="user-content-using-localaffinitymanager-for-pinning-local-workers-to-specific-cores" class="anchor" aria-hidden="true" href="#using-localaffinitymanager-for-pinning-local-workers-to-specific-cores"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using <code>LocalAffinityManager</code> (for pinning local workers to specific cores)</h3>
<ul>
<li>Linux only feature.</li>
<li>Requires the Linux <code>taskset</code> command to be installed.</li>
<li>Usage : <code>addprocs(LocalAffinityManager(;np=CPU_CORES, mode::AffinityMode=BALANCED, affinities=[]); kwargs...)</code>.</li>
</ul>
<p>where</p>
<ul>
<li><code>np</code> is the number of workers to be started.</li>
<li><code>affinities</code>, if specified, is a list of CPU IDs. As many workers as entries in <code>affinities</code> are launched. Each worker is pinned
to the specified CPU ID.</li>
<li><code>mode</code> (used only when <code>affinities</code> is not specified, can be either <code>COMPACT</code> or <code>BALANCED</code>) - <code>COMPACT</code> results in the requested number
of workers pinned to cores in increasing order, For example, worker1 =&gt; CPU0, worker2 =&gt; CPU1 and so on. <code>BALANCED</code> tries to spread
the workers. Useful when we have multiple CPU sockets, with each socket having multiple cores. A <code>BALANCED</code> mode results in workers
spread across CPU sockets. Default is <code>BALANCED</code>.</li>
</ul>
<h3><a id="user-content-using-elasticmanager-dynamically-adding-workers-to-a-cluster" class="anchor" aria-hidden="true" href="#using-elasticmanager-dynamically-adding-workers-to-a-cluster"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using <code>ElasticManager</code> (dynamically adding workers to a cluster)</h3>
<p>The <code>ElasticManager</code> is useful in scenarios where we want to dynamically add workers to a cluster.
It achieves this by listening on a known port on the master. The launched workers connect to this
port and publish their own host/port information for other workers to connect to.</p>
<h5><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h5>
<p>On the master, you need to instantiate an instance of <code>ElasticManager</code>. The constructors defined are:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">ElasticManager</span>(;addr<span class="pl-k">=</span><span class="pl-c1">IPv4</span>(<span class="pl-s"><span class="pl-pds">"</span>127.0.0.1<span class="pl-pds">"</span></span>), port<span class="pl-k">=</span><span class="pl-c1">9009</span>, cookie<span class="pl-k">=</span><span class="pl-c1">nothing</span>, topology<span class="pl-k">=</span><span class="pl-c1">:all_to_all</span>)
<span class="pl-en">ElasticManager</span>(port) <span class="pl-k">=</span> <span class="pl-c1">ElasticManager</span>(;port<span class="pl-k">=</span>port)
<span class="pl-en">ElasticManager</span>(addr, port) <span class="pl-k">=</span> <span class="pl-c1">ElasticManager</span>(;addr<span class="pl-k">=</span>addr, port<span class="pl-k">=</span>port)
<span class="pl-en">ElasticManager</span>(addr, port, cookie) <span class="pl-k">=</span> <span class="pl-c1">ElasticManager</span>(;addr<span class="pl-k">=</span>addr, port<span class="pl-k">=</span>port, cookie<span class="pl-k">=</span>cookie)</pre></div>
<p>On the worker, you need to call <code>ClusterManagers.elastic_worker</code> with the addr/port that the master
is listening on and the same cookie. <code>elastic_worker</code> is defined as:</p>
<pre><code>ClusterManagers.elastic_worker(cookie, addr="127.0.0.1", port=9009; stdout_to_master=true)
</code></pre>
<p>For example, on the master:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ClusterManagers
em<span class="pl-k">=</span><span class="pl-c1">ElasticManager</span>(cookie<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>foobar<span class="pl-pds">"</span></span>)</pre></div>
<p>and launch each worker locally as
<code>echo "using ClusterManagers; ClusterManagers.elastic_worker(\"foobar\")" | julia  &amp;</code></p>
<p>or if you want a REPL on the worker, you can start a julia process normally and manually enter</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ClusterManagers
<span class="pl-c1">@schedule</span> ClusterManagers<span class="pl-k">.</span><span class="pl-c1">elastic_worker</span>(<span class="pl-s"><span class="pl-pds">"</span>foobar<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>addr_of_master<span class="pl-pds">"</span></span>, port_of_master; stdout_to_master<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p>The above will yield back the REPL prompt and also display any printed output locally.</p>
</article></div>