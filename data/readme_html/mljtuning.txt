<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-mljtuning" class="anchor" aria-hidden="true" href="#mljtuning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MLJTuning</h1>
<p>Hyperparameter optimization for
<a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine
learning models.</p>
<p><a href="https://travis-ci.com/alan-turing-institute/MLJTuning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/cd6db5adecb8abb05d61bba92b3d42fbee86c6c9/68747470733a2f2f7472617669732d63692e636f6d2f616c616e2d747572696e672d696e737469747574652f4d4c4a54756e696e672e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/alan-turing-institute/MLJTuning.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/alan-turing-institute/MLJTuning.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/3750554e827975d650a771168055228251c00ca7/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f616c616e2d747572696e672d696e737469747574652f4d4c4a54756e696e672e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/alan-turing-institute/MLJTuning.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<h3><a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contents</h3>
<ul>
<li><a href="#who-is-this-repo-for">Who is this repo for?</a></li>
<li><a href="#what-is-provided-here">What's provided here?</a></li>
<li><a href="#How-do-I-implement-a-new-tuning-strategy">How do I implement a new tuning strategy?</a></li>
</ul>
<p><em>Note:</em> This component of the <a href="https://github.com/alan-turing-institute/MLJ.jl#the-mlj-universe">MLJ
stack</a>
applies to MLJ versions 0.8.0 and higher. Prior to 0.8.0, tuning
algorithms resided in
<a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a>.</p>
<h2><a id="user-content-who-is-this-repo-for" class="anchor" aria-hidden="true" href="#who-is-this-repo-for"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Who is this repo for?</h2>
<p>This repository is not intended to be directly imported by the general
MLJ user. Rather, MLJTuning is a dependency of the
<a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> machine
learning platform, which allows MLJ users to perform a variety of
hyperparameter optimization tasks from there.</p>
<p>MLJTuning is the place for developers to integrate hyperparameter
optimization algorithms (here called <em>tuning strategies</em>) into MLJ,
either by adding code to <a href="/src/strategies">/src/strategies</a>, or by
importing MLJTuning into a third-pary package and implementing
MLJTuning's <a href="#implementing-a-new-tuning-strategy">tuning strategy
interface</a>.</p>
<p>MLJTuning is a component of the <a href="https://github.com/alan-turing-institute/MLJ.jl#the-mlj-universe">MLJ
stack</a>
which does not have
<a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels</a>
as a dependency (no ability to search and load registered MLJ
models). It does however depend on
<a href="https://github.com/alan-turing-institute/MLJBase.jl">MLJBase</a> and,
in particular, on the resampling functionality currently residing
there.</p>
<h2><a id="user-content-what-is-provided-here" class="anchor" aria-hidden="true" href="#what-is-provided-here"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is provided here?</h2>
<p>This repository contains:</p>
<ul>
<li>
<p>a <strong>tuning wrapper</strong> called <code>TunedModel</code> for transforming arbitrary
MLJ models into "self-tuning" ones - that is, into models which,
when fit, automatically optimize a specified subset of the original
hyperparameters (using cross-validation or other resampling
strategy) before training the optimal model on all supplied data</p>
</li>
<li>
<p>an abstract <strong><a href="(#implementing-a-new-tuning-strategy)">tuning strategy
interface</a></strong> to allow
developers to conveniently implement common hyperparameter
optimization strategies, such as:</p>
<ul class="contains-task-list">
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> search models generated by an arbitrary iterator, eg <code>models = [model1, model2, ...]</code> (built-in <code>Explicit</code> strategy)</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> grid search (built-in <code>Grid</code> strategy)</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> Latin hypercubes</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> random search (built-in <code>RandomSearch</code> strategy)</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> bandit</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> simulated annealing</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> Bayesian optimization using Gaussian processes</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> structured tree Parzen estimators</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> multi-objective (Pareto) optimization</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> genetic algorithms</p>
</li>
<li class="task-list-item">
<p><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> AD-powered gradient descent methods</p>
</li>
</ul>
</li>
<li>
<p>a selection of <strong>implementations</strong> of the tuning strategy interface,
currently all those accessible from
<a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> itself.</p>
</li>
<li>
<p>the code defining the MLJ functions <code>learning_curves!</code> and <code>learning_curve</code> as
these are essentially one-dimensional grid searches</p>
</li>
</ul>
<h2><a id="user-content-how-do-i-implement-a-new-tuning-strategy" class="anchor" aria-hidden="true" href="#how-do-i-implement-a-new-tuning-strategy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How do I implement a new tuning strategy?</h2>
<p>This document assumes familiarity with the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/" rel="nofollow">Evaluating Model
Performance</a>
and <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/" rel="nofollow">Performance
Measures</a>
sections of the MLJ manual. Tuning itself, from the user's
perspective, is described in <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/" rel="nofollow">Tuning
Models</a>.</p>
<h3><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overview</h3>
<p>What follows is an overview of tuning in MLJ. After the overview is an
elaboration on those terms given in <em>italics</em>.</p>
<p>All tuning in MLJ is conceptualized as an iterative procedure, each
iteration corresponding to a performance <em>evaluation</em> of a single
<em>model</em>. Each such model is a mutated clone of a fixed prototype. In the
general case, this prototype is a composite model, i.e., a model with
other models as hyperparameters, and while the type of the prototype
mutations is fixed, the types of the sub-models are allowed to vary.</p>
<p>When all iterations of the algorithm are complete, the optimal model
is selected based entirely on a <em>history</em> generated according to the
specified <em>tuning strategy</em>. Iterations are generally performed in
batches, which are evaluated in parallel (sequential tuning strategies
degenerating into semi-sequential strategies, unless the batch size is
one). At the beginning of each batch, both the history and an internal
<em>state</em> object are consulted, and, on the basis of the tuning
strategy, a new batch of models to be evaluated is generated. On the
basis of these evaluations, and the strategy, the history and internal
state are updated.</p>
<p>The tuning algorithm initializes the state object before iterations
begin, on the basis of the specific strategy and a user-specified
<em>range</em> object.</p>
<ul>
<li>
<p>Recall that in MLJ a <em>model</em> is an object storing the
hyperparameters of some learning algorithm indicated by the name of
the model type (e.g., <code>DecisionTreeRegressor</code>). Models do not
store learned parameters.</p>
</li>
<li>
<p>An <em>evaluation</em> is the value returned by some call to the
<code>evaluate!</code> method, when passed the resampling strategy (e.g.,
<code>CV(nfolds=9)</code> and performance measures specified by the user when
specifying the tuning task (e.g., <code>cross_entropy</code>b,
<code>accuracy</code>). Recall that such a value is a named tuple of vectors
with keys <code>measure</code>, <code>measurement</code>, <code>per_fold</code>, and
<code>per_observation</code>. See <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/" rel="nofollow">Evaluating Model
Performance</a>
for details. Recall also that some measures in MLJ (e.g.,
<code>cross_entropy</code>) report a loss (or score) for each provided
observation, while others (e.g., <code>auc</code>) report only an aggregated
value (the <code>per_observation</code> entries being recorded as
<code>missing</code>). This and other behavior can be inspected using trait
functions. Do <code>info(rms)</code> to view the trait values for the <code>rms</code>
loss, for example, and see <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/" rel="nofollow">Performance
measures</a>
for details.</p>
</li>
<li>
<p>The <em>history</em> is a vector of tuples of the form <code>(m, r)</code> generated
by the tuning algorithm - one tuple per iteration - where <code>m</code> is a
model instance that has been evaluated, and <code>r</code> (called the
<em>result</em>) contains three kinds of information: (i) whatever parts of
the evaluation needed to determine the optimal model; (ii)
additional user-inspectable statistics that may be of interest - for
example, evaluations of a measure (loss or score) different from one
being explicitly optimized; and (iii) any model "metadata" that a
tuning strategy implementation may need to be recorded for
generating the next batch of model candidates - for example an
implementation-specific representation of the model.</p>
</li>
<li>
<p>A <em>tuning strategy</em> is an instance of some subtype <code>S &lt;: TuningStrategy</code>, the name <code>S</code> (e.g., <code>Grid</code>) indicating the tuning
algorithm to be applied. The fields of the tuning strategy - called
<em>tuning hyperparameters</em> - are those tuning parameters specific to the
strategy that <strong>do not refer to specific models or specific model
hyperparameters</strong>. So, for example, a default resolution to be used
in a grid search is a hyperparameter of <code>Grid</code>, but the resolution
to be applied to a <em>specific</em> hyperparameter (such as the maximum
depth of a decision tree) is <strong>not</strong>. This latter parameter would be
part of the user-specified range object.</p>
</li>
<li>
<p>A <em>range</em> is any object whose specification completes the
specification of the tuning task, after the prototype, tuning
strategy, resampling strategy, performance measure(s), and total
iteration count are given - and is essentially the space of models
to be searched. This definition is intentionally broad and the
interface places no restriction on the allowed types of this
object. It may be generally viewed as the "space" of models being
searched <em>plus</em> strategy-specific data explaining how models from
that space are actually to be generated (e.g.,
hyperparameter-specific grid resolutions or probability
distributions). For more on range types see <a href="#range-types">Range
types</a> below.</p>
</li>
</ul>
<h3><a id="user-content-interface-points-for-user-input" class="anchor" aria-hidden="true" href="#interface-points-for-user-input"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Interface points for user input</h3>
<p>Recall, for context, that in MLJ tuning is implemented as a model
wrapper. A model is tuned by <em>fitting</em> the wrapped model to data
(which also trains the optimal model on all available data). To use
the optimal model one <em>predicts</em> using the wrapped model. For more
detail, see the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/" rel="nofollow">Tuning
Models</a>
section of the MLJ manual.</p>
<p>In setting up a tuning task, the user constructs an instance of the
<code>TunedModel</code> wrapper type, which has these principal fields:</p>
<ul>
<li>
<p><code>model</code>: the prototype model instance mutated during tuning (the
model being wrapped)</p>
</li>
<li>
<p><code>tuning</code>: the tuning strategy, an instance of a concrete
<code>TuningStrategy</code> subtype, such as <code>Grid</code></p>
</li>
<li>
<p><code>resampling</code>: the resampling strategy used for performance
evaluations, which must be an instance of a concrete
<code>ResamplingStrategy</code> subtype, such as <code>Holdout</code> or <code>CV</code></p>
</li>
<li>
<p><code>measure</code>: a measure (loss or score) or vector of measures available
to the tuning algorithm, the first of which is optimized in the
common case of single-objective tuning strategies</p>
</li>
<li>
<p><code>range</code>: as defined above - roughly, the space of models to be searched</p>
</li>
<li>
<p><code>n</code>: the number of iterations (number of distinct models to be
evaluated)</p>
</li>
<li>
<p><code>acceleration</code>: the computational resources to be applied (e.g.,
<code>CPUProcesses()</code> for distributed computing and <code>CPUThreads()</code> for
multi-threaded processing)</p>
</li>
<li>
<p><code>acceleration_resampling</code>: the computational resources to be applied
at the level of resampling (e.g., in cross-validation)</p>
</li>
</ul>
<h3><a id="user-content-implementation-requirements-for-new-tuning-strategies" class="anchor" aria-hidden="true" href="#implementation-requirements-for-new-tuning-strategies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementation requirements for new tuning strategies</h3>
<p>As sample implementations, see <a href="/src/strategies">/src/strategies/</a></p>
<h4><a id="user-content-summary-of-functions" class="anchor" aria-hidden="true" href="#summary-of-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Summary of functions</h4>
<p>Several functions are part of the tuning strategy API:</p>
<ul>
<li>
<p><code>setup</code>: for initialization of state (compulsory)</p>
</li>
<li>
<p><code>result</code>: for building each element of the history</p>
</li>
<li>
<p><code>models!</code>: for generating batches of new models and updating the
state (compulsory)</p>
</li>
<li>
<p><code>best</code>: for extracting the entry in the history corresponding to the
optimal model from the full history</p>
</li>
<li>
<p><code>tuning_report</code>: for selecting what to report to the user apart from
details on the optimal model</p>
</li>
<li>
<p><code>default_n</code>: to specify the total number of models to be evaluated when
<code>n</code> is not specified by the user</p>
</li>
</ul>
<p><strong>Important note on the history.</strong> The initialization and update of the
history is carried out internally, i.e., is not the responsibility of
the tuning strategy implementation. The history is always initialized to
<code>nothing</code>, rather than an empty vector.</p>
<p>The above functions are discussed further below, after discussing types.</p>
<h4><a id="user-content-the-tuning-strategy-type" class="anchor" aria-hidden="true" href="#the-tuning-strategy-type"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The tuning strategy type</h4>
<p>Each tuning algorithm must define a subtype of <code>TuningStrategy</code> whose
fields are the hyperparameters controlling the strategy that do not
directly refer to models or model hyperparameters. These would
include, for example, the default resolution of a grid search, or the
initial temperature in simulated annealing.</p>
<p>The algorithm implementation must include a keyword constructor with
defaults. Here's an example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">mutable struct</span> Grid <span class="pl-k">&lt;:</span> <span class="pl-c1">TuningStrategy</span>
	goal<span class="pl-k">::</span><span class="pl-c1">Union{Nothing,Int}</span>
	resolution<span class="pl-k">::</span><span class="pl-c1">Int</span>
	shuffle<span class="pl-k">::</span><span class="pl-c1">Bool</span>
	rng<span class="pl-k">::</span><span class="pl-c1">Random.AbstractRNG</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Constructor with keywords</span>
<span class="pl-c1">Grid</span>(; goal<span class="pl-k">=</span><span class="pl-c1">nothing</span>, resolution<span class="pl-k">=</span><span class="pl-c1">10</span>, shuffle<span class="pl-k">=</span><span class="pl-c1">true</span>,
	 rng<span class="pl-k">=</span>Random<span class="pl-k">.</span>GLOBAL_RNG) <span class="pl-k">=</span>
	<span class="pl-c1">Grid</span>(goal, resolution, MLJBase<span class="pl-k">.</span><span class="pl-c1">shuffle_and_rng</span>(shuffle, rng)<span class="pl-k">...</span>)</pre></div>
<h4><a id="user-content-range-types" class="anchor" aria-hidden="true" href="#range-types"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Range types</h4>
<p>Generally new types are defined for each class of range object a
tuning strategy should like to handle, and the tuning strategy
functions to be implemented are dispatched on these types. It is
recommended that every tuning strategy support at least these types:</p>
<ul>
<li>
<p>one-dimensional ranges <code>r</code>, where <code>r</code> is a <code>MLJBase.ParamRange</code> instance</p>
</li>
<li>
<p>(optional) pairs of the form <code>(r, data)</code>, where <code>data</code> is metadata,
such as a resolution in a grid search, or a distribution in a random
search</p>
</li>
<li>
<p>abstract vectors whose elements are of the above form</p>
</li>
</ul>
<p>Recall that <code>ParamRange</code> has two concrete subtypes <code>NumericRange</code> and
<code>NominalRange</code>, whose instances are constructed with the <code>MLJBase</code>
extension to the <code>range</code> function.</p>
<p>Note in particular that a <code>NominalRange</code> has a <code>values</code> field, while
<code>NumericRange</code> has the fields <code>upper</code>, <code>lower</code>, <code>scale</code>, <code>unit</code> and
<code>origin</code>. The <code>unit</code> field specifies a preferred length scale, while
<code>origin</code> a preferred "central value". These default to <code>(upper - lower)/2</code> and <code>(upper + lower)/2</code>, respectively, in the bounded case
(neither <code>upper = Inf</code> nor <code>lower = -Inf</code>). The fields <code>origin</code> and
<code>unit</code> are used in generating grids or fitting probability
distributions to unbounded ranges.</p>
<p>A <code>ParamRange</code> object is always associated with the name of a
hyperparameter (a field of the prototype in the context of tuning)
which is recorded in its <code>field</code> attribute, a <code>Symbol</code>, but for
composite models this might be a be an <code>Expr</code>, such as
<code>:(atom.max_depth)</code>.</p>
<p>Use the <code>iterator</code> and <code>sampler</code> methods to convert ranges into
one-dimensional grids or for random sampling, respectively. See the
<a href="https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/#API-1" rel="nofollow">tuning
section</a>
of the MLJ manual or doc-strings for more on these methods and the
<code>Grid</code> and <code>RandomSearch</code> implementations.</p>
<h4><a id="user-content-the-result-method-for-building-each-entry-of-the-history" class="anchor" aria-hidden="true" href="#the-result-method-for-building-each-entry-of-the-history"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The <code>result</code> method: For building each entry of the history</h4>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-c1">result</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, history, state, e, metadata)</pre></div>
<p>This method is for constructing the result object <code>r</code> in each tuple
<code>(m, r)</code> written to the history. Here <code>e</code> is the evaluation of the
model <code>m</code> (as returned by a call to <code>evaluation!</code>) and <code>metadata</code> is
any metadata associated with <code>m</code> when this is included in the output
of <code>models!</code> (see below), and <code>nothing</code> otherwise. The value of <code>r</code> is
also allowed to depend on previous events in the history. The fallback
is:</p>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-en">result</span>(tuning, history, state, e, metadata) <span class="pl-k">=</span> (measure<span class="pl-k">=</span>e<span class="pl-k">.</span>measure, measurement<span class="pl-k">=</span>e<span class="pl-k">.</span>measurement)</pre></div>
<p>Note in this case that the result is always a named tuple of
<em>vectors</em>, since multiple measures can be specified (and singleton
measures provided by the user are promoted to vectors with a
single element).</p>
<p>The history must contain everything needed for the <code>best</code> method to
determine the optimal model, and everything needed by the
<code>report_history</code> method, which generates a report on tuning to the
user (for use in visualization, for example). These methods are
detailed below.</p>
<h4><a id="user-content-the-setup-method-to-initialize-state" class="anchor" aria-hidden="true" href="#the-setup-method-to-initialize-state"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The <code>setup</code> method: To initialize state</h4>
<div class="highlight highlight-source-julia"><pre>state <span class="pl-k">=</span> <span class="pl-c1">setup</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, model, range, verbosity)</pre></div>
<p>The <code>setup</code> function is for initializing the <code>state</code> of the tuning
algorithm (available to the <code>models!</code> method). Be sure to make this
object mutable if it needs to be updated by the <code>models!</code> method.</p>
<p>The <code>state</code> is a place to record the outcomes of any necessary
intialization of the tuning algorithm (performed by <code>setup</code>) and a
place for the <code>models!</code> method to save and read transient information
that does not need to be recorded in the history.</p>
<p>The <code>setup</code> function is called once only, when a <code>TunedModel</code> machine
is <code>fit!</code> the first time, and not on subsequent calls (unless
<code>force=true</code>). (Specifically, <code>MLJBase.fit(::TunedModel, ...)</code> calls
<code>setup</code> but <code>MLJBase.update(::TunedModel, ...)</code> does not.)</p>
<p>The <code>setup</code> function is called once only, when a <code>TunedModel</code> machine
is <code>fit!</code> the first time, and not on subsequent calls (unless
<code>force=true</code>). (Specifically, <code>MLJBase.fit(::TunedModel, ...)</code> calls
<code>setup</code> but <code>MLJBase.update(::TunedModel, ...)</code> does not.)</p>
<p>The <code>verbosity</code> is an integer indicating the level of logging: <code>0</code>
means logging should be restricted to warnings, <code>-1</code>, means completely
silent.</p>
<p>The fallback for <code>setup</code> is:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">setup</span>(tuning<span class="pl-k">::</span><span class="pl-c1">TuningStrategy</span>, model, range, verbosity) <span class="pl-k">=</span> range</pre></div>
<p>However, a tuning strategy will generally want to implement a <code>setup</code>
method for each range type it is going to support:</p>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-en">setup</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, model, range<span class="pl-k">::</span><span class="pl-c1">RangeType1</span>, verbosity) <span class="pl-k">=</span> <span class="pl-k">...</span>
MLJTuning<span class="pl-k">.</span><span class="pl-en">setup</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, model, range<span class="pl-k">::</span><span class="pl-c1">RangeType2</span>, verbosity) <span class="pl-k">=</span> <span class="pl-k">...</span>
etc.</pre></div>
<h4><a id="user-content-the-models-method-for-generating-model-batches-to-evaluate" class="anchor" aria-hidden="true" href="#the-models-method-for-generating-model-batches-to-evaluate"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The <code>models!</code> method: For generating model batches to evaluate</h4>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-c1">models!</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, model, history, state, verbosity)</pre></div>
<p>This is the core method of a new implementation. Given the existing
<code>history</code> and <code>state</code>, it must return a vector ("batch") of <em>new</em>
model instances to be evaluated. Any number of models can be returned
(and this includes an empty vector or <code>nothing</code>, if models have been
exhausted) and the evaluations will be performed in parallel (using
the mode of parallelization defined by the <code>acceleration</code> field of the
<code>TunedModel</code> instance). <em>An update of the history, performed
automatically under the hood, only occurs after these evaluations.</em></p>
<p>Most sequential tuning strategies will want include the batch size as
a hyperparameter, which we suggest they call <code>batch_size</code>, but this
field is not part of the tuning interface. In tuning, whatever models
are returned by <code>models!</code> get evaluated in parallel.</p>
<p>In a <code>Grid</code> tuning strategy, for example, <code>models!</code> returns a random
selection of <code>n - length(history)</code> models from the grid, so that
<code>models!</code> is called only once (in each call to
<code>MLJBase.fit(::TunedModel, ...)</code> or <code>MLJBase.update(::TunedModel, ...)</code>). In a bona fide sequential method which is generating models
non-deterministically (such as simulated annealing), <code>models!</code> might
return a single model, or return a small batch of models to make use
of parallelization (the method becoming "semi-sequential" in that
case).</p>
<h5><a id="user-content-including-model-metadata" class="anchor" aria-hidden="true" href="#including-model-metadata"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Including model metadata</h5>
<p>If a tuning strategy implementation needs to pass additional
"metadata" along with each model, to be passed to <code>result</code> for
recording in the history, then instead of model instances, <code>models!</code>
should returne a vector of <em>tuples</em> of the form <code>(m, metadata)</code>, where
<code>m</code> is a model instance, and <code>metadata</code> the associated data. See the
discussion above on <code>result</code>.</p>
<p>If the tuning algorithm exhausts it's supply of new models (because,
for example, there is only a finite supply) then <code>models!</code> should
return an empty vector or <code>nothing</code>. Under the hood, there is no fixed
"batch-size" parameter, and the tuning algorithm is happy to receive
any number of models. If <code>models!</code> returns a number of models
exceeding the number needed to complete the history, the list returned
is simply truncated.</p>
<p>Some simple tuning strategies, such as <code>RandomSearch</code>, will want to
return as many models as possible in one hit. The argument
<code>n_remaining</code> is the difference between the current length of the
history and the target number of iterations <code>tuned_model.n</code> set by the
user when constructing his <code>TunedModel</code> instance, <code>tuned_model</code> (or
<code>default_n(tuning, range)</code> if left unspecified).</p>
<h4><a id="user-content-the-best-method-to-define-what-constitutes-the-optimal-model" class="anchor" aria-hidden="true" href="#the-best-method-to-define-what-constitutes-the-optimal-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The <code>best</code> method: To define what constitutes the "optimal model"</h4>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-c1">best</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, history)</pre></div>
<p>Returns the entry <code>(best_model, r)</code> from the history corresponding to
the optimal model <code>best_model</code>.</p>
<p>A fallback whose definition is given below may be used, <em>provided the
fallback for <code>result</code> detailed above has not been overloaded</em>. In this
fallback for <code>best</code>, the best model is the one optimizing performance
estimates for the first measure in the <code>TunedModel</code> field <code>measure</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">function</span> <span class="pl-en">best</span>(tuning<span class="pl-k">::</span><span class="pl-c1">TuningStrategy</span>, history)
   measurements <span class="pl-k">=</span> [h[<span class="pl-c1">2</span>]<span class="pl-k">.</span>measurement[<span class="pl-c1">1</span>] <span class="pl-k">for</span> h <span class="pl-k">in</span> history]
   measure <span class="pl-k">=</span> <span class="pl-c1">first</span>(history)[<span class="pl-c1">2</span>]<span class="pl-k">.</span>measure[<span class="pl-c1">1</span>]
   <span class="pl-k">if</span> <span class="pl-c1">orientation</span>(measure) <span class="pl-k">==</span> <span class="pl-c1">:score</span>
	   measurements <span class="pl-k">=</span> <span class="pl-k">-</span>measurements
   <span class="pl-k">end</span>
   best_index <span class="pl-k">=</span> <span class="pl-c1">argmin</span>(measurements)
   <span class="pl-k">return</span> history[best_index]
<span class="pl-k">end</span></pre></div>
<h4><a id="user-content-the-tuning_report-method-to-build-the-user-accessible-report" class="anchor" aria-hidden="true" href="#the-tuning_report-method-to-build-the-user-accessible-report"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The <code>tuning_report</code> method: To build the user-accessible report</h4>
<p>As with any model, fitting a <code>TunedModel</code> instance generates a
user-accessible report. In the case of tuning, the report is
constructed with this code:</p>
<div class="highlight highlight-source-julia"><pre>report <span class="pl-k">=</span> <span class="pl-c1">merge</span>((best_model<span class="pl-k">=</span>best_model, best_result<span class="pl-k">=</span>best_result, best_report<span class="pl-k">=</span>best_report,),
				<span class="pl-c1">tuning_report</span>(tuning, history, state))</pre></div>
<p>where:</p>
<ul>
<li>
<p><code>best_model</code> is the optimal model instance</p>
</li>
<li>
<p><code>best_result</code> is the corresponding "result" entry in the history (e.g., performance evaluation)</p>
</li>
<li>
<p><code>best_report</code> is the report generated by fitting the optimal
model</p>
</li>
<li>
<p><code>tuning_report(::MyTuningStrategy, ...)</code> is a method the implementer
may overload. It should return a named tuple with <code>history</code> as one
of the keys (the format up to the implementation.) The fallback is
to return the raw history:</p>
</li>
</ul>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-en">tuning_report</span>(tuning, history, state) <span class="pl-k">=</span> (history<span class="pl-k">=</span>history,)</pre></div>
<h4><a id="user-content-the-default_n-method-for-declaring-the-default-number-of-iterations" class="anchor" aria-hidden="true" href="#the-default_n-method-for-declaring-the-default-number-of-iterations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The <code>default_n</code> method: For declaring the default number of iterations</h4>
<div class="highlight highlight-source-julia"><pre>MLJTuning<span class="pl-k">.</span><span class="pl-c1">default_n</span>(tuning<span class="pl-k">::</span><span class="pl-c1">MyTuningStrategy</span>, range)</pre></div>
<p>The <code>models!</code> method (which is allowed to return multiple models) is
called until one of the following occurs:</p>
<ul>
<li>
<p>The length of the history matches the number of iterations specified
by the user, namely <code>tuned_model.n</code> where <code>tuned_model</code> is the user's
<code>TunedModel</code> instance. If <code>tuned_model.n</code> is <code>nothing</code> (because the
user has not specified a value) then <code>default_n(tuning, range)</code> is
used instead.</p>
</li>
<li>
<p><code>models!</code> returns an empty list or <code>nothing</code>.</p>
</li>
</ul>
<p>The fallback is</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-en">default_n</span>(tuning<span class="pl-k">::</span><span class="pl-c1">TuningStrategy</span>, range) <span class="pl-k">=</span> DEFAULT_N</pre></div>
<p>where <code>DEFAULT_N</code> is a global constant. Do <code>using MLJTuning;  MLJTuning.DEFAULT_N</code> to see check the current value.</p>
<h3><a id="user-content-implementation-example-search-through-an-explicit-list" class="anchor" aria-hidden="true" href="#implementation-example-search-through-an-explicit-list"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementation example: Search through an explicit list</h3>
<p>The most rudimentary tuning strategy just evaluates every model
generated by some iterator, such iterators constituting the only kind
of supported range. The models generated must all have a common type
and, in th implementation below, the type information is conveyed by
the specified prototype <code>model</code> (which is otherwise ignored).  The
fallback implementations for <code>result</code>, <code>best</code> and <code>report_history</code>
suffice.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">mutable struct</span> Explicit <span class="pl-k">&lt;:</span> <span class="pl-c1">TuningStrategy</span> <span class="pl-k">end</span>

<span class="pl-k">mutable struct</span> ExplicitState{R,N}
    range<span class="pl-k">::</span><span class="pl-c1">R</span>
    next<span class="pl-k">::</span><span class="pl-c1">Union{Nothing,N}</span> <span class="pl-c"><span class="pl-c">#</span> to hold output of `iterate(range)`</span>
<span class="pl-k">end</span>

<span class="pl-en">ExplicitState</span>(r<span class="pl-k">::</span><span class="pl-c1">R</span>, <span class="pl-k">::</span><span class="pl-c1">Nothing</span>) <span class="pl-k">where</span> R <span class="pl-k">=</span> <span class="pl-c1">ExplicitState</span><span class="pl-c1">{R,Nothing}</span>(r,<span class="pl-c1">nothing</span>)
<span class="pl-en">ExplictState</span>(r<span class="pl-k">::</span><span class="pl-c1">R</span>, n<span class="pl-k">::</span><span class="pl-c1">N</span>) <span class="pl-k">where</span> {R,N} <span class="pl-k">=</span> <span class="pl-c1">ExplicitState</span><span class="pl-c1">{R,Union{Nothing,N}}</span>(r,n)

<span class="pl-k">function</span> MLJTuning<span class="pl-k">.</span><span class="pl-en">setup</span>(tuning<span class="pl-k">::</span><span class="pl-c1">Explicit</span>, model, range, verbosity)
    next <span class="pl-k">=</span> <span class="pl-c1">iterate</span>(range)
    <span class="pl-k">return</span> <span class="pl-c1">ExplicitState</span>(range, next)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> models! returns all available models in the range at once:</span>
<span class="pl-k">function</span> MLJTuning<span class="pl-k">.</span><span class="pl-en">models!</span>(tuning<span class="pl-k">::</span><span class="pl-c1">Explicit</span>,
                           model,
                           history,
                           state,
                           n_remaining,
                           verbosity)

    range, next  <span class="pl-k">=</span> state<span class="pl-k">.</span>range, state<span class="pl-k">.</span>next

    next <span class="pl-k">===</span> <span class="pl-c1">nothing</span> <span class="pl-k">&amp;&amp;</span> <span class="pl-k">return</span> <span class="pl-c1">nothing</span>

    m, s <span class="pl-k">=</span> next
    models <span class="pl-k">=</span> [m, ]

    next <span class="pl-k">=</span> <span class="pl-c1">iterate</span>(range, s)

    i <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-c"><span class="pl-c">#</span> current length of `models`</span>
    <span class="pl-k">while</span> i <span class="pl-k">&lt;</span> n_remaining
        next <span class="pl-k">===</span> <span class="pl-c1">nothing</span> <span class="pl-k">&amp;&amp;</span> <span class="pl-k">break</span>
        m, s <span class="pl-k">=</span> next
        <span class="pl-c1">push!</span>(models, m)
        i <span class="pl-k">+=</span> <span class="pl-c1">1</span>
        next <span class="pl-k">=</span> <span class="pl-c1">iterate</span>(range, s)
    <span class="pl-k">end</span>

    state<span class="pl-k">.</span>next <span class="pl-k">=</span> next

    <span class="pl-k">return</span> models

<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">default_n</span>(tuning<span class="pl-k">::</span><span class="pl-c1">Explicit</span>, range)
    <span class="pl-k">try</span>
        <span class="pl-c1">length</span>(range)
    <span class="pl-k">catch</span> MethodError
        DEFAULT_N
    <span class="pl-k">end</span>
<span class="pl-k">end</span>
</pre></div>
<p>For slightly less trivial example, see
<a href="/src/strategies/grid.jl">/src/strategies/grid.jl</a></p>
</article></div>