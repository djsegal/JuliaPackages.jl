<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-subsetselection" class="anchor" aria-hidden="true" href="#subsetselection"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SubsetSelection</h1>
<p><a href="http://pkg.julialang.org/?pkg=SubsetSelection" rel="nofollow"><img src="https://camo.githubusercontent.com/91ef48104cc3c872f4d697fdefc091a40d56d8fd/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f53756273657453656c656374696f6e5f302e362e737667" alt="0.6" data-canonical-src="http://pkg.julialang.org/badges/SubsetSelection_0.6.svg" style="max-width:100%;"></a></p>
<p>SubsetSelection is a Julia package that computes sparse L2-regularized estimators. Sparsity is enforced through explicit cardinality constraint or L0-penalty. Supported loss functions for regression are least squares, L1 and L2 SVR; for classification, logistic, L1 and L2 Hinge loss. The algorithm formulates the problem as a mixed-integer saddle-point problem and solves its boolean relaxation using a dual sub-gradient approach.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quick start</h2>
<p>To install the package:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">install</span>(<span class="pl-s"><span class="pl-pds">"</span>SubsetSelection<span class="pl-pds">"</span></span>)</pre></div>
<p>or the have the latest version</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">clone</span>(<span class="pl-s"><span class="pl-pds">"</span>git://github.com/jeanpauphilet/SubsetSelection.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<p>To fit a basic model:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> SubsetSelection, StatsBase

julia<span class="pl-k">&gt;</span> n <span class="pl-k">=</span> <span class="pl-c1">100</span>; p <span class="pl-k">=</span> <span class="pl-c1">10000</span>; k <span class="pl-k">=</span> <span class="pl-c1">10</span>;
julia<span class="pl-k">&gt;</span> indices <span class="pl-k">=</span> <span class="pl-c1">sort</span>(<span class="pl-c1">sample</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>p, StatsBase<span class="pl-k">.</span><span class="pl-c1">Weights</span>(<span class="pl-c1">ones</span>(p)<span class="pl-k">/</span>p), k, replace<span class="pl-k">=</span><span class="pl-c1">false</span>));
julia<span class="pl-k">&gt;</span> w <span class="pl-k">=</span> <span class="pl-c1">sample</span>(<span class="pl-k">-</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">1</span>, k);
julia<span class="pl-k">&gt;</span> X <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n,p); Y <span class="pl-k">=</span> X[:,indices]<span class="pl-k">*</span>w;
julia<span class="pl-k">&gt;</span> Sparse_Regressor <span class="pl-k">=</span> <span class="pl-c1">subsetSelection</span>(<span class="pl-c1">OLS</span>(), <span class="pl-c1">Constraint</span>(k), Y, X)
SubsetSelection<span class="pl-k">.</span><span class="pl-c1">SparseEstimator</span>(SubsetSelection<span class="pl-k">.</span><span class="pl-c1">OLS</span>(),SubsetSelection<span class="pl-k">.</span><span class="pl-c1">Constraint</span>(<span class="pl-c1">10</span>),<span class="pl-c1">10.0</span>,[<span class="pl-c1">362</span>,<span class="pl-c1">1548</span>,<span class="pl-c1">2361</span>,<span class="pl-c1">3263</span>,<span class="pl-c1">3369</span>,<span class="pl-c1">3598</span>,<span class="pl-c1">5221</span>,<span class="pl-c1">7314</span>,<span class="pl-c1">7748</span>,<span class="pl-c1">9267</span>],[<span class="pl-c1">5.37997</span>,<span class="pl-k">-</span><span class="pl-c1">5.51019</span>,<span class="pl-k">-</span><span class="pl-c1">5.77256</span>,<span class="pl-k">-</span><span class="pl-c1">7.27197</span>,<span class="pl-k">-</span><span class="pl-c1">6.32432</span>,<span class="pl-k">-</span><span class="pl-c1">4.97585</span>,<span class="pl-c1">5.94814</span>,<span class="pl-c1">4.75648</span>,<span class="pl-c1">5.48098</span>,<span class="pl-k">-</span><span class="pl-c1">5.91967</span>],[<span class="pl-k">-</span><span class="pl-c1">0.224588</span>,<span class="pl-k">-</span><span class="pl-c1">1.1446</span>,<span class="pl-c1">2.81566</span>,<span class="pl-c1">0.582427</span>,<span class="pl-k">-</span><span class="pl-c1">0.923311</span>,<span class="pl-c1">4.1153</span>,<span class="pl-k">-</span><span class="pl-c1">2.43833</span>,<span class="pl-c1">0.117831</span>,<span class="pl-c1">0.0982258</span>,<span class="pl-k">-</span><span class="pl-c1">1.60631</span>  …  <span class="pl-c1">0.783925</span>,<span class="pl-k">-</span><span class="pl-c1">1.1055</span>,<span class="pl-c1">0.841752</span>,<span class="pl-k">-</span><span class="pl-c1">1.09645</span>,<span class="pl-k">-</span><span class="pl-c1">0.397962</span>,<span class="pl-c1">3.48083</span>,<span class="pl-k">-</span><span class="pl-c1">1.33903</span>,<span class="pl-c1">1.44676</span>,<span class="pl-c1">4.03583</span>,<span class="pl-c1">1.05817</span>],<span class="pl-c1">0.0</span>,<span class="pl-c1">19</span>)</pre></div>
<p>The algorithm returns a SparseEstimator object with the following fields: <code>loss</code> (loss function used), <code>sparsity</code> (model to enforce sparsity), <code>indices</code> (features selected), <code>w</code> (value of the estimator on the selected features only), <code>α</code> (values of the associated dual variables), <code>b</code> (bias term), <code>iter</code> (number of iterations required by the algorithm).</p>
<p>For instance, you can access the selected features directly in the <code>indices</code> field:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Sparse_Regressor<span class="pl-k">.</span>indices
<span class="pl-c1">10</span><span class="pl-k">-</span>element Array{Int64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
  <span class="pl-c1">362</span>
 <span class="pl-c1">1548</span>
 <span class="pl-c1">2361</span>
 <span class="pl-c1">3263</span>
 <span class="pl-c1">3369</span>
 <span class="pl-c1">3598</span>
 <span class="pl-c1">5221</span>
 <span class="pl-c1">7314</span>
 <span class="pl-c1">7748</span>
 <span class="pl-c1">9267</span></pre></div>
<p>or compute predictions</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Y_pred <span class="pl-k">=</span> X[:,Sparse_Regressor<span class="pl-k">.</span>indices]<span class="pl-k">*</span>Sparse_Regressor<span class="pl-k">.</span>w
<span class="pl-c1">100</span><span class="pl-k">-</span>element Array{Float64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
   <span class="pl-c1">4.62918</span>
   <span class="pl-c1">8.59952</span>
 <span class="pl-k">-</span><span class="pl-c1">16.2796</span>
  <span class="pl-k">-</span><span class="pl-c1">5.611</span>
   <span class="pl-c1">1.62764</span>
 <span class="pl-k">-</span><span class="pl-c1">50.4562</span>
  <span class="pl-c1">37.407</span>
 <span class="pl-k">-</span><span class="pl-c1">12.3341</span>
  <span class="pl-k">-</span><span class="pl-c1">4.75339</span>
  <span class="pl-c1">25.122</span>
   ⋮
  <span class="pl-k">-</span><span class="pl-c1">7.98349</span>
  <span class="pl-c1">11.0327</span>
  <span class="pl-k">-</span><span class="pl-c1">8.58172</span>
  <span class="pl-c1">16.904</span>
  <span class="pl-k">-</span><span class="pl-c1">9.04211</span>
 <span class="pl-k">-</span><span class="pl-c1">36.5475</span>
  <span class="pl-c1">17.2558</span>
 <span class="pl-k">-</span><span class="pl-c1">22.3915</span>
 <span class="pl-k">-</span><span class="pl-c1">57.9727</span>
  <span class="pl-k">-</span><span class="pl-c1">6.06553</span></pre></div>
<p>For classification, we use +1/-1 labels and the convention
<code>P ( Y = y | X = x ) = 1 / (1+e^{- y x^T w})</code>.</p>
<h2><a id="user-content-required-and-optional-parameters" class="anchor" aria-hidden="true" href="#required-and-optional-parameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Required and optional parameters</h2>
<p><code>subsetSelection</code> has four required parameters:</p>
<ul>
<li>the loss function to be minimized, to be chosen among least squares (<code>OLS()</code>), L1SVR (<code>L1SVR(ɛ)</code>), L2SVR (<code>L2SVR(ɛ)</code>), Logistic loss (<code>LogReg()</code>), Hinge Loss (<code>L1SVM()</code>), L2-SVM (<code>L2SVM()</code>). For classification, we recommend using Hinge loss or L2-SVM functions. Indeed, the Fenchel conjugate of the Logistic loss exhibits unbounded gradients, which largely hinders convergence of the algorithm and might require smaller and more steps (see optional parameters).</li>
<li>the model used to enforce sparsity; either by adding a hard constraint of the form "||w||_0 &lt; k" (<code>Constraint(k)</code>) or by adding a penalty of the form "+ λ ||w||_0" (<code>Penalty(λ)</code>) to the objective. For tractability issues, we highly recommend using an explicit constraint instead of a penalty, for it ensures the size of the support remains bounded through the algorithm.</li>
<li>the vector of outputs <code>Y</code> of size <code>n</code>, the sample size. In classification settings, <code>Y</code> should be a vector of ±1s.</li>
<li>the matrix of covariates <code>X</code> of size <code>n</code>×<code>p</code>, where <code>n</code> and <code>p</code> are the number of samples and features respectively.</li>
</ul>
<p>In addition, <code>subsetSelection</code> accepts the following optional parameters:</p>
<ul>
<li>an initialization for the selected features, <code>indInit</code>.</li>
<li>an initialization for the dual variables, <code>αInit</code>.</li>
<li>the value of the ℓ2-regularization parameter <code>γ</code>, set to 1/√n by default.</li>
<li><code>intercept</code>, a boolean. If true, an intercept/bias term is computed as well. By default, set to false.</li>
<li>the maximum number of iterations in the sub-gradient algorithm, <code>maxIter</code>.</li>
<li>the value of the gradient stepsize <code>δ</code>. By default, the stepsize is set to 1e-3, which demonstrates very good empirical performance. However, smaller stepsizes might be needed when dealing with very large datasets or when the Logistic loss is used.</li>
<li>the number of gradient updates of dual variable α performed per update of primal variable s, <code>gradUp</code>.</li>
<li><code>anticycling</code> a boolean. If true, the algorithm stops as soon as the support is not unchanged from one iteration to another. Empirically, the accuracy of the resulting support is strongly sensitive to noise - to use with caution. By default, set to false.</li>
<li><code>averaging</code> a boolean. If true, the dual solution is averaged over past iterates. By default, set to true.</li>
</ul>
<h2><a id="user-content-best-practices" class="anchor" aria-hidden="true" href="#best-practices"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Best practices</h2>
<ul>
<li>Tuning the regularization parameter <code>γ</code>: By default, <code>γ</code> is set to 1/√n, which is an appropriate scaling in most regression instances. For an optimal performance, and especially in classification or noisy settings, we recommend performing a grid search and using cross-validation to assess out-of-sample performance. The grid search should start with a very low value for <code>γ</code>, such as</li>
</ul>
<div class="highlight highlight-source-julia"><pre>    γ <span class="pl-k">=</span> <span class="pl-c1">1.</span><span class="pl-k">*</span>p <span class="pl-k">/</span> k <span class="pl-k">/</span> n <span class="pl-k">/</span> <span class="pl-c1">maximum</span>(<span class="pl-c1">sum</span>(X[train,:]<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>,<span class="pl-c1">2</span>))</pre></div>
<p>and iteratively increase it by a factor 2. Mean square error or Area Under the Curve (see <a href="https://github.com/davidavdav/ROCAnalysis.jl">ROCAnalysis</a> for implementation) are commonly used performance metrics for regression and classification tasks respectively.</p>
<ul>
<li>Instances where the algorithm fails to converge have been reported. If you occur such cases, try normalize the data matrix <code>X</code> and relaunch the algorithm. If the algorithm still fails to converge, reduce the stepsize <code>δ</code> by a factor 10 or 100 and increase the number of iterations <code>maxIter</code> by a factor at least 2.</li>
</ul>
<h2><a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reference</h2>
</article></div>