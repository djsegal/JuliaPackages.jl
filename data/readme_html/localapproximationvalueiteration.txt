<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://github.com/JuliaPOMDP/LocalApproximationValueIteration.jl/actions/workflows/CI.yml"><img src="https://github.com/JuliaPOMDP/LocalApproximationValueIteration.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/github/JuliaPOMDP/LocalApproximationValueIteration.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9b1a8355dbffc5664540cb2f9514abf16b4f0a6698e6e7c63f69f84ddf1fab29/68747470733a2f2f636f6465636f762e696f2f6769746875622f4a756c6961504f4d44502f4c6f63616c417070726f78696d6174696f6e56616c7565497465726174696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d68653231633458796550" alt="codecov" data-canonical-src="https://codecov.io/github/JuliaPOMDP/LocalApproximationValueIteration.jl/branch/master/graph/badge.svg?token=he21c4XyeP" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-localapproximationvalueiteration" class="anchor" aria-hidden="true" href="#localapproximationvalueiteration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LocalApproximationValueIteration</h1>
<p dir="auto">This package implements the Local Approximation Value Iteration algorithm in Julia for solving
Markov Decision Processes (MDPs). Algorithmically it is very similar to the <a href="https://github.com/JuliaPOMDP/DiscreteValueIteration.jl">DiscreteValueIteration.jl</a>
package, but it represents the state space in a fundamentally different manner, as explained below.
As with <code>DiscreteValueIteration</code>, the user should define the problem according to the API in
<a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a>. Examples of problem definitions can be found in
<a href="https://github.com/JuliaPOMDP/POMDPModels.jl">POMDPModels.jl</a>.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">You need to have <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a> already and the registry added (see its README).
Thereafter, you can add LocalApproximationValueIteration from package manager mode in the Julia REPL</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;LocalApproximationValueIteration&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>LocalApproximationValueIteration<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it Works</h2>
<p dir="auto">This solver is one example of <em>Approximate Dynamic Programming</em>, which tries to find approximately optimal
value functions and policies for large or continuous state spaces. The approach of Local Approximation Value
Iteration assumes that states that are <em>near</em> each other (by some appropriate distance metric) will have similar
values, and it computes the values at some (user-defined) finite set of states, and interpolates the value
function over the entire state space using some local function approximation technique. Details of this approach
are described in <strong>Section 4.5.1</strong> of the book <a href="https://dl.acm.org/citation.cfm?id=2815660" rel="nofollow">Decision Making Under Uncertainty : Theory and Application</a>.</p>
<h2 dir="auto"><a id="user-content-state-space-representation" class="anchor" aria-hidden="true" href="#state-space-representation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>State Space Representation</h2>
<p dir="auto">For value function approximation, the solver depends on the <a href="https://github.com/sisl/LocalFunctionApproximation.jl">LocalFunctionApproximation.jl</a>
package. The <code>LocalApproximationValueIteration</code> solver must be
initialized with an appropriate <code>LocalFunctionApproximator</code> object that approximates
the computed value function over the entire state space by either interpolation over a multi-dimensional grid discretization
of the state space, or by k-nearest-neighbor averaging
with a randomly drawn set of state space samples. The resulting policy uses this object to compute the action-value
function or the best action for any arbitrary state query.</p>
<p dir="auto">A key operational requirement that the solver has from the MDP is that any state can be represented via an equivalent
real-valued vector. This is enforced by the two <a href="https://juliapomdp.github.io/POMDPs.jl/stable/api/#POMDPs.convert_s" rel="nofollow"><code>convert_s</code></a> function requirements that convert an instance of
the MDP State type to a real-valued vector and vice versa.</p>
<p dir="auto">The user is required to implement the above two functions for the <code>State</code> type of their MDP problem model.</p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto"><a href="https://github.com/JuliaPOMDP/POMDPLinter.jl"><code>POMDPLinter.jl</code></a> has a macro <a href="https://juliapomdp.github.io/POMDPLinter.jl/stable/requirements/#POMDPLinter.@show_requirements" rel="nofollow"><code>@show_requirements</code></a> that determines the functions necessary to use some solver on some specific MDP model. As mentioned above, the
<code>LocalApproximationValueIteration</code> solver depends on a <code>LocalFunctionApproximator</code> object and so that object must first be created to invoke
the requirements of the solver accordingly. From our running example in <code>test/runtests_versus_discrete_vi.jl</code>, a function approximation object that uses grid interpolation
(<code>LocalGIFunctionApproximator</code>) is created, after the appropriate <code>RectangleGrid</code> is
constructed (Look at <a href="https://github.com/sisl/GridInterpolations.jl/">GridInterpolations.jl</a> for more details about this).</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using POMDPs, POMDPModels
using GridInterpolations
using LocalFunctionApproximation
using LocalApproximationValueIteration

VERTICES_PER_AXIS = 10 # Controls the resolutions along the grid axis
grid = RectangleGrid(
    range(1, 100, length=VERTICES_PER_AXIS), # x
    range(1, 100, length=VERTICES_PER_AXIS)  # y
)
interp = LocalGIFunctionApproximator(grid)"><pre><span class="pl-k">using</span> POMDPs, POMDPModels
<span class="pl-k">using</span> GridInterpolations
<span class="pl-k">using</span> LocalFunctionApproximation
<span class="pl-k">using</span> LocalApproximationValueIteration

VERTICES_PER_AXIS <span class="pl-k">=</span> <span class="pl-c1">10</span> <span class="pl-c"><span class="pl-c">#</span> Controls the resolutions along the grid axis</span>
grid <span class="pl-k">=</span> <span class="pl-c1">RectangleGrid</span>(
    <span class="pl-c1">range</span>(<span class="pl-c1">1</span>, <span class="pl-c1">100</span>, length<span class="pl-k">=</span>VERTICES_PER_AXIS), <span class="pl-c"><span class="pl-c">#</span> x</span>
    <span class="pl-c1">range</span>(<span class="pl-c1">1</span>, <span class="pl-c1">100</span>, length<span class="pl-k">=</span>VERTICES_PER_AXIS)  <span class="pl-c"><span class="pl-c">#</span> y</span>
)
interp <span class="pl-k">=</span> <span class="pl-c1">LocalGIFunctionApproximator</span>(grid)</pre></div>
<p dir="auto">The user should modify the above steps depending on the kind of interpolation and the necessary parameters they want. We have delegated this step to the user
as it is extremely problem and domain specific. Note that the solver supports both explicit and generative transition models for the MDP (more on that <a href="http://juliapomdp.github.io/POMDPs.jl/latest/def_pomdp" rel="nofollow">here</a>).
The <code>is_mdp_generative</code> and <code>n_generative_samples</code> arguments of the <code>LocalApproximationValueIteration</code> solver should be set accordingly, and there are different
<code>@requirements</code> depending on which kind of model the MDP has.</p>
<p dir="auto">Once all the necessary functions have been defined, the solver can be created.  A <code>GridWorld</code> MDP is defined with grid size 100 x 100 and appropriate reward states:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="mdp = SimpleGridWorld(
    size = (100,100),
    rewards = Dict(GWPos(x,y)=&gt;10. for x ∈ 40:60, y ∈ 40:60)
)"><pre>mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>(
    size <span class="pl-k">=</span> (<span class="pl-c1">100</span>,<span class="pl-c1">100</span>),
    rewards <span class="pl-k">=</span> <span class="pl-c1">Dict</span>(<span class="pl-c1">GWPos</span>(x,y)<span class="pl-k">=&gt;</span><span class="pl-c1">10.</span> <span class="pl-k">for</span> x <span class="pl-k">∈</span> <span class="pl-c1">40</span><span class="pl-k">:</span><span class="pl-c1">60</span>, y <span class="pl-k">∈</span> <span class="pl-c1">40</span><span class="pl-k">:</span><span class="pl-c1">60</span>)
)</pre></div>
<p dir="auto">Finally, the solver can be created using the function approximation object and other necessary parameters
(this model is explicit), and the MDP can be solved:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="approx_solver = LocalApproximationValueIterationSolver(interp, verbose=true, max_iterations=1000, is_mdp_generative=false)
approx_policy = solve(approx_solver, mdp)"><pre>approx_solver <span class="pl-k">=</span> <span class="pl-c1">LocalApproximationValueIterationSolver</span>(interp, verbose<span class="pl-k">=</span><span class="pl-c1">true</span>, max_iterations<span class="pl-k">=</span><span class="pl-c1">1000</span>, is_mdp_generative<span class="pl-k">=</span><span class="pl-c1">false</span>)
approx_policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(approx_solver, mdp)</pre></div>
<p dir="auto">The API for querying the final policy object is identical to <code>DiscreteValueIteration</code>, i.e. the <code>action</code> and <code>value</code> functions can be called for the solved MDP:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="v = value(approx_policy, s)  # returns the approximately optimal value for state s
a = action(approx_policy, s) # returns the approximately optimal action for state s"><pre>v <span class="pl-k">=</span> <span class="pl-c1">value</span>(approx_policy, s)  <span class="pl-c"><span class="pl-c">#</span> returns the approximately optimal value for state s</span>
a <span class="pl-k">=</span> <span class="pl-c1">action</span>(approx_policy, s) <span class="pl-c"><span class="pl-c">#</span> returns the approximately optimal action for state s</span></pre></div>
</article></div>