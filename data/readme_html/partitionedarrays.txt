<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/fverdugo/PartitionedArrays.jl/raw/master/assets/logo.png"><img src="https://github.com/fverdugo/PartitionedArrays.jl/raw/master/assets/logo.png" width="300" title="Logo" style="max-width:100%;"></a></p>
<p><a href="https://github.com/fverdugo/PartitionedArrays.jl/actions"><img src="https://github.com/fverdugo/PartitionedArrays.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/fverdugo/PartitionedArrays.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/5bbdbc2f99b500585a769fda86f02de90de63d584e97465a08e929c08bc6d478/68747470733a2f2f636f6465636f762e696f2f67682f667665726475676f2f506172746974696f6e65644172726179732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/fverdugo/PartitionedArrays.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-what" class="anchor" aria-hidden="true" href="#what"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What</h2>
<p>This package provides a data-oriented parallel implementation of partitioned vectors and sparse matrices needed in FD, FV, and FE simulations. The long-term goal of this package is to provide (when combined with other Julia packages as <code>IterativeSolvers.jl</code>) a Julia alternative to well-known distributed algebra back ends such as <code>PETSc</code>.</p>
<p>At this moment, a simple FD or FE system can be assembled and solved in parallel with this package together with a Conjugate Gradient method from <code>IterativeSolvers.jl</code> . See the files <a href="https://github.com/fverdugo/PartitionedArrays.jl/blob/master/test/test_fdm.jl">test_fdm.jl</a> and <a href="https://github.com/fverdugo/PartitionedArrays.jl/blob/master/test/test_fem_sa.jl">test_fem_sa.jl</a>.</p>
<p>These basic types are currently implemented:</p>
<ul>
<li><code>AbstractBackend</code>: Abstract type to be specialized for different execution models. Now, this package provides <code>SequentialBackend</code> and <code>MPIBackend</code>.</li>
<li><code>AbstractPData</code>: The low level abstract type representing some data partitioned over several chunks or parts. This is the core component of the data-oriented parallel implementation. Now, this package provides <code>SequentialData</code> and <code>MPIData</code>.</li>
<li><code>PRange</code>: A specialization of <code>AbstractUnitRange</code> that has information about how the ids in the range are partitioned in different chunks. This type is used to describe the parallel data layout of rows and cols in <code>PVector</code> and <code>PSparseMatrix</code> objects.</li>
<li><code>PVector</code>: A vector partitioned in (overlapping or non-overlapping) chunks.</li>
<li><code>PSparseMatrix</code>: A sparse matrix partitioned in (overlapping or non-overlapping) chunks of rows.</li>
<li><code>PTimer</code>: A time measuring mechanism designed for the execution model of this package.</li>
</ul>
<p>On these types, several communication operations are defined:</p>
<ul>
<li><code>gather!</code>, <code>gather</code>, <code>gather_all!</code>, <code>gather_all</code></li>
<li><code>reduce</code>, <code>reduce_all</code>, <code>reduce_main</code></li>
<li><code>scatter</code></li>
<li><code>emit</code> (aka broadcast)</li>
<li><code>iscan</code>, <code>iscan_all</code>, <code>iscan_main</code></li>
<li><code>xscan</code>, <code>xscan_all</code>, <code>xscan_main</code></li>
<li><code>exchange!</code> <code>exchange</code>, <code>async_exchange!</code> <code>async_exchange</code></li>
<li><code>assemble!</code>, <code>async_assemble!</code></li>
</ul>
<h2><a id="user-content-why" class="anchor" aria-hidden="true" href="#why"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why</h2>
<p>One can use PETSc bindings like <a href="https://github.com/JuliaParallel/PETSc.jl">PETSc.jl</a> for parallel computations in Julia, but this approach has some limitations:</p>
<ul>
<li>
<p>PETSc is hard-codded for vectors/matrices of some particular element types (e.g. Float64 and ComplexF64).</p>
</li>
<li>
<p>PETSc forces one to use MPI as the parallel execution model. Drivers are executed as <code>mpirun -np 4 julia --project=. input.jl</code>, which means no interactive Julia sessions, no <code>Revise</code>, no <code>Debugger</code>. This is a major limitation to develop parallel algorithms.</p>
</li>
</ul>
<p>This package aims to overcome these limitations. It implements (and allows to implement) parallel algorithms in a generic way independently of the underlying hardware / message passing software that is eventually used. At this moment, this library provides two back ends for running the generic parallel algorithms:</p>
<ul>
<li><code>SequentialBackend</code>: The parallel data is split in chunks, which are stored in a conventional (sequential) Julia session (typically in an <code>Array</code>). The tasks in the parallel algorithms are executed one after the other. Note that the sequential back end does not mean to distribute the data in a single part. The data can be split in an arbitrary number of parts.</li>
<li><code>MPIBackend</code>: Chunks of parallel data and parallel tasks are mapped to different MPI processes. The drivers are to be executed in MPI mode, e.g., <code>mpirun -n 4 julia --project=. input.jl</code>.</li>
</ul>
<p>The <code>SequentialBackend</code> is specially handy for developing new code. Since it runs in a standard Julia session, one can use tools like <code>Revise</code> and <code>Debugger</code> that will certainly do your live easier at the developing stage. Once the code works with the <code>SequentialBackend</code>, it can be automatically deployed in a super computer via the <code>MPIBackend</code>.  Other back ends like a <code>ThreadedBacked</code>, <code>DistributedBackend</code>, or <code>MPIXBackend</code> can be added in the future.</p>
<h2><a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Performance</h2>
<p>This figure shows a strong scaling test of the total time spent in setting up the main components of a FE simulation using <code>PartitionedArrays</code> as the distributed linear algebra backend. A good scaling is obtained beyond 25 thousand degrees of freedom (DOFs) per cpu core, which is usualy considered the point in which scalability starts to significanlty degrade in FE computations.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/fverdugo/PartitionedArrays.jl/raw/master/assets/total_scaling.png"><img src="https://github.com/fverdugo/PartitionedArrays.jl/raw/master/assets/total_scaling.png" alt="" style="max-width:100%;"></a></p>
<p>The wall time includes</p>
<ul>
<li>Generation of a distributed Cartesian FE mesh</li>
<li>Generation of local FE spaces</li>
<li>Generation of a global DOF numbering</li>
<li>Assembly of the distribtued sparse linear system</li>
<li>Interpolation of a manufactured solution</li>
<li>Computation of the residual (incudes a matrix-vector product) and its norm.</li>
</ul>
<p>The results are obtained with the package <a href="https://github.com/fverdugo/PartitionedPoisson.jl"><code>PartitionedPoisson.jl</code></a>.</p>
<h2><a id="user-content-how-to-collaborate" class="anchor" aria-hidden="true" href="#how-to-collaborate"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to collaborate</h2>
<p>We have a number of <a href="https://github.com/fverdugo/PartitionedArrays.jl/labels/help%20wanted">issues waiting for help</a>. You can start contributing to <code>PartitionedArrays.jl</code> by solving some of those issues. Contact with us to coordinate.</p>
</article></div>