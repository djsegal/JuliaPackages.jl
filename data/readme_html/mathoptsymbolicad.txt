<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-mathoptsymbolicad" class="anchor" aria-hidden="true" href="#mathoptsymbolicad"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MathOptSymbolicAD</h1>
<p dir="auto">This package implements an experimental symbolic automatic differentiation
backend for JuMP.</p>
<p dir="auto">For more details, see Oscar's <a href="https://www.youtube.com/watch?v=d_X3gj3Iz-k" rel="nofollow">JuMP-dev 2022 talk</a>.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">Install MathOptSymbolicAD as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Pkg
Pkg.add(&quot;MathOptSymbolicAD&quot;)"><pre><span class="pl-k">import</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MathOptSymbolicAD<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-use-with-jump" class="anchor" aria-hidden="true" href="#use-with-jump"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Use with JuMP</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using JuMP
import Ipopt
import MathOptSymbolicAD
model = Model(Ipopt.Optimizer)
@variable(model, x[1:2])
@NLobjective(model, Min, (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2)
optimize!(model; _differentiation_backend = MathOptSymbolicAD.DefaultBackend())"><pre><span class="pl-k">using</span> JuMP
<span class="pl-k">import</span> Ipopt
<span class="pl-k">import</span> MathOptSymbolicAD
model <span class="pl-k">=</span> <span class="pl-c1">Model</span>(Ipopt<span class="pl-k">.</span>Optimizer)
<span class="pl-c1">@variable</span>(model, x[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span>])
<span class="pl-c1">@NLobjective</span>(model, Min, (<span class="pl-c1">1</span> <span class="pl-k">-</span> x[<span class="pl-c1">1</span>])<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">+</span> <span class="pl-c1">100</span> <span class="pl-k">*</span> (x[<span class="pl-c1">2</span>] <span class="pl-k">-</span> x[<span class="pl-c1">1</span>]<span class="pl-k">^</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span>)
<span class="pl-c1">optimize!</span>(model; _differentiation_backend <span class="pl-k">=</span> MathOptSymbolicAD<span class="pl-k">.</span><span class="pl-c1">DefaultBackend</span>())</pre></div>
<h2 dir="auto"><a id="user-content-background" class="anchor" aria-hidden="true" href="#background"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Background</h2>
<p dir="auto"><code>MathOptSymbolicAD</code> is inspired by Hassan Hijazi's work on
<a href="https://github.com/coin-or/Gravity">coin-or/gravity</a>, a high-performance
algebraic modeling language in C++.</p>
<p dir="auto">Hassan made the following observations:</p>
<ul dir="auto">
<li>For large scale models, symbolic differentiation is slower than other
automatic differentiation techniques.</li>
<li>However, most large-scale nonlinear programs have a lot of structure.</li>
<li>Gravity asks the user to provide structure in the form of
<em>template constraints</em>, where the user gives the symbolic form of the
constraint as well as a set of data to convert from a symbolic form to the
numerical form.</li>
<li>Instead of differentiating each constraint in its numerical form, we can
compute one symbolic derivative of the constraint in symbolic form, and then
plug in the data in to get the numerical derivative of each function.</li>
<li>As a final step, if users don't provide the structure, we can still infer it
--perhaps with less accuracy--by comparing the expression tree of each
constraint.</li>
</ul>
<p dir="auto">The symbolic differentiation approach of Gravity works well when the problem is
large with few unique constraints. For example, a model like:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="model = Model()
@variable(model, 0 &lt;= x[1:10_000] &lt;= 1)
@NLconstraint(model, [i=1:10_000], sin(x[i]) &lt;= 1)
@objective(model, Max, sum(x))"><pre>model <span class="pl-k">=</span> <span class="pl-c1">Model</span>()
<span class="pl-c1">@variable</span>(model, <span class="pl-c1">0</span> <span class="pl-k">&lt;=</span> x[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10_000</span>] <span class="pl-k">&lt;=</span> <span class="pl-c1">1</span>)
<span class="pl-c1">@NLconstraint</span>(model, [i<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">10_000</span>], <span class="pl-c1">sin</span>(x[i]) <span class="pl-k">&lt;=</span> <span class="pl-c1">1</span>)
<span class="pl-c1">@objective</span>(model, Max, <span class="pl-c1">sum</span>(x))</pre></div>
<p dir="auto">is ideal, because although the Jacobian matrix has 10,000 rows, we can compute
the derivative of <code>sin(x[i])</code> as <code>cos(x[i])</code>, and then fill in the Jacobian by
evaluating the derivative function instead of having to differentiation 10,000
expressions.</p>
<p dir="auto">The symbolic differentiation approach of Gravity works poorly if there are a
large number of unique constraints in the model (which would require a lot of
expressions to be symbolically differentiated), or if the nonlinear functions
contain a large number of nonlinear terms (which would make the symbolic
derivative expensive to compute).</p>
</article></div>