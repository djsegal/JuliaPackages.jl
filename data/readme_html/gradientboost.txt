<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gradientboost" class="anchor" aria-hidden="true" href="#gradientboost"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GradientBoost</h1>
<p dir="auto"><a href="https://travis-ci.org/svs14/GradientBoost.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e17e3ec544429cbb3c1f545f1b61a781da87d6a84742eb66ff052780af19c11a/68747470733a2f2f7472617669732d63692e6f72672f73767331342f4772616469656e74426f6f73742e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/svs14/GradientBoost.jl.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://coveralls.io/r/svs14/GradientBoost.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/88b1a97cb3b17d74bafbc542ca5e01ec887c50d4e3f9275a2d6a224d8a0c610c/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f73767331342f4772616469656e74426f6f73742e6a6c2f62616467652e706e673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/svs14/GradientBoost.jl/badge.png?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">This package covers the gradient boosting paradigm: a framework that builds
additive expansions based on any fitting criteria.</p>
<p dir="auto">In machine learning parlance, this is typically referred to as
gradient boosting machines, generalized boosted models and stochastic gradient
boosting.</p>
<p dir="auto">Normally, gradient boosting implementations cover a specific algorithm: gradient
boosted decision trees. This package covers the framework itself, including such
implementations.</p>
<p dir="auto">References:</p>
<ul dir="auto">
<li> Friedman, Jerome H. "Greedy function approximation: a gradient boosting
machine." Annals of Statistics (2001): 1189-1232. </li>
<li> Friedman, Jerome H. "Stochastic gradient boosting."
Computational Statistics &amp; Data Analysis 38.4 (2002): 367-378. </li>
<li> Hastie, Trevor, et al. The elements of statistical learning.
Vol. 2. No. 1. New York: Springer, 2009. </li>
<li> Ridgeway, Greg. "Generalized Boosted Models: A guide to the gbm package."
Update 1.1 (2007). </li>
<li> Pedregosa, Fabian, et al. "Scikit-learn: Machine learning in Python."
The Journal of Machine Learning Research 12 (2011): 2825-2830. </li>
<li> Natekin, Alexey, and Alois Knoll.
"Gradient boosting machines, a tutorial."
Frontiers in neurorobotics 7 (2013). </li>
</ul>
<h2 dir="auto"><a id="user-content-machine-learning-api" class="anchor" aria-hidden="true" href="#machine-learning-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Machine Learning API</h2>
<p dir="auto">Module <code>GradientBoost.ML</code> is provided for users who are only interested in
using existing gradient boosting algorithms for prediction.
To get a feel for the API,
we will run a demonstration
of gradient boosted decision trees on the iris dataset.</p>
<h3 dir="auto"><a id="user-content-obtain-data" class="anchor" aria-hidden="true" href="#obtain-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Obtain Data</h3>
<p dir="auto">At the moment only two-class classification is handled,
so our learner will attempt to separate "setosa" from the other species.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using GradientBoost.ML
using RDatasets

# Obtain iris dataset
iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)
instances = array(iris[:, 1:end-1])
labels = [species == &quot;setosa&quot; ? 1.0 : 0.0 for species in array(iris[:, end])]

# Obtain training and test set (20% test)
num_instances = size(instances, 1)
train_ind, test_ind = GradientBoost.Util.holdout(num_instances, 0.2)"><pre><span class="pl-k">using</span> GradientBoost<span class="pl-k">.</span>ML
<span class="pl-k">using</span> RDatasets

<span class="pl-c"><span class="pl-c">#</span> Obtain iris dataset</span>
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)
instances <span class="pl-k">=</span> <span class="pl-c1">array</span>(iris[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>])
labels <span class="pl-k">=</span> [species <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span> <span class="pl-k">?</span> <span class="pl-c1">1.0</span> <span class="pl-k">:</span> <span class="pl-c1">0.0</span> <span class="pl-k">for</span> species <span class="pl-k">in</span> <span class="pl-c1">array</span>(iris[:, <span class="pl-c1">end</span>])]

<span class="pl-c"><span class="pl-c">#</span> Obtain training and test set (20% test)</span>
num_instances <span class="pl-k">=</span> <span class="pl-c1">size</span>(instances, <span class="pl-c1">1</span>)
train_ind, test_ind <span class="pl-k">=</span> GradientBoost<span class="pl-k">.</span>Util<span class="pl-k">.</span><span class="pl-c1">holdout</span>(num_instances, <span class="pl-c1">0.2</span>)</pre></div>
<h3 dir="auto"><a id="user-content-build-learner" class="anchor" aria-hidden="true" href="#build-learner"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Build Learner</h3>
<p dir="auto">The gradient boosting (GB) learner comprises of a GB algorithm
and what output it must produce.
In this case, we shall assign a gradient boosted decision tree to output classes.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Build GBLearner
gbdt = GBDT(;
  loss_function = BinomialDeviance(),
  sampling_rate = 0.6,
  learning_rate = 0.1,
  num_iterations = 100
)
gbl = GBLearner(
  gbdt,  # Gradient boosting algorithm
  :class # Output (:class, :class_prob, :regression)
)"><pre><span class="pl-c"><span class="pl-c">#</span> Build GBLearner</span>
gbdt <span class="pl-k">=</span> <span class="pl-c1">GBDT</span>(;
  loss_function <span class="pl-k">=</span> <span class="pl-c1">BinomialDeviance</span>(),
  sampling_rate <span class="pl-k">=</span> <span class="pl-c1">0.6</span>,
  learning_rate <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,
  num_iterations <span class="pl-k">=</span> <span class="pl-c1">100</span>
)
gbl <span class="pl-k">=</span> <span class="pl-c1">GBLearner</span>(
  gbdt,  <span class="pl-c"><span class="pl-c">#</span> Gradient boosting algorithm</span>
  <span class="pl-c1">:class</span> <span class="pl-c"><span class="pl-c">#</span> Output (:class, :class_prob, :regression)</span>
)</pre></div>
<h3 dir="auto"><a id="user-content-train-and-predict" class="anchor" aria-hidden="true" href="#train-and-predict"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Train and Predict</h3>
<p dir="auto">Currently <code>Matrix{Float64}</code> instances and <code>Vector{Float64}</code> labels are
the only handled types for training and prediction.
In this case, it is not an issue.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Train
ML.fit!(gbl, instances[train_ind, :], labels[train_ind])

# Predict
predictions = ML.predict!(gbl, instances[test_ind, :])"><pre><span class="pl-c"><span class="pl-c">#</span> Train</span>
ML<span class="pl-k">.</span><span class="pl-c1">fit!</span>(gbl, instances[train_ind, :], labels[train_ind])

<span class="pl-c"><span class="pl-c">#</span> Predict</span>
predictions <span class="pl-k">=</span> ML<span class="pl-k">.</span><span class="pl-c1">predict!</span>(gbl, instances[test_ind, :])</pre></div>
<h3 dir="auto"><a id="user-content-evaluate" class="anchor" aria-hidden="true" href="#evaluate"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Evaluate</h3>
<p dir="auto">If all is well, we should obtain better than baseline accuracy (67%).</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Obtain accuracy
accuracy = mean(predictions .== labels[test_ind]) * 100.0
println(&quot;GBDT accuracy: $(accuracy)&quot;)"><pre><span class="pl-c"><span class="pl-c">#</span> Obtain accuracy</span>
accuracy <span class="pl-k">=</span> <span class="pl-c1">mean</span>(predictions <span class="pl-k">.==</span> labels[test_ind]) <span class="pl-k">*</span> <span class="pl-c1">100.0</span>
<span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>GBDT accuracy: <span class="pl-v">$(accuracy)</span><span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">That concludes the demonstration. Detailed below are the available GB learners.</p>
<h2 dir="auto"><a id="user-content-algorithms" class="anchor" aria-hidden="true" href="#algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Algorithms</h2>
<p dir="auto">Documented below are the currently implemented gradient boosting algorithms.</p>
<h3 dir="auto"><a id="user-content-gb-decision-tree" class="anchor" aria-hidden="true" href="#gb-decision-tree"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GB Decision Tree</h3>
<p dir="auto">Gradient Boosted Decision Tree algorithm backed by
<a href="https://github.com/bensadeghi/DecisionTree.jl#regression-example">DecisionTree.jl</a>
regression trees.
Current loss functions covered are:
<code>LeastSquares</code>, <code>LeastAbsoluteDeviation</code> and <code>BinomialDeviance</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="gbdt = GBDT(;
  loss_function = BinomialDeviance(), # Loss function
  sampling_rate = 0.6,                # Sampling rate
  learning_rate = 0.1,                # Learning rate
  num_iterations = 100,               # Number of iterations
  tree_options = {                    # Tree options (DecisionTree.jl regressor)
    :maxlabels =&gt; 5,
    :nsubfeatures =&gt; 0
  }
)"><pre>gbdt <span class="pl-k">=</span> <span class="pl-c1">GBDT</span>(;
  loss_function <span class="pl-k">=</span> <span class="pl-c1">BinomialDeviance</span>(), <span class="pl-c"><span class="pl-c">#</span> Loss function</span>
  sampling_rate <span class="pl-k">=</span> <span class="pl-c1">0.6</span>,                <span class="pl-c"><span class="pl-c">#</span> Sampling rate</span>
  learning_rate <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,                <span class="pl-c"><span class="pl-c">#</span> Learning rate</span>
  num_iterations <span class="pl-k">=</span> <span class="pl-c1">100</span>,               <span class="pl-c"><span class="pl-c">#</span> Number of iterations</span>
  tree_options <span class="pl-k">=</span> {                    <span class="pl-c"><span class="pl-c">#</span> Tree options (DecisionTree.jl regressor)</span>
    <span class="pl-c1">:maxlabels</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">5</span>,
    <span class="pl-c1">:nsubfeatures</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span>
  }
)</pre></div>
<h3 dir="auto"><a id="user-content-gb-base-learner" class="anchor" aria-hidden="true" href="#gb-base-learner"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GB Base Learner</h3>
<p dir="auto">Gradient boosting with a given base learner.
Current loss functions covered are: <code>LeastSquares</code> and <code>LeastAbsoluteDeviation</code>.
In order to use this,
<code>ML.learner_fit</code> and <code>ML.learner_predict</code> functions must be extended.
Example provided below for linear regression found in
<a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import GLM: fit, predict, LinearModel

# Extend functions
function ML.learner_fit(lf::LossFunction, 
  learner::Type{LinearModel}, instances, labels)
  
  model = fit(learner, instances, labels)
end
function ML.learner_predict(lf::LossFunction,
  learner::Type{LinearModel}, model, instances)
  
  predict(model, instances)
end"><pre><span class="pl-k">import</span> GLM<span class="pl-k">:</span> fit, predict, LinearModel

<span class="pl-c"><span class="pl-c">#</span> Extend functions</span>
<span class="pl-k">function</span> ML<span class="pl-k">.</span><span class="pl-en">learner_fit</span>(lf<span class="pl-k">::</span><span class="pl-c1">LossFunction</span>, 
  learner<span class="pl-k">::</span><span class="pl-c1">Type{LinearModel}</span>, instances, labels)
  
  model <span class="pl-k">=</span> <span class="pl-c1">fit</span>(learner, instances, labels)
<span class="pl-k">end</span>
<span class="pl-k">function</span> ML<span class="pl-k">.</span><span class="pl-en">learner_predict</span>(lf<span class="pl-k">::</span><span class="pl-c1">LossFunction</span>,
  learner<span class="pl-k">::</span><span class="pl-c1">Type{LinearModel}</span>, model, instances)
  
  <span class="pl-c1">predict</span>(model, instances)
<span class="pl-k">end</span></pre></div>
<p dir="auto">Once this is done,
the algorithm can be instantiated with the respective base learner.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="gbl = GBBL(
  LinearModel;                    # Base Learner
  loss_function = LeastSquares(), # Loss function
  sampling_rate = 0.8,            # Sampling rate
  learning_rate = 0.1,            # Learning rate
  num_iterations = 100            # Number of iterations
)
gbl = GBLearner(gbl, :regression)"><pre>gbl <span class="pl-k">=</span> <span class="pl-c1">GBBL</span>(
  LinearModel;                    <span class="pl-c"><span class="pl-c">#</span> Base Learner</span>
  loss_function <span class="pl-k">=</span> <span class="pl-c1">LeastSquares</span>(), <span class="pl-c"><span class="pl-c">#</span> Loss function</span>
  sampling_rate <span class="pl-k">=</span> <span class="pl-c1">0.8</span>,            <span class="pl-c"><span class="pl-c">#</span> Sampling rate</span>
  learning_rate <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,            <span class="pl-c"><span class="pl-c">#</span> Learning rate</span>
  num_iterations <span class="pl-k">=</span> <span class="pl-c1">100</span>            <span class="pl-c"><span class="pl-c">#</span> Number of iterations</span>
)
gbl <span class="pl-k">=</span> <span class="pl-c1">GBLearner</span>(gbl, <span class="pl-c1">:regression</span>)</pre></div>
<h2 dir="auto"><a id="user-content-gradient-boosting-framework" class="anchor" aria-hidden="true" href="#gradient-boosting-framework"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Gradient Boosting Framework</h2>
<p dir="auto">All previously developed algorithms follow the framework
provided by <code>GradientBoost.GB</code>.</p>
<p dir="auto">As this package is in its preliminary stage,
major changes may occur in the near future and as such
we provide minimal README documentation.</p>
<p dir="auto">All of what is required to be implemented is exampled below:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import GradientBoost.GB
import GradientBoost.LossFunctions: LossFunction

# Must subtype from GBAlgorithm defined in GB module.
type ExampleGB &lt;: GB.GBAlgorithm
  loss_function::LossFunction
  sampling_rate::FloatingPoint
  learning_rate::FloatingPoint
  num_iterations::Int
end

# Model training and co-efficient optimization should be done here.
function GB.build_base_func(
  gb::ExampleGB, instances, labels, prev_func_pred, psuedo)

  model_const = 0.5
  model_pred = (instances) -&gt; Float64[
    sum(instances[i,:]) for i = 1:size(instances, 1)
  ]

  return (instances) -&gt; model_const .* model_pred(instances)
end"><pre><span class="pl-k">import</span> GradientBoost<span class="pl-k">.</span>GB
<span class="pl-k">import</span> GradientBoost<span class="pl-k">.</span>LossFunctions<span class="pl-k">:</span> LossFunction

<span class="pl-c"><span class="pl-c">#</span> Must subtype from GBAlgorithm defined in GB module.</span>
type ExampleGB <span class="pl-k">&lt;:</span> <span class="pl-c1">GB.GBAlgorithm</span>
  loss_function<span class="pl-k">::</span><span class="pl-c1">LossFunction</span>
  sampling_rate<span class="pl-k">::</span><span class="pl-c1">FloatingPoint</span>
  learning_rate<span class="pl-k">::</span><span class="pl-c1">FloatingPoint</span>
  num_iterations<span class="pl-k">::</span><span class="pl-c1">Int</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Model training and co-efficient optimization should be done here.</span>
<span class="pl-k">function</span> GB<span class="pl-k">.</span><span class="pl-en">build_base_func</span>(
  gb<span class="pl-k">::</span><span class="pl-c1">ExampleGB</span>, instances, labels, prev_func_pred, psuedo)

  model_const <span class="pl-k">=</span> <span class="pl-c1">0.5</span>
  model_pred <span class="pl-k">=</span> (instances) <span class="pl-k">-&gt;</span> Float64[
    <span class="pl-c1">sum</span>(instances[i,:]) <span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(instances, <span class="pl-c1">1</span>)
  ]

  <span class="pl-k">return</span> (instances) <span class="pl-k">-&gt;</span> model_const <span class="pl-k">.*</span> <span class="pl-c1">model_pred</span>(instances)
<span class="pl-k">end</span></pre></div>
<p dir="auto">A relatively light algorithm
that implements <code>GBAlgorithm</code> is <code>GBBL</code>, found in <code>src/gb_bl.jl</code>.</p>
<h2 dir="auto"><a id="user-content-misc" class="anchor" aria-hidden="true" href="#misc"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Misc</h2>
<p dir="auto">The links provided below will only work if you are viewing this in the GitHub repository.</p>
<h3 dir="auto"><a id="user-content-changes" class="anchor" aria-hidden="true" href="#changes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Changes</h3>
<p dir="auto">See <a href="CHANGELOG.yml">CHANGELOG.yml</a>.</p>
<h3 dir="auto"><a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Future Work</h3>
<p dir="auto">See <a href="FUTUREWORK.md">FUTUREWORK.md</a>.</p>
<h3 dir="auto"><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h3>
<p dir="auto">See <a href="CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<h3 dir="auto"><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h3>
<p dir="auto">MIT "Expat" License. See <a href="LICENSE.md">LICENSE.md</a>.</p>
</article></div>