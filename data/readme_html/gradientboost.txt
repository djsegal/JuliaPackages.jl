<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-gradientboost" class="anchor" aria-hidden="true" href="#gradientboost"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GradientBoost</h1>
<p><a href="https://travis-ci.org/svs14/GradientBoost.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/294d1392e025fff6e2ca44939328ee42246c10e2/68747470733a2f2f7472617669732d63692e6f72672f73767331342f4772616469656e74426f6f73742e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/svs14/GradientBoost.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/r/svs14/GradientBoost.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/851637860355f28c67519c49042df8da2499cb45/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f73767331342f4772616469656e74426f6f73742e6a6c2f62616467652e706e673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/svs14/GradientBoost.jl/badge.png?branch=master" style="max-width:100%;"></a></p>
<p>This package covers the gradient boosting paradigm: a framework that builds
additive expansions based on any fitting criteria.</p>
<p>In machine learning parlance, this is typically referred to as
gradient boosting machines, generalized boosted models and stochastic gradient
boosting.</p>
<p>Normally, gradient boosting implementations cover a specific algorithm: gradient
boosted decision trees. This package covers the framework itself, including such
implementations.</p>
<p>References:</p>
<ul>
<li> Friedman, Jerome H. "Greedy function approximation: a gradient boosting
machine." Annals of Statistics (2001): 1189-1232. </li>
<li> Friedman, Jerome H. "Stochastic gradient boosting."
Computational Statistics &amp; Data Analysis 38.4 (2002): 367-378. </li>
<li> Hastie, Trevor, et al. The elements of statistical learning.
Vol. 2. No. 1. New York: Springer, 2009. </li>
<li> Ridgeway, Greg. "Generalized Boosted Models: A guide to the gbm package."
Update 1.1 (2007). </li>
<li> Pedregosa, Fabian, et al. "Scikit-learn: Machine learning in Python."
The Journal of Machine Learning Research 12 (2011): 2825-2830. </li>
<li> Natekin, Alexey, and Alois Knoll.
"Gradient boosting machines, a tutorial."
Frontiers in neurorobotics 7 (2013). </li>
</ul>
<h2><a id="user-content-machine-learning-api" class="anchor" aria-hidden="true" href="#machine-learning-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Machine Learning API</h2>
<p>Module <code>GradientBoost.ML</code> is provided for users who are only interested in
using existing gradient boosting algorithms for prediction.
To get a feel for the API,
we will run a demonstration
of gradient boosted decision trees on the iris dataset.</p>
<h3><a id="user-content-obtain-data" class="anchor" aria-hidden="true" href="#obtain-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Obtain Data</h3>
<p>At the moment only two-class classification is handled,
so our learner will attempt to separate "setosa" from the other species.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> GradientBoost<span class="pl-k">.</span>ML
<span class="pl-k">using</span> RDatasets

<span class="pl-c"><span class="pl-c">#</span> Obtain iris dataset</span>
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)
instances <span class="pl-k">=</span> <span class="pl-c1">array</span>(iris[:, <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>])
labels <span class="pl-k">=</span> [species <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span> ? <span class="pl-c1">1.0</span> : <span class="pl-c1">0.0</span> <span class="pl-k">for</span> species <span class="pl-k">in</span> <span class="pl-c1">array</span>(iris[:, <span class="pl-c1">end</span>])]

<span class="pl-c"><span class="pl-c">#</span> Obtain training and test set (20% test)</span>
num_instances <span class="pl-k">=</span> <span class="pl-c1">size</span>(instances, <span class="pl-c1">1</span>)
train_ind, test_ind <span class="pl-k">=</span> GradientBoost<span class="pl-k">.</span>Util<span class="pl-k">.</span><span class="pl-c1">holdout</span>(num_instances, <span class="pl-c1">0.2</span>)</pre></div>
<h3><a id="user-content-build-learner" class="anchor" aria-hidden="true" href="#build-learner"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Build Learner</h3>
<p>The gradient boosting (GB) learner comprises of a GB algorithm
and what output it must produce.
In this case, we shall assign a gradient boosted decision tree to output classes.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Build GBLearner</span>
gbdt <span class="pl-k">=</span> <span class="pl-c1">GBDT</span>(;
  loss_function <span class="pl-k">=</span> <span class="pl-c1">BinomialDeviance</span>(),
  sampling_rate <span class="pl-k">=</span> <span class="pl-c1">0.6</span>,
  learning_rate <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,
  num_iterations <span class="pl-k">=</span> <span class="pl-c1">100</span>
)
gbl <span class="pl-k">=</span> <span class="pl-c1">GBLearner</span>(
  gbdt,  <span class="pl-c"><span class="pl-c">#</span> Gradient boosting algorithm</span>
  <span class="pl-c1">:class</span> <span class="pl-c"><span class="pl-c">#</span> Output (:class, :class_prob, :regression)</span>
)</pre></div>
<h3><a id="user-content-train-and-predict" class="anchor" aria-hidden="true" href="#train-and-predict"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Train and Predict</h3>
<p>Currently <code>Matrix{Float64}</code> instances and <code>Vector{Float64}</code> labels are
the only handled types for training and prediction.
In this case, it is not an issue.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Train</span>
ML<span class="pl-k">.</span><span class="pl-c1">fit!</span>(gbl, instances[train_ind, :], labels[train_ind])

<span class="pl-c"><span class="pl-c">#</span> Predict</span>
predictions <span class="pl-k">=</span> ML<span class="pl-k">.</span><span class="pl-c1">predict!</span>(gbl, instances[test_ind, :])</pre></div>
<h3><a id="user-content-evaluate" class="anchor" aria-hidden="true" href="#evaluate"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluate</h3>
<p>If all is well, we should obtain better than baseline accuracy (67%).</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Obtain accuracy</span>
accuracy <span class="pl-k">=</span> <span class="pl-c1">mean</span>(predictions <span class="pl-k">.==</span> labels[test_ind]) <span class="pl-k">*</span> <span class="pl-c1">100.0</span>
<span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>GBDT accuracy: <span class="pl-v">$(accuracy)</span><span class="pl-pds">"</span></span>)</pre></div>
<p>That concludes the demonstration. Detailed below are the available GB learners.</p>
<h2><a id="user-content-algorithms" class="anchor" aria-hidden="true" href="#algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Algorithms</h2>
<p>Documented below are the currently implemented gradient boosting algorithms.</p>
<h3><a id="user-content-gb-decision-tree" class="anchor" aria-hidden="true" href="#gb-decision-tree"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GB Decision Tree</h3>
<p>Gradient Boosted Decision Tree algorithm backed by
<a href="https://github.com/bensadeghi/DecisionTree.jl#regression-example">DecisionTree.jl</a>
regression trees.
Current loss functions covered are:
<code>LeastSquares</code>, <code>LeastAbsoluteDeviation</code> and <code>BinomialDeviance</code>.</p>
<div class="highlight highlight-source-julia"><pre>gbdt <span class="pl-k">=</span> <span class="pl-c1">GBDT</span>(;
  loss_function <span class="pl-k">=</span> <span class="pl-c1">BinomialDeviance</span>(), <span class="pl-c"><span class="pl-c">#</span> Loss function</span>
  sampling_rate <span class="pl-k">=</span> <span class="pl-c1">0.6</span>,                <span class="pl-c"><span class="pl-c">#</span> Sampling rate</span>
  learning_rate <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,                <span class="pl-c"><span class="pl-c">#</span> Learning rate</span>
  num_iterations <span class="pl-k">=</span> <span class="pl-c1">100</span>,               <span class="pl-c"><span class="pl-c">#</span> Number of iterations</span>
  tree_options <span class="pl-k">=</span> {                    <span class="pl-c"><span class="pl-c">#</span> Tree options (DecisionTree.jl regressor)</span>
    <span class="pl-c1">:maxlabels</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">5</span>,
    <span class="pl-c1">:nsubfeatures</span> <span class="pl-k">=&gt;</span> <span class="pl-c1">0</span>
  }
)</pre></div>
<h3><a id="user-content-gb-base-learner" class="anchor" aria-hidden="true" href="#gb-base-learner"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GB Base Learner</h3>
<p>Gradient boosting with a given base learner.
Current loss functions covered are: <code>LeastSquares</code> and <code>LeastAbsoluteDeviation</code>.
In order to use this,
<code>ML.learner_fit</code> and <code>ML.learner_predict</code> functions must be extended.
Example provided below for linear regression found in
<a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> GLM<span class="pl-k">:</span> fit, predict, LinearModel

<span class="pl-c"><span class="pl-c">#</span> Extend functions</span>
<span class="pl-k">function</span> ML<span class="pl-k">.</span><span class="pl-en">learner_fit</span>(lf<span class="pl-k">::</span><span class="pl-c1">LossFunction</span>, 
  learner<span class="pl-k">::</span><span class="pl-c1">Type{LinearModel}</span>, instances, labels)
  
  model <span class="pl-k">=</span> <span class="pl-c1">fit</span>(learner, instances, labels)
<span class="pl-k">end</span>
<span class="pl-k">function</span> ML<span class="pl-k">.</span><span class="pl-en">learner_predict</span>(lf<span class="pl-k">::</span><span class="pl-c1">LossFunction</span>,
  learner<span class="pl-k">::</span><span class="pl-c1">Type{LinearModel}</span>, model, instances)
  
  <span class="pl-c1">predict</span>(model, instances)
<span class="pl-k">end</span></pre></div>
<p>Once this is done,
the algorithm can be instantiated with the respective base learner.</p>
<div class="highlight highlight-source-julia"><pre>gbl <span class="pl-k">=</span> <span class="pl-c1">GBBL</span>(
  LinearModel;                    <span class="pl-c"><span class="pl-c">#</span> Base Learner</span>
  loss_function <span class="pl-k">=</span> <span class="pl-c1">LeastSquares</span>(), <span class="pl-c"><span class="pl-c">#</span> Loss function</span>
  sampling_rate <span class="pl-k">=</span> <span class="pl-c1">0.8</span>,            <span class="pl-c"><span class="pl-c">#</span> Sampling rate</span>
  learning_rate <span class="pl-k">=</span> <span class="pl-c1">0.1</span>,            <span class="pl-c"><span class="pl-c">#</span> Learning rate</span>
  num_iterations <span class="pl-k">=</span> <span class="pl-c1">100</span>            <span class="pl-c"><span class="pl-c">#</span> Number of iterations</span>
)
gbl <span class="pl-k">=</span> <span class="pl-c1">GBLearner</span>(gbl, <span class="pl-c1">:regression</span>)</pre></div>
<h2><a id="user-content-gradient-boosting-framework" class="anchor" aria-hidden="true" href="#gradient-boosting-framework"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Gradient Boosting Framework</h2>
<p>All previously developed algorithms follow the framework
provided by <code>GradientBoost.GB</code>.</p>
<p>As this package is in its preliminary stage,
major changes may occur in the near future and as such
we provide minimal README documentation.</p>
<p>All of what is required to be implemented is exampled below:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> GradientBoost<span class="pl-k">.</span>GB
<span class="pl-k">import</span> GradientBoost<span class="pl-k">.</span>LossFunctions<span class="pl-k">:</span> LossFunction

<span class="pl-c"><span class="pl-c">#</span> Must subtype from GBAlgorithm defined in GB module.</span>
type ExampleGB <span class="pl-k">&lt;:</span> <span class="pl-c1">GB.GBAlgorithm</span>
  loss_function<span class="pl-k">::</span><span class="pl-c1">LossFunction</span>
  sampling_rate<span class="pl-k">::</span><span class="pl-c1">FloatingPoint</span>
  learning_rate<span class="pl-k">::</span><span class="pl-c1">FloatingPoint</span>
  num_iterations<span class="pl-k">::</span><span class="pl-c1">Int</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Model training and co-efficient optimization should be done here.</span>
<span class="pl-k">function</span> GB<span class="pl-k">.</span><span class="pl-en">build_base_func</span>(
  gb<span class="pl-k">::</span><span class="pl-c1">ExampleGB</span>, instances, labels, prev_func_pred, psuedo)

  model_const <span class="pl-k">=</span> <span class="pl-c1">0.5</span>
  model_pred <span class="pl-k">=</span> (instances) <span class="pl-k">-&gt;</span> Float64[
    <span class="pl-c1">sum</span>(instances[i,:]) <span class="pl-k">for</span> i <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(instances, <span class="pl-c1">1</span>)
  ]

  <span class="pl-k">return</span> (instances) <span class="pl-k">-&gt;</span> model_const <span class="pl-k">.*</span> <span class="pl-c1">model_pred</span>(instances)
<span class="pl-k">end</span></pre></div>
<p>A relatively light algorithm
that implements <code>GBAlgorithm</code> is <code>GBBL</code>, found in <code>src/gb_bl.jl</code>.</p>
<h2><a id="user-content-misc" class="anchor" aria-hidden="true" href="#misc"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Misc</h2>
<p>The links provided below will only work if you are viewing this in the GitHub repository.</p>
<h3><a id="user-content-changes" class="anchor" aria-hidden="true" href="#changes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Changes</h3>
<p>See <a href="CHANGELOG.yml">CHANGELOG.yml</a>.</p>
<h3><a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Work</h3>
<p>See <a href="FUTUREWORK.md">FUTUREWORK.md</a>.</p>
<h3><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributing</h3>
<p>See <a href="CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<h3><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>License</h3>
<p>MIT "Expat" License. See <a href="LICENSE.md">LICENSE.md</a>.</p>
</article></div>