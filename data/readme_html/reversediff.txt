<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-reversediff" class="anchor" aria-hidden="true" href="#reversediff"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ReverseDiff</h1>
<p><a href="https://travis-ci.org/JuliaDiff/ReverseDiff.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6744409ddc7f083e73e0d191f9767dec90cd3f43/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961446966662f52657665727365446966662e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaDiff/ReverseDiff.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/JuliaDiff/ReverseDiff.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/74d54806eba1d5cd989529dec6e658dd698a5ad6/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4a756c6961446966662f52657665727365446966662e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/JuliaDiff/ReverseDiff.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p><a href="http://www.juliadiff.org/ReverseDiff.jl/" rel="nofollow"><strong>Go To ReverseDiff's Documentation</strong></a></p>
<p><a href="https://github.com/JuliaDiff/ReverseDiff.jl/tree/master/examples"><strong>See ReverseDiff Usage Examples</strong></a></p>
<p>ReverseDiff is a fast and compile-able tape-based <strong>reverse mode automatic differentiation (AD)</strong> that
implements methods to take <strong>gradients</strong>, <strong>Jacobians</strong>, <strong>Hessians</strong>, and
higher-order derivatives of native Julia functions (or any callable object, really).</p>
<p>While performance can vary depending on the functions you evaluate, the algorithms
implemented by ReverseDiff <strong>generally outperform non-AD algorithms in both speed and
accuracy.</strong></p>
<p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="nofollow">Wikipedia's entry on automatic differentiation</a>
is a useful resource for learning about the advantages of AD techniques over other common
differentiation methods (such as <a href="https://en.wikipedia.org/wiki/Numerical_differentiation" rel="nofollow">finite differencing</a>).</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>To install ReverseDiff, simply use Julia's package manager:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>ReverseDiff<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-why-use-reversediff" class="anchor" aria-hidden="true" href="#why-use-reversediff"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why use ReverseDiff?</h2>
<p>Other Julia packages may provide some of these features, but only ReverseDiff provides all
of them (as far as I know at the time of this writing):</p>
<ul>
<li>supports a large subset of the Julia language, including loops, recursion, and control flow</li>
<li>user-friendly API for reusing and compiling tapes</li>
<li>user-friendly performance annotations such as <code>@forward</code> and <code>@skip</code> (with more to come!)</li>
<li>compatible with ForwardDiff, enabling mixed-mode AD</li>
<li>built-in definitions leverage the benefits of ForwardDiff's <code>Dual</code> numbers (e.g. SIMD, zero-overhead arithmetic)</li>
<li>a familiar differentiation API for ForwardDiff users</li>
<li>non-allocating linear algebra optimizations</li>
<li>nested differentiation</li>
<li>suitable as an execution backend for graphical machine learning libraries</li>
<li>ReverseDiff doesn't need to record scalar indexing operations (a huge cost for many similar libraries)</li>
<li>higher-order <code>map</code> and <code>broadcast</code> optimizations</li>
<li>it's well tested</li>
</ul>
<p>...and, simply put, it's fast (for gradients, at least). Using the code from <code>examples/gradient.jl</code>:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BenchmarkTools, Pkg

<span class="pl-c"><span class="pl-c">#</span> this script defines f and ∇f!</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">include</span>(<span class="pl-c1">joinpath</span>(Pkg<span class="pl-k">.</span><span class="pl-c1">dir</span>(<span class="pl-s"><span class="pl-pds">"</span>ReverseDiff<span class="pl-pds">"</span></span>), <span class="pl-s"><span class="pl-pds">"</span>examples/gradient.jl<span class="pl-pds">"</span></span>));

julia<span class="pl-k">&gt;</span> a, b <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>, <span class="pl-c1">100</span>), <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>, <span class="pl-c1">100</span>);

julia<span class="pl-k">&gt;</span> inputs <span class="pl-k">=</span> (a, b);

julia<span class="pl-k">&gt;</span> results <span class="pl-k">=</span> (<span class="pl-c1">similar</span>(a), <span class="pl-c1">similar</span>(b));

<span class="pl-c"><span class="pl-c">#</span> Benchmark the original objective function, sum(a' * b + a * b')</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">f</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">234.61</span> kb
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">6</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">110.000</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">137.416</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">173.085</span> μs (<span class="pl-c1">11.63</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">3.613</span> ms (<span class="pl-c1">91.47</span><span class="pl-k">%</span> GC)

<span class="pl-c"><span class="pl-c">#</span> Benchmark ∇f! at the same inputs (this is executing the function,</span>
<span class="pl-c"><span class="pl-c">#</span> getting the gradient w.r.t. `a`, and getting the gradient w.r.t</span>
<span class="pl-c"><span class="pl-c">#</span> to `b` simultaneously). Notice that the whole thing is</span>
<span class="pl-c"><span class="pl-c">#</span> non-allocating.</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">∇f!</span>(<span class="pl-k">$</span>results, <span class="pl-k">$</span>inputs)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0.00</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">429.650</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">431.460</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">469.916</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">937.512</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)</pre></div>
<p>I've used this benchmark (and others) to pit ReverseDiff against every other native
Julia reverse-mode AD package that I know of (including source-to-source packages),
and have found ReverseDiff to be faster and use less memory in most cases.</p>
<h2><a id="user-content-should-i-use-reversediff-or-forwarddiff" class="anchor" aria-hidden="true" href="#should-i-use-reversediff-or-forwarddiff"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Should I use ReverseDiff or ForwardDiff?</h2>
<p>ForwardDiff is algorithmically more efficient for differentiating functions where the input
dimension is less than the output dimension, while ReverseDiff is algorithmically more
efficient for differentiating functions where the output dimension is less than the input
dimension.</p>
<p>Thus, ReverseDiff is generally a better choice for gradients, but Jacobians and Hessians are
trickier to determine. For example, optimized methods for computing nested derivatives might
use a combination of forward-mode and reverse-mode AD.</p>
<p>ForwardDiff is often faster than ReverseDiff for lower dimensional gradients (<code>length(input) &lt; 100</code>), or gradients of functions where the number of input parameters is small compared
to the number of operations performed on them. ReverseDiff is often faster if your code
is expressed as a series of array operations, e.g. a composition of Julia's Base linear
algebra methods.</p>
<p>In general, your choice of algorithms will depend on the function being differentiated, and
you should benchmark different methods to see how they fare.</p>
</article></div>