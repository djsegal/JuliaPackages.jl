<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-xgboostjl" class="anchor" aria-hidden="true" href="#xgboostjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>XGBoost.jl</h1>
<p><a href="https://travis-ci.org/dmlc/XGBoost.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dc920f74fc1bd76aa9eb985ddd7a21fa63a2944b/68747470733a2f2f7472617669732d63692e6f72672f646d6c632f5847426f6f73742e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/dmlc/XGBoost.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>eXtreme Gradient Boosting in Julia</p>
<h2><a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Abstract</h2>
<p>This package is a Julia interface of <a href="https://github.com/dmlc/xgboost">XGBoost</a>.
It is an efficient and scalable implementation of distributed gradient boosting
framework. The package includes efficient linear model solver and tree learning algorithms. The
library is parallelized using OpenMP, and it can be more than 10 times faster than some existing
gradient boosting packages. It supports various objective functions, including regression,
classification and ranking. The package is also made to be extensible, so that users are also
allowed to define their own objectives easily.</p>
<h2><a id="user-content-features" class="anchor" aria-hidden="true" href="#features"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Features</h2>
<ul>
<li>Sparse feature format, it allows easy handling of missing values, and improve computation
efficiency.</li>
<li>Advanced features, such as customized loss function, cross validation, see <a href="demo">demo folder</a>
for walkthrough examples.</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia"><pre>] add XGBoost</pre></div>
<p>or</p>
<div class="highlight highlight-source-julia"><pre>] develop <span class="pl-s"><span class="pl-pds">"</span>https://github.com/dmlc/XGBoost.jl.git<span class="pl-pds">"</span></span>
] build XGBoost</pre></div>
<p>By default, the package installs prebuilt binaries for XGBoost <code>v0.82.0</code> on Linux, MacOS and Windows. Only the linux version is built with OpenMP.</p>
<h2><a id="user-content-minimal-examples" class="anchor" aria-hidden="true" href="#minimal-examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Minimal examples</h2>
<p>To show how XGBoost works, here is an example of dataset Mushroom</p>
<ul>
<li>Prepare Data</li>
</ul>
<p>XGBoost support Julia <code>Array</code>, <code>SparseMatrixCSC</code>, libSVM format text and XGBoost binary
file as input. Here is an example of Mushroom classification. This example will use the function
<code>readlibsvm</code> in <a href="demo/basic_walkthrough.jl#L5">basic_walkthrough.jl</a>. This function load libsvm
format text into Julia dense matrix.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> XGBoost

train_X, train_Y <span class="pl-k">=</span> <span class="pl-c1">readlibsvm</span>(<span class="pl-s"><span class="pl-pds">"</span>data/agaricus.txt.train<span class="pl-pds">"</span></span>, (<span class="pl-c1">6513</span>, <span class="pl-c1">126</span>))
test_X, test_Y <span class="pl-k">=</span> <span class="pl-c1">readlibsvm</span>(<span class="pl-s"><span class="pl-pds">"</span>data/agaricus.txt.test<span class="pl-pds">"</span></span>, (<span class="pl-c1">1611</span>, <span class="pl-c1">126</span>))
</pre></div>
<ul>
<li>Fit Model</li>
</ul>
<div class="highlight highlight-source-julia"><pre>num_round <span class="pl-k">=</span> <span class="pl-c1">2</span>
bst <span class="pl-k">=</span> <span class="pl-c1">xgboost</span>(train_X, num_round, label <span class="pl-k">=</span> train_Y, eta <span class="pl-k">=</span> <span class="pl-c1">1</span>, max_depth <span class="pl-k">=</span> <span class="pl-c1">2</span>)</pre></div>
<h2><a id="user-content-predict" class="anchor" aria-hidden="true" href="#predict"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Predict</h2>
<div class="highlight highlight-source-julia"><pre>pred <span class="pl-k">=</span> <span class="pl-c1">predict</span>(bst, test_X)
<span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">"</span>test-error=<span class="pl-pds">"</span></span>, <span class="pl-c1">sum</span>((pred <span class="pl-k">.&gt;</span> <span class="pl-c1">0.5</span>) <span class="pl-k">.!=</span> test_Y) <span class="pl-k">/</span> <span class="pl-c1">float</span>(<span class="pl-c1">size</span>(pred)[<span class="pl-c1">1</span>]), <span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\n</span><span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-cross-validation" class="anchor" aria-hidden="true" href="#cross-validation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cross-Validation</h2>
<div class="highlight highlight-source-julia"><pre>nfold <span class="pl-k">=</span> <span class="pl-c1">5</span>
param <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>max_depth<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span>,
         <span class="pl-s"><span class="pl-pds">"</span>eta<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span>,
         <span class="pl-s"><span class="pl-pds">"</span>objective<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-s"><span class="pl-pds">"</span>binary:logistic<span class="pl-pds">"</span></span>]
metrics <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>auc<span class="pl-pds">"</span></span>]
<span class="pl-c1">nfold_cv</span>(train_X, num_round, nfold, label <span class="pl-k">=</span> train_Y, param <span class="pl-k">=</span> param, metrics <span class="pl-k">=</span> metrics)</pre></div>
<h2><a id="user-content-feature-walkthrough" class="anchor" aria-hidden="true" href="#feature-walkthrough"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Feature Walkthrough</h2>
<p>Check <a href="https://github.com/antinucleon/XGBoost.jl/blob/master/demo/">demo</a></p>
<ul>
<li><a href="demo/basic_walkthrough.jl">Basic walkthrough of features</a></li>
<li><a href="demo/custom_objective.jl">Customize loss function, and evaluation metric</a></li>
<li><a href="demo/boost_from_prediction.jl">Boosting from existing prediction</a></li>
<li><a href="demo/predict_first_ntree.jl">Predicting using first n trees</a></li>
<li><a href="demo/generalized_linear_model.jl">Generalized Linear Model</a></li>
<li><a href="demo/cross_validation.jl">Cross validation</a></li>
</ul>
<h2><a id="user-content-model-parameter-setting" class="anchor" aria-hidden="true" href="#model-parameter-setting"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Model Parameter Setting</h2>
<p>Check <a href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="nofollow">XGBoost Documentation</a></p>
</article></div>