<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-sgdoptim" class="anchor" aria-hidden="true" href="#sgdoptim"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SGDOptim</h1>
<p>A Julia package for Stochastic Gradient Descent (SGD) and its variants.</p>
<p><a href="https://travis-ci.org/lindahua/SGDOptim.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/07569ecae6806e80c0bfd06c9cffb386bf4099de/68747470733a2f2f7472617669732d63692e6f72672f6c696e64616875612f5347444f7074696d2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/lindahua/SGDOptim.jl.svg?branch=master" style="max-width:100%;"></a></p>
<hr>
<p>With the advent of <em>Big Data</em>, <em>Stochastic Gradient Descent (SGD)</em> has become increasingly popular in recent years, especially in machine learning and related areas. This package implements the SGD algorithm and its variants under a generic setting to facilitate the use of SGD in practice.</p>
<p>Here is an <a href="http://nbviewer.ipython.org/github/lindahua/SGDOptim.jl/blob/master/example.ipynb" rel="nofollow">example</a> that demonstrates the use of this package in solving a ridge regression problem.</p>
<h2><a id="user-content-optimization-algorithms" class="anchor" aria-hidden="true" href="#optimization-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Optimization Algorithms</h2>
<p>This package depends on <a href="https://github.com/lindahua/EmpiricalRisks.jl">EmpiricalRisks.jl</a>, which provides the basic components, including <em>predictors</em>, <em>loss functions</em>, and <em>regularizers</em>.</p>
<p>On top of that, we provide a variety of algorithms, including SGD and its variants, and you may choose one that is suitable for your need:</p>
<p><strong>For streaming settings:</strong></p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Stochastic Gradient Descent</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> Accelerated Stochastic Gradient Descent</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> Stochastic Proximal Gradient Descent</li>
</ul>
<p><strong>For distributed settings:</strong></p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> Parallel Alternate Direction Methods of Multipliers (ADMM)</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> ADMM with Variable Splitting</li>
</ul>
<p><strong>Learning rate:</strong></p>
<p>The setting of the <em>learning rate</em> has significant impact on the algorithm's behavior. This package allows the learning rate setting to be provided as a function on <code>t</code> as a keyword argument.</p>
<p>The default setting is <code>t -&gt; 1.0 / (1.0 + t)</code>.</p>
<h4><a id="user-content-key-functions" class="anchor" aria-hidden="true" href="#key-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Key Functions</h4>
<ul>
<li>
<p><strong>sgd</strong>(rmodel, theta, stream; ...)</p>
<p>Performs stochastic gradient descent to solve a (regularized) risk minimization problem.</p>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>rmodel</code></td>
<td>the risk model, which can be constructed using <a href="http://empiricalrisksjl.readthedocs.org/en/latest/riskmodels.html#risk-models" rel="nofollow">riskmodel</a> method.</td>
</tr>
<tr>
<td><code>theta</code></td>
<td>The initial guess of the model parameter.</td>
</tr>
<tr>
<td><code>stream</code></td>
<td>The input data stream. (See the <em>Streams</em> section below for details)</td>
</tr>
</tbody>
</table>
<p>This function also accepts keyword arguments:</p>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>reg</code></td>
<td>the regularizer (default = <code>ZeroReg()</code>, means no regularization). See the <a href="http://empiricalrisksjl.readthedocs.org/en/latest/regularizers.html" rel="nofollow">documentation on regularizers</a> for details.</td>
</tr>
<tr>
<td><code>lrate</code></td>
<td>the learning rate rule, which should be a function of <code>t</code> (default as mentioned above).</td>
</tr>
<tr>
<td><code>callback</code></td>
<td>the callback function, which will be invoked during iterations. default is <code>simple_trace</code>. See the <em>Callbacks</em> section below for detail.</td>
</tr>
<tr>
<td><code>cbinterval</code></td>
<td>the interval of invoking the callback, <em>i.e.</em> the function invokes the callback every <code>cbinterval</code> iterations. (default is <code>0</code>, meaning that it never invokes the callback).</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h2><a id="user-content-streams" class="anchor" aria-hidden="true" href="#streams"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Streams</h2>
<p>Unlike conventional methods, SGD and its variants look at a single sample or a small batch of samples at each iteration. In other words, data are viewed as a stream of samples or minibatches.</p>
<p>This package provides a variety of ways to construct data streams. Each data stream is essentially an iterator that implements the <code>start</code>, <code>done</code>, and <code>next</code> methods (see [here]( &lt;<a href="http://julia.readthedocs.org/en/latest/stdlib/collections/#iteration" rel="nofollow">http://julia.readthedocs.org/en/latest/stdlib/collections/#iteration</a>) for details of Julia's iteration patterns). Each item from a data stream can be either a sample (as a pair of input and output) or a mini-batch (as a pair of multi-input array and multi-output array).</p>
<p><strong>Note:</strong> All SGD algorithms in this package support both sample streams and mini-batch streams. At each iteration, the algorithm works on a single item from the stream, which can be either a sample or a mini-batch.</p>
<p>The package provides several methods to construct streams of samples or minibatches.</p>
<ul>
<li>
<p><strong>sample_seq</strong>(X, Y[, ord])</p>
<p>Wrap an input array <code>X</code> and an output array <code>Y</code> into a stream of individual samples.</p>
<p>Each item of the stream is a pair, comprised of an item from <code>X</code> and a corresponding item from <code>Y</code>. If <code>X</code> is a vector, then each item of <code>X</code> is a scalar, if <code>X</code> is a matrix, then each item of <code>X</code> is a column vector. The same applies to <code>Y</code>.</p>
<p>The <code>ord</code> argument is an instance of <code>AbstractVector</code> that specifies the order in which the samples are scanned. If <code>ord</code> is omitted, it is, by default, set to the natural order, namely,
<code>1:n</code>, where <code>n</code> is the number of samples in the data set.</p>
</li>
<li>
<p><strong>minibatch_seq</strong>(X, Y, bsize[, ord])</p>
<p>Wrap an input array <code>X</code> and an output array <code>Y</code> into a stream of mini-batches of size <code>bsize</code> or smaller.</p>
<p>For example, if <code>X</code> and <code>Y</code> have <code>28</code> samples, by setting <code>bsize</code> to <code>10</code>, we partition the data set into three minibatches, respectively corresponding to the indices <code>1:10</code>, <code>11:20</code>, and <code>21:28</code>.</p>
<p>The <code>ord</code> argument specifies the order in which the mini-batches are used. For example, if <code>ord</code> is set to <code>[3, 2, 1]</code>, it first takes the 3rd batch, then 2nd, and finally 1st. If <code>ord</code> is omitted, it is, by default, set to the natural order, namely, <code>1:m</code>, where <code>m</code> is the number of mini-batches.</p>
</li>
</ul>
<h2><a id="user-content-callbacks" class="anchor" aria-hidden="true" href="#callbacks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Callbacks</h2>
<p>The algorithms provided in this package interoperate with the rest of the world through <em>callbacks</em>. In particular, it allows a third party (<em>e.g.</em> a higher-level script, a user, a GUI, etc) to monitor the progress of the optimization and take proper actions.</p>
<p>Generally, a <em>callback</em> is an arbitrary function (or closure) that can be called in the following way:</p>
<pre><code>callback(theta, t, n, v)
</code></pre>
<table>
<thead>
<tr>
<th>params</th>
<th>descriptions</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>theta</code></td>
<td>The current solution.</td>
</tr>
<tr>
<td><code>t</code></td>
<td>The number of elapsed iterations.</td>
</tr>
<tr>
<td><code>n</code></td>
<td>The number of samples that have been used.</td>
</tr>
<tr>
<td><code>v</code></td>
<td>The objective value of the last item, which can be an objective evaluated on a single sample or the total objective value evaluated on the last batch of samples.</td>
</tr>
</tbody>
</table>
<p>The package already provides some callbacks for simple use:</p>
<ul>
<li>
<p><code>simple_trace</code></p>
<p>Simply print the optimization trace, including the number of iterations, and the average loss of the last iteration.</p>
<p>This is the default choice for most algorithms.</p>
</li>
<li>
<p><code>gtcompare_trace(theta_g)</code></p>
<p>In addition to printing the optimization trace, it also computes and shows the deviation from a given oracle <code>theta_g</code>.</p>
<p><strong>Note:</strong> <code>gtcompare_trace</code> is a high-level function, and <code>gtcompare_trace(theta_g)</code> produces a callback function.</p>
</li>
</ul>
</article></div>