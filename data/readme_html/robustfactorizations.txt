<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-robustfactorizations" class="anchor" aria-hidden="true" href="#robustfactorizations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>RobustFactorizations</h1>
<p dir="auto"><a href="https://github.com/baggepinnen/RobustFactorizations.jl/actions"><img src="https://github.com/baggepinnen/RobustFactorizations.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/baggepinnen/RobustFactorizations.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/88b904a0886b9dfe3d737e650abe152ce5baef9ce6e5f39b35fca27f10dba42a/68747470733a2f2f636f6465636f762e696f2f67682f626167676570696e6e656e2f526f62757374466163746f72697a6174696f6e732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/baggepinnen/RobustFactorizations.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">This package provides some utilities for robust factorization of matrices, useful for, e.g., matrix completion and denoising.</p>
<p dir="auto">We try to find the low-rank matrix <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$L$</math-renderer> when given matrix <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$L_n$</math-renderer> corrupted by sparse noise <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$S$</math-renderer> and dense noise <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$D$</math-renderer> according to <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$L_n = L + D + S$</math-renderer>. Typically, <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$S$</math-renderer> contains very few entries, but they may be very large, while the entries in <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$D$</math-renderer> are much smaller, and maybe normally distributed.</p>
<h2 dir="auto">
<a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<h3 dir="auto">
<a id="user-content-only-sparse-noise" class="anchor" aria-hidden="true" href="#only-sparse-noise"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Only sparse noise</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="L = lowrank(100,10,3)
S = 10sparserandn(100,10)
Ln = L + S
res = rpca(Ln, verbose=false)
@show opnorm(L - res.L)/opnorm(L)"><pre>L <span class="pl-k">=</span> <span class="pl-c1">lowrank</span>(<span class="pl-c1">100</span>,<span class="pl-c1">10</span>,<span class="pl-c1">3</span>)
S <span class="pl-k">=</span> <span class="pl-c1">10</span><span class="pl-c1">sparserandn</span>(<span class="pl-c1">100</span>,<span class="pl-c1">10</span>)
Ln <span class="pl-k">=</span> L <span class="pl-k">+</span> S
res <span class="pl-k">=</span> <span class="pl-c1">rpca</span>(Ln, verbose<span class="pl-k">=</span><span class="pl-c1">false</span>)
<span class="pl-c1">@show</span> <span class="pl-c1">opnorm</span>(L <span class="pl-k">-</span> res<span class="pl-k">.</span>L)<span class="pl-k">/</span><span class="pl-c1">opnorm</span>(L)</pre></div>
<h3 dir="auto">
<a id="user-content-dense-and-sparse-noise" class="anchor" aria-hidden="true" href="#dense-and-sparse-noise"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dense and sparse noise</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="L = lowrank(100,10,3)      # A low-rank matrix
D = randn(100,10)          # A dense noise matrix
S = 10sparserandn(100,10)  # A sparse noise matrix (large noise)
Ln = L + D + S             # Ln is the sum of them all
λ = 1/sqrt(maximum(size(L)))
res1 = rpca(Ln, verbose=false)
res2 = rpca(Ln, verbose=false, proxD=SqrNormL2(λ/std(D))) # proxD parameter might need tuning
@show opnorm(L - res1.L)/opnorm(L), opnorm(L - res2.L)/opnorm(L)"><pre>L <span class="pl-k">=</span> <span class="pl-c1">lowrank</span>(<span class="pl-c1">100</span>,<span class="pl-c1">10</span>,<span class="pl-c1">3</span>)      <span class="pl-c"><span class="pl-c">#</span> A low-rank matrix</span>
D <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">100</span>,<span class="pl-c1">10</span>)          <span class="pl-c"><span class="pl-c">#</span> A dense noise matrix</span>
S <span class="pl-k">=</span> <span class="pl-c1">10</span><span class="pl-c1">sparserandn</span>(<span class="pl-c1">100</span>,<span class="pl-c1">10</span>)  <span class="pl-c"><span class="pl-c">#</span> A sparse noise matrix (large noise)</span>
Ln <span class="pl-k">=</span> L <span class="pl-k">+</span> D <span class="pl-k">+</span> S             <span class="pl-c"><span class="pl-c">#</span> Ln is the sum of them all</span>
λ <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">/</span><span class="pl-c1">sqrt</span>(<span class="pl-c1">maximum</span>(<span class="pl-c1">size</span>(L)))
res1 <span class="pl-k">=</span> <span class="pl-c1">rpca</span>(Ln, verbose<span class="pl-k">=</span><span class="pl-c1">false</span>)
res2 <span class="pl-k">=</span> <span class="pl-c1">rpca</span>(Ln, verbose<span class="pl-k">=</span><span class="pl-c1">false</span>, proxD<span class="pl-k">=</span><span class="pl-c1">SqrNormL2</span>(λ<span class="pl-k">/</span><span class="pl-c1">std</span>(D))) <span class="pl-c"><span class="pl-c">#</span> proxD parameter might need tuning</span>
<span class="pl-c1">@show</span> <span class="pl-c1">opnorm</span>(L <span class="pl-k">-</span> res1<span class="pl-k">.</span>L)<span class="pl-k">/</span><span class="pl-c1">opnorm</span>(L), <span class="pl-c1">opnorm</span>(L <span class="pl-k">-</span> res2<span class="pl-k">.</span>L)<span class="pl-k">/</span><span class="pl-c1">opnorm</span>(L)</pre></div>
<h2 dir="auto">
<a id="user-content-functions" class="anchor" aria-hidden="true" href="#functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Functions</h2>
<ul dir="auto">
<li>
<code>rpca</code> Works very well, uses "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices", Zhouchen Lin, Minming Chen, Leqin Wu, Yi Ma, <a href="https://people.eecs.berkeley.edu/~yima/psfile/Lin09-MP.pdf" rel="nofollow">https://people.eecs.berkeley.edu/~yima/psfile/Lin09-MP.pdf</a>
</li>
<li>
<code>rpca_fista</code> requires tuning.</li>
<li>
<code>rpca_admm</code> requires tuning.</li>
</ul>
<p dir="auto">The <code>rpca</code> function is the recommended default choice:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="rpca(Ln::Matrix; λ=1.0 / √(maximum(size(A))), iters=1000, tol=1.0e-7, ρ=1.5, verbose=false, nonnegL=false, nonnegS=false, nukeA=true)"><pre><span class="pl-c1">rpca</span>(Ln<span class="pl-k">::</span><span class="pl-c1">Matrix</span>; λ<span class="pl-k">=</span><span class="pl-c1">1.0</span> <span class="pl-k">/</span> <span class="pl-k">√</span>(<span class="pl-c1">maximum</span>(<span class="pl-c1">size</span>(A))), iters<span class="pl-k">=</span><span class="pl-c1">1000</span>, tol<span class="pl-k">=</span><span class="pl-c1">1.0e-7</span>, ρ<span class="pl-k">=</span><span class="pl-c1">1.5</span>, verbose<span class="pl-k">=</span><span class="pl-c1">false</span>, nonnegL<span class="pl-k">=</span><span class="pl-c1">false</span>, nonnegS<span class="pl-k">=</span><span class="pl-c1">false</span>, nukeA<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<p dir="auto">It solves the following problem:</p>
<math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="fa7dd46b6e7266f0658d13660809d713">$$\text{minimize}_{L,D,S} ||L||_* + \lambda ||S||_1 + \gamma ||D||^2_2 \quad \text{s.t. } L_n = L+D+S$$</math-renderer>
<p dir="auto">Reference:</p>
<blockquote>
<p dir="auto">"The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices", Zhouchen Lin, Minming Chen, Leqin Wu, Yi Ma, <a href="https://people.eecs.berkeley.edu/~yima/psfile/Lin09-MP.pdf" rel="nofollow">https://people.eecs.berkeley.edu/~yima/psfile/Lin09-MP.pdf</a></p>
</blockquote>
<p dir="auto"><strong>Arguments:</strong></p>
<ul dir="auto">
<li>
<code>Ln</code>: Input data matrix</li>
<li>
<code>λ</code>: Sparsity regularization</li>
<li>
<code>iters</code>: Maximum number of iterations</li>
<li>
<code>tol</code>: Tolerance</li>
<li>
<code>ρ</code>: Algorithm tuning param</li>
<li>
<code>verbose</code>: Print status</li>
<li>
<code>nonnegL</code>: Hard thresholding on A</li>
<li>
<code>nonnegS</code>: Hard thresholding on E</li>
<li>
<code>proxL</code>: Defaults to <code>NuclearNorm(1/2)</code>
</li>
<li>
<code>proxD</code>: Defaults to <code>nothing</code>
</li>
<li>
<code>proxS</code>: Defaults to <code>NormL1(λ))</code>
</li>
</ul>
<p dir="auto">To speed up convergence you may either increase the tolerance or increase <code>ρ</code>. Increasing <code>tol</code> is often the best solution.</p>
</article></div>