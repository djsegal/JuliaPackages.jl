<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-supervisedlearning" class="anchor" aria-hidden="true" href="#supervisedlearning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SupervisedLearning</h1>
<p><a href="http://www.repostatus.org/#suspended" rel="nofollow"><img src="https://camo.githubusercontent.com/292e8a8a2576abd63982798528afdb61457d96be6ef0b07b84d2dc1a7893b08f/687474703a2f2f7777772e7265706f7374617475732e6f72672f6261646765732f6c61746573742f73757370656e6465642e737667" alt="Project Status: Suspended - Initial development has started, but there has not yet been a stable, usable release; work has been stopped for the time being but the author(s) intend on resuming work." data-canonical-src="http://www.repostatus.org/badges/latest/suspended.svg" style="max-width:100%;"></a>
<a href="LICENSE.md"><img src="https://camo.githubusercontent.com/bbf49a2eb96e6f718803f2493bd7aa3baae61abb09b7f8fc185a94e08c504dc6/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d627269676874677265656e2e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" style="max-width:100%;"></a></p>
<p>Work in progress for a front-end supervised learning framework. Currently the focus is on creating a pure Julia package for SVMs in <a href="https://github.com/Evizero/KSVM.jl">KSVM.jl</a></p>
<p><a href="https://travis-ci.org/Evizero/SupervisedLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ca8020ad0f5d0b154349f3485a3fd0303593b739d3b317ad66e44744e817ae7d/68747470733a2f2f7472617669732d63692e6f72672f4576697a65726f2f537570657276697365644c6561726e696e672e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Evizero/SupervisedLearning.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>The goal of this library is manyfold:</p>
<ul>
<li><strong>Education:</strong> allow the user to play around with the models, solvers, etc. for educational purposes. Provide a good base for course exercises. For example visualizing the learning curve of neural networks using different optimization algorithms.</li>
<li><strong>Research:</strong> Swap out parts of the machine learning pipeline with custom implementations without losing the ability to utilize the rest of the framework. For example to prototype new prediction models.</li>
<li><strong>Application:</strong> Porcelain interface to apply machine learning to given datasets in a convenient way. There might be multiple high-level interface for different usergroups (e.g. one that mimics R's caret package)</li>
</ul>
<h2><a id="user-content-planned-high-level-api" class="anchor" aria-hidden="true" href="#planned-high-level-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Planned High-level API</h2>
<p>The following code should already work</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using SupervisedLearning
using RDatasets
using UnicodePlots

data = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)

# In this case the dataset will be in-memory and encoded to -1, 1
# There will also be support for datastreaming from HDF5
problemSet = dataSource(AM ~ DRat + WT + DRat&amp;WT, data, SignedClassEncoding)

# Convenient to use with UnicodePlots
print(barplot(classDistribution(problemSet)...))

# Methods for splitting the abstract data sets
trainSet, testSet = splitTrainTest!(problemSet, p_train = .75)

# Specifies the model and modelspecific parameter
model = Classifier.LogisticRegression(l2_coef=0.1)

# Backend for neural networks will be Mocha.jl or OnlineAI.jl
# model = Classifier.FeedForwardNeuralNetwork([4,5,7],[ReLu,ReLu,ReLu])

# train! mutates the model state
#  * the do-block is the callback function which also allows for early stopping
#  * In the regression case Solver.GradientDescent() will result in using Regression.jl, 
#    otherwise (in most deterministic cases) Optim.jl
#  * There will also be stochastic gradient descent with minibatches
train!(model, trainSet, Solver.GradientDescent(), max_iter = 10000, break_every = 100) do
  # You can also use the callback to execute any code
  # For example to print informative messages
  println(&quot;Testset accuracy: &quot;, accuracy(model, testSet))
  
  # You can easily store custom learning curves or other arbitrary values
  # They will be linked to the correct iteration automatically
  remember!(model, :testsetCost, cost(model, testSet))
end

# The loss of the training set is stored by default and can be accessed with trainingCurve
# x is a Vector{Int} of iterations with stepsize break_every,
# y is a Vector{Float64} where y[i] is the cost of the trainSet at x[i]
x, y = trainingCurve(model)
print(lineplot(x, y, title = &quot;Learning curve for trainSet&quot;))

# Customly stored curves can be accessed with &quot;history&quot;
# x is a Vector{Int} of iterations (exact values depend on when you called remember!),
# y is a Vector{Float64} where y[i] is the cost of the testSet at x[i]
x, y = history(model, :testsetCost)
print(lineplot(x, y, title = &quot;Learning curve for testSet&quot;))

ŷ = predict(model, testSet) # what the model says
t = groundtruth(testSet) # what it should be
"><pre><span class="pl-k">using</span> SupervisedLearning
<span class="pl-k">using</span> RDatasets
<span class="pl-k">using</span> UnicodePlots

data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>mtcars<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> In this case the dataset will be in-memory and encoded to -1, 1</span>
<span class="pl-c"><span class="pl-c">#</span> There will also be support for datastreaming from HDF5</span>
problemSet <span class="pl-k">=</span> <span class="pl-c1">dataSource</span>(AM <span class="pl-k">~</span> DRat <span class="pl-k">+</span> WT <span class="pl-k">+</span> DRat<span class="pl-k">&amp;</span>WT, data, SignedClassEncoding)

<span class="pl-c"><span class="pl-c">#</span> Convenient to use with UnicodePlots</span>
<span class="pl-c1">print</span>(<span class="pl-c1">barplot</span>(<span class="pl-c1">classDistribution</span>(problemSet)<span class="pl-k">...</span>))

<span class="pl-c"><span class="pl-c">#</span> Methods for splitting the abstract data sets</span>
trainSet, testSet <span class="pl-k">=</span> <span class="pl-c1">splitTrainTest!</span>(problemSet, p_train <span class="pl-k">=</span> <span class="pl-c1">.75</span>)

<span class="pl-c"><span class="pl-c">#</span> Specifies the model and modelspecific parameter</span>
model <span class="pl-k">=</span> Classifier<span class="pl-k">.</span><span class="pl-c1">LogisticRegression</span>(l2_coef<span class="pl-k">=</span><span class="pl-c1">0.1</span>)

<span class="pl-c"><span class="pl-c">#</span> Backend for neural networks will be Mocha.jl or OnlineAI.jl</span>
<span class="pl-c"><span class="pl-c">#</span> model = Classifier.FeedForwardNeuralNetwork([4,5,7],[ReLu,ReLu,ReLu])</span>

<span class="pl-c"><span class="pl-c">#</span> train! mutates the model state</span>
<span class="pl-c"><span class="pl-c">#</span>  * the do-block is the callback function which also allows for early stopping</span>
<span class="pl-c"><span class="pl-c">#</span>  * In the regression case Solver.GradientDescent() will result in using Regression.jl, </span>
<span class="pl-c"><span class="pl-c">#</span>    otherwise (in most deterministic cases) Optim.jl</span>
<span class="pl-c"><span class="pl-c">#</span>  * There will also be stochastic gradient descent with minibatches</span>
<span class="pl-c1">train!</span>(model, trainSet, Solver<span class="pl-k">.</span><span class="pl-c1">GradientDescent</span>(), max_iter <span class="pl-k">=</span> <span class="pl-c1">10000</span>, break_every <span class="pl-k">=</span> <span class="pl-c1">100</span>) <span class="pl-k">do</span>
  <span class="pl-c"><span class="pl-c">#</span> You can also use the callback to execute any code</span>
  <span class="pl-c"><span class="pl-c">#</span> For example to print informative messages</span>
  <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Testset accuracy: <span class="pl-pds">"</span></span>, <span class="pl-c1">accuracy</span>(model, testSet))
  
  <span class="pl-c"><span class="pl-c">#</span> You can easily store custom learning curves or other arbitrary values</span>
  <span class="pl-c"><span class="pl-c">#</span> They will be linked to the correct iteration automatically</span>
  <span class="pl-c1">remember!</span>(model, <span class="pl-c1">:testsetCost</span>, <span class="pl-c1">cost</span>(model, testSet))
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> The loss of the training set is stored by default and can be accessed with trainingCurve</span>
<span class="pl-c"><span class="pl-c">#</span> x is a Vector{Int} of iterations with stepsize break_every,</span>
<span class="pl-c"><span class="pl-c">#</span> y is a Vector{Float64} where y[i] is the cost of the trainSet at x[i]</span>
x, y <span class="pl-k">=</span> <span class="pl-c1">trainingCurve</span>(model)
<span class="pl-c1">print</span>(<span class="pl-c1">lineplot</span>(x, y, title <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Learning curve for trainSet<span class="pl-pds">"</span></span>))

<span class="pl-c"><span class="pl-c">#</span> Customly stored curves can be accessed with "history"</span>
<span class="pl-c"><span class="pl-c">#</span> x is a Vector{Int} of iterations (exact values depend on when you called remember!),</span>
<span class="pl-c"><span class="pl-c">#</span> y is a Vector{Float64} where y[i] is the cost of the testSet at x[i]</span>
x, y <span class="pl-k">=</span> <span class="pl-c1">history</span>(model, <span class="pl-c1">:testsetCost</span>)
<span class="pl-c1">print</span>(<span class="pl-c1">lineplot</span>(x, y, title <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Learning curve for testSet<span class="pl-pds">"</span></span>))

ŷ <span class="pl-k">=</span> <span class="pl-c1">predict</span>(model, testSet) <span class="pl-c"><span class="pl-c">#</span> what the model says</span>
t <span class="pl-k">=</span> <span class="pl-c1">groundtruth</span>(testSet) <span class="pl-c"><span class="pl-c">#</span> what it should be</span></pre></div>
<h2><a id="user-content-planned-mid-level-api" class="anchor" aria-hidden="true" href="#planned-mid-level-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Planned Mid-level API</h2>
<p>This is just a rough draft and still object to change</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using SupervisedLearning
using RDatasets

data = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)

# In this case the dataset will be in-memory.
# Specifying the encoding is not necessary.
# The model will select the encoding it needs automatically
# Trees for example don't need an encoding at all.
problemSet = dataSource(AM ~ DRat + WT, data)

# Methods for splitting the abstract data sets
trainSet, testSet = splitTrainTest!(problemSet, p_train = .75)

# Perform a gridsearch over an arbitrary modelspace
gsResult = gridsearch([.001, .01, .1], [.0001, .0003]) do lr, lambda

  # Perform cross validation to get a good estimate for the hyperparameter performance
  cvResult = crossvalidate(trainSet, k = 5) do trainFold, valFold

    # Specify the model and model-specific parameters
    model = Classifier.LogisticRegression(l2_coef = lambda)

    # Specify the solver and solver-specific parameters
    solver = Solver.NaiveGradientDescent(learning_rate = lr, normalize_gradient = false)

    # train! mutates the model state
    train!(model, trainFold, solver, max_iter = 1000)

    # make sure to return the trained model
    model
  end

  # You can return a model or a cvResult to gridsearch
  cvResult
end

# Plot the final accuracy of all trained models using UnicodePlots
print(barplot(accuracy(gsResult, testSet)...))

# Get the best model
bestModel = gsResult.bestModel
ŷ = predict(bestModel, testSet)
"><pre><span class="pl-k">using</span> SupervisedLearning
<span class="pl-k">using</span> RDatasets

data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>mtcars<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> In this case the dataset will be in-memory.</span>
<span class="pl-c"><span class="pl-c">#</span> Specifying the encoding is not necessary.</span>
<span class="pl-c"><span class="pl-c">#</span> The model will select the encoding it needs automatically</span>
<span class="pl-c"><span class="pl-c">#</span> Trees for example don't need an encoding at all.</span>
problemSet <span class="pl-k">=</span> <span class="pl-c1">dataSource</span>(AM <span class="pl-k">~</span> DRat <span class="pl-k">+</span> WT, data)

<span class="pl-c"><span class="pl-c">#</span> Methods for splitting the abstract data sets</span>
trainSet, testSet <span class="pl-k">=</span> <span class="pl-c1">splitTrainTest!</span>(problemSet, p_train <span class="pl-k">=</span> <span class="pl-c1">.75</span>)

<span class="pl-c"><span class="pl-c">#</span> Perform a gridsearch over an arbitrary modelspace</span>
gsResult <span class="pl-k">=</span> <span class="pl-c1">gridsearch</span>([<span class="pl-c1">.001</span>, <span class="pl-c1">.01</span>, <span class="pl-c1">.1</span>], [<span class="pl-c1">.0001</span>, <span class="pl-c1">.0003</span>]) <span class="pl-k">do</span> lr, lambda

  <span class="pl-c"><span class="pl-c">#</span> Perform cross validation to get a good estimate for the hyperparameter performance</span>
  cvResult <span class="pl-k">=</span> <span class="pl-c1">crossvalidate</span>(trainSet, k <span class="pl-k">=</span> <span class="pl-c1">5</span>) <span class="pl-k">do</span> trainFold, valFold

    <span class="pl-c"><span class="pl-c">#</span> Specify the model and model-specific parameters</span>
    model <span class="pl-k">=</span> Classifier<span class="pl-k">.</span><span class="pl-c1">LogisticRegression</span>(l2_coef <span class="pl-k">=</span> lambda)

    <span class="pl-c"><span class="pl-c">#</span> Specify the solver and solver-specific parameters</span>
    solver <span class="pl-k">=</span> Solver<span class="pl-k">.</span><span class="pl-c1">NaiveGradientDescent</span>(learning_rate <span class="pl-k">=</span> lr, normalize_gradient <span class="pl-k">=</span> <span class="pl-c1">false</span>)

    <span class="pl-c"><span class="pl-c">#</span> train! mutates the model state</span>
    <span class="pl-c1">train!</span>(model, trainFold, solver, max_iter <span class="pl-k">=</span> <span class="pl-c1">1000</span>)

    <span class="pl-c"><span class="pl-c">#</span> make sure to return the trained model</span>
    model
  <span class="pl-k">end</span>

  <span class="pl-c"><span class="pl-c">#</span> You can return a model or a cvResult to gridsearch</span>
  cvResult
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Plot the final accuracy of all trained models using UnicodePlots</span>
<span class="pl-c1">print</span>(<span class="pl-c1">barplot</span>(<span class="pl-c1">accuracy</span>(gsResult, testSet)<span class="pl-k">...</span>))

<span class="pl-c"><span class="pl-c">#</span> Get the best model</span>
bestModel <span class="pl-k">=</span> gsResult<span class="pl-k">.</span>bestModel
ŷ <span class="pl-k">=</span> <span class="pl-c1">predict</span>(bestModel, testSet)</pre></div>
</article></div>