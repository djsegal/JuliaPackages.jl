<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-supervisedlearning" class="anchor" aria-hidden="true" href="#supervisedlearning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SupervisedLearning</h1>
<p><a href="http://www.repostatus.org/#suspended" rel="nofollow"><img src="https://camo.githubusercontent.com/e79bdb70997a3d100eee22a68424c486a6a72b1f/687474703a2f2f7777772e7265706f7374617475732e6f72672f6261646765732f6c61746573742f73757370656e6465642e737667" alt="Project Status: Suspended - Initial development has started, but there has not yet been a stable, usable release; work has been stopped for the time being but the author(s) intend on resuming work." data-canonical-src="http://www.repostatus.org/badges/latest/suspended.svg" style="max-width:100%;"></a>
<a href="LICENSE.md"><img src="https://camo.githubusercontent.com/4440d5deb3a53c4f8661ee765378e6071e7878e8/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d627269676874677265656e2e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" style="max-width:100%;"></a></p>
<p>Work in progress for a front-end supervised learning framework. Currently the focus is on creating a pure Julia package for SVMs in <a href="https://github.com/Evizero/KSVM.jl">KSVM.jl</a></p>
<p><a href="https://travis-ci.org/Evizero/SupervisedLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dc0032fa5a24dd6e11760fadf661a92a468e5372/68747470733a2f2f7472617669732d63692e6f72672f4576697a65726f2f537570657276697365644c6561726e696e672e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Evizero/SupervisedLearning.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>The goal of this library is manyfold:</p>
<ul>
<li><strong>Education:</strong> allow the user to play around with the models, solvers, etc. for educational purposes. Provide a good base for course exercises. For example visualizing the learning curve of neural networks using different optimization algorithms.</li>
<li><strong>Research:</strong> Swap out parts of the machine learning pipeline with custom implementations without losing the ability to utilize the rest of the framework. For example to prototype new prediction models.</li>
<li><strong>Application:</strong> Porcelain interface to apply machine learning to given datasets in a convenient way. There might be multiple high-level interface for different usergroups (e.g. one that mimics R's caret package)</li>
</ul>
<h2><a id="user-content-planned-high-level-api" class="anchor" aria-hidden="true" href="#planned-high-level-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Planned High-level API</h2>
<p>The following code should already work</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> SupervisedLearning
<span class="pl-k">using</span> RDatasets
<span class="pl-k">using</span> UnicodePlots

data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>mtcars<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> In this case the dataset will be in-memory and encoded to -1, 1</span>
<span class="pl-c"><span class="pl-c">#</span> There will also be support for datastreaming from HDF5</span>
problemSet <span class="pl-k">=</span> <span class="pl-c1">dataSource</span>(AM <span class="pl-k">~</span> DRat <span class="pl-k">+</span> WT <span class="pl-k">+</span> DRat<span class="pl-k">&amp;</span>WT, data, SignedClassEncoding)

<span class="pl-c"><span class="pl-c">#</span> Convenient to use with UnicodePlots</span>
<span class="pl-c1">print</span>(<span class="pl-c1">barplot</span>(<span class="pl-c1">classDistribution</span>(problemSet)<span class="pl-k">...</span>))

<span class="pl-c"><span class="pl-c">#</span> Methods for splitting the abstract data sets</span>
trainSet, testSet <span class="pl-k">=</span> <span class="pl-c1">splitTrainTest!</span>(problemSet, p_train <span class="pl-k">=</span> <span class="pl-c1">.75</span>)

<span class="pl-c"><span class="pl-c">#</span> Specifies the model and modelspecific parameter</span>
model <span class="pl-k">=</span> Classifier<span class="pl-k">.</span><span class="pl-c1">LogisticRegression</span>(l2_coef<span class="pl-k">=</span><span class="pl-c1">0.1</span>)

<span class="pl-c"><span class="pl-c">#</span> Backend for neural networks will be Mocha.jl or OnlineAI.jl</span>
<span class="pl-c"><span class="pl-c">#</span> model = Classifier.FeedForwardNeuralNetwork([4,5,7],[ReLu,ReLu,ReLu])</span>

<span class="pl-c"><span class="pl-c">#</span> train! mutates the model state</span>
<span class="pl-c"><span class="pl-c">#</span>  * the do-block is the callback function which also allows for early stopping</span>
<span class="pl-c"><span class="pl-c">#</span>  * In the regression case Solver.GradientDescent() will result in using Regression.jl, </span>
<span class="pl-c"><span class="pl-c">#</span>    otherwise (in most deterministic cases) Optim.jl</span>
<span class="pl-c"><span class="pl-c">#</span>  * There will also be stochastic gradient descent with minibatches</span>
<span class="pl-c1">train!</span>(model, trainSet, Solver<span class="pl-k">.</span><span class="pl-c1">GradientDescent</span>(), max_iter <span class="pl-k">=</span> <span class="pl-c1">10000</span>, break_every <span class="pl-k">=</span> <span class="pl-c1">100</span>) <span class="pl-k">do</span>
  <span class="pl-c"><span class="pl-c">#</span> You can also use the callback to execute any code</span>
  <span class="pl-c"><span class="pl-c">#</span> For example to print informative messages</span>
  <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Testset accuracy: <span class="pl-pds">"</span></span>, <span class="pl-c1">accuracy</span>(model, testSet))
  
  <span class="pl-c"><span class="pl-c">#</span> You can easily store custom learning curves or other arbitrary values</span>
  <span class="pl-c"><span class="pl-c">#</span> They will be linked to the correct iteration automatically</span>
  <span class="pl-c1">remember!</span>(model, <span class="pl-c1">:testsetCost</span>, <span class="pl-c1">cost</span>(model, testSet))
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> The loss of the training set is stored by default and can be accessed with trainingCurve</span>
<span class="pl-c"><span class="pl-c">#</span> x is a Vector{Int} of iterations with stepsize break_every,</span>
<span class="pl-c"><span class="pl-c">#</span> y is a Vector{Float64} where y[i] is the cost of the trainSet at x[i]</span>
x, y <span class="pl-k">=</span> <span class="pl-c1">trainingCurve</span>(model)
<span class="pl-c1">print</span>(<span class="pl-c1">lineplot</span>(x, y, title <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Learning curve for trainSet<span class="pl-pds">"</span></span>))

<span class="pl-c"><span class="pl-c">#</span> Customly stored curves can be accessed with "history"</span>
<span class="pl-c"><span class="pl-c">#</span> x is a Vector{Int} of iterations (exact values depend on when you called remember!),</span>
<span class="pl-c"><span class="pl-c">#</span> y is a Vector{Float64} where y[i] is the cost of the testSet at x[i]</span>
x, y <span class="pl-k">=</span> <span class="pl-c1">history</span>(model, <span class="pl-c1">:testsetCost</span>)
<span class="pl-c1">print</span>(<span class="pl-c1">lineplot</span>(x, y, title <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Learning curve for testSet<span class="pl-pds">"</span></span>))

yÌ‚ <span class="pl-k">=</span> <span class="pl-c1">predict</span>(model, testSet) <span class="pl-c"><span class="pl-c">#</span> what the model says</span>
t <span class="pl-k">=</span> <span class="pl-c1">groundtruth</span>(testSet) <span class="pl-c"><span class="pl-c">#</span> what it should be</span></pre></div>
<h2><a id="user-content-planned-mid-level-api" class="anchor" aria-hidden="true" href="#planned-mid-level-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Planned Mid-level API</h2>
<p>This is just a rough draft and still object to change</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> SupervisedLearning
<span class="pl-k">using</span> RDatasets

data <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>mtcars<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> In this case the dataset will be in-memory.</span>
<span class="pl-c"><span class="pl-c">#</span> Specifying the encoding is not necessary.</span>
<span class="pl-c"><span class="pl-c">#</span> The model will select the encoding it needs automatically</span>
<span class="pl-c"><span class="pl-c">#</span> Trees for example don't need an encoding at all.</span>
problemSet <span class="pl-k">=</span> <span class="pl-c1">dataSource</span>(AM <span class="pl-k">~</span> DRat <span class="pl-k">+</span> WT, data)

<span class="pl-c"><span class="pl-c">#</span> Methods for splitting the abstract data sets</span>
trainSet, testSet <span class="pl-k">=</span> <span class="pl-c1">splitTrainTest!</span>(problemSet, p_train <span class="pl-k">=</span> <span class="pl-c1">.75</span>)

<span class="pl-c"><span class="pl-c">#</span> Perform a gridsearch over an arbitrary modelspace</span>
gsResult <span class="pl-k">=</span> <span class="pl-c1">gridsearch</span>([<span class="pl-c1">.001</span>, <span class="pl-c1">.01</span>, <span class="pl-c1">.1</span>], [<span class="pl-c1">.0001</span>, <span class="pl-c1">.0003</span>]) <span class="pl-k">do</span> lr, lambda

  <span class="pl-c"><span class="pl-c">#</span> Perform cross validation to get a good estimate for the hyperparameter performance</span>
  cvResult <span class="pl-k">=</span> <span class="pl-c1">crossvalidate</span>(trainSet, k <span class="pl-k">=</span> <span class="pl-c1">5</span>) <span class="pl-k">do</span> trainFold, valFold

    <span class="pl-c"><span class="pl-c">#</span> Specify the model and model-specific parameters</span>
    model <span class="pl-k">=</span> Classifier<span class="pl-k">.</span><span class="pl-c1">LogisticRegression</span>(l2_coef <span class="pl-k">=</span> lambda)

    <span class="pl-c"><span class="pl-c">#</span> Specify the solver and solver-specific parameters</span>
    solver <span class="pl-k">=</span> Solver<span class="pl-k">.</span><span class="pl-c1">NaiveGradientDescent</span>(learning_rate <span class="pl-k">=</span> lr, normalize_gradient <span class="pl-k">=</span> <span class="pl-c1">false</span>)

    <span class="pl-c"><span class="pl-c">#</span> train! mutates the model state</span>
    <span class="pl-c1">train!</span>(model, trainFold, solver, max_iter <span class="pl-k">=</span> <span class="pl-c1">1000</span>)

    <span class="pl-c"><span class="pl-c">#</span> make sure to return the trained model</span>
    model
  <span class="pl-k">end</span>

  <span class="pl-c"><span class="pl-c">#</span> You can return a model or a cvResult to gridsearch</span>
  cvResult
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Plot the final accuracy of all trained models using UnicodePlots</span>
<span class="pl-c1">print</span>(<span class="pl-c1">barplot</span>(<span class="pl-c1">accuracy</span>(gsResult, testSet)<span class="pl-k">...</span>))

<span class="pl-c"><span class="pl-c">#</span> Get the best model</span>
bestModel <span class="pl-k">=</span> gsResult<span class="pl-k">.</span>bestModel
yÌ‚ <span class="pl-k">=</span> <span class="pl-c1">predict</span>(bestModel, testSet)</pre></div>
</article></div>