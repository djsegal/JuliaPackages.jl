<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-cudasimdtypesjl" class="anchor" aria-hidden="true" href="#cudasimdtypesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CUDASIMDTypes.jl</h1>
<p dir="auto">Explicit SIMD types that live in 32 bits, optimized for CUDA with
fallbacks for regular CPUs.</p>
<ul dir="auto">
<li><a href="https://eschnett.github.io/CUDASIMDTypes.jl/dev/" rel="nofollow">Documentation</a></li>
<li><a href="https://github.com/eschnett/CUDASIMDTypes.jl/actions"><img src="https://github.com/eschnett/CUDASIMDTypes.jl/workflows/CI/badge.svg" alt="GitHub CI" style="max-width: 100%;"></a></li>
<li><a href="https://codecov.io/gh/eschnett/CUDASIMDTypes.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/af4ba7d0c29d07c04208ce9ad8b7aee10bc725d7e27b2a24630725b9fc48b372/68747470733a2f2f636f6465636f762e696f2f67682f657363686e6574742f4355444153494d4454797065732e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d373546543033554c4844" alt="codecov" data-canonical-src="https://codecov.io/gh/eschnett/CUDASIMDTypes.jl/branch/main/graph/badge.svg?token=75FT03ULHD" style="max-width: 100%;"></a></li>
</ul>
<p dir="auto">(CI code coverage is bad because the CI tests don't test the CUDA
code; these tests need to be run manually.)</p>
<h2 dir="auto"><a id="user-content-description" class="anchor" aria-hidden="true" href="#description"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Description</h2>
<p dir="auto">CUDA supports storing SIMD integer for floating-point in its 32-bit
registers. These types are today most prominently used in the tensor
core operations. These types are <code>Int4x8</code>, <code>Int8x4</code>, <code>Int16x2</code>,
<code>Float16x2</code>, and <code>BFloat16x2</code>. Each such type stores multiple small
integer or floating point numbers in a single 32-bit register.</p>
<p dir="auto">Unfortunately, plain CUDA has very little support for these types.
This Julia package <code>CUDASIMDTypes.jl</code> defines respective data types,
constructors, conversion routines to tuples (to decompose the SIMD
types), and simple arithmetic operations. When executing in CUDA,
these operations are highly optimized. These operations are also
supported on CPUs, but are usually less efficient there. (This could
be remedied by interfacing this package with
<a href="https://github.com/eschnett/SIMD.jl"><code>SIMD.jl</code></a>.</p>
<p dir="auto">This package also defines and exports a few helper functions that
correspond to certain CUDA PTX instructions, such as <code>prmt</code> and
<code>lop3</code>, and defines a function <code>bitifelse</code>. These are used internally
but might also be useful in other CUDA packages.</p>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">Create two <code>Int8x4</code> numbers, add them, and convert the result into a tuple:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using CUDASIMDTypes
[ Info: Precompiling CUDASIMDTypes [ba1ee33b-8807-41fd-9812-6d5f2ce04139]

julia&gt; i = Int8x4(1, 2, 3, 4)
(1, 2, 3, 4)

julia&gt; j = Int8x4(5, 6, 7, 8)
(5, 6, 7, 8)

julia&gt; k = i + j
(6, 8, 10, 12)

julia&gt; convert(NTuple{4,Int32}, k)
(6, 8, 10, 12)"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> CUDASIMDTypes
[ Info<span class="pl-k">:</span> Precompiling CUDASIMDTypes [ba1ee33b<span class="pl-k">-</span><span class="pl-c1">8807</span><span class="pl-k">-</span><span class="pl-c1">41</span>fd<span class="pl-k">-</span><span class="pl-c1">9812</span><span class="pl-k">-</span><span class="pl-c1">6</span>d5f2ce04139]

julia<span class="pl-k">&gt;</span> i <span class="pl-k">=</span> <span class="pl-c1">Int8x4</span>(<span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>)
(<span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>)

julia<span class="pl-k">&gt;</span> j <span class="pl-k">=</span> <span class="pl-c1">Int8x4</span>(<span class="pl-c1">5</span>, <span class="pl-c1">6</span>, <span class="pl-c1">7</span>, <span class="pl-c1">8</span>)
(<span class="pl-c1">5</span>, <span class="pl-c1">6</span>, <span class="pl-c1">7</span>, <span class="pl-c1">8</span>)

julia<span class="pl-k">&gt;</span> k <span class="pl-k">=</span> i <span class="pl-k">+</span> j
(<span class="pl-c1">6</span>, <span class="pl-c1">8</span>, <span class="pl-c1">10</span>, <span class="pl-c1">12</span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">convert</span>(NTuple{<span class="pl-c1">4</span>,Int32}, k)
(<span class="pl-c1">6</span>, <span class="pl-c1">8</span>, <span class="pl-c1">10</span>, <span class="pl-c1">12</span>)</pre></div>
<p dir="auto">Create an <code>Int4x8</code> vector, and split it into its even and odd
components, converted into 2 <code>Int8x4</code> vectors. Note that <code>Int4</code> is a
rather small type, so that our input <code>8</code> overflows to <code>-8</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using CUDASIMDTypes

julia&gt; i = Int4x8(1, 2, 3, 4, 5, 6, 7, 8)
(1, 2, 3, 4, 5, 6, 7, -8)

julia&gt; jlo, jhi = convert(NTuple{2,Int8x4}, i)
((1, 3, 5, 7), (2, 4, 6, -8))"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> CUDASIMDTypes

julia<span class="pl-k">&gt;</span> i <span class="pl-k">=</span> <span class="pl-c1">Int4x8</span>(<span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>, <span class="pl-c1">5</span>, <span class="pl-c1">6</span>, <span class="pl-c1">7</span>, <span class="pl-c1">8</span>)
(<span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>, <span class="pl-c1">5</span>, <span class="pl-c1">6</span>, <span class="pl-c1">7</span>, <span class="pl-k">-</span><span class="pl-c1">8</span>)

julia<span class="pl-k">&gt;</span> jlo, jhi <span class="pl-k">=</span> <span class="pl-c1">convert</span>(NTuple{<span class="pl-c1">2</span>,Int8x4}, i)
((<span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">5</span>, <span class="pl-c1">7</span>), (<span class="pl-c1">2</span>, <span class="pl-c1">4</span>, <span class="pl-c1">6</span>, <span class="pl-k">-</span><span class="pl-c1">8</span>))</pre></div>
<p dir="auto">Create <code>Float16x2</code> numbers, multiply and add them, and sum the result:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; x = Float16x2(1.0, 2.0)
(1.0f0, 2.0f0)

julia&gt; y = Float16x2(3.0, 4.0)
(3.0f0, 4.0f0)

julia&gt; z = Float16x2(5.0, 6.0)
(5.0f0, 6.0f0)

julia&gt; r = muladd(x, y, z)
(8.0f0, 14.0f0)

julia&gt; convert(NTuple{2,Float32}, r)
(8.0f0, 14.0f0)

julia&gt; convert(NTuple{2,Float32}, r) |&gt; sum
22.0f0"><pre>julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">Float16x2</span>(<span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>)
(<span class="pl-c1">1.0f0</span>, <span class="pl-c1">2.0f0</span>)

julia<span class="pl-k">&gt;</span> y <span class="pl-k">=</span> <span class="pl-c1">Float16x2</span>(<span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>)
(<span class="pl-c1">3.0f0</span>, <span class="pl-c1">4.0f0</span>)

julia<span class="pl-k">&gt;</span> z <span class="pl-k">=</span> <span class="pl-c1">Float16x2</span>(<span class="pl-c1">5.0</span>, <span class="pl-c1">6.0</span>)
(<span class="pl-c1">5.0f0</span>, <span class="pl-c1">6.0f0</span>)

julia<span class="pl-k">&gt;</span> r <span class="pl-k">=</span> <span class="pl-c1">muladd</span>(x, y, z)
(<span class="pl-c1">8.0f0</span>, <span class="pl-c1">14.0f0</span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">convert</span>(NTuple{<span class="pl-c1">2</span>,Float32}, r)
(<span class="pl-c1">8.0f0</span>, <span class="pl-c1">14.0f0</span>)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">convert</span>(NTuple{<span class="pl-c1">2</span>,Float32}, r) <span class="pl-k">|&gt;</span> sum
<span class="pl-c1">22.0f0</span></pre></div>
</article></div>