<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aicenter/GenerativeModels.jl/workflows/Run%20tests/badge.svg"><img src="https://github.com/aicenter/GenerativeModels.jl/workflows/Run%20tests/badge.svg" alt="Run tests" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/aicenter/GenerativeModels.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/edaf2a2e39060df9b7e85846923800eca729d3cb34678c98d34945af89be1b9f/68747470733a2f2f636f6465636f762e696f2f67682f616963656e7465722f47656e657261746976654d6f64656c732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/aicenter/GenerativeModels.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-generativemodelsjl" class="anchor" aria-hidden="true" href="#generativemodelsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GenerativeModels.jl</h1>
<p dir="auto">This library contains a collection of generative models.
It uses trainable
<a href="https://github.com/aicenter/ConditionalDists.jl"><code>ConditionalDists.jl</code></a> that
can be used in conjuction with <a href="https://github.com/FluxML/Flux.jl"><code>Flux.jl</code></a>
models.  Probability measures such as KL divergence are defined in
<a href="https://github.com/aicenter/IPMeasures.jl"><code>IPMeasures.jl</code></a> This package aims
to make experimenting with new models as easy as possible.</p>
<p dir="auto">As an example, check out how to build a conventional variational autoencoder (VAE)
that reconstructs MNIST below.</p>
<h1 dir="auto"><a id="user-content-reconstructing-mnist" class="anchor" aria-hidden="true" href="#reconstructing-mnist"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reconstructing MNIST</h1>
<p dir="auto">First we load the MNIST training dataset</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLDatasets, Flux
train_x, _ = MNIST.traindata(Float32)
flat_x = reshape(train_x, :, size(train_x,3)) |&gt; gpu
data = Flux.Data.DataLoader(flat_x, batchsize=200, shuffle=true);"><pre><span class="pl-k">using</span> MLDatasets, Flux
train_x, _ <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>(Float32)
flat_x <span class="pl-k">=</span> <span class="pl-c1">reshape</span>(train_x, :, <span class="pl-c1">size</span>(train_x,<span class="pl-c1">3</span>)) <span class="pl-k">|&gt;</span> gpu
data <span class="pl-k">=</span> Flux<span class="pl-k">.</span>Data<span class="pl-k">.</span><span class="pl-c1">DataLoader</span>(flat_x, batchsize<span class="pl-k">=</span><span class="pl-c1">200</span>, shuffle<span class="pl-k">=</span><span class="pl-c1">true</span>);</pre></div>
<p dir="auto">and define some parameters for a VAE with an input length <code>xlength</code> and latent
vector of <code>zlength</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ConditionalDists

xlength = size(flat_x, 1)
zlength = 2
hdim    = 512
hd2     = Int(hdim/2)"><pre><span class="pl-k">using</span> ConditionalDists

xlength <span class="pl-k">=</span> <span class="pl-c1">size</span>(flat_x, <span class="pl-c1">1</span>)
zlength <span class="pl-k">=</span> <span class="pl-c1">2</span>
hdim    <span class="pl-k">=</span> <span class="pl-c1">512</span>
hd2     <span class="pl-k">=</span> <span class="pl-c1">Int</span>(hdim<span class="pl-k">/</span><span class="pl-c1">2</span>)</pre></div>
<p dir="auto">We define an <code>encoder</code> with diagonal variance on the latent dimension,
which is just a Flux model wrapped in a <code>ConditionalMvNormal</code>.  The Flux model
must return a tuple with the appropriate number of parameters - in case of a
<code>MvNormal</code> two: mean and variance.  Hence, the <code>SplitLayer</code> returns two vectors
of <code>zlength</code>, one of which (the variance) is constrained to be positive.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ConditionalDists: SplitLayer

# mapping that will be trained to output mean and variance
enc_map = Chain(Dense(xlength, hdim, relu),
                Dense(hdim, hd2, relu),
                SplitLayer(hd2, [zlength,zlength], [identity,softplus]))
# conditional encoder (can be called e.g. like `rand(encoder,x)`, see ConditionalDists.jl)
encoder = ConditionalMvNormal(enc_map)"><pre><span class="pl-k">using</span> ConditionalDists<span class="pl-k">:</span> SplitLayer

<span class="pl-c"><span class="pl-c">#</span> mapping that will be trained to output mean and variance</span>
enc_map <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(xlength, hdim, relu),
                <span class="pl-c1">Dense</span>(hdim, hd2, relu),
                <span class="pl-c1">SplitLayer</span>(hd2, [zlength,zlength], [identity,softplus]))
<span class="pl-c"><span class="pl-c">#</span> conditional encoder (can be called e.g. like `rand(encoder,x)`, see ConditionalDists.jl)</span>
encoder <span class="pl-k">=</span> <span class="pl-c1">ConditionalMvNormal</span>(enc_map)</pre></div>
<p dir="auto">The decoder will return a Multivariate Normal with scalar variance:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="dec_map = Chain(Dense(zlength, hd2, relu),
                Dense(hd2, hdim, relu),
                SplitLayer(hdim, [xlength,1], σ))
decoder = ConditionalMvNormal(dec_map)"><pre>dec_map <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(zlength, hd2, relu),
                <span class="pl-c1">Dense</span>(hd2, hdim, relu),
                <span class="pl-c1">SplitLayer</span>(hdim, [xlength,<span class="pl-c1">1</span>], σ))
decoder <span class="pl-k">=</span> <span class="pl-c1">ConditionalMvNormal</span>(dec_map)</pre></div>
<p dir="auto">Now we can create the VAE model and train it to maximize the ELBO.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using GenerativeModels

model = VAE(zlength, encoder, decoder) |&gt; gpu
loss(x) = -elbo(model,x)

ps = Flux.params(model)
opt = ADAM()

for e in 1:50
    @info &quot;Epoch $e&quot; loss(flat_x)
    Flux.train!(loss, ps, data, opt)
end"><pre><span class="pl-k">using</span> GenerativeModels

model <span class="pl-k">=</span> <span class="pl-c1">VAE</span>(zlength, encoder, decoder) <span class="pl-k">|&gt;</span> gpu
<span class="pl-en">loss</span>(x) <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">elbo</span>(model,x)

ps <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">params</span>(model)
opt <span class="pl-k">=</span> <span class="pl-c1">ADAM</span>()

<span class="pl-k">for</span> e <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">50</span>
    <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Epoch <span class="pl-v">$e</span><span class="pl-pds">"</span></span> <span class="pl-c1">loss</span>(flat_x)
    Flux<span class="pl-k">.</span><span class="pl-c1">train!</span>(loss, ps, data, opt)
<span class="pl-k">end</span></pre></div>
<p dir="auto">Some test reconstructions and the corresponding latent space are shown below:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="model = model |&gt; cpu
test_x, test_y = MNIST.testdata(Float32)
p1 = plot_reconstructions(model, test_x[:,:,1:6])"><pre>model <span class="pl-k">=</span> model <span class="pl-k">|&gt;</span> cpu
test_x, test_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">testdata</span>(Float32)
p1 <span class="pl-k">=</span> <span class="pl-c1">plot_reconstructions</span>(model, test_x[:,:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">6</span>])</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="figures/README_7_1.png"><img src="figures/README_7_1.png" alt="" style="max-width: 100%;"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="figures/README_8_1.png"><img src="figures/README_8_1.png" alt="" style="max-width: 100%;"></a></p>
</article></div>