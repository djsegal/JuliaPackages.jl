<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-embeddings" class="anchor" aria-hidden="true" href="#embeddings"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Embeddings</h1>
<p><a href="https://travis-ci.org/JuliaText/Embeddings.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/7786cf223277d90c0b60475d736ab6c18ff93d1c15725bfcd44811a6d52a5c41/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961546578742f456d62656464696e67732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaText/Embeddings.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/JuliaText/Embeddings.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/53f6c3ac41162584a27f61ead6539a2e1ff72d54bde5f55302c03f92df8794d2/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961546578742f456d62656464696e67732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="CodeCov" data-canonical-src="https://codecov.io/gh/JuliaText/Embeddings.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Introduction</h2>
<p>Word Embeddings present words as high-dimensional vectors, where every dimension corresponds to some latent feature [1]. This makes it possible to utilize different mathematical operations between words. With these we can discover semantic relationships between words. E.g. when using <a href="https://code.google.com/archive/p/word2vec/" rel="nofollow">Word2Vec</a> embeddings and utilizing cosine similarity between vectors, the calculation <code>vector(“Madrid”) - vector(“Spain”) + vector(“France”)</code> gives as an answer the vector for word “Paris” [2].
Pretraining Word Embeddings are commonly uses to initialize the bottom layer of a more advanced NLP method, such as a LSTM [3].
Simply summing the embeddings in a sentence or phrase can in and of itself be a surprisingly powerful way to represent the sentence/phrase, and can be used as a input to simple ML models like SVM 4].</p>
<p>This package gives access to pretrained embeddings. At its current state it includes following word embeddings: <a href="https://code.google.com/archive/p/word2vec/" rel="nofollow">Word2Vec</a> (English), <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow">GloVe</a> (English), and <a href="https://fasttext.cc/" rel="nofollow">FastText</a> (hundreds of languages).</p>
<h3><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h3>
<p>The package can be installed using the <a href="https://julialang.github.io/Pkg.jl/v1/managing-packages/#Adding-packages-1" rel="nofollow">julia package manager in the normal way.</a>.</p>
<p>Open the REPL, press <kbd>]</kbd> to enter package mode, and then:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="pkg&gt; add Embeddings
"><pre>pkg<span class="pl-k">&gt;</span> add Embeddings</pre></div>
<p>There are no further steps.
Pretrained embeddings will be downloaded the first time you use them.</p>
<h2><a id="user-content-details" class="anchor" aria-hidden="true" href="#details"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Details</h2>
<h3><a id="user-content-load_embeddings" class="anchor" aria-hidden="true" href="#load_embeddings"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><code>load_embeddings</code></h3>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="load_embeddings(EmbeddingSystem, [embedding_file|default_file_number])
load_embeddings(EmbeddingSystem{:lang}, [embedding_file|default_file_number])
"><pre><code>load_embeddings(EmbeddingSystem, [embedding_file|default_file_number])
load_embeddings(EmbeddingSystem{:lang}, [embedding_file|default_file_number])
</code></pre></div>
<p>Loaded the embeddings from a embedding file.
The embeddings should be of the type given by the Embedding system.</p>
<p>If the <code>embedding file</code> is not provided, a default embedding file will be used.
(It will be automatically installed if required).
EmbeddingSystems have a language type parameter.
For example <code>FastText_Text{:fr}</code> or <code>Word2Vec{:en}</code>, if that language parameter is not given it defaults to English.
(I am sorry for the poor state of the NLP field that many embedding formats are only available pretrained in English.)
Using this the correct  default embedding file will be installed for that language.
For some languages and embedding systems there are multiple possible files.
You can check the list of them using for example <code>language_files(FastText_Text{:de})</code>.
The first is nominally the most popular, but if you want to default to another you can do so by setting the <code>default_file_num</code>.</p>
<h3><a id="user-content-this-returns-an-embeddingtable-object" class="anchor" aria-hidden="true" href="#this-returns-an-embeddingtable-object"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>This returns an <code>EmbeddingTable</code> object.</h3>
<p>This has 2 fields.</p>
<ul>
<li><code>embeddings</code> is a matrix, each column is the embedding for a word.</li>
<li><code>vocab</code> is a vector of strings, ordered as per the columns of <code>embeddings</code>, such that the first string in vocab is the first column of <code>embeddings</code> etc</li>
</ul>
<p>We do not include a method for getting the index of a column from a word.
This is trivial to define in code (<code>vocab2ind(vocab)=Dict(word=&gt;ii for (ii,word) in enumerate(vocab))</code>),
and you might like to be doing this in a more consistant way, e.g using <a href="https://github.com/JuliaML/MLLabelUtils.jl">MLLabelUtils.jl</a>,
or you might like to build a much faster Dict solution on top of <a href="https://github.com/JuliaString/InternedStrings.jl">InternedStrings.jl</a></p>
<h2><a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Configuration</h2>
<p>This package is build on top of <a href="https://github.com/oxinabox/DataDeps.jl">DataDeps.jl</a>.
To configure, e.g., where downloaded files save to, and read from (and to understand how that works),
see the DataDeps.jl readme.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<p>Load the package with</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; using Embeddings
"><pre><code>julia&gt; using Embeddings
</code></pre></div>
<h3><a id="user-content-basic-example" class="anchor" aria-hidden="true" href="#basic-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic example</h3>
<p>The Following script loads up the embeddings,
and defines a <code>Dict</code> to map from vocabulary word to index, in the embedding matrix,
and a function that used it to get an embedding vector.
This is a basic way to access the embedding for a word.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using Embeddings
const embtable = load_embeddings(Word2Vec) # or load_embeddings(FastText_Text) or ...

const get_word_index = Dict(word=&gt;ii for (ii,word) in enumerate(embtable.vocab))

function get_embedding(word)
    ind = get_word_index[word]
    emb = embtable.embeddings[:,ind]
    return emb
end
"><pre><code>using Embeddings
const embtable = load_embeddings(Word2Vec) # or load_embeddings(FastText_Text) or ...

const get_word_index = Dict(word=&gt;ii for (ii,word) in enumerate(embtable.vocab))

function get_embedding(word)
    ind = get_word_index[word]
    emb = embtable.embeddings[:,ind]
    return emb
end
</code></pre></div>
<p>This can be used like so:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; get_embedding(&quot;blue&quot;)
300-element Array{Float32,1}:
  0.01540828
  0.03409082
  0.0882124
  0.04680265
 -0.03409082
...
"><pre><code>julia&gt; get_embedding("blue")
300-element Array{Float32,1}:
  0.01540828
  0.03409082
  0.0882124
  0.04680265
 -0.03409082
...
</code></pre></div>
<h3><a id="user-content-loading-different-embeddings" class="anchor" aria-hidden="true" href="#loading-different-embeddings"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Loading different Embeddings</h3>
<p>load up the default word2vec embeddings:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; load_embeddings(Word2Vec) 
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.0673199 0.0529562 … -0.21143 0.0136373; -0.0534466 0.0654598 … -0.0087888 -0.0742876; … ; -0.00733469 0.0108946 … -0.00405157 0.0156112; -0.00514565 -0.0470722 … -0.0341579 0.0396559], String[&quot;&lt;/s&gt;&quot;, &quot;in&quot;, &quot;for&quot;, &quot;that&quot;, &quot;is&quot;, &quot;on&quot;, &quot;##&quot;, &quot;The&quot;, &quot;with&quot;, &quot;said&quot;  …  &quot;#-###-PA-PARKS&quot;, &quot;Lackmeyer&quot;, &quot;PERVEZ&quot;, &quot;KUNDI&quot;, &quot;Budhadeb&quot;, &quot;Nautsch&quot;, &quot;Antuane&quot;, &quot;tricorne&quot;, &quot;VISIONPAD&quot;, &quot;RAFFAELE&quot;])
"><pre><code>julia&gt; load_embeddings(Word2Vec) 
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.0673199 0.0529562 … -0.21143 0.0136373; -0.0534466 0.0654598 … -0.0087888 -0.0742876; … ; -0.00733469 0.0108946 … -0.00405157 0.0156112; -0.00514565 -0.0470722 … -0.0341579 0.0396559], String["&lt;/s&gt;", "in", "for", "that", "is", "on", "##", "The", "with", "said"  …  "#-###-PA-PARKS", "Lackmeyer", "PERVEZ", "KUNDI", "Budhadeb", "Nautsch", "Antuane", "tricorne", "VISIONPAD", "RAFFAELE"])
</code></pre></div>
<p>Load up the first 100 embeddings from the default French FastText embeddings:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; load_embeddings(FastText_Text{:fr}; max_vocab_size=100) 
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.0058 -0.0842 … -0.062 -0.0687; 0.0478 -0.0388 … 0.0064 -0.339; … ; 0.023 -0.0106 … -0.022 -0.1581; 0.0378 0.0579 … 0.0417 0.0714], String[&quot;,&quot;, &quot;de&quot;, &quot;.&quot;, &quot;&lt;/s&gt;&quot;, &quot;la&quot;, &quot;et&quot;, &quot;:&quot;, &quot;à&quot;, &quot;le&quot;, &quot;\&quot;&quot;  …  &quot;faire&quot;, &quot;c'&quot;, &quot;aussi&quot;, &quot;&gt;&quot;, &quot;leur&quot;, &quot;%&quot;, &quot;si&quot;, &quot;entre&quot;, &quot;qu&quot;, &quot;€&quot;])
"><pre><code>julia&gt; load_embeddings(FastText_Text{:fr}; max_vocab_size=100) 
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.0058 -0.0842 … -0.062 -0.0687; 0.0478 -0.0388 … 0.0064 -0.339; … ; 0.023 -0.0106 … -0.022 -0.1581; 0.0378 0.0579 … 0.0417 0.0714], String[",", "de", ".", "&lt;/s&gt;", "la", "et", ":", "à", "le", "\""  …  "faire", "c'", "aussi", "&gt;", "leur", "%", "si", "entre", "qu", "€"])
</code></pre></div>
<p>List all the default files for FastText in English:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; language_files(FastText_Text{:en}) # List all the possible default files for FastText in English
3-element Array{String,1}:
 &quot;FastText Common Crawl/crawl-300d-2M.vec&quot;
 &quot;FastText Wiki News/wiki-news-300d-1M.vec&quot;
 &quot;FastText en Wiki Text/wiki.en.vec&quot;
"><pre><code>julia&gt; language_files(FastText_Text{:en}) # List all the possible default files for FastText in English
3-element Array{String,1}:
 "FastText Common Crawl/crawl-300d-2M.vec"
 "FastText Wiki News/wiki-news-300d-1M.vec"
 "FastText en Wiki Text/wiki.en.vec"
</code></pre></div>
<p>From the second of those default files, load the embeddings just for "red", "green", and "blue":</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; load_embeddings(FastText_Text{:en}, 2; keep_words=Set([&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;]))
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[-0.0054 0.0404 -0.0293; 0.0406 0.0621 0.0224; … ; 0.218 0.1542 0.2256; 0.1315 0.1528 0.1051], String[&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;])
"><pre><code>julia&gt; load_embeddings(FastText_Text{:en}, 2; keep_words=Set(["red", "green", "blue"]))
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[-0.0054 0.0404 -0.0293; 0.0406 0.0621 0.0224; … ; 0.218 0.1542 0.2256; 0.1315 0.1528 0.1051], String["red", "green", "blue"])
</code></pre></div>
<p>List all the default files for GloVe in English:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; language_files(GloVe{:en})
10-element Array{String,1}:
 &quot;glove.6B/glove.6B.50d.txt&quot;
 &quot;glove.6B/glove.6B.100d.txt&quot;
 &quot;glove.6B/glove.6B.200d.txt&quot;
 &quot;glove.6B/glove.6B.300d.txt&quot;
 &quot;glove.42B.300d/glove.42B.300d.txt&quot;
 &quot;glove.840B.300d/glove.840B.300d.txt&quot;
 &quot;glove.twitter.27B/glove.twitter.27B.25d.txt&quot;
 &quot;glove.twitter.27B/glove.twitter.27B.50d.txt&quot;
 &quot;glove.twitter.27B/glove.twitter.27B.100d.txt&quot;
 &quot;glove.twitter.27B/glove.twitter.27B.200d.txt&quot;
"><pre><code>julia&gt; language_files(GloVe{:en})
10-element Array{String,1}:
 "glove.6B/glove.6B.50d.txt"
 "glove.6B/glove.6B.100d.txt"
 "glove.6B/glove.6B.200d.txt"
 "glove.6B/glove.6B.300d.txt"
 "glove.42B.300d/glove.42B.300d.txt"
 "glove.840B.300d/glove.840B.300d.txt"
 "glove.twitter.27B/glove.twitter.27B.25d.txt"
 "glove.twitter.27B/glove.twitter.27B.50d.txt"
 "glove.twitter.27B/glove.twitter.27B.100d.txt"
 "glove.twitter.27B/glove.twitter.27B.200d.txt"
</code></pre></div>
<p>Load the 200d GloVe embedding matrix for the top 10000 words trained on 6B words:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; glove = load_embeddings(GloVe{:en}, 3, max_vocab_size=10000)
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[-0.071549 0.17651 … 0.19765 -0.22419; 0.093459 0.29208 … -0.31357 0.039311; … ; 0.030591 -0.23189 … -0.72917 0.49645; 0.25577 -0.10814 … 0.07403 0.41581], [&quot;the&quot;, &quot;,&quot;, &quot;.&quot;, &quot;of&quot;, &quot;to&quot;, &quot;and&quot;, &quot;in&quot;, &quot;a&quot;, &quot;\&quot;&quot;, &quot;'s&quot;  …  &quot;slashed&quot;, &quot;23-year&quot;, &quot;communique&quot;, &quot;hawk&quot;, &quot;necessity&quot;, &quot;petty&quot;, &quot;stretching&quot;, &quot;taxpayer&quot;, &quot;resistant&quot;, &quot;quinn&quot;])

julia&gt; size(glove)
(200, 10000)
"><pre><code>julia&gt; glove = load_embeddings(GloVe{:en}, 3, max_vocab_size=10000)
Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[-0.071549 0.17651 … 0.19765 -0.22419; 0.093459 0.29208 … -0.31357 0.039311; … ; 0.030591 -0.23189 … -0.72917 0.49645; 0.25577 -0.10814 … 0.07403 0.41581], ["the", ",", ".", "of", "to", "and", "in", "a", "\"", "'s"  …  "slashed", "23-year", "communique", "hawk", "necessity", "petty", "stretching", "taxpayer", "resistant", "quinn"])

julia&gt; size(glove)
(200, 10000)
</code></pre></div>
<h2><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<p>Contributions, in the form of bug-reports, pull requests, additional documentation are encouraged.
They can be made to the Github repository.</p>
<p><strong>All contributions and communications should abide by the <a href="https://julialang.org/community/standards/" rel="nofollow">Julia Community Standards</a>.</strong></p>
<p>The following software contributions would particularly be appreciated:</p>
<ul>
<li>Provide Hashstrings: I have only filled in the checksums for the FastText Embeddings that I have downloaded, which is only a small fraction. If you using embeddings files for a language that doesn't have its hashstring set, then DataDeps.jl will tell you the hashstring that need to be added to the file. It is a quick and easy PR.</li>
<li>Provide Implementations of other loaders: If you have implementations of code to load another format (e.g. Binary FastText) it would be great if you could contribute them. I know I have a few others kicking around somewhere.</li>
</ul>
<p>Software contributions should follow the prevailing style within the code-base.
If your pull request (or issues) are not getting responses within a few days do not hesitate to "bump" them,
by posting a comment such as "Any update on the status of this?".
Sometimes Github notifications get lost.</p>
<h2><a id="user-content-support" class="anchor" aria-hidden="true" href="#support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Support</h2>
<p>Feel free to ask for help on the <a href="https://discourse.julialang.org/" rel="nofollow">Julia Discourse forum</a>,
or in the <code>#natural-language</code> channel on julia-slack. (Which you can <a href="https://slackinvite.julialang.org/" rel="nofollow">join here</a>).
You can also raise issues in this repository to request improvements to the documentation.</p>
<h2><a id="user-content-sources" class="anchor" aria-hidden="true" href="#sources"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Sources</h2>
<p>[1]: <a href="https://www.aclweb.org/anthology/P10-1040/" rel="nofollow">Turian, Joseph, Lev Ratinov, and Yoshua Bengio. "Word representations: a simple and general method for semi-supervised learning." Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational Linguistics, 2010.</a></p>
<p>[2]: <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="nofollow">Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." Advances in neural information processing systems. 2013.</a></p>
<p>[3]: <a href="https://www.springer.com/us/book/9789811300615" rel="nofollow">White, Lyndon et al. Neural Representations of Natural Language. Springer: Studies in Computational Intelligence. 2018.</a></p>
<p>[4]: <a href="https://research-repository.uwa.edu.au/en/publications/on-the-surprising-capacity-of-linear-combinations-of-embeddings-f" rel="nofollow">White, Lyndon. On the surprising capacity of linear combinations of embeddings for natural language processing.
Doctorial Thesis, The University of Western Australia. 2019</a></p>
</article></div>