<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-distancesjl" class="anchor" aria-hidden="true" href="#distancesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Distances.jl</h1>
<p><a href="https://travis-ci.org/JuliaStats/Distances.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4762745181f362684d2002f5a72816cc23938ef3/68747470733a2f2f7472617669732d63692e6f72672f4a756c696153746174732f44697374616e6365732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaStats/Distances.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/JuliaStats/Distances.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/5cc34438e7d82e1de6c24ff0ec4dcce826c9dd6d/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f4a756c696153746174732f44697374616e6365732e6a6c2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/JuliaStats/Distances.jl/badge.svg?branch=master&amp;service=github" style="max-width:100%;"></a></p>
<p>A Julia package for evaluating distances(metrics) between vectors.</p>
<p>This package also provides optimized functions to compute column-wise and pairwise distances, which are often substantially faster than a straightforward loop implementation. (See the benchmark section below for details).</p>
<h2><a id="user-content-supported-distances" class="anchor" aria-hidden="true" href="#supported-distances"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supported distances</h2>
<ul>
<li>Euclidean distance</li>
<li>Squared Euclidean distance</li>
<li>Periodic Euclidean distance</li>
<li>Cityblock distance</li>
<li>Total variation distance</li>
<li>Jaccard distance</li>
<li>Rogers-Tanimoto distance</li>
<li>Chebyshev distance</li>
<li>Minkowski distance</li>
<li>Hamming distance</li>
<li>Cosine distance</li>
<li>Correlation distance</li>
<li>Chi-square distance</li>
<li>Kullback-Leibler divergence</li>
<li>Generalized Kullback-Leibler divergence</li>
<li>Rényi divergence</li>
<li>Jensen-Shannon divergence</li>
<li>Mahalanobis distance</li>
<li>Squared Mahalanobis distance</li>
<li>Bhattacharyya distance</li>
<li>Hellinger distance</li>
<li>Haversine distance</li>
<li>Mean absolute deviation</li>
<li>Mean squared deviation</li>
<li>Root mean squared deviation</li>
<li>Normalized root mean squared deviation</li>
<li>Bray-Curtis dissimilarity</li>
<li>Bregman divergence</li>
</ul>
<p>For <code>Euclidean distance</code>, <code>Squared Euclidean distance</code>, <code>Cityblock distance</code>, <code>Minkowski distance</code>, and <code>Hamming distance</code>, a weighted version is also provided.</p>
<h2><a id="user-content-basic-use" class="anchor" aria-hidden="true" href="#basic-use"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic use</h2>
<p>The library supports three ways of computation: <em>computing the distance between two vectors</em>, <em>column-wise computation</em>, and <em>pairwise computation</em>.</p>
<h4><a id="user-content-computing-the-distance-between-two-vectors" class="anchor" aria-hidden="true" href="#computing-the-distance-between-two-vectors"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computing the distance between two vectors</h4>
<p>Each distance corresponds to a <em>distance type</em>. You can always compute a certain distance between two vectors using the following syntax</p>
<div class="highlight highlight-source-julia"><pre>r <span class="pl-k">=</span> <span class="pl-c1">evaluate</span>(dist, x, y)
r <span class="pl-k">=</span> <span class="pl-c1">dist</span>(x, y)</pre></div>
<p>Here, <code>dist</code> is an instance of a distance type. For example, the type for Euclidean distance is <code>Euclidean</code> (more distance types will be introduced in the next section), then you can compute the Euclidean distance between <code>x</code> and <code>y</code> as</p>
<div class="highlight highlight-source-julia"><pre>r <span class="pl-k">=</span> <span class="pl-c1">evaluate</span>(<span class="pl-c1">Euclidean</span>(), x, y)
r <span class="pl-k">=</span> <span class="pl-c1">Euclidean</span>()(x, y)</pre></div>
<p>Common distances also come with convenient functions for distance evaluation. For example, you may also compute Euclidean distance between two vectors as below</p>
<div class="highlight highlight-source-julia"><pre>r <span class="pl-k">=</span> <span class="pl-c1">euclidean</span>(x, y)</pre></div>
<h4><a id="user-content-computing-distances-between-corresponding-columns" class="anchor" aria-hidden="true" href="#computing-distances-between-corresponding-columns"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computing distances between corresponding columns</h4>
<p>Suppose you have two <code>m-by-n</code> matrix <code>X</code> and <code>Y</code>, then you can compute all distances between corresponding columns of <code>X</code> and <code>Y</code> in one batch, using the <code>colwise</code> function, as</p>
<div class="highlight highlight-source-julia"><pre>r <span class="pl-k">=</span> <span class="pl-c1">colwise</span>(dist, X, Y)</pre></div>
<p>The output <code>r</code> is a vector of length <code>n</code>. In particular, <code>r[i]</code> is the distance between <code>X[:,i]</code> and <code>Y[:,i]</code>. The batch computation typically runs considerably faster than calling <code>evaluate</code> column-by-column.</p>
<p>Note that either of <code>X</code> and <code>Y</code> can be just a single vector -- then the <code>colwise</code> function will compute the distance between this vector and each column of the other parameter.</p>
<h4><a id="user-content-computing-pairwise-distances" class="anchor" aria-hidden="true" href="#computing-pairwise-distances"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computing pairwise distances</h4>
<p>Let <code>X</code> and <code>Y</code> respectively have <code>m</code> and <code>n</code> columns. Then the <code>pairwise</code> function with the <code>dims=2</code> argument computes distances between each pair of columns in <code>X</code> and <code>Y</code>:</p>
<div class="highlight highlight-source-julia"><pre>R <span class="pl-k">=</span> <span class="pl-c1">pairwise</span>(dist, X, Y, dims<span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<p>In the output, <code>R</code> is a matrix of size <code>(m, n)</code>, such that <code>R[i,j]</code> is the distance between <code>X[:,i]</code> and <code>Y[:,j]</code>. Computing distances for all pairs using <code>pairwise</code> function is often remarkably faster than evaluting for each pair individually.</p>
<p>If you just want to just compute distances between columns of a matrix <code>X</code>, you can write</p>
<div class="highlight highlight-source-julia"><pre>R <span class="pl-k">=</span> <span class="pl-c1">pairwise</span>(dist, X, dims<span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<p>This statement will result in an <code>m-by-m</code> matrix, where <code>R[i,j]</code> is the distance between <code>X[:,i]</code> and <code>X[:,j]</code>.
<code>pairwise(dist, X)</code> is typically more efficient than <code>pairwise(dist, X, X)</code>, as the former will take advantage of the symmetry when <code>dist</code> is a semi-metric (including metric).</p>
<p>For performance reasons, it is recommended to use matrices with observations in columns (as shown above). Indeed,
the <code>Array</code> type in Julia is column-major, making it more efficient to access memory column by column. However,
matrices with observations stored in rows are also supported via the argument <code>dims=1</code>.</p>
<h4><a id="user-content-computing-column-wise-and-pairwise-distances-inplace" class="anchor" aria-hidden="true" href="#computing-column-wise-and-pairwise-distances-inplace"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computing column-wise and pairwise distances inplace</h4>
<p>If the vector/matrix to store the results are pre-allocated, you may use the storage (without creating a new array) using the following syntax (<code>i</code> being either <code>1</code> or <code>2</code>):</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">colwise!</span>(r, dist, X, Y)
<span class="pl-c1">pairwise!</span>(R, dist, X, Y, dims<span class="pl-k">=</span>i)
<span class="pl-c1">pairwise!</span>(R, dist, X, dims<span class="pl-k">=</span>i)</pre></div>
<p>Please pay attention to the difference, the functions for inplace computation are <code>colwise!</code> and <code>pairwise!</code> (instead of <code>colwise</code> and <code>pairwise</code>).</p>
<h2><a id="user-content-distance-type-hierarchy" class="anchor" aria-hidden="true" href="#distance-type-hierarchy"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Distance type hierarchy</h2>
<p>The distances are organized into a type hierarchy.</p>
<p>At the top of this hierarchy is an abstract class <strong>PreMetric</strong>, which is defined to be a function <code>d</code> that satisfies</p>
<pre><code>d(x, x) == 0  for all x
d(x, y) &gt;= 0  for all x, y
</code></pre>
<p><strong>SemiMetric</strong> is a abstract type that refines <strong>PreMetric</strong>. Formally, a <em>semi-metric</em> is a <em>pre-metric</em> that is also symmetric, as</p>
<pre><code>d(x, y) == d(y, x)  for all x, y
</code></pre>
<p><strong>Metric</strong> is a abstract type that further refines <strong>SemiMetric</strong>. Formally, a <em>metric</em> is a <em>semi-metric</em> that also satisfies triangle inequality, as</p>
<pre><code>d(x, z) &lt;= d(x, y) + d(y, z)  for all x, y, z
</code></pre>
<p>This type system has practical significance. For example, when computing pairwise distances
between a set of vectors, you may only perform computation for half of the pairs, derive the
values immediately for the remaining half by leveraging the symmetry of <em>semi-metrics</em>. Note
that the types of <code>SemiMetric</code> and <code>Metric</code> do not completely follow the definition in
mathematics as they do not require the "distance" to be able to distinguish between points:
for these types <code>x != y</code> does not imply that <code>d(x, y) != 0</code> in general compared to the
mathematical definition of semi-metric and metric, as this property does not change
computations in practice.</p>
<p>Each distance corresponds to a distance type. The type name and the corresponding mathematical definitions of the distances are listed in the following table.</p>
<table>
<thead>
<tr>
<th>type name</th>
<th>convenient syntax</th>
<th>math definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Euclidean</td>
<td><code>euclidean(x, y)</code></td>
<td><code>sqrt(sum((x - y) .^ 2))</code></td>
</tr>
<tr>
<td>SqEuclidean</td>
<td><code>sqeuclidean(x, y)</code></td>
<td><code>sum((x - y).^2)</code></td>
</tr>
<tr>
<td>PeriodicEuclidean</td>
<td><code>peuclidean(x, y, p)</code></td>
<td><code>sqrt(sum(min(mod(abs(x - y), p), p - mod(abs(x - y), p)).^2))</code></td>
</tr>
<tr>
<td>Cityblock</td>
<td><code>cityblock(x, y)</code></td>
<td><code>sum(abs(x - y))</code></td>
</tr>
<tr>
<td>TotalVariation</td>
<td><code>totalvariation(x, y)</code></td>
<td><code>sum(abs(x - y)) / 2</code></td>
</tr>
<tr>
<td>Chebyshev</td>
<td><code>chebyshev(x, y)</code></td>
<td><code>max(abs(x - y))</code></td>
</tr>
<tr>
<td>Minkowski</td>
<td><code>minkowski(x, y, p)</code></td>
<td><code>sum(abs(x - y).^p) ^ (1/p)</code></td>
</tr>
<tr>
<td>Hamming</td>
<td><code>hamming(k, l)</code></td>
<td><code>sum(k .!= l)</code></td>
</tr>
<tr>
<td>RogersTanimoto</td>
<td><code>rogerstanimoto(a, b)</code></td>
<td><code>2(sum(a&amp;!b) + sum(!a&amp;b)) / (2(sum(a&amp;!b) + sum(!a&amp;b)) + sum(a&amp;b) + sum(!a&amp;!b))</code></td>
</tr>
<tr>
<td>Jaccard</td>
<td><code>jaccard(x, y)</code></td>
<td><code>1 - sum(min(x, y)) / sum(max(x, y))</code></td>
</tr>
<tr>
<td>BrayCurtis</td>
<td><code>braycurtis(x, y)</code></td>
<td><code>sum(abs(x - y)) / sum(abs(x + y))</code></td>
</tr>
<tr>
<td>CosineDist</td>
<td><code>cosine_dist(x, y)</code></td>
<td><code>1 - dot(x, y) / (norm(x) * norm(y))</code></td>
</tr>
<tr>
<td>CorrDist</td>
<td><code>corr_dist(x, y)</code></td>
<td><code>cosine_dist(x - mean(x), y - mean(y))</code></td>
</tr>
<tr>
<td>ChiSqDist</td>
<td><code>chisq_dist(x, y)</code></td>
<td><code>sum((x - y).^2 / (x + y))</code></td>
</tr>
<tr>
<td>KLDivergence</td>
<td><code>kl_divergence(p, q)</code></td>
<td><code>sum(p .* log(p ./ q))</code></td>
</tr>
<tr>
<td>GenKLDivergence</td>
<td><code>gkl_divergence(x, y)</code></td>
<td><code>sum(p .* log(p ./ q) - p + q)</code></td>
</tr>
<tr>
<td>RenyiDivergence</td>
<td><code>renyi_divergence(p, q, k)</code></td>
<td><code>log(sum( p .* (p ./ q) .^ (k - 1))) / (k - 1)</code></td>
</tr>
<tr>
<td>JSDivergence</td>
<td><code>js_divergence(p, q)</code></td>
<td><code>KL(p, m) / 2 + KL(p, m) / 2 with m = (p + q) / 2</code></td>
</tr>
<tr>
<td>SpanNormDist</td>
<td><code>spannorm_dist(x, y)</code></td>
<td><code>max(x - y) - min(x - y)</code></td>
</tr>
<tr>
<td>BhattacharyyaDist</td>
<td><code>bhattacharyya(x, y)</code></td>
<td><code>-log(sum(sqrt(x .* y) / sqrt(sum(x) * sum(y)))</code></td>
</tr>
<tr>
<td>HellingerDist</td>
<td><code>hellinger(x, y) </code></td>
<td><code>sqrt(1 - sum(sqrt(x .* y) / sqrt(sum(x) * sum(y))))</code></td>
</tr>
<tr>
<td>Haversine</td>
<td><code>haversine(x, y, r)</code></td>
<td><a href="https://en.wikipedia.org/wiki/Haversine_formula" rel="nofollow">Haversine formula</a></td>
</tr>
<tr>
<td>Mahalanobis</td>
<td><code>mahalanobis(x, y, Q)</code></td>
<td><code>sqrt((x - y)' * Q * (x - y))</code></td>
</tr>
<tr>
<td>SqMahalanobis</td>
<td><code>sqmahalanobis(x, y, Q)</code></td>
<td><code>(x - y)' * Q * (x - y)</code></td>
</tr>
<tr>
<td>MeanAbsDeviation</td>
<td><code>meanad(x, y)</code></td>
<td><code>mean(abs.(x - y))</code></td>
</tr>
<tr>
<td>MeanSqDeviation</td>
<td><code>msd(x, y)</code></td>
<td><code>mean(abs2.(x - y))</code></td>
</tr>
<tr>
<td>RMSDeviation</td>
<td><code>rmsd(x, y)</code></td>
<td><code>sqrt(msd(x, y))</code></td>
</tr>
<tr>
<td>NormRMSDeviation</td>
<td><code>nrmsd(x, y)</code></td>
<td><code>rmsd(x, y) / (maximum(x) - minimum(x))</code></td>
</tr>
<tr>
<td>WeightedEuclidean</td>
<td><code>weuclidean(x, y, w)</code></td>
<td><code>sqrt(sum((x - y).^2 .* w))</code></td>
</tr>
<tr>
<td>WeightedSqEuclidean</td>
<td><code>wsqeuclidean(x, y, w)</code></td>
<td><code>sum((x - y).^2 .* w)</code></td>
</tr>
<tr>
<td>WeightedCityblock</td>
<td><code>wcityblock(x, y, w)</code></td>
<td><code>sum(abs(x - y) .* w)</code></td>
</tr>
<tr>
<td>WeightedMinkowski</td>
<td><code>wminkowski(x, y, w, p)</code></td>
<td><code>sum(abs(x - y).^p .* w) ^ (1/p)</code></td>
</tr>
<tr>
<td>WeightedHamming</td>
<td><code>whamming(x, y, w)</code></td>
<td><code>sum((x .!= y) .* w)</code></td>
</tr>
<tr>
<td>Bregman</td>
<td><code>bregman(F, ∇, x, y; inner = LinearAlgebra.dot)</code></td>
<td><code>F(x) - F(y) - inner(∇(y), x - y)</code></td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> The formulas above are using <em>Julia</em>'s functions. These formulas are mainly for conveying the math concepts in a concise way. The actual implementation may use a faster way. The arguments <code>x</code> and <code>y</code> are arrays of real numbers; <code>k</code> and <code>l</code> are arrays of distinct elements of any kind; a and b are arrays of Bools; and finally, <code>p</code> and <code>q</code> are arrays forming a discrete probability distribution and are therefore both expected to sum to one.</p>
<h3><a id="user-content-precision-for-euclidean-and-sqeuclidean" class="anchor" aria-hidden="true" href="#precision-for-euclidean-and-sqeuclidean"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Precision for Euclidean and SqEuclidean</h3>
<p>For efficiency (see the benchmarks below), <code>Euclidean</code> and
<code>SqEuclidean</code> make use of BLAS3 matrix-matrix multiplication to
calculate distances.  This corresponds to the following expansion:</p>
<div class="highlight highlight-source-julia"><pre>(x<span class="pl-k">-</span>y)<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">==</span> x<span class="pl-k">^</span><span class="pl-c1">2</span> <span class="pl-k">-</span> <span class="pl-c1">2</span>xy <span class="pl-k">+</span> y<span class="pl-k">^</span><span class="pl-c1">2</span></pre></div>
<p>However, equality is not precise in the presence of roundoff error,
and particularly when <code>x</code> and <code>y</code> are nearby points this may not be
accurate.  Consequently, <code>Euclidean</code> and <code>SqEuclidean</code> allow you to
supply a relative tolerance to force recalculation:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">reshape</span>([<span class="pl-c1">0.1</span>, <span class="pl-c1">0.3</span>, <span class="pl-k">-</span><span class="pl-c1">0.1</span>], <span class="pl-c1">3</span>, <span class="pl-c1">1</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">pairwise</span>(<span class="pl-c1">Euclidean</span>(), x, x)
<span class="pl-c1">1</span><span class="pl-k">×</span><span class="pl-c1">1</span> Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-c1">7.45058e-9</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">pairwise</span>(<span class="pl-c1">Euclidean</span>(<span class="pl-c1">1e-12</span>), x, x)
<span class="pl-c1">1</span><span class="pl-k">×</span><span class="pl-c1">1</span> Array{Float64,<span class="pl-c1">2</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.0</span></pre></div>
<h2><a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmarks</h2>
<p>The implementation has been carefully optimized based on benchmarks. The script in <code>benchmark/benchmarks.jl</code> defines a benchmark suite
for a variety of distances, under column-wise and pairwise settings.</p>
<p>Here are benchmarks obtained running Julia 1.0 on a computer with a dual-core Intel Core i5-2300K processor @ 2.3 GHz.
The tables below can be replicated using the script in <code>benchmark/print_table.jl</code>.</p>
<h4><a id="user-content-column-wise-benchmark" class="anchor" aria-hidden="true" href="#column-wise-benchmark"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Column-wise benchmark</h4>
<p>The table below compares the performance (measured in terms of average elapsed time of each iteration) of a straightforward loop implementation and an optimized implementation provided in <em>Distances.jl</em>. The task in each iteration is to compute a specific distance between corresponding columns in two <code>200-by-10000</code> matrices.</p>
<table>
<thead>
<tr>
<th>distance</th>
<th>loop</th>
<th>colwise</th>
<th>gain</th>
</tr>
</thead>
<tbody>
<tr>
<td>SqEuclidean</td>
<td>0.004432s</td>
<td>0.001049s</td>
<td>4.2270</td>
</tr>
<tr>
<td>Euclidean</td>
<td>0.004537s</td>
<td>0.001054s</td>
<td>4.3031</td>
</tr>
<tr>
<td>PeriodicEuclidean</td>
<td>0.012092s</td>
<td>0.006714s</td>
<td>1.8011</td>
</tr>
<tr>
<td>Cityblock</td>
<td>0.004515s</td>
<td>0.001060s</td>
<td>4.2585</td>
</tr>
<tr>
<td>TotalVariation</td>
<td>0.004496s</td>
<td>0.001062s</td>
<td>4.2337</td>
</tr>
<tr>
<td>Chebyshev</td>
<td>0.009123s</td>
<td>0.005034s</td>
<td>1.8123</td>
</tr>
<tr>
<td>Minkowski</td>
<td>0.047573s</td>
<td>0.042508s</td>
<td>1.1191</td>
</tr>
<tr>
<td>Hamming</td>
<td>0.004355s</td>
<td>0.001099s</td>
<td>3.9638</td>
</tr>
<tr>
<td>CosineDist</td>
<td>0.006432s</td>
<td>0.002282s</td>
<td>2.8185</td>
</tr>
<tr>
<td>CorrDist</td>
<td>0.010273s</td>
<td>0.012500s</td>
<td>0.8219</td>
</tr>
<tr>
<td>ChiSqDist</td>
<td>0.005291s</td>
<td>0.001271s</td>
<td>4.1635</td>
</tr>
<tr>
<td>KLDivergence</td>
<td>0.031491s</td>
<td>0.025643s</td>
<td>1.2281</td>
</tr>
<tr>
<td>RenyiDivergence</td>
<td>0.052420s</td>
<td>0.048075s</td>
<td>1.0904</td>
</tr>
<tr>
<td>RenyiDivergence</td>
<td>0.017317s</td>
<td>0.009023s</td>
<td>1.9193</td>
</tr>
<tr>
<td>JSDivergence</td>
<td>0.047905s</td>
<td>0.044006s</td>
<td>1.0886</td>
</tr>
<tr>
<td>BhattacharyyaDist</td>
<td>0.007761s</td>
<td>0.003796s</td>
<td>2.0445</td>
</tr>
<tr>
<td>HellingerDist</td>
<td>0.007636s</td>
<td>0.003665s</td>
<td>2.0836</td>
</tr>
<tr>
<td>WeightedSqEuclidean</td>
<td>0.004550s</td>
<td>0.001151s</td>
<td>3.9541</td>
</tr>
<tr>
<td>WeightedEuclidean</td>
<td>0.004687s</td>
<td>0.001168s</td>
<td>4.0125</td>
</tr>
<tr>
<td>WeightedCityblock</td>
<td>0.004493s</td>
<td>0.001157s</td>
<td>3.8849</td>
</tr>
<tr>
<td>WeightedMinkowski</td>
<td>0.049442s</td>
<td>0.042145s</td>
<td>1.1732</td>
</tr>
<tr>
<td>WeightedHamming</td>
<td>0.004431s</td>
<td>0.001153s</td>
<td>3.8440</td>
</tr>
<tr>
<td>SqMahalanobis</td>
<td>0.082493s</td>
<td>0.019843s</td>
<td>4.1574</td>
</tr>
<tr>
<td>Mahalanobis</td>
<td>0.082180s</td>
<td>0.019618s</td>
<td>4.1891</td>
</tr>
<tr>
<td>BrayCurtis</td>
<td>0.004464s</td>
<td>0.001121s</td>
<td>3.9809</td>
</tr>
</tbody>
</table>
<p>We can see that using <code>colwise</code> instead of a simple loop yields considerable gain (2x - 4x), especially when the internal computation of each distance is simple. Nonetheless, when the computation of a single distance is heavy enough (e.g. <em>KLDivergence</em>,  <em>RenyiDivergence</em>), the gain is not as significant.</p>
<h4><a id="user-content-pairwise-benchmark" class="anchor" aria-hidden="true" href="#pairwise-benchmark"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pairwise benchmark</h4>
<p>The table below compares the performance (measured in terms of average elapsed time of each iteration) of a straightforward loop implementation and an optimized implementation provided in <em>Distances.jl</em>. The task in each iteration is to compute a specific distance in a pairwise manner between columns in a <code>100-by-200</code> and <code>100-by-250</code> matrices, which will result in a <code>200-by-250</code> distance matrix.</p>
<table>
<thead>
<tr>
<th>distance</th>
<th>loop</th>
<th>pairwise</th>
<th>gain</th>
</tr>
</thead>
<tbody>
<tr>
<td>SqEuclidean</td>
<td>0.012498s</td>
<td>0.000170s</td>
<td><strong>73.6596</strong></td>
</tr>
<tr>
<td>Euclidean</td>
<td>0.012583s</td>
<td>0.000257s</td>
<td>48.9628</td>
</tr>
<tr>
<td>PeriodicEuclidean</td>
<td>0.030935s</td>
<td>0.017572s</td>
<td>1.7605</td>
</tr>
<tr>
<td>Cityblock</td>
<td>0.012416s</td>
<td>0.000910s</td>
<td>13.6464</td>
</tr>
<tr>
<td>TotalVariation</td>
<td>0.012763s</td>
<td>0.000959s</td>
<td>13.3080</td>
</tr>
<tr>
<td>Chebyshev</td>
<td>0.023800s</td>
<td>0.012042s</td>
<td>1.9763</td>
</tr>
<tr>
<td>Minkowski</td>
<td>0.121388s</td>
<td>0.107333s</td>
<td>1.1310</td>
</tr>
<tr>
<td>Hamming</td>
<td>0.012171s</td>
<td>0.000689s</td>
<td>17.6538</td>
</tr>
<tr>
<td>CosineDist</td>
<td>0.017474s</td>
<td>0.000214s</td>
<td><strong>81.6546</strong></td>
</tr>
<tr>
<td>CorrDist</td>
<td>0.028195s</td>
<td>0.000259s</td>
<td><strong>108.7360</strong></td>
</tr>
<tr>
<td>ChiSqDist</td>
<td>0.014372s</td>
<td>0.003129s</td>
<td>4.5932</td>
</tr>
<tr>
<td>KLDivergence</td>
<td>0.079669s</td>
<td>0.063491s</td>
<td>1.2548</td>
</tr>
<tr>
<td>RenyiDivergence</td>
<td>0.134093s</td>
<td>0.117737s</td>
<td>1.1389</td>
</tr>
<tr>
<td>RenyiDivergence</td>
<td>0.047658s</td>
<td>0.024960s</td>
<td>1.9094</td>
</tr>
<tr>
<td>JSDivergence</td>
<td>0.121999s</td>
<td>0.110984s</td>
<td>1.0993</td>
</tr>
<tr>
<td>BhattacharyyaDist</td>
<td>0.021788s</td>
<td>0.009414s</td>
<td>2.3145</td>
</tr>
<tr>
<td>HellingerDist</td>
<td>0.020735s</td>
<td>0.008784s</td>
<td>2.3606</td>
</tr>
<tr>
<td>WeightedSqEuclidean</td>
<td>0.012671s</td>
<td>0.000186s</td>
<td><strong>68.0345</strong></td>
</tr>
<tr>
<td>WeightedEuclidean</td>
<td>0.012867s</td>
<td>0.000276s</td>
<td><strong>46.6634</strong></td>
</tr>
<tr>
<td>WeightedCityblock</td>
<td>0.012803s</td>
<td>0.001539s</td>
<td>8.3200</td>
</tr>
<tr>
<td>WeightedMinkowski</td>
<td>0.127386s</td>
<td>0.107257s</td>
<td>1.1877</td>
</tr>
<tr>
<td>WeightedHamming</td>
<td>0.012240s</td>
<td>0.001462s</td>
<td>8.3747</td>
</tr>
<tr>
<td>SqMahalanobis</td>
<td>0.214285s</td>
<td>0.000330s</td>
<td><strong>650.0722</strong></td>
</tr>
<tr>
<td>Mahalanobis</td>
<td>0.197294s</td>
<td>0.000420s</td>
<td><strong>470.2354</strong></td>
</tr>
<tr>
<td>BrayCurtis</td>
<td>0.012872s</td>
<td>0.001489s</td>
<td>8.6456</td>
</tr>
</tbody>
</table>
<p>For distances of which a major part of the computation is a quadratic form (e.g. <em>Euclidean</em>, <em>CosineDist</em>, <em>Mahalanobis</em>), the performance can be drastically improved by restructuring the computation and delegating the core part to <code>GEMM</code> in <em>BLAS</em>. The use of this strategy can easily lead to 100x performance gain over simple loops (see the highlighted part of the table above).</p>
</article></div>