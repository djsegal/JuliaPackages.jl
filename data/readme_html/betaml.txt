<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-beta-machine-learning-toolkit" class="anchor" aria-hidden="true" href="#beta-machine-learning-toolkit"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Beta Machine Learning Toolkit</h1>
<p><em>Machine Learning made simple :-)</em></p>

<p><a target="_blank" rel="noopener noreferrer" href="assets/BetaML_logo.png"><img src="assets/BetaML_logo.png" width="300" valign="middle" style="max-width:100%;"></a>    <a target="_blank" rel="noopener noreferrer" href="assets/microExample_white.png"><img src="assets/microExample_white.png" width="500" valign="middle" style="max-width:100%;"></a></p>
<p>The <strong>Beta Machine Learning Toolkit</strong> is a repository with several Machine Learning algorithms, started from implementing in the Julia language the concepts taught in the <a href="https://www.edx.org/course/machine-learning-with-python-from-linear-models-to" rel="nofollow">MITX 6.86x - Machine Learning with Python: from Linear Models to Deep Learning</a> course (note we bear no affiliation with that course).</p>
<p><a href="https://sylvaticus.github.io/BetaML.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://sylvaticus.github.io/BetaML.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://doi.org/10.21105/joss.02849" rel="nofollow"><img src="https://camo.githubusercontent.com/f3e975c272488246ef6ecf4ec40238b8c4374aaff7c59e796a3dd585f41cddaf/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f31302e32313130352f6a6f73732e30323834392f7374617475732e737667" alt="DOI" data-canonical-src="https://joss.theoj.org/papers/10.21105/joss.02849/status.svg" style="max-width:100%;"></a>
<a href="https://github.com/sylvaticus/BetaML.jl/actions"><img src="https://github.com/sylvaticus/BetaML.jl/workflows/CI/badge.svg" alt="Build status" style="max-width:100%;"></a>
<a href="http://codecov.io/github/sylvaticus/BetaML.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/8daddfec12ab92a28b23c4f109a8f436b4010a27035092ac2350ec5b148cbfc1/687474703a2f2f636f6465636f762e696f2f6769746875622f73796c766174696375732f426574614d4c2e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/sylvaticus/BetaML.jl/coverage.svg?branch=master" style="max-width:100%;"></a></p>
<p>Theoretical notes describing most of these algorithms are at the companion repository <a href="https://github.com/sylvaticus/MITx_6.86x">https://github.com/sylvaticus/MITx_6.86x</a>.</p>
<p>The focus of the library is skewed toward user-friendliness rather than computational efficiency, the code is (relatively) easy to read and, for the algorithms implementation, uses mostly Julia core without external libraries (a little bit like <a href="https://github.com/ddbourgin/numpy-ml">numpy-ml</a> for Python-Numpy) but it is not heavily optimised (and GPU is not supported).
. For excellent and mature machine learning algorithms in Julia that support huge datasets or to organise complex and partially automated pipelines of algorithms please consider the packages in the above section "Alternative packages".</p>
<p>As the focus is on simplicity, functions have pretty longer but more explicit names than usual.. for example the <code>Dense</code> layer is a <code>DenseLayer</code>, the <code>RBF</code> kernel is <code>radialKernel</code>, etc.
As we didn't aim for heavy optimisation, we were able to keep the API (Application Programming Interface) both beginner-friendly and flexible. Contrary to established packages, most methods provide reasonable defaults that can be overridden when needed (like the neural network optimiser, the verbosity level, or the loss function).
For example, one can implement its own layer as a subtype of the abstract type <code>Layer</code> or its own optimisation algorithm as a subtype of <code>OptimisationAlgorithm</code> or even specify its own distance metric in the clustering <code>Kmedoids</code> algorithm..</p>
<p>That said, Julia is a relatively fast language and most hard job is done in multithreaded functions or using matrix operations whose underlying libraries may be multithreaded, so it is reasonably fast for small exploratory tasks and mid-size analysis (basically everything that holds in your PC's memory).</p>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h2>
<p>Please refer to the package documentation (<a href="https://sylvaticus.github.io/BetaML.jl/stable" rel="nofollow">stable</a> | <a href="https://sylvaticus.github.io/BetaML.jl/dev" rel="nofollow">dev</a>) or use the Julia inline package system (just press the question mark <code>?</code> and then, on the special help prompt <code>help?&gt;</code>, type the module or function name). The package documentation is made of two distinct parts. The first one is an extensively commented tutorial that covers most of the library, the second one is the reference manual covering the library's API.</p>
<p>We currently implemented the following modules in BetaML: <a href="https://sylvaticus.github.io/BetaML.jl/dev/Perceptron.html" rel="nofollow">Perceptron</a> (linear and kernel-based classifiers), <a href="https://sylvaticus.github.io/BetaML.jl/dev/Trees.html" rel="nofollow">Trees</a> (Decision Trees and Random Forests), <a href="https://sylvaticus.github.io/BetaML.jl/dev/Nn.html" rel="nofollow">Nn</a> (Neural Networks), <a href="https://sylvaticus.github.io/BetaML.jl/dev/Clustering.html" rel="nofollow">Clustering</a> (Kmean, Kmenoids, Expectation-Maximisation, Missing value imputation, ...) and <a href="https://sylvaticus.github.io/BetaML.jl/dev/Utils.html" rel="nofollow">Utils</a>.</p>
<p>If you are looking for an introductory book on Julia, have a look on "<a href="https://www.julia-book.com/" rel="nofollow">Julia Quick Syntax Reference</a>"(Apress,2019).</p>
<p>The package can be easily used in R or Python employing <a href="https://github.com/Non-Contradiction/JuliaCall">JuliaCall</a> or <a href="https://github.com/JuliaPy/pyjulia">PyJulia</a> respectively, see the documentation tutorial on the "Getting started" section.</p>
<h3><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h3>
<p>We see how to use three different algorithms to learn the relation between floral sepals and petal measures (first 4 columns) and the specie's name (5th column) in the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="nofollow">iris flower dataset</a>.</p>
<p>The first two algorithms are example of <em>supervised</em> learning, the third one of <em>unsupervised</em> learning.</p>
<ul>
<li><strong>Using Random Forests for classification</strong></li>
</ul>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using DelimitedFiles, BetaML

iris  = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),',',skipstart=1)
x = iris[:,1:4]
y = iris[:,5]
((xtrain,xtest),(ytrain,ytest)) = partition([x,y],[0.7,0.3])
(ytrain,ytest) = dropdims.([ytrain,ytest],dims=2)
myForest       = buildForest(xtrain,ytrain,30)
ŷtrain         = predict(myForest, xtrain)
ŷtest          = predict(myForest, xtest)
trainAccuracy  = accuracy(ŷtrain,ytrain) # 1.00
testAccuracy   = accuracy(ŷtest,ytest)   # 0.956
"><pre><span class="pl-k">using</span> DelimitedFiles, BetaML

iris  <span class="pl-k">=</span> <span class="pl-c1">readdlm</span>(<span class="pl-c1">joinpath</span>(<span class="pl-c1">dirname</span>(Base<span class="pl-k">.</span><span class="pl-c1">find_package</span>(<span class="pl-s"><span class="pl-pds">"</span>BetaML<span class="pl-pds">"</span></span>)),<span class="pl-s"><span class="pl-pds">"</span>..<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>test<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>iris.csv<span class="pl-pds">"</span></span>),<span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>,skipstart<span class="pl-k">=</span><span class="pl-c1">1</span>)
x <span class="pl-k">=</span> iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>]
y <span class="pl-k">=</span> iris[:,<span class="pl-c1">5</span>]
((xtrain,xtest),(ytrain,ytest)) <span class="pl-k">=</span> <span class="pl-c1">partition</span>([x,y],[<span class="pl-c1">0.7</span>,<span class="pl-c1">0.3</span>])
(ytrain,ytest) <span class="pl-k">=</span> <span class="pl-c1">dropdims</span>.([ytrain,ytest],dims<span class="pl-k">=</span><span class="pl-c1">2</span>)
myForest       <span class="pl-k">=</span> <span class="pl-c1">buildForest</span>(xtrain,ytrain,<span class="pl-c1">30</span>)
ŷtrain         <span class="pl-k">=</span> <span class="pl-c1">predict</span>(myForest, xtrain)
ŷtest          <span class="pl-k">=</span> <span class="pl-c1">predict</span>(myForest, xtest)
trainAccuracy  <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ŷtrain,ytrain) <span class="pl-c"><span class="pl-c">#</span> 1.00</span>
testAccuracy   <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ŷtest,ytest)   <span class="pl-c"><span class="pl-c">#</span> 0.956</span></pre></div>
<ul>
<li><strong>Using an Artificial Neural Network for multinomial categorisation</strong></li>
</ul>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Load Modules
using BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules
Random.seed!(123); # Fix the random seed (to obtain reproducible results)

# Load the data
iris     = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),',',skipstart=1)
iris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default
x        = convert(Array{Float64,2}, iris[:,1:4])
y        = map(x-&gt;Dict(&quot;setosa&quot; =&gt; 1, &quot;versicolor&quot; =&gt; 2, &quot;virginica&quot; =&gt;3)[x],iris[:, 5]) # Convert the target column to numbers
y_oh     = oneHotEncoder(y) # Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])

# Split the data in training/testing sets
((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,0.2],shuffle=false)
(ntrain, ntest) = size.([xtrain,xtest],1)

# Define the Artificial Neural Network model
l1   = DenseLayer(4,10,f=relu) # Activation function is ReLU
l2   = DenseLayer(10,3)        # Activation function is identity by default
l3   = VectorFunctionLayer(3,3,f=softmax) # Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once
mynn = buildNetwork([l1,l2,l3],squaredCost,name=&quot;Multinomial logistic regression Model Sepal&quot;) # Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)

# Training it (default to ADAM)
res = train!(mynn,scale(xtrain),ytrain_oh,epochs=100,batchSize=6) # Use optAlg=SGD() to use Stochastic Gradient Descent instead

# Test it
ŷtrain        = predict(mynn,scale(xtrain))   # Note the scaling function
ŷtest         = predict(mynn,scale(xtest))
trainAccuracy = accuracy(ŷtrain,ytrain,tol=1) # 0.983
testAccuracy  = accuracy(ŷtest,ytest,tol=1)   # 1.0

# Visualise results
testSize    = size(ŷtest,1)
ŷtestChosen =  [argmax(ŷtest[i,:]) for i in 1:testSize]
groupedbar([ytest ŷtestChosen], label=[&quot;ytest&quot; &quot;ŷtest (est)&quot;], title=&quot;True vs estimated categories&quot;) # All records correctly labelled !
plot(0:res.epochs,res.ϵ_epochs, ylabel=&quot;epochs&quot;,xlabel=&quot;error&quot;,legend=nothing,title=&quot;Avg. error per epoch on the Sepal dataset&quot;)
"><pre><span class="pl-c"><span class="pl-c">#</span> Load Modules</span>
<span class="pl-k">using</span> BetaML, DelimitedFiles, Random, StatsPlots <span class="pl-c"><span class="pl-c">#</span> Load the main module and ausiliary modules</span>
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">123</span>); <span class="pl-c"><span class="pl-c">#</span> Fix the random seed (to obtain reproducible results)</span>

<span class="pl-c"><span class="pl-c">#</span> Load the data</span>
iris     <span class="pl-k">=</span> <span class="pl-c1">readdlm</span>(<span class="pl-c1">joinpath</span>(<span class="pl-c1">dirname</span>(Base<span class="pl-k">.</span><span class="pl-c1">find_package</span>(<span class="pl-s"><span class="pl-pds">"</span>BetaML<span class="pl-pds">"</span></span>)),<span class="pl-s"><span class="pl-pds">"</span>..<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>test<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>iris.csv<span class="pl-pds">"</span></span>),<span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>,skipstart<span class="pl-k">=</span><span class="pl-c1">1</span>)
iris     <span class="pl-k">=</span> iris[<span class="pl-c1">shuffle</span>(<span class="pl-c1">axes</span>(iris, <span class="pl-c1">1</span>)), :] <span class="pl-c"><span class="pl-c">#</span> Shuffle the records, as they aren't by default</span>
x        <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Array{Float64,<span class="pl-c1">2</span>}, iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>])
y        <span class="pl-k">=</span> <span class="pl-c1">map</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span>, <span class="pl-s"><span class="pl-pds">"</span>versicolor<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span>, <span class="pl-s"><span class="pl-pds">"</span>virginica<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span><span class="pl-c1">3</span>)[x],iris[:, <span class="pl-c1">5</span>]) <span class="pl-c"><span class="pl-c">#</span> Convert the target column to numbers</span>
y_oh     <span class="pl-k">=</span> <span class="pl-c1">oneHotEncoder</span>(y) <span class="pl-c"><span class="pl-c">#</span> Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])</span>

<span class="pl-c"><span class="pl-c">#</span> Split the data in training/testing sets</span>
((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) <span class="pl-k">=</span> <span class="pl-c1">partition</span>([x,y,y_oh],[<span class="pl-c1">0.8</span>,<span class="pl-c1">0.2</span>],shuffle<span class="pl-k">=</span><span class="pl-c1">false</span>)
(ntrain, ntest) <span class="pl-k">=</span> <span class="pl-c1">size</span>.([xtrain,xtest],<span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> Define the Artificial Neural Network model</span>
l1   <span class="pl-k">=</span> <span class="pl-c1">DenseLayer</span>(<span class="pl-c1">4</span>,<span class="pl-c1">10</span>,f<span class="pl-k">=</span>relu) <span class="pl-c"><span class="pl-c">#</span> Activation function is ReLU</span>
l2   <span class="pl-k">=</span> <span class="pl-c1">DenseLayer</span>(<span class="pl-c1">10</span>,<span class="pl-c1">3</span>)        <span class="pl-c"><span class="pl-c">#</span> Activation function is identity by default</span>
l3   <span class="pl-k">=</span> <span class="pl-c1">VectorFunctionLayer</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>,f<span class="pl-k">=</span>softmax) <span class="pl-c"><span class="pl-c">#</span> Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once</span>
mynn <span class="pl-k">=</span> <span class="pl-c1">buildNetwork</span>([l1,l2,l3],squaredCost,name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Multinomial logistic regression Model Sepal<span class="pl-pds">"</span></span>) <span class="pl-c"><span class="pl-c">#</span> Build the NN and use the squared cost (aka MSE) as error function (crossEntropy could also be used)</span>

<span class="pl-c"><span class="pl-c">#</span> Training it (default to ADAM)</span>
res <span class="pl-k">=</span> <span class="pl-c1">train!</span>(mynn,<span class="pl-c1">scale</span>(xtrain),ytrain_oh,epochs<span class="pl-k">=</span><span class="pl-c1">100</span>,batchSize<span class="pl-k">=</span><span class="pl-c1">6</span>) <span class="pl-c"><span class="pl-c">#</span> Use optAlg=SGD() to use Stochastic Gradient Descent instead</span>

<span class="pl-c"><span class="pl-c">#</span> Test it</span>
ŷtrain        <span class="pl-k">=</span> <span class="pl-c1">predict</span>(mynn,<span class="pl-c1">scale</span>(xtrain))   <span class="pl-c"><span class="pl-c">#</span> Note the scaling function</span>
ŷtest         <span class="pl-k">=</span> <span class="pl-c1">predict</span>(mynn,<span class="pl-c1">scale</span>(xtest))
trainAccuracy <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ŷtrain,ytrain,tol<span class="pl-k">=</span><span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 0.983</span>
testAccuracy  <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ŷtest,ytest,tol<span class="pl-k">=</span><span class="pl-c1">1</span>)   <span class="pl-c"><span class="pl-c">#</span> 1.0</span>

<span class="pl-c"><span class="pl-c">#</span> Visualise results</span>
testSize    <span class="pl-k">=</span> <span class="pl-c1">size</span>(ŷtest,<span class="pl-c1">1</span>)
ŷtestChosen <span class="pl-k">=</span>  [<span class="pl-c1">argmax</span>(ŷtest[i,:]) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>testSize]
<span class="pl-c1">groupedbar</span>([ytest ŷtestChosen], label<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>ytest<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>ŷtest (est)<span class="pl-pds">"</span></span>], title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>True vs estimated categories<span class="pl-pds">"</span></span>) <span class="pl-c"><span class="pl-c">#</span> All records correctly labelled !</span>
<span class="pl-c1">plot</span>(<span class="pl-c1">0</span><span class="pl-k">:</span>res<span class="pl-k">.</span>epochs,res<span class="pl-k">.</span>ϵ_epochs, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>epochs<span class="pl-pds">"</span></span>,xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>error<span class="pl-pds">"</span></span>,legend<span class="pl-k">=</span><span class="pl-c1">nothing</span>,title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Avg. error per epoch on the Sepal dataset<span class="pl-pds">"</span></span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="assets/sepalOutput_results.png"><img src="assets/sepalOutput_results.png" width="400" style="max-width:100%;"></a> <a target="_blank" rel="noopener noreferrer" href="assets/sepalOutput_errors.png"><img src="assets/sepalOutput_errors.png" width="400" style="max-width:100%;"></a></p>
<ul>
<li><strong>Using the Expectation-Maximisation algorithm for clustering</strong></li>
</ul>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using BetaML, DelimitedFiles, Random, StatsPlots # Load the main module and ausiliary modules
Random.seed!(123); # Fix the random seed (to obtain reproducible results)

# Load the data
iris     = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),',',skipstart=1)
iris     = iris[shuffle(axes(iris, 1)), :] # Shuffle the records, as they aren't by default
x        = convert(Array{Float64,2}, iris[:,1:4])
x        = scale(x) # normalise all dimensions to (μ=0, σ=1)
y        = map(x-&gt;Dict(&quot;setosa&quot; =&gt; 1, &quot;versicolor&quot; =&gt; 2, &quot;virginica&quot; =&gt;3)[x],iris[:, 5]) # Convert the target column to numbers

# Get some ranges of minVariance and minCovariance to test
minVarRange   = collect(0.04:0.05:1.5)
minCovarRange = collect(0:0.05:1.45)

# Run the gmm(em) algorithm for the various cases...
sphOut   = [gmm(x,3,mixtures=[SphericalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE) for v in minVarRange, cv in minCovarRange[1:1]]
diagOut  = [gmm(x,3,mixtures=[DiagonalGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange[1:1]]
fullOut  = [gmm(x,3,mixtures=[FullGaussian() for i in 1:3],minVariance=v, minCovariance=cv, verbosity=NONE)  for v in minVarRange, cv in minCovarRange]

# Get the Bayesian information criterion (AIC is also available)
sphBIC  = [sphOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]
diagBIC = [diagOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:1]
fullBIC = [fullOut[v,cv].BIC for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]

# Compare the accuracy with true categories
sphAcc  = [accuracy(sphOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]
diagAcc = [accuracy(diagOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:1]
fullAcc = [accuracy(fullOut[v,cv].pₙₖ,y,ignoreLabels=true) for v in 1:length(minVarRange), cv in 1:length(minCovarRange)]

plot(minVarRange,[sphBIC diagBIC fullBIC[:,1] fullBIC[:,15] fullBIC[:,30]], markershape=:circle, label=[&quot;sph&quot; &quot;diag&quot; &quot;full (cov=0)&quot; &quot;full (cov=0.7)&quot; &quot;full (cov=1.45)&quot;], title=&quot;BIC&quot;, xlabel=&quot;minVariance&quot;)
plot(minVarRange,[sphAcc diagAcc fullAcc[:,1] fullAcc[:,15] fullAcc[:,30]], markershape=:circle, label=[&quot;sph&quot; &quot;diag&quot; &quot;full (cov=0)&quot; &quot;full (cov=0.7)&quot; &quot;full (cov=1.45)&quot;], title=&quot;Accuracies&quot;, xlabel=&quot;minVariance&quot;)
"><pre><span class="pl-k">using</span> BetaML, DelimitedFiles, Random, StatsPlots <span class="pl-c"><span class="pl-c">#</span> Load the main module and ausiliary modules</span>
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">123</span>); <span class="pl-c"><span class="pl-c">#</span> Fix the random seed (to obtain reproducible results)</span>

<span class="pl-c"><span class="pl-c">#</span> Load the data</span>
iris     <span class="pl-k">=</span> <span class="pl-c1">readdlm</span>(<span class="pl-c1">joinpath</span>(<span class="pl-c1">dirname</span>(Base<span class="pl-k">.</span><span class="pl-c1">find_package</span>(<span class="pl-s"><span class="pl-pds">"</span>BetaML<span class="pl-pds">"</span></span>)),<span class="pl-s"><span class="pl-pds">"</span>..<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>test<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>iris.csv<span class="pl-pds">"</span></span>),<span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>,skipstart<span class="pl-k">=</span><span class="pl-c1">1</span>)
iris     <span class="pl-k">=</span> iris[<span class="pl-c1">shuffle</span>(<span class="pl-c1">axes</span>(iris, <span class="pl-c1">1</span>)), :] <span class="pl-c"><span class="pl-c">#</span> Shuffle the records, as they aren't by default</span>
x        <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Array{Float64,<span class="pl-c1">2</span>}, iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>])
x        <span class="pl-k">=</span> <span class="pl-c1">scale</span>(x) <span class="pl-c"><span class="pl-c">#</span> normalise all dimensions to (μ=0, σ=1)</span>
y        <span class="pl-k">=</span> <span class="pl-c1">map</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span>, <span class="pl-s"><span class="pl-pds">"</span>versicolor<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span>, <span class="pl-s"><span class="pl-pds">"</span>virginica<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span><span class="pl-c1">3</span>)[x],iris[:, <span class="pl-c1">5</span>]) <span class="pl-c"><span class="pl-c">#</span> Convert the target column to numbers</span>

<span class="pl-c"><span class="pl-c">#</span> Get some ranges of minVariance and minCovariance to test</span>
minVarRange   <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">0.04</span><span class="pl-k">:</span><span class="pl-c1">0.05</span><span class="pl-k">:</span><span class="pl-c1">1.5</span>)
minCovarRange <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">0</span><span class="pl-k">:</span><span class="pl-c1">0.05</span><span class="pl-k">:</span><span class="pl-c1">1.45</span>)

<span class="pl-c"><span class="pl-c">#</span> Run the gmm(em) algorithm for the various cases...</span>
sphOut   <span class="pl-k">=</span> [<span class="pl-c1">gmm</span>(x,<span class="pl-c1">3</span>,mixtures<span class="pl-k">=</span>[<span class="pl-c1">SphericalGaussian</span>() <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>],minVariance<span class="pl-k">=</span>v, minCovariance<span class="pl-k">=</span>cv, verbosity<span class="pl-k">=</span>NONE) <span class="pl-k">for</span> v <span class="pl-k">in</span> minVarRange, cv <span class="pl-k">in</span> minCovarRange[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>]]
diagOut  <span class="pl-k">=</span> [<span class="pl-c1">gmm</span>(x,<span class="pl-c1">3</span>,mixtures<span class="pl-k">=</span>[<span class="pl-c1">DiagonalGaussian</span>() <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>],minVariance<span class="pl-k">=</span>v, minCovariance<span class="pl-k">=</span>cv, verbosity<span class="pl-k">=</span>NONE)  <span class="pl-k">for</span> v <span class="pl-k">in</span> minVarRange, cv <span class="pl-k">in</span> minCovarRange[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>]]
fullOut  <span class="pl-k">=</span> [<span class="pl-c1">gmm</span>(x,<span class="pl-c1">3</span>,mixtures<span class="pl-k">=</span>[<span class="pl-c1">FullGaussian</span>() <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">3</span>],minVariance<span class="pl-k">=</span>v, minCovariance<span class="pl-k">=</span>cv, verbosity<span class="pl-k">=</span>NONE)  <span class="pl-k">for</span> v <span class="pl-k">in</span> minVarRange, cv <span class="pl-k">in</span> minCovarRange]

<span class="pl-c"><span class="pl-c">#</span> Get the Bayesian information criterion (AIC is also available)</span>
sphBIC  <span class="pl-k">=</span> [sphOut[v,cv]<span class="pl-k">.</span>BIC <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minVarRange), cv <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>]
diagBIC <span class="pl-k">=</span> [diagOut[v,cv]<span class="pl-k">.</span>BIC <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minVarRange), cv <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>]
fullBIC <span class="pl-k">=</span> [fullOut[v,cv]<span class="pl-k">.</span>BIC <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minVarRange), cv <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minCovarRange)]

<span class="pl-c"><span class="pl-c">#</span> Compare the accuracy with true categories</span>
sphAcc  <span class="pl-k">=</span> [<span class="pl-c1">accuracy</span>(sphOut[v,cv]<span class="pl-k">.</span>pₙₖ,y,ignoreLabels<span class="pl-k">=</span><span class="pl-c1">true</span>) <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minVarRange), cv <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>]
diagAcc <span class="pl-k">=</span> [<span class="pl-c1">accuracy</span>(diagOut[v,cv]<span class="pl-k">.</span>pₙₖ,y,ignoreLabels<span class="pl-k">=</span><span class="pl-c1">true</span>) <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minVarRange), cv <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1</span>]
fullAcc <span class="pl-k">=</span> [<span class="pl-c1">accuracy</span>(fullOut[v,cv]<span class="pl-k">.</span>pₙₖ,y,ignoreLabels<span class="pl-k">=</span><span class="pl-c1">true</span>) <span class="pl-k">for</span> v <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minVarRange), cv <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(minCovarRange)]

<span class="pl-c1">plot</span>(minVarRange,[sphBIC diagBIC fullBIC[:,<span class="pl-c1">1</span>] fullBIC[:,<span class="pl-c1">15</span>] fullBIC[:,<span class="pl-c1">30</span>]], markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>, label<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>sph<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>diag<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>full (cov=0)<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>full (cov=0.7)<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>full (cov=1.45)<span class="pl-pds">"</span></span>], title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>BIC<span class="pl-pds">"</span></span>, xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>minVariance<span class="pl-pds">"</span></span>)
<span class="pl-c1">plot</span>(minVarRange,[sphAcc diagAcc fullAcc[:,<span class="pl-c1">1</span>] fullAcc[:,<span class="pl-c1">15</span>] fullAcc[:,<span class="pl-c1">30</span>]], markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>, label<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>sph<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>diag<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>full (cov=0)<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>full (cov=0.7)<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>full (cov=1.45)<span class="pl-pds">"</span></span>], title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Accuracies<span class="pl-pds">"</span></span>, xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>minVariance<span class="pl-pds">"</span></span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="assets/sepalClustersBIC.png"><img src="assets/sepalClustersBIC.png" width="400" style="max-width:100%;"></a> <a target="_blank" rel="noopener noreferrer" href="assets/sepalClustersAccuracy.png"><img src="assets/sepalClustersAccuracy.png" width="400" style="max-width:100%;"></a></p>
<ul>
<li><strong>Other examples</strong></li>
</ul>
<p>Further examples, with more advanced techniques in order to improve predictions, are provided in the documentation tutorial. At the opposite, very "micro" examples of usage of the various functions can be studied in the unit-tests available in the <a href="https://github.com/sylvaticus/BetaML.jl/tree/master/test"><code>test</code></a> folder</p>
<h2><a id="user-content-alternative-packages" class="anchor" aria-hidden="true" href="#alternative-packages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Alternative packages</h2>
<table>
<thead>
<tr>
<th>Category</th>
<th>Packages</th>
</tr>
</thead>
<tbody>
<tr>
<td>ML toolkits/pipelines</td>
<td><a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a>, <a href="https://github.com/IBM/AutoMLPipeline.jl">AutoMLPipeline.jl</a>, <a href="https://joss.theoj.org/papers/10.21105/joss.02704" rel="nofollow">MLJ.jl</a></td>
</tr>
<tr>
<td>Neural Networks</td>
<td><a href="https://fluxml.ai/" rel="nofollow">Flux.jl</a>, <a href="https://github.com/denizyuret/Knet.jl">Knet</a></td>
</tr>
<tr>
<td>Decision Trees</td>
<td><a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl</a></td>
</tr>
<tr>
<td>Clustering</td>
<td><a href="https://github.com/JuliaStats/Clustering.jl">Clustering.jl</a>, <a href="https://github.com/davidavdav/GaussianMixtures.jl">GaussianMixtures.jl</a></td>
</tr>
<tr>
<td>Missing imputation</td>
<td><a href="https://github.com/invenia/Impute.jl">Impute.jl</a></td>
</tr>
</tbody>
</table>
<h2><a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TODO</h2>
<h3><a id="user-content-short-term" class="anchor" aria-hidden="true" href="#short-term"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Short term</h3>
<ul>
<li>Implement utility functions to do hyper-parameter tuning using cross-validation as back-end</li>
</ul>
<h3><a id="user-content-midlong-term" class="anchor" aria-hidden="true" href="#midlong-term"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Mid/Long term</h3>
<ul>
<li>Add convolutional layers and RNN support</li>
<li>Reinforcement learning (Markov decision processes)</li>
</ul>
<h2><a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contribute</h2>
<p>Contributions to the library are welcome. We are particularly interested in the areas covered in the "TODO" list above, but we are open to other areas as well.
Please however consider that the focus is mostly didactic/research, so clear, easy to read (and well documented) code and simple API with reasonable defaults are more important that highly optimised algorithms. For the same reason, it is fine to use verbose names.
Please open an issue to discuss your ideas or make directly a well-documented pull request to the repository.
While not required by any means, if you are customising BetaML and writing for example your own neural network layer type (by subclassing <code>AbstractLayer</code>), your own sampler (by subclassing <code>AbstractDataSampler</code>) or your own mixture component (by subclassing <code>AbstractMixture</code>), please consider to give it back to the community and open a pull request to integrate them in BetaML.</p>
<h2><a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Citations</h2>
<p>If you use <code>BetaML</code> please cite as:</p>
<ul>
<li>Lobianco, A., (2021). BetaML: The Beta Machine Learning Toolkit, a self-contained repository of Machine Learning algorithms in Julia. Journal of Open Source Software, 6(60), 2849, <a href="https://doi.org/10.21105/joss.02849" rel="nofollow">https://doi.org/10.21105/joss.02849</a></li>
</ul>
<div class="highlight highlight-text-bibtex position-relative" data-snippet-clipboard-copy-content="@article{Lobianco2021,
  doi       = {10.21105/joss.02849},
  url       = {https://doi.org/10.21105/joss.02849},
  year      = {2021},
  publisher = {The Open Journal},
  volume    = {6},
  number    = {60},
  pages     = {2849},
  author    = {Antonello Lobianco},
  title     = {BetaML: The Beta Machine Learning Toolkit, a self-contained repository of Machine Learning algorithms in Julia},
  journal   = {Journal of Open Source Software}
}
"><pre><span class="pl-k">@article</span>{<span class="pl-en">Lobianco2021</span>,
  <span class="pl-s">doi</span>       = <span class="pl-s"><span class="pl-pds">{</span>10.21105/joss.02849<span class="pl-pds">}</span></span>,
  <span class="pl-s">url</span>       = <span class="pl-s"><span class="pl-pds">{</span>https://doi.org/10.21105/joss.02849<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>      = <span class="pl-s"><span class="pl-pds">{</span>2021<span class="pl-pds">}</span></span>,
  <span class="pl-s">publisher</span> = <span class="pl-s"><span class="pl-pds">{</span>The Open Journal<span class="pl-pds">}</span></span>,
  <span class="pl-s">volume</span>    = <span class="pl-s"><span class="pl-pds">{</span>6<span class="pl-pds">}</span></span>,
  <span class="pl-s">number</span>    = <span class="pl-s"><span class="pl-pds">{</span>60<span class="pl-pds">}</span></span>,
  <span class="pl-s">pages</span>     = <span class="pl-s"><span class="pl-pds">{</span>2849<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>    = <span class="pl-s"><span class="pl-pds">{</span>Antonello Lobianco<span class="pl-pds">}</span></span>,
  <span class="pl-s">title</span>     = <span class="pl-s"><span class="pl-pds">{</span>BetaML: The Beta Machine Learning Toolkit, a self-contained repository of Machine Learning algorithms in Julia<span class="pl-pds">}</span></span>,
  <span class="pl-s">journal</span>   = <span class="pl-s"><span class="pl-pds">{</span>Journal of Open Source Software<span class="pl-pds">}</span></span>
}</pre></div>
<h2><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgements</h2>
<p>The development of this package at the <em>Bureau d'Economie Théorique et Appliquée</em> (BETA, Nancy) was supported by the French National Research Agency through the <a href="http://mycor.nancy.inra.fr/ARBRE/" rel="nofollow">Laboratory of Excellence ARBRE</a>, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).</p>
<p><a target="_blank" rel="noopener noreferrer" href="assets/logos_betaumr.png"><img src="assets/logos_betaumr.png" alt="BLogos" style="max-width:100%;"></a></p>
</article></div>