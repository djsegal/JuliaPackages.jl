<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-beta-machine-learning-toolkit" class="anchor" aria-hidden="true" href="#beta-machine-learning-toolkit"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Beta Machine Learning Toolkit</h1>
<p><a target="_blank" rel="noopener noreferrer" href="assets/BetaML_logo.png"><img src="assets/BetaML_logo.png" width="300" valign="middle" style="max-width:100%;"></a>    <a target="_blank" rel="noopener noreferrer" href="assets/microExample_white.png"><img src="assets/microExample_white.png" width="500" valign="middle" style="max-width:100%;"></a></p>
<p>The <strong>Beta Machine Learning Toolkit</strong> is a repository with several basic Machine Learning algorithms, started from implementing in the Julia language the concepts taught in the <a href="https://www.edx.org/course/machine-learning-with-python-from-linear-models-to" rel="nofollow">MITX 6.86x - Machine Learning with Python: from Linear Models to Deep Learning</a> course.</p>
<p><a href="https://sylvaticus.github.io/BetaML.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://sylvaticus.github.io/BetaML.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3e353c26ddfe819150acbc732248f4f2a37f5175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.com/sylvaticus/BetaML.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/fae4c17200aa0cee2c2456c78e1a7daf7e95f1fe/68747470733a2f2f7472617669732d63692e636f6d2f73796c766174696375732f426574614d4c2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/sylvaticus/BetaML.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="http://codecov.io/github/sylvaticus/BetaML.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/b7f4cd89f84f87129888352a72798f6bfc65c91e/687474703a2f2f636f6465636f762e696f2f6769746875622f73796c766174696375732f426574614d4c2e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/sylvaticus/BetaML.jl/coverage.svg?branch=master" style="max-width:100%;"></a>
<a href="https://mybinder.org/v2/gh/sylvaticus/BetaML.jl/master" rel="nofollow"><img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"></a></p>
<p>Theoretical notes describing most of these algorithms are at the companion repository <a href="https://github.com/sylvaticus/MITx_6.86x">https://github.com/sylvaticus/MITx_6.86x</a>.</p>
<p>This stuff most likely has value only didactically, as the approaches are the "vanilla" ones, i.e. the simplest possible ones, and GPU is not supported here.
For "serious" machine learning work in Julia I suggest to use either <a href="https://fluxml.ai/" rel="nofollow">Flux</a> or <a href="https://github.com/denizyuret/Knet.jl">Knet</a>.</p>
<p>As the focus is mainly didactic, functions have pretty longer but more explicit names than usual.. for example the <code>Dense</code> layer is a <code>DenseLayer</code>, the <code>RBF</code> kernel is <code>radialKernel</code>, etc.</p>
<p>That said, Julia is a relatively fast language and most hard job is done in multithreaded functions or using matrix operations whose underlying libraries may be multithreaded, so it is reasonably fast for small exploratory tasks. Also it is already very flexible. For example, one can implement its own layer as a subtype of the abstract type <code>Layer</code> or its own optimisation algorithm as a subtype of <code>OptimisationAlgorithm</code> or even specify its own distance metric in the Kmedoids algorithm..</p>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h2>
<p>Please refer to the package documentation (<a href="https://sylvaticus.github.io/BetaML.jl/dev" rel="nofollow">stable</a> | <a href="https://sylvaticus.github.io/BetaML.jl/dev" rel="nofollow">dev</a>) or use the Julia inline package system (just press the question mark <code>?</code> and then, on the special help prompt <code>help?&gt;</code>, type the module or function name).</p>
<p>Module currently implemented are <a href="https://sylvaticus.github.io/BetaML.jl/dev/Perceptron.html" rel="nofollow">Perceptron</a>, <a href="https://sylvaticus.github.io/BetaML.jl/dev/Nn.html" rel="nofollow">Nn</a>, <a href="https://sylvaticus.github.io/BetaML.jl/dev/Clustering.html" rel="nofollow">Clustering</a> and <a href="https://sylvaticus.github.io/BetaML.jl/dev/Utils.html" rel="nofollow">Utils</a>.</p>
<p>We also provide some <a href="https://sylvaticus.github.io/BetaML.jl/dev/Notebooks.html" rel="nofollow">notebooks</a> that can be run online without installing anything, so you can start playing with the library in minutes.</p>
<p>If you are looking for an introductory book on Julia, have a look on "<a href="https://www.julia-book.com/" rel="nofollow">Julia Quick Syntax Reference</a>"(Apress,2019).</p>
<h3><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h3>
<h4><a id="user-content-using-an-artificial-neural-network-for-multinomial-categorisation" class="anchor" aria-hidden="true" href="#using-an-artificial-neural-network-for-multinomial-categorisation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using an Artificial Neural Network for multinomial categorisation</h4>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Load Modules</span>
<span class="pl-k">using</span> BetaML<span class="pl-k">.</span>Nn, DelimitedFiles, Random, StatsPlots <span class="pl-c"><span class="pl-c">#</span> Load the main module and ausiliary modules</span>
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">123</span>); <span class="pl-c"><span class="pl-c">#</span> Fix the random seed (to obtain reproducible results)</span>

<span class="pl-c"><span class="pl-c">#</span> Load the data</span>
iris     <span class="pl-k">=</span> <span class="pl-c1">readdlm</span>(<span class="pl-c1">joinpath</span>(<span class="pl-c1">dirname</span>(Base<span class="pl-k">.</span><span class="pl-c1">find_package</span>(<span class="pl-s"><span class="pl-pds">"</span>BetaML<span class="pl-pds">"</span></span>)),<span class="pl-s"><span class="pl-pds">"</span>..<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>test<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>iris.csv<span class="pl-pds">"</span></span>),<span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>,skipstart<span class="pl-k">=</span><span class="pl-c1">1</span>)
iris     <span class="pl-k">=</span> iris[<span class="pl-c1">shuffle</span>(<span class="pl-c1">axes</span>(iris, <span class="pl-c1">1</span>)), :] <span class="pl-c"><span class="pl-c">#</span> Shuffle the records, as they aren't by default</span>
x        <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Array{Float64,<span class="pl-c1">2</span>}, iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>])
y        <span class="pl-k">=</span> <span class="pl-c1">map</span>(x<span class="pl-k">-&gt;</span><span class="pl-c1">Dict</span>(<span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">1</span>, <span class="pl-s"><span class="pl-pds">"</span>versicolor<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span> <span class="pl-c1">2</span>, <span class="pl-s"><span class="pl-pds">"</span>virginica<span class="pl-pds">"</span></span> <span class="pl-k">=&gt;</span><span class="pl-c1">3</span>)[x],iris[:, <span class="pl-c1">5</span>]) <span class="pl-c"><span class="pl-c">#</span> Convert the target column to numbers</span>
y_oh     <span class="pl-k">=</span> <span class="pl-c1">oneHotEncoder</span>(y) <span class="pl-c"><span class="pl-c">#</span> Convert to One-hot representation (e.g. 2 =&gt; [0 1 0], 3 =&gt; [0 0 1])</span>

<span class="pl-c"><span class="pl-c">#</span> Split the data in training/testing sets</span>
ntrain    <span class="pl-k">=</span> <span class="pl-c1">Int64</span>(<span class="pl-c1">round</span>(<span class="pl-c1">size</span>(x,<span class="pl-c1">1</span>)<span class="pl-k">*</span><span class="pl-c1">0.8</span>))
xtrain    <span class="pl-k">=</span> x[<span class="pl-c1">1</span><span class="pl-k">:</span>ntrain,:]
ytrain    <span class="pl-k">=</span> y[<span class="pl-c1">1</span><span class="pl-k">:</span>ntrain]
ytrain_oh <span class="pl-k">=</span> y_oh[<span class="pl-c1">1</span><span class="pl-k">:</span>ntrain,:]
xtest     <span class="pl-k">=</span> x[ntrain<span class="pl-k">+</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span>,:]
ytest     <span class="pl-k">=</span> y[ntrain<span class="pl-k">+</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">end</span>]

<span class="pl-c"><span class="pl-c">#</span> Define the Artificial Neural Network model</span>
l1   <span class="pl-k">=</span> <span class="pl-c1">DenseLayer</span>(<span class="pl-c1">4</span>,<span class="pl-c1">10</span>,f<span class="pl-k">=</span>relu) <span class="pl-c"><span class="pl-c">#</span> Activation function is ReLU</span>
l2   <span class="pl-k">=</span> <span class="pl-c1">DenseLayer</span>(<span class="pl-c1">10</span>,<span class="pl-c1">3</span>)        <span class="pl-c"><span class="pl-c">#</span> Activation function is identity by default</span>
l3   <span class="pl-k">=</span> <span class="pl-c1">VectorFunctionLayer</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>,f<span class="pl-k">=</span>softMax) <span class="pl-c"><span class="pl-c">#</span> Add a (parameterless) layer whose activation function (softMax in this case) is defined to all its nodes at once</span>
mynn <span class="pl-k">=</span> <span class="pl-c1">buildNetwork</span>([l1,l2,l3],squaredCost,name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Multinomial logistic regression Model Sepal<span class="pl-pds">"</span></span>) <span class="pl-c"><span class="pl-c">#</span> Build the NN and use the squared cost (aka MSE) as error function</span>

<span class="pl-c"><span class="pl-c">#</span> Training it (default to SGD)</span>
res <span class="pl-k">=</span> <span class="pl-c1">train!</span>(mynn,<span class="pl-c1">scale</span>(xtrain),ytrain_oh,epochs<span class="pl-k">=</span><span class="pl-c1">100</span>,batchSize<span class="pl-k">=</span><span class="pl-c1">6</span>) <span class="pl-c"><span class="pl-c">#</span> Use optAlg=SGD (Stochastic Gradient Descent) by default</span>

<span class="pl-c"><span class="pl-c">#</span> Test it</span>
ŷtrain        <span class="pl-k">=</span> <span class="pl-c1">predict</span>(mynn,<span class="pl-c1">scale</span>(xtrain))   <span class="pl-c"><span class="pl-c">#</span> Note the scaling function</span>
ŷtest         <span class="pl-k">=</span> <span class="pl-c1">predict</span>(mynn,<span class="pl-c1">scale</span>(xtest))
trainAccuracy <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ŷtrain,ytrain,tol<span class="pl-k">=</span><span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> 0.983</span>
testAccuracy  <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ŷtest,ytest,tol<span class="pl-k">=</span><span class="pl-c1">1</span>)   <span class="pl-c"><span class="pl-c">#</span> 1.0</span>

<span class="pl-c"><span class="pl-c">#</span> Visualise results</span>
testSize <span class="pl-k">=</span> <span class="pl-c1">size</span>(ŷtest,<span class="pl-c1">1</span>)
ŷtestChosen <span class="pl-k">=</span>  [<span class="pl-c1">argmax</span>(ŷtest[i,:]) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span>testSize]
<span class="pl-c1">groupedbar</span>([ytest ŷtestChosen], label<span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>ytest<span class="pl-pds">"</span></span> <span class="pl-s"><span class="pl-pds">"</span>ŷtest (est)<span class="pl-pds">"</span></span>], title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>True vs estimated categories<span class="pl-pds">"</span></span>) <span class="pl-c"><span class="pl-c">#</span> All records correctly labelled !</span>
<span class="pl-c1">plot</span>(<span class="pl-c1">0</span><span class="pl-k">:</span>res<span class="pl-k">.</span>epochs,res<span class="pl-k">.</span>ϵ_epochs, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>epochs<span class="pl-pds">"</span></span>,xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>error<span class="pl-pds">"</span></span>,legend<span class="pl-k">=</span><span class="pl-c1">nothing</span>,title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Avg. error per epoch on the Sepal dataset<span class="pl-pds">"</span></span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="assets/sepalOutput_results.png"><img src="assets/sepalOutput_results.png" width="400" style="max-width:100%;"></a> <a target="_blank" rel="noopener noreferrer" href="assets/sepalOutput_errors.png"><img src="assets/sepalOutput_errors.png" width="400" style="max-width:100%;"></a></p>
<h2><a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TODO</h2>
<h3><a id="user-content-short-term" class="anchor" aria-hidden="true" href="#short-term"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Short term</h3>
<ul>
<li>adding other optimisation algorithms to NN</li>
<li>sorting out cluster API (EM for generic mixtures)</li>
</ul>
<h3><a id="user-content-long-term" class="anchor" aria-hidden="true" href="#long-term"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Long term</h3>
<ul>
<li>Support Vector Machine (if anyone request it)</li>
<li>Add convolutional layers and RNN support</li>
</ul>
<h2><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgements</h2>
<p>The development of this package at the <em>Bureau d'Economie Théorique et Appliquée</em> (BETA, Nancy) was supported by the French National Research Agency through the <a href="http://mycor.nancy.inra.fr/ARBRE/" rel="nofollow">Laboratory of Excellence ARBRE</a>, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).</p>
<p><a target="_blank" rel="noopener noreferrer" href="assets/logos_betaumr.png"><img src="assets/logos_betaumr.png" alt="BLogos" style="max-width:100%;"></a></p>
</article></div>