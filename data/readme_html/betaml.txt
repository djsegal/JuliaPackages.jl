<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-beta-machine-learning-toolkit" class="anchor" aria-hidden="true" href="#beta-machine-learning-toolkit"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Beta Machine Learning Toolkit</h1>
<p dir="auto"><em>Machine Learning made simple :-)</em></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/BetaML_logo.png"><img src="assets/BetaML_logo.png" width="300" valign="middle" style="max-width: 100%;"></a>    <a target="_blank" rel="noopener noreferrer" href="assets/microExample_white.png"><img src="assets/microExample_white.png" width="500" valign="middle" style="max-width: 100%;"></a></p>
<p dir="auto">The <strong>Beta Machine Learning Toolkit</strong> is a package including many algorithms and utilities to implement machine learning workflows in Julia, <a href="https://sylvaticus.github.io/BetaML.jl/stable/tutorials/Betaml_tutorial_getting_started.html#Use-BetaML-in-Python" rel="nofollow">Python</a>, <a href="https://sylvaticus.github.io/BetaML.jl/stable/tutorials/Betaml_tutorial_getting_started.html#Use-BetaML-in-R" rel="nofollow">R</a> and any other language with a Julia binding.</p>
<p dir="auto"><a href="https://sylvaticus.github.io/BetaML.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://sylvaticus.github.io/BetaML.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://doi.org/10.21105/joss.02849" rel="nofollow"><img src="https://camo.githubusercontent.com/f3e975c272488246ef6ecf4ec40238b8c4374aaff7c59e796a3dd585f41cddaf/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f31302e32313130352f6a6f73732e30323834392f7374617475732e737667" alt="DOI" data-canonical-src="https://joss.theoj.org/papers/10.21105/joss.02849/status.svg" style="max-width: 100%;"></a>
<a href="https://github.com/sylvaticus/BetaML.jl/actions"><img src="https://github.com/sylvaticus/BetaML.jl/workflows/CI/badge.svg" alt="Build status" style="max-width: 100%;"></a>
<a href="http://codecov.io/github/sylvaticus/BetaML.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/8daddfec12ab92a28b23c4f109a8f436b4010a27035092ac2350ec5b148cbfc1/687474703a2f2f636f6465636f762e696f2f6769746875622f73796c766174696375732f426574614d4c2e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/sylvaticus/BetaML.jl/coverage.svg?branch=master" style="max-width: 100%;"></a></p>
<p dir="auto">Currently the following models are available:</p>
<table>
<thead>
<tr>
<th>BetaML name</th>
<th>MLJ Interface</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Perceptron.html#BetaML.Perceptron.PerceptronClassifier" rel="nofollow"><code>PerceptronClassifier</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Perceptron.html#BetaML.Perceptron.LinearPerceptron" rel="nofollow"><code>LinearPerceptron</code></a></td>
<td><em>Supervised classifier</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Perceptron.html#BetaML.Perceptron.KernelPerceptronClassifier" rel="nofollow"><code>KernelPerceptronClassifier</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Perceptron.html#BetaML.Perceptron.KernelPerceptron" rel="nofollow"><code>KernelPerceptron</code></a></td>
<td><em>Supervised classifier</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Perceptron.html#BetaML.Perceptron.PegasosClassifier" rel="nofollow"><code>PegasosClassifier</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Perceptron.html#BetaML.Perceptron.Pegasos" rel="nofollow"><code>Pegasos</code></a></td>
<td><em>Supervised classifier</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Trees.html#BetaML.Trees.DecisionTreeEstimator" rel="nofollow"><code>DecisionTreeEstimator</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Trees.html#BetaML.Trees.DecisionTreeClassifier" rel="nofollow"><code>DecisionTreeClassifier</code></a>, <a href="https://sylvaticus.github.io/BetaML.jl/stable/Trees.html#BetaML.Trees.DecisionTreeRegressor" rel="nofollow"><code>DecisionTreeRegressor</code></a></td>
<td><em>Supervised regressor and classifier</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Trees.html#BetaML.Trees.RandomForestEstimator" rel="nofollow"><code>RandomForestEstimator</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Trees.html#BetaML.Trees.RandomForestClassifier" rel="nofollow"><code>RandomForestClassifier</code></a>, <a href="https://sylvaticus.github.io/BetaML.jl/stable/Trees.html#BetaML.Trees.RandomForestRegressor" rel="nofollow"><code>RandomForestRegressor</code></a></td>
<td><em>Supervised regressor and classifier</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Nn.html#BetaML.Nn.NeuralNetworkEstimator" rel="nofollow"><code>NeuralNetworkEstimator</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Nn.html#BetaML.Nn.NeuralNetworkRegressor" rel="nofollow"><code>NeuralNetworkRegressor</code></a>, <a href="https://sylvaticus.github.io/BetaML.jl/stable/Nn.html#BetaML.Nn.MultitargetNeuralNetworkRegressor" rel="nofollow"><code>MultitargetNeuralNetworkRegressor</code></a>, <a href="https://sylvaticus.github.io/BetaML.jl/stable/Nn.html#BetaML.Nn.NeuralNetworkClassifier" rel="nofollow"><code>NeuralNetworkClassifier</code></a></td>
<td><em>Supervised regressor and classifier</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/GMM.html#BetaML.GMM.GMMRegressor1" rel="nofollow"><code>GMMRegressor1</code></a></td>
<td></td>
<td><em>Supervised regressor</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/GMM.html#BetaML.GMM.GMMRegressor2" rel="nofollow"><code>GMMRegressor2</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/GMM.html#BetaML.GMM.GaussianMixtureRegressor" rel="nofollow"><code>GaussianMixtureRegressor</code></a>, <a href="https://sylvaticus.github.io/BetaML.jl/stable/GMM.html#BetaML.GMM.MultitargetGaussianMixtureRegressor" rel="nofollow"><code>MultitargetGaussianMixtureRegressor</code></a></td>
<td><em>Supervised regressor</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Clustering.html#BetaML.Clustering.KMeansClusterer" rel="nofollow"><code>KMeansClusterer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Clustering.html#BetaML.Clustering.KMeans" rel="nofollow"><code>KMeans</code></a></td>
<td><em>Unsupervised hard clusterer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Clustering.html#BetaML.Clustering.KMedoidsClusterer" rel="nofollow"><code>KMedoidsClusterer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Clustering.html#BetaML.Clustering.KMedoids" rel="nofollow"><code>KMedoids</code></a></td>
<td><em>Unsupervised hard clusterer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/GMM.html#BetaML.GMM.GMMClusterer" rel="nofollow"><code>GMMClusterer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/GMM.html#BetaML.GMM.GaussianMixtureClusterer" rel="nofollow"><code>GaussianMixtureClusterer</code></a></td>
<td><em>Unsupervised soft clusterer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.FeatureBasedImputer" rel="nofollow"><code>FeatureBasedImputer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.SimpleImputer" rel="nofollow"><code>SimpleImputer</code></a></td>
<td><em>Unsupervised missing data imputer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.GMMImputer" rel="nofollow"><code>GMMImputer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.GaussianMixtureImputer" rel="nofollow"><code>GaussianMixtureImputer</code></a></td>
<td><em>Unsupervised missing data imputer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.RFImputer" rel="nofollow"><code>RFImputer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.RandomForestImputer" rel="nofollow"><code>RandomForestImputer</code></a></td>
<td><em>Unsupervised missing data imputer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.UniversalImputer" rel="nofollow"><code>UniversalImputer</code></a></td>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Imputation.html#BetaML.Imputation.GeneralImputer" rel="nofollow"><code>GeneralImputer</code></a></td>
<td><em>Unsupervised missing data imputer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.MinMaxScaler" rel="nofollow"><code>MinMaxScaler</code></a></td>
<td></td>
<td><em>Data transformer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.StandardScaler" rel="nofollow"><code>StandardScaler</code></a></td>
<td></td>
<td><em>Data transformer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.Scaler" rel="nofollow"><code>Scaler</code></a></td>
<td></td>
<td><em>Data transformer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.PCA" rel="nofollow"><code>PCA</code></a></td>
<td></td>
<td><em>Unsupervised dimensionality reduction transformer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.OneHotEncoder" rel="nofollow"><code>OneHotEncoder</code></a></td>
<td></td>
<td><em>Data transformer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.OrdinalEncoder" rel="nofollow"><code>OrdinalEncoder</code></a></td>
<td></td>
<td><em>Data transformer</em></td>
</tr>
<tr>
<td><a href="https://sylvaticus.github.io/BetaML.jl/stable/Utils.html#BetaML.Utils.ConfusionMatrix" rel="nofollow"><code>ConfusionMatrix</code></a></td>
<td></td>
<td><em>Predictions assessment</em></td>
</tr>
</tbody>
</table>
<p dir="auto">Theoretical notes describing many of these algorithms are at the companion repository <a href="https://github.com/sylvaticus/MITx_6.86x">https://github.com/sylvaticus/MITx_6.86x</a>.</p>
<p dir="auto">All models are implemented entirely in Julia and are hosted in the repository itself (i.e. they are not wrapper to third-party models).
If your favorite option or model is missing, you can try implement it yourself and <a href="https://github.com/sylvaticus/BetaML.jl/pulls">open a pull request</a> to share it (see the section <a href="#contribute">Contribute</a> below) or request its implementation (<a href="https://github.com/sylvaticus/BetaML.jl/issues">open an issue</a>). Thanks to its JIT compiler, Julia is indeed in the sweet spot where we can easily write models in a high-level language and still having them running efficiently.</p>
<h2 dir="auto"><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Documentation</h2>
<p dir="auto">Please refer to the <a href="https://sylvaticus.github.io/BetaML.jl/stable" rel="nofollow">package documentation</a> or use the Julia inline package system (just press the question mark <code>?</code> and then, on the special help prompt <code>help?&gt;</code>, type the module or function name). The package documentation is made of two distinct parts. The first one is an extensively commented tutorial that covers most of the library, the second one is the reference manual covering the library's API.</p>
<p dir="auto">If you are looking for an introductory material on Julia, have a look on the book "<a href="https://www.julia-book.com/" rel="nofollow">Julia Quick Syntax Reference</a>"(Apress,2019) or the online course "<a href="https://sylvaticus.github.io/SPMLJ/stable/" rel="nofollow">Scientific Programming and Machine Learning in Julia</a>.</p>
<p dir="auto">While implemented in Julia, this package can be easily used in R or Python employing <a href="https://github.com/Non-Contradiction/JuliaCall">JuliaCall</a> or <a href="https://github.com/JuliaPy/pyjulia">PyJulia</a> respectively, see <a href="https://sylvaticus.github.io/BetaML.jl/stable/tutorials/Betaml_tutorial_getting_started.html#using_betaml_from_other_languages" rel="nofollow">the relevant section</a> in the documentation.</p>
<h3 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h3>
<ul dir="auto">
<li><strong>Using an Artificial Neural Network for multinomial categorisation</strong></li>
</ul>
<p dir="auto">In this example we see how to train a neural networks model to predict the specie's name (5th column) given floral sepals and petals measures (first 4 columns) in the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="nofollow">iris flower dataset</a>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Load Modules
using DelimitedFiles, Random
using Pipe, Plots, BetaML # Load BetaML and other auxiliary modules
Random.seed!(123);  # Fix the random seed (to obtain reproducible results).

# Load the data
iris     = readdlm(joinpath(dirname(Base.find_package(&quot;BetaML&quot;)),&quot;..&quot;,&quot;test&quot;,&quot;data&quot;,&quot;iris.csv&quot;),',',skipstart=1)
x        = convert(Array{Float64,2}, iris[:,1:4])
y        = convert(Array{String,1}, iris[:,5])
# Encode the categories (levels) of y using a separate column per each category (aka &quot;one-hot&quot; encoding) 
ohmod    = OneHotEncoder()
y_oh     = fit!(ohmod,y) 
# Split the data in training/testing sets
((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) = partition([x,y,y_oh],[0.8,0.2])
(ntrain, ntest) = size.([xtrain,xtest],1)

# Define the Artificial Neural Network model
l1   = DenseLayer(4,10,f=relu) # The activation function is `ReLU`
l2   = DenseLayer(10,3)        # The activation function is `identity` by default
l3   = VectorFunctionLayer(3,f=softmax) # Add a (parameterless) layer whose activation function (`softmax` in this case) is defined to all its nodes at once
mynn = NeuralNetworkEstimator(layers=[l1,l2,l3],loss=crossentropy,descr=&quot;Multinomial logistic regression Model Sepal&quot;, batch_size=2, epochs=200) # Build the NN and use the cross-entropy as error function. Swith to auto-tuning with `autotune=true`

# Train the model (using the ADAM optimizer by default)
res = fit!(mynn,fit!(Scaler(),xtrain),ytrain_oh) # Fit the model to the (scaled) data

# Obtain predictions and test them against the ground true observations
ŷtrain         = @pipe predict(mynn,fit!(Scaler(),xtrain)) |&gt; inverse_predict(ohmod,_)  # Note the scaling and reverse one-hot encoding functions
ŷtest          = @pipe predict(mynn,fit!(Scaler(),xtest))  |&gt; inverse_predict(ohmod,_) 
train_accuracy = accuracy(ytrain,ŷtrain) # 0.975
test_accuracy  = accuracy(ytest,ŷtest)   # 0.96

# Analyse model performances
cm = ConfusionMatrix()
fit!(cm,ytest,ŷtest)
print(cm)"><pre><span class="pl-c"><span class="pl-c">#</span> Load Modules</span>
<span class="pl-k">using</span> DelimitedFiles, Random
<span class="pl-k">using</span> Pipe, Plots, BetaML <span class="pl-c"><span class="pl-c">#</span> Load BetaML and other auxiliary modules</span>
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">123</span>);  <span class="pl-c"><span class="pl-c">#</span> Fix the random seed (to obtain reproducible results).</span>

<span class="pl-c"><span class="pl-c">#</span> Load the data</span>
iris     <span class="pl-k">=</span> <span class="pl-c1">readdlm</span>(<span class="pl-c1">joinpath</span>(<span class="pl-c1">dirname</span>(Base<span class="pl-k">.</span><span class="pl-c1">find_package</span>(<span class="pl-s"><span class="pl-pds">"</span>BetaML<span class="pl-pds">"</span></span>)),<span class="pl-s"><span class="pl-pds">"</span>..<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>test<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>data<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>iris.csv<span class="pl-pds">"</span></span>),<span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>,skipstart<span class="pl-k">=</span><span class="pl-c1">1</span>)
x        <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Array{Float64,<span class="pl-c1">2</span>}, iris[:,<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4</span>])
y        <span class="pl-k">=</span> <span class="pl-c1">convert</span>(Array{String,<span class="pl-c1">1</span>}, iris[:,<span class="pl-c1">5</span>])
<span class="pl-c"><span class="pl-c">#</span> Encode the categories (levels) of y using a separate column per each category (aka "one-hot" encoding) </span>
ohmod    <span class="pl-k">=</span> <span class="pl-c1">OneHotEncoder</span>()
y_oh     <span class="pl-k">=</span> <span class="pl-c1">fit!</span>(ohmod,y) 
<span class="pl-c"><span class="pl-c">#</span> Split the data in training/testing sets</span>
((xtrain,xtest),(ytrain,ytest),(ytrain_oh,ytest_oh)) <span class="pl-k">=</span> <span class="pl-c1">partition</span>([x,y,y_oh],[<span class="pl-c1">0.8</span>,<span class="pl-c1">0.2</span>])
(ntrain, ntest) <span class="pl-k">=</span> <span class="pl-c1">size</span>.([xtrain,xtest],<span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> Define the Artificial Neural Network model</span>
l1   <span class="pl-k">=</span> <span class="pl-c1">DenseLayer</span>(<span class="pl-c1">4</span>,<span class="pl-c1">10</span>,f<span class="pl-k">=</span>relu) <span class="pl-c"><span class="pl-c">#</span> The activation function is `ReLU`</span>
l2   <span class="pl-k">=</span> <span class="pl-c1">DenseLayer</span>(<span class="pl-c1">10</span>,<span class="pl-c1">3</span>)        <span class="pl-c"><span class="pl-c">#</span> The activation function is `identity` by default</span>
l3   <span class="pl-k">=</span> <span class="pl-c1">VectorFunctionLayer</span>(<span class="pl-c1">3</span>,f<span class="pl-k">=</span>softmax) <span class="pl-c"><span class="pl-c">#</span> Add a (parameterless) layer whose activation function (`softmax` in this case) is defined to all its nodes at once</span>
mynn <span class="pl-k">=</span> <span class="pl-c1">NeuralNetworkEstimator</span>(layers<span class="pl-k">=</span>[l1,l2,l3],loss<span class="pl-k">=</span>crossentropy,descr<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Multinomial logistic regression Model Sepal<span class="pl-pds">"</span></span>, batch_size<span class="pl-k">=</span><span class="pl-c1">2</span>, epochs<span class="pl-k">=</span><span class="pl-c1">200</span>) <span class="pl-c"><span class="pl-c">#</span> Build the NN and use the cross-entropy as error function. Swith to auto-tuning with `autotune=true`</span>

<span class="pl-c"><span class="pl-c">#</span> Train the model (using the ADAM optimizer by default)</span>
res <span class="pl-k">=</span> <span class="pl-c1">fit!</span>(mynn,<span class="pl-c1">fit!</span>(<span class="pl-c1">Scaler</span>(),xtrain),ytrain_oh) <span class="pl-c"><span class="pl-c">#</span> Fit the model to the (scaled) data</span>

<span class="pl-c"><span class="pl-c">#</span> Obtain predictions and test them against the ground true observations</span>
ŷtrain         <span class="pl-k">=</span> <span class="pl-c1">@pipe</span> <span class="pl-c1">predict</span>(mynn,<span class="pl-c1">fit!</span>(<span class="pl-c1">Scaler</span>(),xtrain)) <span class="pl-k">|&gt;</span> <span class="pl-c1">inverse_predict</span>(ohmod,_)  <span class="pl-c"><span class="pl-c">#</span> Note the scaling and reverse one-hot encoding functions</span>
ŷtest          <span class="pl-k">=</span> <span class="pl-c1">@pipe</span> <span class="pl-c1">predict</span>(mynn,<span class="pl-c1">fit!</span>(<span class="pl-c1">Scaler</span>(),xtest))  <span class="pl-k">|&gt;</span> <span class="pl-c1">inverse_predict</span>(ohmod,_) 
train_accuracy <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ytrain,ŷtrain) <span class="pl-c"><span class="pl-c">#</span> 0.975</span>
test_accuracy  <span class="pl-k">=</span> <span class="pl-c1">accuracy</span>(ytest,ŷtest)   <span class="pl-c"><span class="pl-c">#</span> 0.96</span>

<span class="pl-c"><span class="pl-c">#</span> Analyse model performances</span>
cm <span class="pl-k">=</span> <span class="pl-c1">ConfusionMatrix</span>()
<span class="pl-c1">fit!</span>(cm,ytest,ŷtest)
<span class="pl-c1">print</span>(cm)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="A ConfusionMatrix BetaMLModel (fitted)

-----------------------------------------------------------------

*** CONFUSION MATRIX ***

Scores actual (rows) vs predicted (columns):

4×4 Matrix{Any}:
 &quot;Labels&quot;       &quot;virginica&quot;    &quot;versicolor&quot;   &quot;setosa&quot;
 &quot;virginica&quot;   8              1              0
 &quot;versicolor&quot;  0             14              0
 &quot;setosa&quot;      0              0              7
Normalised scores actual (rows) vs predicted (columns):

4×4 Matrix{Any}:
 &quot;Labels&quot;       &quot;virginica&quot;   &quot;versicolor&quot;   &quot;setosa&quot;
 &quot;virginica&quot;   0.888889      0.111111       0.0
 &quot;versicolor&quot;  0.0           1.0            0.0
 &quot;setosa&quot;      0.0           0.0            1.0

 *** CONFUSION REPORT ***

- Accuracy:               0.9666666666666667
- Misclassification rate: 0.033333333333333326
- Number of classes:      3

  N Class      precision   recall  specificity  f1score  actual_count  predicted_count
                             TPR       TNR                 support                  

  1 virginica      1.000    0.889        1.000    0.941            9               8
  2 versicolor     0.933    1.000        0.938    0.966           14              15
  3 setosa         1.000    1.000        1.000    1.000            7               7

- Simple   avg.    0.978    0.963        0.979    0.969
- Weigthed avg.    0.969    0.967        0.971    0.966"><pre lang="text" class="notranslate"><code>A ConfusionMatrix BetaMLModel (fitted)

-----------------------------------------------------------------

*** CONFUSION MATRIX ***

Scores actual (rows) vs predicted (columns):

4×4 Matrix{Any}:
 "Labels"       "virginica"    "versicolor"   "setosa"
 "virginica"   8              1              0
 "versicolor"  0             14              0
 "setosa"      0              0              7
Normalised scores actual (rows) vs predicted (columns):

4×4 Matrix{Any}:
 "Labels"       "virginica"   "versicolor"   "setosa"
 "virginica"   0.888889      0.111111       0.0
 "versicolor"  0.0           1.0            0.0
 "setosa"      0.0           0.0            1.0

 *** CONFUSION REPORT ***

- Accuracy:               0.9666666666666667
- Misclassification rate: 0.033333333333333326
- Number of classes:      3

  N Class      precision   recall  specificity  f1score  actual_count  predicted_count
                             TPR       TNR                 support                  

  1 virginica      1.000    0.889        1.000    0.941            9               8
  2 versicolor     0.933    1.000        0.938    0.966           14              15
  3 setosa         1.000    1.000        1.000    1.000            7               7

- Simple   avg.    0.978    0.963        0.979    0.969
- Weigthed avg.    0.969    0.967        0.971    0.966
</code></pre></div>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="ϵ = info(mynn)[&quot;lossPerEpoch&quot;]
plot(1:length(ϵ),ϵ, ylabel=&quot;epochs&quot;,xlabel=&quot;error&quot;,legend=nothing,title=&quot;Avg. error per epoch on the Sepal dataset&quot;)
heatmap(info(cm)[&quot;categories&quot;],info(cm)[&quot;categories&quot;],info(cm)[&quot;normalised_scores&quot;],c=cgrad([:white,:blue]),xlabel=&quot;Predicted&quot;,ylabel=&quot;Actual&quot;, title=&quot;Confusion Matrix&quot;)"><pre>ϵ <span class="pl-k">=</span> <span class="pl-c1">info</span>(mynn)[<span class="pl-s"><span class="pl-pds">"</span>lossPerEpoch<span class="pl-pds">"</span></span>]
<span class="pl-c1">plot</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(ϵ),ϵ, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>epochs<span class="pl-pds">"</span></span>,xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>error<span class="pl-pds">"</span></span>,legend<span class="pl-k">=</span><span class="pl-c1">nothing</span>,title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Avg. error per epoch on the Sepal dataset<span class="pl-pds">"</span></span>)
<span class="pl-c1">heatmap</span>(<span class="pl-c1">info</span>(cm)[<span class="pl-s"><span class="pl-pds">"</span>categories<span class="pl-pds">"</span></span>],<span class="pl-c1">info</span>(cm)[<span class="pl-s"><span class="pl-pds">"</span>categories<span class="pl-pds">"</span></span>],<span class="pl-c1">info</span>(cm)[<span class="pl-s"><span class="pl-pds">"</span>normalised_scores<span class="pl-pds">"</span></span>],c<span class="pl-k">=</span><span class="pl-c1">cgrad</span>([<span class="pl-c1">:white</span>,<span class="pl-c1">:blue</span>]),xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Predicted<span class="pl-pds">"</span></span>,ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Actual<span class="pl-pds">"</span></span>, title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Confusion Matrix<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/sepal_errorsPerEpoch.png"><img src="assets/sepal_errorsPerEpoch.png" width="400" style="max-width: 100%;"></a> <a target="_blank" rel="noopener noreferrer" href="assets/sepal_confusionMatrix.png"><img src="assets/sepal_confusionMatrix.png" width="400" style="max-width: 100%;"></a></p>
<ul dir="auto">
<li><strong>Other examples</strong></li>
</ul>
<p dir="auto">Further examples, with more models and more advanced techniques in order to improve predictions, are provided in the documentation tutorial.
Basic examples in Python and R are given <a href="https://sylvaticus.github.io/BetaML.jl/stable/tutorials/Betaml_tutorial_getting_started.html#using_betaml_from_other_languages" rel="nofollow">here</a>.
Very "micro" examples of usage of the various functions can also be studied in the unit-tests available in the <a href="https://github.com/sylvaticus/BetaML.jl/tree/master/test"><code>test</code></a> folder.</p>
<h2 dir="auto"><a id="user-content-limitations-and-alternative-packages" class="anchor" aria-hidden="true" href="#limitations-and-alternative-packages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Limitations and alternative packages</h2>
<p dir="auto">The focus of the library is skewed toward user-friendliness rather than computational efficiency. While the code is (relatively) easy to read, it is not heavily optimised, and currently all models operate on the CPU and only with data that fits in the pc's memory.
For very large data we suggest specialised packages. See the list below:</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Packages</th>
</tr>
</thead>
<tbody>
<tr>
<td>ML toolkits/pipelines</td>
<td><a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a>, <a href="https://github.com/IBM/AutoMLPipeline.jl">AutoMLPipeline.jl</a>, <a href="https://joss.theoj.org/papers/10.21105/joss.02704" rel="nofollow">MLJ.jl</a></td>
</tr>
<tr>
<td>Neural Networks</td>
<td><a href="https://fluxml.ai/" rel="nofollow">Flux.jl</a>, <a href="https://github.com/denizyuret/Knet.jl">Knet</a></td>
</tr>
<tr>
<td>Decision Trees</td>
<td><a href="https://github.com/bensadeghi/DecisionTree.jl">DecisionTree.jl</a></td>
</tr>
<tr>
<td>Clustering</td>
<td><a href="https://github.com/JuliaStats/Clustering.jl">Clustering.jl</a>, <a href="https://github.com/davidavdav/GaussianMixtures.jl">GaussianMixtures.jl</a></td>
</tr>
<tr>
<td>Missing imputation</td>
<td><a href="https://github.com/invenia/Impute.jl">Impute.jl</a></td>
</tr>
</tbody>
</table>
<h2 dir="auto"><a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TODO</h2>
<h3 dir="auto"><a id="user-content-short-term" class="anchor" aria-hidden="true" href="#short-term"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Short term</h3>
<ul dir="auto">
<li>Implement autotuning of <code>GMMClusterer</code> using  <code>BIC</code> or <code>AIC</code></li>
<li>Add Silhouette method to check cluster validity</li>
<li>Implement PAM and/or variants for kmedoids</li>
</ul>
<h3 dir="auto"><a id="user-content-midlong-term" class="anchor" aria-hidden="true" href="#midlong-term"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Mid/Long term</h3>
<ul dir="auto">
<li>Add RNN support and improve convolutional layers speed</li>
<li>Reinforcement learning (Markov decision processes)</li>
</ul>
<h2 dir="auto"><a id="user-content-contribute" class="anchor" aria-hidden="true" href="#contribute"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contribute</h2>
<p dir="auto">Contributions to the library are welcome. We are particularly interested in the areas covered in the "TODO" list above, but we are open to other areas as well.
Please however consider that the focus is mostly didactic/research, so clear, easy to read (and well documented) code and simple API with reasonable defaults are more important that highly optimised algorithms. For the same reason, it is fine to use verbose names.
Please open an issue to discuss your ideas or make directly a well-documented pull request to the repository.
While not required by any means, if you are customising BetaML and writing for example your own neural network layer type (by subclassing <code>AbstractLayer</code>), your own sampler (by subclassing <code>AbstractDataSampler</code>) or your own mixture component (by subclassing <code>AbstractMixture</code>), please consider to give it back to the community and open a pull request to integrate them in BetaML.</p>
<h2 dir="auto"><a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citations</h2>
<p dir="auto">If you use <code>BetaML</code> please cite it as:</p>
<ul dir="auto">
<li>Lobianco, A., (2021). BetaML: The Beta Machine Learning Toolkit, a self-contained repository of Machine Learning algorithms in Julia. Journal of Open Source Software, 6(60), 2849, <a href="https://doi.org/10.21105/joss.02849" rel="nofollow">https://doi.org/10.21105/joss.02849</a></li>
</ul>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@article{Lobianco2021,
  doi       = {10.21105/joss.02849},
  url       = {https://doi.org/10.21105/joss.02849},
  year      = {2021},
  publisher = {The Open Journal},
  volume    = {6},
  number    = {60},
  pages     = {2849},
  author    = {Antonello Lobianco},
  title     = {BetaML: The Beta Machine Learning Toolkit, a self-contained repository of Machine Learning algorithms in Julia},
  journal   = {Journal of Open Source Software}
}"><pre><span class="pl-k">@article</span>{<span class="pl-en">Lobianco2021</span>,
  <span class="pl-s">doi</span>       = <span class="pl-s"><span class="pl-pds">{</span>10.21105/joss.02849<span class="pl-pds">}</span></span>,
  <span class="pl-s">url</span>       = <span class="pl-s"><span class="pl-pds">{</span>https://doi.org/10.21105/joss.02849<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>      = <span class="pl-s"><span class="pl-pds">{</span>2021<span class="pl-pds">}</span></span>,
  <span class="pl-s">publisher</span> = <span class="pl-s"><span class="pl-pds">{</span>The Open Journal<span class="pl-pds">}</span></span>,
  <span class="pl-s">volume</span>    = <span class="pl-s"><span class="pl-pds">{</span>6<span class="pl-pds">}</span></span>,
  <span class="pl-s">number</span>    = <span class="pl-s"><span class="pl-pds">{</span>60<span class="pl-pds">}</span></span>,
  <span class="pl-s">pages</span>     = <span class="pl-s"><span class="pl-pds">{</span>2849<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>    = <span class="pl-s"><span class="pl-pds">{</span>Antonello Lobianco<span class="pl-pds">}</span></span>,
  <span class="pl-s">title</span>     = <span class="pl-s"><span class="pl-pds">{</span>BetaML: The Beta Machine Learning Toolkit, a self-contained repository of Machine Learning algorithms in Julia<span class="pl-pds">}</span></span>,
  <span class="pl-s">journal</span>   = <span class="pl-s"><span class="pl-pds">{</span>Journal of Open Source Software<span class="pl-pds">}</span></span>
}</pre></div>
<h2 dir="auto"><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">The development of this package at the <em>Bureau d'Economie Théorique et Appliquée</em> (BETA, Nancy) was supported by the French National Research Agency through the <a href="http://mycor.nancy.inra.fr/ARBRE/" rel="nofollow">Laboratory of Excellence ARBRE</a>, a part of the “Investissements d'Avenir” Program (ANR 11 – LABX-0002-01).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="assets/logos_betaumr.png"><img src="assets/logos_betaumr.png" alt="BLogos" style="max-width: 100%;"></a></p>
</article></div>