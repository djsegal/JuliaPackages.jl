<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-egr-julia" class="anchor" aria-hidden="true" href="#egr-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>EGR-Julia</h1>
<p><a href="https://travis-ci.org/stefanks/EGR.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9b008bf5ad0889a490fe59b9528742e70ada445e/68747470733a2f2f7472617669732d63692e6f72672f73746566616e6b732f4547522e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/stefanks/EGR.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/r/stefanks/EGR.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/83395c31e734722afb2a6e2e5316849fe32630f2/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f73746566616e6b732f4547522e6a6c2f62616467652e737667" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/stefanks/EGR.jl/badge.svg" style="max-width:100%;"></a></p>
<p>The Stochastic Gradient (SG) algorithm is a popular learning algorithm for machine learning. Its main drawback is the high variance of the gradient estimates. In this evolving gradient framework, past history is used to reduce the variance of the SG step direction.</p>
<p>Efficient implementations of gradients for Multi-Class logistic regression, and sparse binary logistic regression are used.</p>
</article></div>