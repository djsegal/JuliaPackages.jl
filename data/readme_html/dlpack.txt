<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h2 dir="auto"><a id="user-content-dlpackjl" class="anchor" aria-hidden="true" href="#dlpackjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DLPack.jl</h2>
<p dir="auto"><a href="https://github.com/pabloferz/DLPack.jl/actions?query=ci"><img src="https://github.com/pabloferz/DLPack.jl/workflows/CI/badge.svg" alt="Tests" style="max-width: 100%;"></a></p>
<p dir="auto">Julia wrapper for <a href="https://github.com/dmlc/dlpack">DLPack</a>.</p>
<p dir="auto">This module provides a Julia interface to facilitate bidirectional data
exchange of tensor objects between Julia and Python libraries such as JAX,
CuPy, PyTorch, among others (all python libraries supporting the
<a href="https://data-apis.org/array-api/latest/design_topics/data_interchange.html" rel="nofollow">DLPack protocol</a>).</p>
<p dir="auto">It can share and wrap CPU and CUDA arrays, and supports interfacing through
both <code>PyCall</code> and <code>PythonCall</code>.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">From the Julia REPL activate the package manager (type <code>]</code>) and run:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="pkg&gt; add DLPack"><pre class="notranslate"><code>pkg&gt; add DLPack
</code></pre></div>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">As an example, let us wrap a JAX array instantiated via the <code>PyCall</code> package:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using DLPack
using PyCall

np = pyimport(&quot;jax.numpy&quot;)
dl = pyimport(&quot;jax.dlpack&quot;)

pyv = np.arange(10)
v = DLPack.wrap(pyv, o -&gt; @pycall dl.to_dlpack(o)::PyObject)

(pyv[1] == 1).item()  # This is false since the first element is 0

# Let's mutate an immutable jax DeviceArray
v[1] = 1

(pyv[1] == 1).item()  # true"><pre><span class="pl-k">using</span> DLPack
<span class="pl-k">using</span> PyCall

np <span class="pl-k">=</span> <span class="pl-c1">pyimport</span>(<span class="pl-s"><span class="pl-pds">"</span>jax.numpy<span class="pl-pds">"</span></span>)
dl <span class="pl-k">=</span> <span class="pl-c1">pyimport</span>(<span class="pl-s"><span class="pl-pds">"</span>jax.dlpack<span class="pl-pds">"</span></span>)

pyv <span class="pl-k">=</span> np<span class="pl-k">.</span><span class="pl-c1">arange</span>(<span class="pl-c1">10</span>)
v <span class="pl-k">=</span> DLPack<span class="pl-k">.</span><span class="pl-c1">wrap</span>(pyv, o <span class="pl-k">-&gt;</span> <span class="pl-c1">@pycall</span> dl<span class="pl-k">.</span><span class="pl-c1">to_dlpack</span>(o)<span class="pl-k">::</span><span class="pl-c1">PyObject</span>)

(pyv[<span class="pl-c1">1</span>] <span class="pl-k">==</span> <span class="pl-c1">1</span>)<span class="pl-k">.</span><span class="pl-c1">item</span>()  <span class="pl-c"><span class="pl-c">#</span> This is false since the first element is 0</span>

<span class="pl-c"><span class="pl-c">#</span> Let's mutate an immutable jax DeviceArray</span>
v[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-c1">1</span>

(pyv[<span class="pl-c1">1</span>] <span class="pl-k">==</span> <span class="pl-c1">1</span>)<span class="pl-k">.</span><span class="pl-c1">item</span>()  <span class="pl-c"><span class="pl-c">#</span> true</span></pre></div>
<p dir="auto">If the python tensor has more than one dimension and the memory layout is
row-major the array returned by <code>DLPack.wrap</code> has its dimensions reversed.
Let us illustrate this now by importing a <code>torch.Tensor</code> via the
<code>PythonCall</code> package:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using DLPack
using PythonCall

torch = pyimport(&quot;torch&quot;)

pyv = torch.arange(1, 5).reshape(2, 2)
v = DLPack.wrap(pyv, torch.to_dlpack)

Bool(v[2, 1] == 2 == pyv[0, 1])  # dimensions are reversed"><pre><span class="pl-k">using</span> DLPack
<span class="pl-k">using</span> PythonCall

torch <span class="pl-k">=</span> <span class="pl-c1">pyimport</span>(<span class="pl-s"><span class="pl-pds">"</span>torch<span class="pl-pds">"</span></span>)

pyv <span class="pl-k">=</span> torch<span class="pl-k">.</span><span class="pl-c1">arange</span>(<span class="pl-c1">1</span>, <span class="pl-c1">5</span>)<span class="pl-k">.</span><span class="pl-c1">reshape</span>(<span class="pl-c1">2</span>, <span class="pl-c1">2</span>)
v <span class="pl-k">=</span> DLPack<span class="pl-k">.</span><span class="pl-c1">wrap</span>(pyv, torch<span class="pl-k">.</span>to_dlpack)

<span class="pl-c1">Bool</span>(v[<span class="pl-c1">2</span>, <span class="pl-c1">1</span>] <span class="pl-k">==</span> <span class="pl-c1">2</span> <span class="pl-k">==</span> pyv[<span class="pl-c1">0</span>, <span class="pl-c1">1</span>])  <span class="pl-c"><span class="pl-c">#</span> dimensions are reversed</span></pre></div>
<p dir="auto">Likewise, we can share Julia arrays to python:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using DLPack
using PythonCall

torch = pyimport(&quot;torch&quot;)

v = rand(3, 2)
pyv = DLPack.share(v, torch.from_dlpack)

Bool(pyv.shape == torch.Size((2, 3)))  # again, the dimensions are reversed."><pre><span class="pl-k">using</span> DLPack
<span class="pl-k">using</span> PythonCall

torch <span class="pl-k">=</span> <span class="pl-c1">pyimport</span>(<span class="pl-s"><span class="pl-pds">"</span>torch<span class="pl-pds">"</span></span>)

v <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>, <span class="pl-c1">2</span>)
pyv <span class="pl-k">=</span> DLPack<span class="pl-k">.</span><span class="pl-c1">share</span>(v, torch<span class="pl-k">.</span>from_dlpack)

<span class="pl-c1">Bool</span>(pyv<span class="pl-k">.</span>shape <span class="pl-k">==</span> torch<span class="pl-k">.</span><span class="pl-c1">Size</span>((<span class="pl-c1">2</span>, <span class="pl-c1">3</span>)))  <span class="pl-c"><span class="pl-c">#</span> again, the dimensions are reversed.</span></pre></div>
<p dir="auto">Do you want to exchange CUDA tensors? Worry not:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using DLPack
using CUDA
using PyCall

cupy = pyimport(&quot;cupy&quot;)

pyv = cupy.arange(6).reshape(2, 3)
v = DLPack.wrap(pyv, o -&gt; pycall(o.toDlpack, PyObject))

v .= 1
pyv.sum().item() == 6  # true

pyw = DLPack.share(v, cupy.from_dlpack)  # new cupy ndarray holding the same data"><pre><span class="pl-k">using</span> DLPack
<span class="pl-k">using</span> CUDA
<span class="pl-k">using</span> PyCall

cupy <span class="pl-k">=</span> <span class="pl-c1">pyimport</span>(<span class="pl-s"><span class="pl-pds">"</span>cupy<span class="pl-pds">"</span></span>)

pyv <span class="pl-k">=</span> cupy<span class="pl-k">.</span><span class="pl-c1">arange</span>(<span class="pl-c1">6</span>)<span class="pl-k">.</span><span class="pl-c1">reshape</span>(<span class="pl-c1">2</span>, <span class="pl-c1">3</span>)
v <span class="pl-k">=</span> DLPack<span class="pl-k">.</span><span class="pl-c1">wrap</span>(pyv, o <span class="pl-k">-&gt;</span> <span class="pl-c1">pycall</span>(o<span class="pl-k">.</span>toDlpack, PyObject))

v <span class="pl-k">.=</span> <span class="pl-c1">1</span>
pyv<span class="pl-k">.</span><span class="pl-c1">sum</span>()<span class="pl-k">.</span><span class="pl-c1">item</span>() <span class="pl-k">==</span> <span class="pl-c1">6</span>  <span class="pl-c"><span class="pl-c">#</span> true</span>

pyw <span class="pl-k">=</span> DLPack<span class="pl-k">.</span><span class="pl-c1">share</span>(v, cupy<span class="pl-k">.</span>from_dlpack)  <span class="pl-c"><span class="pl-c">#</span> new cupy ndarray holding the same data</span></pre></div>
</article></div>