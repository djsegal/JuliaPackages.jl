<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><p><a href="https://travis-ci.org/baggepinnen/TotalLeastSquares.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/7fffa1b7f5d478788b60cbdc5ed5ac1da971f925/68747470733a2f2f7472617669732d63692e6f72672f626167676570696e6e656e2f546f74616c4c65617374537175617265732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/baggepinnen/TotalLeastSquares.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/report.html" rel="nofollow"><img src="https://camo.githubusercontent.com/9eaf0bcdca2b18d25a7141dfa4a958eda67e214d/68747470733a2f2f6a756c696163692e6769746875622e696f2f4e616e6f736f6c646965725265706f7274732f706b676576616c5f6261646765732f542f546f74616c4c65617374537175617265732e737667" alt="PkgEval" data-canonical-src="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/T/TotalLeastSquares.svg" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/baggepinnen/TotalLeastSquares.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6045dbc3f64ed4a0925d4f4bac619dafafee1d36/68747470733a2f2f636f6465636f762e696f2f67682f626167676570696e6e656e2f546f74616c4c65617374537175617265732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/baggepinnen/TotalLeastSquares.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h1><a id="user-content-totalleastsquaresjl" class="anchor" aria-hidden="true" href="#totalleastsquaresjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TotalLeastSquares.jl</h1>
<p>Solve (weighted or robust) total least-squares problems, impute missing data and robustly filter time series.</p>
<p>These functions are exported:</p>
<h4><a id="user-content-estimation" class="anchor" aria-hidden="true" href="#estimation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Estimation</h4>
<ul>
<li><code>x = tls(A,y)</code>
Solves the standard TLS problem using the SVD method. An inplace version <code>tls!(Ay, n)</code> also exists, for this you need to supply <code>Ay = [A y]</code> and the width of <code>A</code>, <code>n = size(A,2)</code>.</li>
<li><code>x = wtls(A,y,Qaa,Qay,Qyy,iters=10)</code>
Solves the weighted TLS problem using algorithm 1 from (Fang, 2013)
The Q-matrices are the covariance matrices of the noise terms in <code>vec(A)</code> and <code>y</code> respectively.</li>
<li><code>Qaa,Qay,Qyy = rowcovariance(rowQ::AbstractVector{&lt;:AbstractMatrix})</code>
Takes row-wise covariance matrices <code>QAy[i]</code> and returns the full (sparse) covariance matrices. <code>rowQ = [cov([A[i,:] y[i]]) for i = 1:length(y)]</code></li>
<li><code>x = wls(A,y,Qyy)</code> Solves the weighted standard LS problem. <code>Qyy</code> is the covariance matrix of the residuals with side length equal to the length of <code>y</code>.</li>
<li><code>x = rtls(A,y)</code> Solves a robust TLS problem. Both <code>A</code> and <code>y</code> are assumed to be corrupted with high magnitude, but sparse, noise. See analysis below.</li>
<li><code>x = irls(A,y; tolx=0.001, tol=1.0e-6, verbose=false, iters=100)</code> minimizeₓ ||Ax-y||₁ using iteratively reweighted least squares.</li>
</ul>
<h4><a id="user-content-matrix-recovery" class="anchor" aria-hidden="true" href="#matrix-recovery"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Matrix recovery</h4>
<ul>
<li><code>Â, Ê, s, sv = rpca(D; kwargs...)</code> robust matrix recovery using robust PCA. Solves <code>minimize_{A,E} ||A||_* + λ||E||₁ s.t. D = A+E</code>. Optionally force <code>A</code> or <code>E</code> to be non-negative.</li>
<li><code>Q = rpca_ga(X, r; kwargs...)</code> robust PCA using Grassmann averages. Returns the principal components up to rank <code>r</code>.</li>
</ul>
<h4><a id="user-content-time-series-filtering" class="anchor" aria-hidden="true" href="#time-series-filtering"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Time-series filtering</h4>
<ul>
<li><code>yf = lowrankfilter(y, n; kwargs...)</code> Filter time series <code>y</code> by forming a lag-embedding H (a Hankel matrix) and using <a href="@ref"><code>rpca</code></a> to recover a low-rank matrix from which the a filtered signal <code>yf</code> can be extracted. Robustly filters large sparse outliers.</li>
</ul>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> TotalLeastSquares, FillArrays, Random, Printf
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">0</span>)
x   <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>)
A   <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">50</span>,<span class="pl-c1">3</span>)
σa  <span class="pl-k">=</span> <span class="pl-c1">1</span>
σy  <span class="pl-k">=</span> <span class="pl-c1">0.01</span>
An  <span class="pl-k">=</span> A <span class="pl-k">+</span> σa<span class="pl-k">*</span><span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(A))
y   <span class="pl-k">=</span> A<span class="pl-k">*</span>x
yn  <span class="pl-k">=</span> y <span class="pl-k">+</span> σy<span class="pl-k">*</span><span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(y))
Qaa <span class="pl-k">=</span> σa<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">*</span><span class="pl-c1">Eye</span>(<span class="pl-c1">prod</span>(<span class="pl-c1">size</span>(A)))
Qay <span class="pl-k">=</span> <span class="pl-c1">0</span><span class="pl-c1">Eye</span>(<span class="pl-c1">prod</span>(<span class="pl-c1">size</span>(A)),<span class="pl-c1">length</span>(y))
Qyy <span class="pl-k">=</span> σy<span class="pl-k">^</span><span class="pl-c1">2</span><span class="pl-k">*</span><span class="pl-c1">Eye</span>(<span class="pl-c1">prod</span>(<span class="pl-c1">size</span>(y)))


x̂ <span class="pl-k">=</span> An<span class="pl-k">\</span>yn
<span class="pl-c1">@printf</span> <span class="pl-s"><span class="pl-pds">"</span>Least squares error: %25.3e %10.3e %10.3e, Norm: %10.3e<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> (x<span class="pl-k">-</span>x̂)<span class="pl-k">...</span> <span class="pl-c1">norm</span>(x<span class="pl-k">-</span>x̂)

x̂ <span class="pl-k">=</span> <span class="pl-c1">wls</span>(An,yn,Qyy)
<span class="pl-c1">@printf</span> <span class="pl-s"><span class="pl-pds">"</span>Weighted Least squares error: %16.3e %10.3e %10.3e, Norm: %10.3e<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> (x<span class="pl-k">-</span>x̂)<span class="pl-k">...</span> <span class="pl-c1">norm</span>(x<span class="pl-k">-</span>x̂)

x̂ <span class="pl-k">=</span> <span class="pl-c1">tls</span>(An,yn)
<span class="pl-c1">@printf</span> <span class="pl-s"><span class="pl-pds">"</span>Total Least squares error: %19.3e %10.3e %10.3e, Norm: %10.3e<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> (x<span class="pl-k">-</span>x̂)<span class="pl-k">...</span> <span class="pl-c1">norm</span>(x<span class="pl-k">-</span>x̂)

x̂ <span class="pl-k">=</span> <span class="pl-c1">wtls</span>(An,yn,Qaa,Qay,Qyy,iters<span class="pl-k">=</span><span class="pl-c1">10</span>)
<span class="pl-c1">@printf</span> <span class="pl-s"><span class="pl-pds">"</span>Weighted Total Least squares error: %10.3e %10.3e %10.3e, Norm: %10.3e<span class="pl-cce">\n</span><span class="pl-pds">"</span></span> (x<span class="pl-k">-</span>x̂)<span class="pl-k">...</span> <span class="pl-c1">norm</span>(x<span class="pl-k">-</span>x̂)
<span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>----------------------------<span class="pl-pds">"</span></span>)</pre></div>
<div class="highlight highlight-source-julia"><pre>Least squares error<span class="pl-k">:</span>                 <span class="pl-c1">3.753e-01</span>  <span class="pl-c1">2.530e-01</span> <span class="pl-k">-</span><span class="pl-c1">3.637e-01</span>, Norm<span class="pl-k">:</span>  <span class="pl-c1">5.806e-01</span>
Weighted Least squares error<span class="pl-k">:</span>        <span class="pl-c1">3.753e-01</span>  <span class="pl-c1">2.530e-01</span> <span class="pl-k">-</span><span class="pl-c1">3.637e-01</span>, Norm<span class="pl-k">:</span>  <span class="pl-c1">5.806e-01</span>
Total Least squares error<span class="pl-k">:</span>           <span class="pl-c1">2.897e-01</span>  <span class="pl-c1">1.062e-01</span> <span class="pl-k">-</span><span class="pl-c1">2.976e-01</span>, Norm<span class="pl-k">:</span>  <span class="pl-c1">4.287e-01</span>
Weighted Total Least squares error<span class="pl-k">:</span>  <span class="pl-c1">1.213e-01</span> <span class="pl-k">-</span><span class="pl-c1">1.933e-01</span> <span class="pl-k">-</span><span class="pl-c1">9.527e-02</span>, Norm<span class="pl-k">:</span>  <span class="pl-c1">2.473e-01</span>
<span class="pl-k">----------------------------</span></pre></div>
<h2><a id="user-content-robust-tls-analysis" class="anchor" aria-hidden="true" href="#robust-tls-analysis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Robust TLS analysis</h2>
<p>The code for this analysis is <a href="https://github.com/baggepinnen/TotalLeastSquares.jl/blob/master/total_vs_robust_demo.jl">here</a>.</p>
<p>We generate random data on the form <code>Ax=y</code> where both <code>A</code> and <code>y</code> are corrupted with sparse noise, the entries in <code>A</code> are Gaussian random variables with unit variance and <code>size(A) = (500,5)</code>. The plots below show the norm of the error in the estimated <code>x</code> as functions of the noise variance and the noise sparsity.</p>
<p><a target="_blank" rel="noopener noreferrer" href="figs/e_vs_n.svg"><img src="figs/e_vs_n.svg" alt="window" style="max-width:100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="figs/e_vs_s_5.svg"><img src="figs/e_vs_s_5.svg" alt="window" style="max-width:100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="figs/e_vs_s_50.svg"><img src="figs/e_vs_s_50.svg" alt="window" style="max-width:100%;"></a></p>
<p>The results indicate that the robust method is to be preferred when the noise is large but sparse.</p>
<h2><a id="user-content-missing-data-imputation" class="anchor" aria-hidden="true" href="#missing-data-imputation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Missing data imputation</h2>
<p>The robust methods handle missing data the same way as they handle outliers. You may indicate that an entry is missing simply by setting it to a very large value, e.g.,</p>
<div class="highlight highlight-source-julia"><pre>N <span class="pl-k">=</span> <span class="pl-c1">500</span>
y <span class="pl-k">=</span> <span class="pl-c1">sin</span>.(<span class="pl-c1">0.1</span> <span class="pl-k">.*</span> (<span class="pl-c1">1</span><span class="pl-k">:</span>N)) <span class="pl-c"><span class="pl-c">#</span> Sinus</span>
miss <span class="pl-k">=</span> <span class="pl-c1">rand</span>(N) <span class="pl-k">.&lt;</span> <span class="pl-c1">0.1</span>  <span class="pl-c"><span class="pl-c">#</span> 10% missing values</span>
yn <span class="pl-k">=</span> y <span class="pl-k">.+</span> miss <span class="pl-k">.*</span> <span class="pl-c1">1e2</span> <span class="pl-k">.+</span> <span class="pl-c1">0.1</span><span class="pl-k">*</span><span class="pl-c1">randn</span>(N)   <span class="pl-c"><span class="pl-c">#</span> Set missing values to very large number and add noise</span>
yf <span class="pl-k">=</span> <span class="pl-c1">lowrankfilter</span>(yn,<span class="pl-c1">40</span>)    <span class="pl-c"><span class="pl-c">#</span> Filter</span>
<span class="pl-c1">mean</span>(abs2,y<span class="pl-k">-</span>yf)<span class="pl-k">/</span><span class="pl-c1">mean</span>(abs2,y) <span class="pl-c"><span class="pl-c">#</span> Normalized error</span>
<span class="pl-c"><span class="pl-c">#</span> 0.001500 # Less than 1 percent error in the recovery of y</span></pre></div>
<p>To impute missing data in a matrix, we make use of <code>rpca</code>:</p>
<div class="highlight highlight-source-julia"><pre>H <span class="pl-k">=</span> <span class="pl-c1">hankel</span>(<span class="pl-c1">sin</span>.(<span class="pl-c1">0.1</span> <span class="pl-k">.*</span> (<span class="pl-c1">1</span><span class="pl-k">:</span>N)), <span class="pl-c1">5</span>)  <span class="pl-c"><span class="pl-c">#</span> A low-rank matrix</span>
miss <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">size</span>(H)<span class="pl-k">...</span>) <span class="pl-k">.&lt;</span> <span class="pl-c1">0.1</span>     <span class="pl-c"><span class="pl-c">#</span> 10% missing values</span>
Hn <span class="pl-k">=</span> H <span class="pl-k">.+</span> <span class="pl-c1">0.1</span><span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(H)) <span class="pl-k">.+</span> miss <span class="pl-k">.*</span> <span class="pl-c1">1e2</span>    <span class="pl-c"><span class="pl-c">#</span> Set missing values to very large number</span>
Ĥ, E <span class="pl-k">=</span> <span class="pl-c1">rpca</span>(Hn)
<span class="pl-c1">mean</span>(abs2,H<span class="pl-k">-</span>Ĥ)<span class="pl-k">/</span><span class="pl-c1">mean</span>(abs2,H) <span class="pl-c"><span class="pl-c">#</span> Normalized error</span>
<span class="pl-c"><span class="pl-c">#</span> 0.06 # Six percent error in the recovery of H</span></pre></div>
<p>The matrix <code>E</code> contains the estimated outliers</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">vec</span>(E)<span class="pl-k">'</span><span class="pl-c1">vec</span>(miss)<span class="pl-k">/</span>(<span class="pl-c1">norm</span>(E)<span class="pl-k">*</span><span class="pl-c1">norm</span>(miss)) <span class="pl-c"><span class="pl-c">#</span> These should correlate if all missing values were identified</span>
<span class="pl-c"><span class="pl-c">#</span> 1.00</span></pre></div>
<h1><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Notes</h1>
<p>This package was developed for the thesis<br>
<a href="https://www.control.lth.se/staff/fredrik-bagge-carlson/" rel="nofollow">Bagge Carlson, F.</a>, <a href="https://lup.lub.lu.se/search/publication/ffb8dc85-ce12-4f75-8f2b-0881e492f6c0" rel="nofollow">"Machine Learning and System Identification for Estimation in Physical Systems"</a> (PhD Thesis 2018).</p>
<div class="highlight highlight-text-bibtex"><pre><span class="pl-k">@thesis</span>{<span class="pl-en">bagge2018</span>,
  <span class="pl-s">title</span>        = <span class="pl-s"><span class="pl-pds">{</span>Machine Learning and System Identification for Estimation in Physical Systems<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>       = <span class="pl-s"><span class="pl-pds">{</span>Bagge Carlson, Fredrik<span class="pl-pds">}</span></span>,
  <span class="pl-s">keyword</span>      = <span class="pl-s"><span class="pl-pds">{</span>Machine Learning,System Identification,Robotics,Spectral estimation,Calibration,State estimation<span class="pl-pds">}</span></span>,
  <span class="pl-s">month</span>        = <span class="pl-s"><span class="pl-pds">{</span>12<span class="pl-pds">}</span></span>,
  <span class="pl-s">type</span>         = <span class="pl-s"><span class="pl-pds">{</span>PhD Thesis<span class="pl-pds">}</span></span>,
  <span class="pl-s">number</span>       = <span class="pl-s"><span class="pl-pds">{</span>TFRT-1122<span class="pl-pds">}</span></span>,
  <span class="pl-s">institution</span>  = <span class="pl-s"><span class="pl-pds">{</span>Dept. Automatic Control, Lund University, Sweden<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>         = <span class="pl-s"><span class="pl-pds">{</span>2018<span class="pl-pds">}</span></span>,
  <span class="pl-s">url</span>          = <span class="pl-s"><span class="pl-pds">{</span>https://lup.lub.lu.se/search/publication/ffb8dc85-ce12-4f75-8f2b-0881e492f6c0<span class="pl-pds">}</span></span>,
}</pre></div>
</article></div>