<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-markovify" class="anchor" aria-hidden="true" href="#markovify"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Markovify</h1>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b0224997019dec4e51d692c722ea9bee2818c837/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e737667"><img src="https://camo.githubusercontent.com/b0224997019dec4e51d692c722ea9bee2818c837/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e737667" alt="" data-canonical-src="https://img.shields.io/github/license/mashape/apistatus.svg" style="max-width:100%;"></a>
<a href="https://eugleo.github.io/Markovify.jl/" rel="nofollow"><img src="https://camo.githubusercontent.com/2b7a0f5b24a62798659cc82bc8e71b060874e5b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d627269676874677265656e2e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-brightgreen.svg" style="max-width:100%;"></a></p>
<p>Simple text generation in Julia.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>You can install this package by using the Julia package manager. From the Julia REPL, type <code>]</code> to enter the Pkg REPL mode and run:</p>
<pre><code>pkg&gt; add Markovify
</code></pre>
<h2><a id="user-content-usage-examples" class="anchor" aria-hidden="true" href="#usage-examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage examples</h2>
<p>All functions in this package are documented. You can view the documentation of the public symbols <a href="https://eugleo.github.io/Markovify.jl/public/" rel="nofollow">here</a>.</p>
<p>Let's say we want to build a simplistic <a href="https://cs.wikipedia.org/wiki/Lorem_ipsum" rel="nofollow">Lorem ipsum</a> generator. We can use Markovify for that; the whole process can be split into three main steps:</p>
<ol>
<li>Loading the training texts from a file (or multiple files) and splitting it into tokens.</li>
<li>Training the model on the tokens.</li>
<li>Walking through the model and generating random texts.</li>
</ol>
<p>Let's assume we have our training files in the directory <code>files</code>, named <code>src1</code>, <code>src2</code> and <code>src3</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Markovify
<span class="pl-k">using</span> Tokenizer

<span class="pl-c"><span class="pl-c">#</span> For each supplied file, make a model, and return an iterator of all such models</span>
<span class="pl-c"><span class="pl-c">#</span> This function actually performs both step 1 and step 2</span>
<span class="pl-k">function</span> <span class="pl-en">loadfiles</span>(filenames)
    <span class="pl-k">return</span> (
        <span class="pl-c1">open</span>(filename) <span class="pl-k">do</span> file
            text <span class="pl-k">=</span> <span class="pl-c1">read</span>(file, String)
            <span class="pl-c"><span class="pl-c">#</span> Tokenize on words (we could also tokenize on letters/lines etc.)</span>
            <span class="pl-c"><span class="pl-c">#</span> That means: split the text to sentences and then those sentences to words</span>
            tokens <span class="pl-k">=</span> <span class="pl-c1">tokenize</span>(text; on<span class="pl-k">=</span>words)
            <span class="pl-k">return</span> <span class="pl-c1">Model</span>(tokens; order<span class="pl-k">=</span><span class="pl-c1">1</span>)
        <span class="pl-k">end</span>
        <span class="pl-k">for</span> filename <span class="pl-k">in</span> filenames
    )
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Print n sentences generated with the model</span>
<span class="pl-c"><span class="pl-c">#</span> This function performs step 3</span>
<span class="pl-k">function</span> <span class="pl-en">gensentences</span>(model, n)
    sentences <span class="pl-k">=</span> []
    <span class="pl-c"><span class="pl-c">#</span> Stop only after n sentences were generated</span>
    <span class="pl-c"><span class="pl-c">#</span> and passed through the length test</span>
    <span class="pl-k">while</span> <span class="pl-c1">length</span>(sentences) <span class="pl-k">&lt;</span> n
        seq <span class="pl-k">=</span> <span class="pl-c1">walk</span>(model)
        <span class="pl-c"><span class="pl-c">#</span> Add the sentence to the array iff its length is ok</span>
        <span class="pl-k">if</span> <span class="pl-c1">length</span>(seq) <span class="pl-k">&gt;</span> <span class="pl-c1">5</span> <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">length</span>(seq) <span class="pl-k">&lt;</span> <span class="pl-c1">15</span>
            <span class="pl-c1">push!</span>(sentences, <span class="pl-c1">join</span>(seq, <span class="pl-s"><span class="pl-pds">"</span> <span class="pl-pds">"</span></span>))
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Now we just put them together</span>
FILENAMES <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>files/src1.txt<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>files/src2.txt<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>files/src3.txt<span class="pl-pds">"</span></span>]
MODEL <span class="pl-k">=</span> <span class="pl-c1">combine</span>(<span class="pl-c1">loadfiles</span>(FILENAMES)<span class="pl-k">...</span>)
<span class="pl-c1">gensentences</span>(MODEL, <span class="pl-c1">4</span>)</pre></div>
<p>And the output is 4 lines of random sentences, similar to this example generated from three random French texts on <a href="http://www.gutenberg.org" rel="nofollow">Project Gutenberg</a>.</p>
<pre><code>Mais elle exposa froidement le pria quelquun à dîner.
Les animaux guérissent quelquefois, la duchesse et les mères.
cest une fortune en souriant ses rivaux.
Mais la spécialité des hommes vraiment forts, évitait de Paris.
</code></pre>
<p>The most complicated step is 1: tokenizing. The constructor <code>Model</code> expects an array of arrays of tokens, so keep that in mind. There is also another method of <code>Model</code>, which can build a full model object from just the nodes dictionary (read more on nodes in the docs) — you can thus save the nodes of an existing model to a JSON file, for example, and restore it later.</p>
</article></div>