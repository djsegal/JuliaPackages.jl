<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-pycallchainrules" class="anchor" aria-hidden="true" href="#pycallchainrules"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>PyCallChainRules</h1>
<p dir="auto"><a href="https://rejuvyesh.github.io/PyCallChainRules.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://rejuvyesh.github.io/PyCallChainRules.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a></p>
<p dir="auto">While Julia is great, there are still a lot of existing useful differentiable python code in PyTorch, Jax, etc. Given PyCall.jl is already so great and seamless, one might wonder what it takes to differentiate through those <code>pycall</code>s. This library aims for that ideal.</p>
<p dir="auto">Thanks to <a href="https://github.cim/pabloferz" rel="nofollow">@pabloferz</a>, this works on both CPU and GPU without any array copies via <a href="https://github.com/pabloferz/DLPack.jl">DLPack.jl</a>.</p>
<h2 dir="auto"><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basic Usage</h2>
<h3 dir="auto"><a id="user-content-pytorch" class="anchor" aria-hidden="true" href="#pytorch"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>PyTorch</h3>
<h4 dir="auto"><a id="user-content-cpu-only" class="anchor" aria-hidden="true" href="#cpu-only"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CPU only</h4>
<h5 dir="auto"><a id="user-content-install-python-dependencies" class="anchor" aria-hidden="true" href="#install-python-dependencies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install Python dependencies</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCall
run(`$(PyCall.pyprogramname) -m pip install torch==1.11.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html functorch`)"><pre><span class="pl-k">using</span> PyCall
<span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds">`</span>$(PyCall.pyprogramname) -m pip install torch==1.11.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html functorch<span class="pl-pds">`</span></span>)</pre></div>
<h5 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCallChainRules.Torch: TorchModuleWrapper, torch
using Zygote

indim = 32
outdim = 16
torch_module = torch.nn.Linear(indim, outdim) # Can be anything subclassing torch.nn.Module
jlwrap = TorchModuleWrapper(torch_module)

batchsize = 64
input = randn(Float32, indim, batchsize)
output = jlwrap(input)

target = randn(Float32, outdim, batchsize)
loss(m, x, y) = sum(m(x) .- target)
grad, = Zygote.gradient(m-&gt;loss(m, input, target), jlwrap)"><pre><span class="pl-k">using</span> PyCallChainRules<span class="pl-k">.</span>Torch<span class="pl-k">:</span> TorchModuleWrapper, torch
<span class="pl-k">using</span> Zygote

indim <span class="pl-k">=</span> <span class="pl-c1">32</span>
outdim <span class="pl-k">=</span> <span class="pl-c1">16</span>
torch_module <span class="pl-k">=</span> torch<span class="pl-k">.</span>nn<span class="pl-k">.</span><span class="pl-c1">Linear</span>(indim, outdim) <span class="pl-c"><span class="pl-c">#</span> Can be anything subclassing torch.nn.Module</span>
jlwrap <span class="pl-k">=</span> <span class="pl-c1">TorchModuleWrapper</span>(torch_module)

batchsize <span class="pl-k">=</span> <span class="pl-c1">64</span>
input <span class="pl-k">=</span> <span class="pl-c1">randn</span>(Float32, indim, batchsize)
output <span class="pl-k">=</span> <span class="pl-c1">jlwrap</span>(input)

target <span class="pl-k">=</span> <span class="pl-c1">randn</span>(Float32, outdim, batchsize)
<span class="pl-en">loss</span>(m, x, y) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">m</span>(x) <span class="pl-k">.-</span> target)
grad, <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(m<span class="pl-k">-&gt;</span><span class="pl-c1">loss</span>(m, input, target), jlwrap)</pre></div>
<h4 dir="auto"><a id="user-content-gpu" class="anchor" aria-hidden="true" href="#gpu"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GPU</h4>
<h5 dir="auto"><a id="user-content-install-python-dependencies-1" class="anchor" aria-hidden="true" href="#install-python-dependencies-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install Python dependencies</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCall
# For CUDA 11 and PyTorch 1.11
run(`$(PyCall.pyprogramname) -m pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html functorch`)"><pre><span class="pl-k">using</span> PyCall
<span class="pl-c"><span class="pl-c">#</span> For CUDA 11 and PyTorch 1.11</span>
<span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds">`</span>$(PyCall.pyprogramname) -m pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html functorch<span class="pl-pds">`</span></span>)</pre></div>
<h5 dir="auto"><a id="user-content-example-1" class="anchor" aria-hidden="true" href="#example-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using CUDA
using PyCallChainRules.Torch: TorchModuleWrapper, torch
using Zygote

@assert CUDA.functional()

indim = 32
outdim = 16
torch_module = torch.nn.Linear(indim, outdim).to(device=torch.device(&quot;cuda:0&quot;)) # Can be anything subclassing torch.nn.Module
jlwrap = TorchModuleWrapper(torch_module)

batchsize = 64
input = CUDA.cu(randn(Float32, indim, batchsize))
output = jlwrap(input)

target = CUDA.cu(randn(Float32, outdim, batchsize))
loss(m, x, y) = sum(m(x) .- y)
grad, = Zygote.gradient(m-&gt;loss(m, input, target), jlwrap)"><pre><span class="pl-k">using</span> CUDA
<span class="pl-k">using</span> PyCallChainRules<span class="pl-k">.</span>Torch<span class="pl-k">:</span> TorchModuleWrapper, torch
<span class="pl-k">using</span> Zygote

<span class="pl-c1">@assert</span> CUDA<span class="pl-k">.</span><span class="pl-c1">functional</span>()

indim <span class="pl-k">=</span> <span class="pl-c1">32</span>
outdim <span class="pl-k">=</span> <span class="pl-c1">16</span>
torch_module <span class="pl-k">=</span> torch<span class="pl-k">.</span>nn<span class="pl-k">.</span><span class="pl-c1">Linear</span>(indim, outdim)<span class="pl-k">.</span><span class="pl-c1">to</span>(device<span class="pl-k">=</span>torch<span class="pl-k">.</span><span class="pl-c1">device</span>(<span class="pl-s"><span class="pl-pds">"</span>cuda:0<span class="pl-pds">"</span></span>)) <span class="pl-c"><span class="pl-c">#</span> Can be anything subclassing torch.nn.Module</span>
jlwrap <span class="pl-k">=</span> <span class="pl-c1">TorchModuleWrapper</span>(torch_module)

batchsize <span class="pl-k">=</span> <span class="pl-c1">64</span>
input <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">cu</span>(<span class="pl-c1">randn</span>(Float32, indim, batchsize))
output <span class="pl-k">=</span> <span class="pl-c1">jlwrap</span>(input)

target <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">cu</span>(<span class="pl-c1">randn</span>(Float32, outdim, batchsize))
<span class="pl-en">loss</span>(m, x, y) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">m</span>(x) <span class="pl-k">.-</span> y)
grad, <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(m<span class="pl-k">-&gt;</span><span class="pl-c1">loss</span>(m, input, target), jlwrap)</pre></div>
<h3 dir="auto"><a id="user-content-jax" class="anchor" aria-hidden="true" href="#jax"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Jax</h3>
<h4 dir="auto"><a id="user-content-cpu-only-1" class="anchor" aria-hidden="true" href="#cpu-only-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CPU only</h4>
<h5 dir="auto"><a id="user-content-install-python-dependencies-2" class="anchor" aria-hidden="true" href="#install-python-dependencies-2"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install Python dependencies</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCall
run(`$(PyCall.pyprogramname) -m pip install jax\[&quot;cpu&quot;\]`) # for cpu version"><pre><span class="pl-k">using</span> PyCall
<span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds">`</span>$(PyCall.pyprogramname) -m pip install jax<span class="pl-cce">\[</span>"cpu"<span class="pl-cce">\]</span><span class="pl-pds">`</span></span>) <span class="pl-c"><span class="pl-c">#</span> for cpu version</span></pre></div>
<h5 dir="auto"><a id="user-content-example-2" class="anchor" aria-hidden="true" href="#example-2"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCallChainRules.Jax: JaxFunctionWrapper, jax, stax, pyto_dlpack

batchsize = 64
indim = 32
outdim = 16

init_lin, apply_lin = stax.Dense(outdim)
_, params = init_lin(jax.random.PRNGKey(0), (-1, indim))
params_jl = map(x-&gt;DLPack.wrap(x, pyto_dlpack), params)
jlwrap = JaxFunctionWrapper(jax.jit(apply_lin))
input = randn(Float32, indim, batchsize)
output = jlwrap(params_jl, input)

target = randn(Float32, outdim, batchsize)
loss(p, x, y) = sum(jlwrap(p, x) .- y)
grad, = Zygote.gradient(p-&gt;loss(p, input, target), params_jl)"><pre><span class="pl-k">using</span> PyCallChainRules<span class="pl-k">.</span>Jax<span class="pl-k">:</span> JaxFunctionWrapper, jax, stax, pyto_dlpack

batchsize <span class="pl-k">=</span> <span class="pl-c1">64</span>
indim <span class="pl-k">=</span> <span class="pl-c1">32</span>
outdim <span class="pl-k">=</span> <span class="pl-c1">16</span>

init_lin, apply_lin <span class="pl-k">=</span> stax<span class="pl-k">.</span><span class="pl-c1">Dense</span>(outdim)
_, params <span class="pl-k">=</span> <span class="pl-c1">init_lin</span>(jax<span class="pl-k">.</span>random<span class="pl-k">.</span><span class="pl-c1">PRNGKey</span>(<span class="pl-c1">0</span>), (<span class="pl-k">-</span><span class="pl-c1">1</span>, indim))
params_jl <span class="pl-k">=</span> <span class="pl-c1">map</span>(x<span class="pl-k">-&gt;</span>DLPack<span class="pl-k">.</span><span class="pl-c1">wrap</span>(x, pyto_dlpack), params)
jlwrap <span class="pl-k">=</span> <span class="pl-c1">JaxFunctionWrapper</span>(jax<span class="pl-k">.</span><span class="pl-c1">jit</span>(apply_lin))
input <span class="pl-k">=</span> <span class="pl-c1">randn</span>(Float32, indim, batchsize)
output <span class="pl-k">=</span> <span class="pl-c1">jlwrap</span>(params_jl, input)

target <span class="pl-k">=</span> <span class="pl-c1">randn</span>(Float32, outdim, batchsize)
<span class="pl-en">loss</span>(p, x, y) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">jlwrap</span>(p, x) <span class="pl-k">.-</span> y)
grad, <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(p<span class="pl-k">-&gt;</span><span class="pl-c1">loss</span>(p, input, target), params_jl)</pre></div>
<h4 dir="auto"><a id="user-content-gpu-1" class="anchor" aria-hidden="true" href="#gpu-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GPU</h4>
<h5 dir="auto"><a id="user-content-install-python-dependencies-3" class="anchor" aria-hidden="true" href="#install-python-dependencies-3"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install Python dependencies</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCall
run(`$(PyCall.pyprogramname) -m pip install jax\[&quot;cuda&quot;\] -f https://storage.googleapis.com/jax-releases/jax_releases.html`)"><pre><span class="pl-k">using</span> PyCall
<span class="pl-c1">run</span>(<span class="pl-s"><span class="pl-pds">`</span>$(PyCall.pyprogramname) -m pip install jax<span class="pl-cce">\[</span>"cuda"<span class="pl-cce">\]</span> -f https://storage.googleapis.com/jax-releases/jax_releases.html<span class="pl-pds">`</span></span>)</pre></div>
<h5 dir="auto"><a id="user-content-example-3" class="anchor" aria-hidden="true" href="#example-3"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h5>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using PyCallChainRules.Jax: JaxFunctionWrapper, jax, stax
using CUDA

using PyCallChainRules.Jax: JaxFunctionWrapper, jax, stax, pyto_dlpack

batchsize = 64
indim = 32
outdim = 16

init_lin, apply_lin = stax.Dense(outdim)
_, params = init_lin(jax.random.PRNGKey(0), (-1, indim))
params_jl = map(x-&gt;DLPack.wrap(x, pyto_dlpack), params)
jlwrap = JaxFunctionWrapper(jax.jit(apply_lin))
input = CUDA.cu(randn(Float32, indim, batchsize))
output = jlwrap(params_jl, input)

target = CUDA.cu(randn(Float32, outdim, batchsize))
loss(p, x, y) = sum(jlwrap(p, x) .- y)
grad, = Zygote.gradient(p-&gt;loss(p, input, target), params_jl)"><pre><span class="pl-k">using</span> PyCallChainRules<span class="pl-k">.</span>Jax<span class="pl-k">:</span> JaxFunctionWrapper, jax, stax
<span class="pl-k">using</span> CUDA

<span class="pl-k">using</span> PyCallChainRules<span class="pl-k">.</span>Jax<span class="pl-k">:</span> JaxFunctionWrapper, jax, stax, pyto_dlpack

batchsize <span class="pl-k">=</span> <span class="pl-c1">64</span>
indim <span class="pl-k">=</span> <span class="pl-c1">32</span>
outdim <span class="pl-k">=</span> <span class="pl-c1">16</span>

init_lin, apply_lin <span class="pl-k">=</span> stax<span class="pl-k">.</span><span class="pl-c1">Dense</span>(outdim)
_, params <span class="pl-k">=</span> <span class="pl-c1">init_lin</span>(jax<span class="pl-k">.</span>random<span class="pl-k">.</span><span class="pl-c1">PRNGKey</span>(<span class="pl-c1">0</span>), (<span class="pl-k">-</span><span class="pl-c1">1</span>, indim))
params_jl <span class="pl-k">=</span> <span class="pl-c1">map</span>(x<span class="pl-k">-&gt;</span>DLPack<span class="pl-k">.</span><span class="pl-c1">wrap</span>(x, pyto_dlpack), params)
jlwrap <span class="pl-k">=</span> <span class="pl-c1">JaxFunctionWrapper</span>(jax<span class="pl-k">.</span><span class="pl-c1">jit</span>(apply_lin))
input <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">cu</span>(<span class="pl-c1">randn</span>(Float32, indim, batchsize))
output <span class="pl-k">=</span> <span class="pl-c1">jlwrap</span>(params_jl, input)

target <span class="pl-k">=</span> CUDA<span class="pl-k">.</span><span class="pl-c1">cu</span>(<span class="pl-c1">randn</span>(Float32, outdim, batchsize))
<span class="pl-en">loss</span>(p, x, y) <span class="pl-k">=</span> <span class="pl-c1">sum</span>(<span class="pl-c1">jlwrap</span>(p, x) <span class="pl-k">.-</span> y)
grad, <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(p<span class="pl-k">-&gt;</span><span class="pl-c1">loss</span>(p, input, target), params_jl)</pre></div>
<p dir="auto">When mixing <code>jax</code> and <code>julia</code> it's recommended to disable <code>jax</code>'s preallocation with setting the environment variable <code>XLA_PYTHON_CLIENT_PREALLOCATE=false</code>.</p>
<h2 dir="auto"><a id="user-content-current-limitations" class="anchor" aria-hidden="true" href="#current-limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Current Limitations</h2>
<ul dir="auto">
<li>Input and output types of wrapped python functions can only be python tensors or [nested] tuples of python tensors.</li>
<li>Keyword arguments should not be arrays and do not support differentiation.</li>
</ul>
</article></div>