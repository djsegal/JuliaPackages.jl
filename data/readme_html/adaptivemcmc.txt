<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://mvihola.github.io/docs/AdaptiveMCMC.jl/" rel="nofollow"><img src="https://camo.githubusercontent.com/59e36b5a5201fb0f937bb6b8c0844df8841e78199432584751f8c30f0081fea6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63756d656e746174696f6e2d626c75652e737667" alt="documentation" data-canonical-src="https://img.shields.io/badge/documentation-blue.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-adaptivemcmcjl" class="anchor" aria-hidden="true" href="#adaptivemcmcjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>AdaptiveMCMC.jl</h1>
<p dir="auto">This package provides implementations of some general-purpose random-walk based adaptive MCMC algorithms, including the following:</p>
<ul dir="auto">
<li>Adaptive Metropolis, proposal covariance adaptation, (<a href="https://projecteuclid.org/euclid.bj/1080222083" rel="nofollow">Haario, Saksman and Tamminen, 2001</a>, and <a href="http://dx.doi.org/10.1214/105051606000000286" rel="nofollow">Andrieu and Moulines, 2006</a>)</li>
<li>Adaptive scaling Metropolis, acceptance rate adaptation for scale (e.g. as in <a href="https://doi.org/10.1007/s11222-008-9110-y" rel="nofollow">Andrieu and Thoms, 2008</a>, and <a href="https://projecteuclid.org/euclid.bj/1265984706" rel="nofollow">Atchadé and Fort, 2010</a>)</li>
<li>Robust Adaptive Metropolis, acceptance rate adaptation for shape <a href="http://dx.doi.org/10.1007/s11222-011-9269-5" rel="nofollow">(Vihola, 2012)</a></li>
<li>Adaptive Parallel Tempering, acceptance rate adaptation for temperature levels <a href="http://dx.doi.org/10.1080/10618600.2013.778779" rel="nofollow">(Miasojedow, Moulines and Vihola, 2013)</a></li>
</ul>
<p dir="auto">The aim of the package is to provide a simple and modular general-purpose implementation, which may be easily used to sample from a log-target density, but also used in a variety of custom settings.</p>
<p dir="auto">See also <a href="https://github.com/mvihola/AdaptiveParticleMCMC.jl">AdaptiveParticleMCMC.jl</a> which uses this package with <a href="https://github.com/awllee/SequentialMonteCarlo.jl">SequentialMonteCarlo.jl</a> for adaptive particle MCMC.</p>
<h2 dir="auto"><a id="user-content-getting-the-package" class="anchor" aria-hidden="true" href="#getting-the-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting the package</h2>
<p dir="auto">To get the latest registered version:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;AdaptiveMCMC&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>AdaptiveMCMC<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">To install the latest development version:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(url=&quot;https://github.com/mvihola/AdaptiveMCMC.jl&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(url<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>https://github.com/mvihola/AdaptiveMCMC.jl<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick start</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Load the package
using AdaptiveMCMC

# Define a function which returns log-density values:
log_p(x) = -.5*sum(x.^2)

# Run 10k iterations of the Adaptive Metropolis:
out = adaptive_rwm(zeros(2), log_p, 10_000; algorithm=:am)

# Calculate '95% credible intervals':
using Statistics
mapslices(x-&gt;&quot;$(mean(x)) ± $(1.96std(x))&quot;, out.X, dims=2)"><pre><span class="pl-c"><span class="pl-c">#</span> Load the package</span>
<span class="pl-k">using</span> AdaptiveMCMC

<span class="pl-c"><span class="pl-c">#</span> Define a function which returns log-density values:</span>
<span class="pl-en">log_p</span>(x) <span class="pl-k">=</span> <span class="pl-k">-</span>.<span class="pl-c1">5</span><span class="pl-k">*</span><span class="pl-c1">sum</span>(x<span class="pl-k">.^</span><span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> Run 10k iterations of the Adaptive Metropolis:</span>
out <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), log_p, <span class="pl-c1">10_000</span>; algorithm<span class="pl-k">=</span><span class="pl-c1">:am</span>)

<span class="pl-c"><span class="pl-c">#</span> Calculate '95% credible intervals':</span>
<span class="pl-k">using</span> Statistics
<span class="pl-c1">mapslices</span>(x<span class="pl-k">-&gt;</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-v">$(<span class="pl-c1">mean</span>(x))</span> ± <span class="pl-v">$(<span class="pl-c1">1.96</span><span class="pl-c1">std</span>(x))</span><span class="pl-pds">"</span></span>, out<span class="pl-k">.</span>X, dims<span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<p dir="auto">The function <code>adaptive_rwm</code> ('Adaptive Random walk Metropolis') is a simple implenentation
which does sampling for a given log-target density with the chosen method.</p>
<h2 dir="auto"><a id="user-content-adaptive-parallel-tempering" class="anchor" aria-hidden="true" href="#adaptive-parallel-tempering"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Adaptive parallel tempering</h2>
<p dir="auto">The <code>adaptive_rwm</code> also implements tempering, which is used if an optional argument <code>L≥2</code> (number of temperature levels) is supplied. Here is a simple multimodal distribution sampled with APT:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Multimodal target of dimension d.
function multimodalTarget(d::Int, sigma2=0.1^2, sigman=sigma2)
    # The means of mixtures
    m = [2.18 5.76; 3.25 3.47; 5.41 2.65; 4.93 1.50; 8.67 9.59;
         1.70 0.50; 2.70 7.88; 1.83 0.09; 4.24 8.48; 4.59 5.60;
         4.98 3.70; 2.26 0.31; 8.41 1.68; 6.91 5.81; 1.14 2.39;
         5.54 6.86; 3.93 8.82; 6.87 5.40; 8.33 9.50; 1.69 8.11]'
    n_m = size(m,2)
    @assert d&gt;=2 &quot;Dimension should be &gt;= 2&quot;
    let m=m, n_m=size(m,2), d=d
        function log_p(x::Vector{Float64})
            l_dens = -0.5*(mapslices(sum, (m.-x[1:2]).^2, dims=1)/sigma2)
            if d&gt;2
                l_dens .-= 0.5*mapslices(sum, x[3:d].^2, dims=1)/sigman
            end
            l_max = maximum(l_dens) # Prevent underflow by log-sum trick
            l_max + log(sum(exp.(l_dens.-l_max)))
        end
    end
end

using AdaptiveMCMC
n = 100_000; L = 2
rwm = adaptive_rwm(zeros(2), multimodalTarget(2), n; thin=10)
apt = adaptive_rwm(zeros(2), multimodalTarget(2), div(n,L); L = L, thin=10)

# Assuming you have 'Plots' installed:
using Plots
plot(scatter(rwm.X[1,:], rwm.X[2,:], title=&quot;w/o tempering&quot;, legend=:none),
scatter(apt.X[1,:], apt.X[2,:], title=&quot;w/ tempering&quot;, legend=:none), layout=(1,2))"><pre><span class="pl-c"><span class="pl-c">#</span> Multimodal target of dimension d.</span>
<span class="pl-k">function</span> <span class="pl-en">multimodalTarget</span>(d<span class="pl-k">::</span><span class="pl-c1">Int</span>, sigma2<span class="pl-k">=</span><span class="pl-c1">0.1</span><span class="pl-k">^</span><span class="pl-c1">2</span>, sigman<span class="pl-k">=</span>sigma2)
    <span class="pl-c"><span class="pl-c">#</span> The means of mixtures</span>
    m <span class="pl-k">=</span> [<span class="pl-c1">2.18</span> <span class="pl-c1">5.76</span>; <span class="pl-c1">3.25</span> <span class="pl-c1">3.47</span>; <span class="pl-c1">5.41</span> <span class="pl-c1">2.65</span>; <span class="pl-c1">4.93</span> <span class="pl-c1">1.50</span>; <span class="pl-c1">8.67</span> <span class="pl-c1">9.59</span>;
         <span class="pl-c1">1.70</span> <span class="pl-c1">0.50</span>; <span class="pl-c1">2.70</span> <span class="pl-c1">7.88</span>; <span class="pl-c1">1.83</span> <span class="pl-c1">0.09</span>; <span class="pl-c1">4.24</span> <span class="pl-c1">8.48</span>; <span class="pl-c1">4.59</span> <span class="pl-c1">5.60</span>;
         <span class="pl-c1">4.98</span> <span class="pl-c1">3.70</span>; <span class="pl-c1">2.26</span> <span class="pl-c1">0.31</span>; <span class="pl-c1">8.41</span> <span class="pl-c1">1.68</span>; <span class="pl-c1">6.91</span> <span class="pl-c1">5.81</span>; <span class="pl-c1">1.14</span> <span class="pl-c1">2.39</span>;
         <span class="pl-c1">5.54</span> <span class="pl-c1">6.86</span>; <span class="pl-c1">3.93</span> <span class="pl-c1">8.82</span>; <span class="pl-c1">6.87</span> <span class="pl-c1">5.40</span>; <span class="pl-c1">8.33</span> <span class="pl-c1">9.50</span>; <span class="pl-c1">1.69</span> <span class="pl-c1">8.11</span>]<span class="pl-k">'</span>
    n_m <span class="pl-k">=</span> <span class="pl-c1">size</span>(m,<span class="pl-c1">2</span>)
    <span class="pl-c1">@assert</span> d<span class="pl-k">&gt;=</span><span class="pl-c1">2</span> <span class="pl-s"><span class="pl-pds">"</span>Dimension should be &gt;= 2<span class="pl-pds">"</span></span>
    <span class="pl-k">let</span> m<span class="pl-k">=</span>m, n_m<span class="pl-k">=</span><span class="pl-c1">size</span>(m,<span class="pl-c1">2</span>), d<span class="pl-k">=</span>d
        <span class="pl-k">function</span> <span class="pl-en">log_p</span>(x<span class="pl-k">::</span><span class="pl-c1">Vector{Float64}</span>)
            l_dens <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">0.5</span><span class="pl-k">*</span>(<span class="pl-c1">mapslices</span>(sum, (m<span class="pl-k">.-</span>x[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">2</span>])<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>, dims<span class="pl-k">=</span><span class="pl-c1">1</span>)<span class="pl-k">/</span>sigma2)
            <span class="pl-k">if</span> d<span class="pl-k">&gt;</span><span class="pl-c1">2</span>
                l_dens <span class="pl-k">.-</span><span class="pl-k">=</span> <span class="pl-c1">0.5</span><span class="pl-k">*</span><span class="pl-c1">mapslices</span>(sum, x[<span class="pl-c1">3</span><span class="pl-k">:</span>d]<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>, dims<span class="pl-k">=</span><span class="pl-c1">1</span>)<span class="pl-k">/</span>sigman
            <span class="pl-k">end</span>
            l_max <span class="pl-k">=</span> <span class="pl-c1">maximum</span>(l_dens) <span class="pl-c"><span class="pl-c">#</span> Prevent underflow by log-sum trick</span>
            l_max <span class="pl-k">+</span> <span class="pl-c1">log</span>(<span class="pl-c1">sum</span>(<span class="pl-c1">exp</span>.(l_dens<span class="pl-k">.-</span>l_max)))
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">using</span> AdaptiveMCMC
n <span class="pl-k">=</span> <span class="pl-c1">100_000</span>; L <span class="pl-k">=</span> <span class="pl-c1">2</span>
rwm <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">multimodalTarget</span>(<span class="pl-c1">2</span>), n; thin<span class="pl-k">=</span><span class="pl-c1">10</span>)
apt <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">multimodalTarget</span>(<span class="pl-c1">2</span>), <span class="pl-c1">div</span>(n,L); L <span class="pl-k">=</span> L, thin<span class="pl-k">=</span><span class="pl-c1">10</span>)

<span class="pl-c"><span class="pl-c">#</span> Assuming you have 'Plots' installed:</span>
<span class="pl-k">using</span> Plots
<span class="pl-c1">plot</span>(<span class="pl-c1">scatter</span>(rwm<span class="pl-k">.</span>X[<span class="pl-c1">1</span>,:], rwm<span class="pl-k">.</span>X[<span class="pl-c1">2</span>,:], title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>w/o tempering<span class="pl-pds">"</span></span>, legend<span class="pl-k">=</span><span class="pl-c1">:none</span>),
<span class="pl-c1">scatter</span>(apt<span class="pl-k">.</span>X[<span class="pl-c1">1</span>,:], apt<span class="pl-k">.</span>X[<span class="pl-c1">2</span>,:], title<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>w/ tempering<span class="pl-pds">"</span></span>, legend<span class="pl-k">=</span><span class="pl-c1">:none</span>), layout<span class="pl-k">=</span>(<span class="pl-c1">1</span>,<span class="pl-c1">2</span>))</pre></div>
<h2 dir="auto"><a id="user-content-using-with-distributions-and-labelledarrays" class="anchor" aria-hidden="true" href="#using-with-distributions-and-labelledarrays"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Using with Distributions and LabelledArrays</h2>
<p dir="auto">MCMC is often useful with hierarchical models. These may be conveniently built using <code>Distributions</code> and <code>LabelledArrays</code> packages. The following example assumes these packages to be installed.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Distributions, LabelledArrays, AdaptiveMCMC
# Define convenience log-transform for continuous univariate distributions
struct LogTransformedDistribution{Dist &lt;: ContinuousUnivariateDistribution}
        d::Dist
end
import Distributions.logpdf
logpdf(d::LogTransformedDistribution, x) = logpdf(d.d, exp(x)) + x
import Base.log
log(d::ContinuousUnivariateDistribution) = LogTransformedDistribution(d)

# This example is modified from Turing Getting Started:
# https://turing.ml/dev/docs/using-turing/get-started
function buildModel(x=0.0, y=1.0)
    let x=x, y=y
        function(v)
            p = 0.0
            p += logpdf(log(InverseGamma(2,3)), v.log_s)
            ss = exp(.5*v.log_s)
            p += logpdf(Normal(0, ss), v.m)
            p += logpdf(Normal(v.m, ss), x)
            p += logpdf(Normal(v.m, ss), y)
            p
        end
    end
end

# Initial state vector (labelled with keys `s` and `m`)
x0 = LVector(log_s=1.0, m=0.0); log_p = buildModel(3.3, 4.14)
# Hint: If you do not have a good guess of the mode of log_p (which is
# a good initial value for MCMC), you may use optimisation:
#using Optim; o = optimize(x -&gt; -log_p(x), x0); x0 = o.minimizer
out = adaptive_rwm(x0, log_p, 1_000_000; thin=20)
using StatsPlots # Assuming installed
corrplot(out.X', labels=[keys(x0)...])"><pre><span class="pl-k">using</span> Distributions, LabelledArrays, AdaptiveMCMC
<span class="pl-c"><span class="pl-c">#</span> Define convenience log-transform for continuous univariate distributions</span>
<span class="pl-k">struct</span> LogTransformedDistribution{Dist <span class="pl-k">&lt;:</span> <span class="pl-c1">ContinuousUnivariateDistribution</span>}
        d<span class="pl-k">::</span><span class="pl-c1">Dist</span>
<span class="pl-k">end</span>
<span class="pl-k">import</span> Distributions<span class="pl-k">.</span>logpdf
<span class="pl-en">logpdf</span>(d<span class="pl-k">::</span><span class="pl-c1">LogTransformedDistribution</span>, x) <span class="pl-k">=</span> <span class="pl-c1">logpdf</span>(d<span class="pl-k">.</span>d, <span class="pl-c1">exp</span>(x)) <span class="pl-k">+</span> x
<span class="pl-k">import</span> Base<span class="pl-k">.</span>log
<span class="pl-en">log</span>(d<span class="pl-k">::</span><span class="pl-c1">ContinuousUnivariateDistribution</span>) <span class="pl-k">=</span> <span class="pl-c1">LogTransformedDistribution</span>(d)

<span class="pl-c"><span class="pl-c">#</span> This example is modified from Turing Getting Started:</span>
<span class="pl-c"><span class="pl-c">#</span> https://turing.ml/dev/docs/using-turing/get-started</span>
<span class="pl-k">function</span> <span class="pl-en">buildModel</span>(x<span class="pl-k">=</span><span class="pl-c1">0.0</span>, y<span class="pl-k">=</span><span class="pl-c1">1.0</span>)
    <span class="pl-k">let</span> x<span class="pl-k">=</span>x, y<span class="pl-k">=</span>y
        <span class="pl-k">function</span>(v)
            p <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
            p <span class="pl-k">+=</span> <span class="pl-c1">logpdf</span>(<span class="pl-c1">log</span>(<span class="pl-c1">InverseGamma</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)), v<span class="pl-k">.</span>log_s)
            ss <span class="pl-k">=</span> <span class="pl-c1">exp</span>(.<span class="pl-c1">5</span><span class="pl-k">*</span>v<span class="pl-k">.</span>log_s)
            p <span class="pl-k">+=</span> <span class="pl-c1">logpdf</span>(<span class="pl-c1">Normal</span>(<span class="pl-c1">0</span>, ss), v<span class="pl-k">.</span>m)
            p <span class="pl-k">+=</span> <span class="pl-c1">logpdf</span>(<span class="pl-c1">Normal</span>(v<span class="pl-k">.</span>m, ss), x)
            p <span class="pl-k">+=</span> <span class="pl-c1">logpdf</span>(<span class="pl-c1">Normal</span>(v<span class="pl-k">.</span>m, ss), y)
            p
        <span class="pl-k">end</span>
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Initial state vector (labelled with keys `s` and `m`)</span>
x0 <span class="pl-k">=</span> <span class="pl-c1">LVector</span>(log_s<span class="pl-k">=</span><span class="pl-c1">1.0</span>, m<span class="pl-k">=</span><span class="pl-c1">0.0</span>); log_p <span class="pl-k">=</span> <span class="pl-c1">buildModel</span>(<span class="pl-c1">3.3</span>, <span class="pl-c1">4.14</span>)
<span class="pl-c"><span class="pl-c">#</span> Hint: If you do not have a good guess of the mode of log_p (which is</span>
<span class="pl-c"><span class="pl-c">#</span> a good initial value for MCMC), you may use optimisation:</span>
<span class="pl-c"><span class="pl-c">#</span>using Optim; o = optimize(x -&gt; -log_p(x), x0); x0 = o.minimizer</span>
out <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(x0, log_p, <span class="pl-c1">1_000_000</span>; thin<span class="pl-k">=</span><span class="pl-c1">20</span>)
<span class="pl-k">using</span> StatsPlots <span class="pl-c"><span class="pl-c">#</span> Assuming installed</span>
<span class="pl-c1">corrplot</span>(out<span class="pl-k">.</span>X<span class="pl-k">'</span>, labels<span class="pl-k">=</span>[<span class="pl-c1">keys</span>(x0)<span class="pl-k">...</span>])</pre></div>
<h2 dir="auto"><a id="user-content-resuming-simulation" class="anchor" aria-hidden="true" href="#resuming-simulation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Resuming simulation</h2>
<p dir="auto">(This is available currently only in the development version!)</p>
<p dir="auto">Simulation can be resumed, or continued after one simulation. Here is an example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using AdaptiveMCMC, Random
log_p(x) = -.5*sum(x.^2)
Random.seed!(12345)
# Simulate 200 iterations first:
out = adaptive_rwm(zeros(2), log_p, 200)
# Simulate 100 iterations more:
out2 = adaptive_rwm(out.X[:,end], log_p, 100; Sp=out.S, Rp=out.R, indp=200)
# This results in exactly the same output as simulating 300 samples in one go:
Random.seed!(12345)
out2_ = adaptive_rwm(zeros(2), log_p, 300)"><pre><span class="pl-k">using</span> AdaptiveMCMC, Random
<span class="pl-en">log_p</span>(x) <span class="pl-k">=</span> <span class="pl-k">-</span>.<span class="pl-c1">5</span><span class="pl-k">*</span><span class="pl-c1">sum</span>(x<span class="pl-k">.^</span><span class="pl-c1">2</span>)
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">12345</span>)
<span class="pl-c"><span class="pl-c">#</span> Simulate 200 iterations first:</span>
out <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), log_p, <span class="pl-c1">200</span>)
<span class="pl-c"><span class="pl-c">#</span> Simulate 100 iterations more:</span>
out2 <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(out<span class="pl-k">.</span>X[:,<span class="pl-c1">end</span>], log_p, <span class="pl-c1">100</span>; Sp<span class="pl-k">=</span>out<span class="pl-k">.</span>S, Rp<span class="pl-k">=</span>out<span class="pl-k">.</span>R, indp<span class="pl-k">=</span><span class="pl-c1">200</span>)
<span class="pl-c"><span class="pl-c">#</span> This results in exactly the same output as simulating 300 samples in one go:</span>
Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">12345</span>)
out2_ <span class="pl-k">=</span> <span class="pl-c1">adaptive_rwm</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), log_p, <span class="pl-c1">300</span>)</pre></div>
<h2 dir="auto"><a id="user-content-custom-sampler" class="anchor" aria-hidden="true" href="#custom-sampler"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Custom sampler</h2>
<p dir="auto">The package provides also simple building blocks which you can use within a 'custom' MCMC sampler. Here is an example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using AdaptiveMCMC

# Sampler in R^d
function mySampler(log_p, n, x0)

    # Initialise random walk sampler state: r.x current state, r.y proposal
    r = RWMState(x0)

    # Initialise Adaptive Metropolis state (with default parameters)
    s = AdaptiveMetropolis(x0)
    # Other adaptations are: AdaptiveScalingMetropolis,
    # AdaptiveScalingWithinAdaptiveMetropolis, and RobustAdaptiveMetropolis

    X = zeros(eltype(x0), length(x0), n) # Allocate output storage
    p_x = log_p(r.x)                     # = log_p(x0); the initial log target
    for k = 1:n

        # Draw new proposal r.x -&gt; r.y:
        draw!(r, s)

        p_y = log_p(r.y)                      # Calculate log target at proposal
        alpha = min(one(p_x), exp(p_y - p_x)) # The Metropolis acceptance probability

        if rand() &lt;= alpha
            p_x = p_y

            # This 'accepts', or interchanges r.x &lt;-&gt; r.y:
            # (NB: do not do r.x = r.y; these are (pointers to) vectors!)
            accept!(r)
        end

        # Do the adaptation update:
        adapt!(s, r, alpha, k)

        X[:,k] = r.x   # Save the current sample
     end
    X
end

# Standard normal target for testing
normal_log_p(x) = -mapreduce(e-&gt;e*e, +, x)/2

# Run 1M iterations of the sampler targetting 30d standard Normal:
X = mySampler(normal_log_p, 1_000_000, zeros(30))"><pre><span class="pl-k">using</span> AdaptiveMCMC

<span class="pl-c"><span class="pl-c">#</span> Sampler in R^d</span>
<span class="pl-k">function</span> <span class="pl-en">mySampler</span>(log_p, n, x0)

    <span class="pl-c"><span class="pl-c">#</span> Initialise random walk sampler state: r.x current state, r.y proposal</span>
    r <span class="pl-k">=</span> <span class="pl-c1">RWMState</span>(x0)

    <span class="pl-c"><span class="pl-c">#</span> Initialise Adaptive Metropolis state (with default parameters)</span>
    s <span class="pl-k">=</span> <span class="pl-c1">AdaptiveMetropolis</span>(x0)
    <span class="pl-c"><span class="pl-c">#</span> Other adaptations are: AdaptiveScalingMetropolis,</span>
    <span class="pl-c"><span class="pl-c">#</span> AdaptiveScalingWithinAdaptiveMetropolis, and RobustAdaptiveMetropolis</span>

    X <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(<span class="pl-c1">eltype</span>(x0), <span class="pl-c1">length</span>(x0), n) <span class="pl-c"><span class="pl-c">#</span> Allocate output storage</span>
    p_x <span class="pl-k">=</span> <span class="pl-c1">log_p</span>(r<span class="pl-k">.</span>x)                     <span class="pl-c"><span class="pl-c">#</span> = log_p(x0); the initial log target</span>
    <span class="pl-k">for</span> k <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span>n

        <span class="pl-c"><span class="pl-c">#</span> Draw new proposal r.x -&gt; r.y:</span>
        <span class="pl-c1">draw!</span>(r, s)

        p_y <span class="pl-k">=</span> <span class="pl-c1">log_p</span>(r<span class="pl-k">.</span>y)                      <span class="pl-c"><span class="pl-c">#</span> Calculate log target at proposal</span>
        alpha <span class="pl-k">=</span> <span class="pl-c1">min</span>(<span class="pl-c1">one</span>(p_x), <span class="pl-c1">exp</span>(p_y <span class="pl-k">-</span> p_x)) <span class="pl-c"><span class="pl-c">#</span> The Metropolis acceptance probability</span>

        <span class="pl-k">if</span> <span class="pl-c1">rand</span>() <span class="pl-k">&lt;=</span> alpha
            p_x <span class="pl-k">=</span> p_y

            <span class="pl-c"><span class="pl-c">#</span> This 'accepts', or interchanges r.x &lt;-&gt; r.y:</span>
            <span class="pl-c"><span class="pl-c">#</span> (NB: do not do r.x = r.y; these are (pointers to) vectors!)</span>
            <span class="pl-c1">accept!</span>(r)
        <span class="pl-k">end</span>

        <span class="pl-c"><span class="pl-c">#</span> Do the adaptation update:</span>
        <span class="pl-c1">adapt!</span>(s, r, alpha, k)

        X[:,k] <span class="pl-k">=</span> r<span class="pl-k">.</span>x   <span class="pl-c"><span class="pl-c">#</span> Save the current sample</span>
     <span class="pl-k">end</span>
    X
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Standard normal target for testing</span>
<span class="pl-en">normal_log_p</span>(x) <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">mapreduce</span>(e<span class="pl-k">-&gt;</span>e<span class="pl-k">*</span>e, <span class="pl-k">+</span>, x)<span class="pl-k">/</span><span class="pl-c1">2</span>

<span class="pl-c"><span class="pl-c">#</span> Run 1M iterations of the sampler targetting 30d standard Normal:</span>
X <span class="pl-k">=</span> <span class="pl-c1">mySampler</span>(normal_log_p, <span class="pl-c1">1_000_000</span>, <span class="pl-c1">zeros</span>(<span class="pl-c1">30</span>))</pre></div>
<h2 dir="auto"><a id="user-content-more-documentation" class="anchor" aria-hidden="true" href="#more-documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>More documentation</h2>
<p dir="auto">See the more detailed <a href="https://mvihola.github.io/docs/AdaptiveMCMC.jl/" rel="nofollow">documentation</a>
for more information regarding the implementation. The functions also have help fields, so for instance, <code>? adaptive_mcmc</code> in the Julia REPL gives a brief help of that function.</p>
<h2 dir="auto"><a id="user-content-how-to-cite" class="anchor" aria-hidden="true" href="#how-to-cite"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to cite</h2>
<p dir="auto">The algorithms implemented in the package are discussed in the following reference:</p>
<ul dir="auto">
<li>M. Vihola. Ergonomic and reliable Bayesian inference with adaptive Markov chain Monte Carlo. In <i>Wiley StatsRef: Statistics Reference Online</i>, Davidian, M., Kenett, R.S., Longford, N.T., Molenberghs, G., Piegorsch, W.W., and Ruggeri, F. (eds.), Article No. stat08286, 2020.
<a href="https://doi.org/10.1002/9781118445112.stat08286" rel="nofollow">doi.org/10.1002/9781118445112.stat08286</a></li>
</ul>
<p dir="auto">The above is also published as the following book chapter (which can also be cited):</p>
<ul dir="auto">
<li>M. Vihola. Bayesian inference with Adaptive Markov chain Monte Carlo. In <i>Computational Statistics in Data Science</i>, Piegorsch, W.W., Levine, R.A., Zhang, H.H., and Lee, T.C.M. (eds.), Chichester: John Wiley &amp; Sons, ISBN: 978-1-119-56107-1, 2022.</li>
</ul>
</article></div>