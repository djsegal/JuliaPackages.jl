<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-data-parallel-training-for-fluxjl" class="anchor" aria-hidden="true" href="#data-parallel-training-for-fluxjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Data Parallel Training for Flux.jl</h1>
<p dir="auto"><a href="https://dhairyalgandhi.github.io/FluxDistributed.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/75ec09bb66e3d0ffdf8557e84fd7a866a94e1dfa698e92fb645c5403e0bbb4ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63732d6465762d626c7565" alt="Docs" data-canonical-src="https://img.shields.io/badge/Docs-dev-blue" style="max-width: 100%;"></a></p>
<p dir="auto">Modern large scale deep learning models have increased in size and number of parameters substaintially. This aims to provide tools and mechanisms to scale training of Flux.jl models over multiple GPUs.</p>
<p dir="auto">Supports both task based and process based parallelism. The former is suited to single-node parallelism, and the latter to multi-node training. Multi-node training is handled by the same design as multiple locally installed GPU clusters using process based parallelism.</p>
<h2 dir="auto"><a id="user-content-setting-up-a-dataset" class="anchor" aria-hidden="true" href="#setting-up-a-dataset"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setting Up a DataSet</h2>
<p dir="auto">There is a <code>Data.toml</code> in the repo which has a few example Datasets. The package uses the <code>"imagenet_local"</code> data set by default. Make sure to update the path to where in the system the a dataset is available. In the future, this requirement will be lifted in favour of an API to configure this path in code.</p>
<h2 dir="auto"><a id="user-content-single-node-parallelism" class="anchor" aria-hidden="true" href="#single-node-parallelism"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Single Node Parallelism</h2>
<h3 dir="auto"><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basic Usage:</h3>
<p dir="auto">Start Julia with the environment of the package activated. This is currently necessary. Start julia with more threads than available. Finally, set up the environment via <code>] instantiate</code>.</p>
<p dir="auto">Here is an example of a simple task based parallel training demo.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using FluxDistributed, Metalhead, Flux, CUDA, Optimisers, DataSets

julia&gt; classes = 1:1000
1:1000

julia&gt; model = ResNet34();

julia&gt; key = open(BlobTree, DataSets.dataset(&quot;imagenet&quot;)) do data_tree
         FluxDistributed.train_solutions(data_tree, path&quot;LOC_train_solution.csv&quot;, classes)
       end;

julia&gt; val = open(BlobTree, DataSets.dataset(&quot;imagenet&quot;)) do data_tree
         FluxDistributed.train_solutions(data_tree, path&quot;LOC_val_solution.csv&quot;, classes)
       end;

julia&gt; opt = Optimisers.Momentum()
Optimisers.Momentum{Float32}(0.01f0, 0.9f0)

julia&gt; setup, buffer = prepare_training(model, key,
                                        CUDA.devices(),
                                        opt, # optimizer
                                        96,  # batchsize per GPU
                                        epochs = 2);

julia&gt; loss = Flux.Losses.logitcrossentropy
logitcrossentropy (generic function with 1 method)

julia&gt; FluxDistributed.train(loss, setup, buffer, opt,
                            val = val,
                            sched = identity);"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> FluxDistributed, Metalhead, Flux, CUDA, Optimisers, DataSets

julia<span class="pl-k">&gt;</span> classes <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1000</span>
<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1000</span>

julia<span class="pl-k">&gt;</span> model <span class="pl-k">=</span> <span class="pl-c1">ResNet34</span>();

julia<span class="pl-k">&gt;</span> key <span class="pl-k">=</span> <span class="pl-c1">open</span>(BlobTree, DataSets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>imagenet<span class="pl-pds">"</span></span>)) <span class="pl-k">do</span> data_tree
         FluxDistributed<span class="pl-k">.</span><span class="pl-c1">train_solutions</span>(data_tree, <span class="pl-s"><span class="pl-pds"><span class="pl-c1">path</span>"</span>LOC_train_solution.csv<span class="pl-pds">"</span></span>, classes)
       <span class="pl-k">end</span>;

julia<span class="pl-k">&gt;</span> val <span class="pl-k">=</span> <span class="pl-c1">open</span>(BlobTree, DataSets<span class="pl-k">.</span><span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>imagenet<span class="pl-pds">"</span></span>)) <span class="pl-k">do</span> data_tree
         FluxDistributed<span class="pl-k">.</span><span class="pl-c1">train_solutions</span>(data_tree, <span class="pl-s"><span class="pl-pds"><span class="pl-c1">path</span>"</span>LOC_val_solution.csv<span class="pl-pds">"</span></span>, classes)
       <span class="pl-k">end</span>;

julia<span class="pl-k">&gt;</span> opt <span class="pl-k">=</span> Optimisers<span class="pl-k">.</span><span class="pl-c1">Momentum</span>()
Optimisers<span class="pl-k">.</span><span class="pl-c1">Momentum</span><span class="pl-c1">{Float32}</span>(<span class="pl-c1">0.01f0</span>, <span class="pl-c1">0.9f0</span>)

julia<span class="pl-k">&gt;</span> setup, buffer <span class="pl-k">=</span> <span class="pl-c1">prepare_training</span>(model, key,
                                        CUDA<span class="pl-k">.</span><span class="pl-c1">devices</span>(),
                                        opt, <span class="pl-c"><span class="pl-c">#</span> optimizer</span>
                                        <span class="pl-c1">96</span>,  <span class="pl-c"><span class="pl-c">#</span> batchsize per GPU</span>
                                        epochs <span class="pl-k">=</span> <span class="pl-c1">2</span>);

julia<span class="pl-k">&gt;</span> loss <span class="pl-k">=</span> Flux<span class="pl-k">.</span>Losses<span class="pl-k">.</span>logitcrossentropy
logitcrossentropy (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> FluxDistributed<span class="pl-k">.</span><span class="pl-c1">train</span>(loss, setup, buffer, opt,
                            val <span class="pl-k">=</span> val,
                            sched <span class="pl-k">=</span> identity);</pre></div>
<p dir="auto">Here <code>model</code> describes the model to train, <code>key</code> describes a table of data and how it may be accessed. For the purposes of the demo, this is taken from the <code>LOC_train_solution.csv</code> published by ImageNet alongside the images. Look at <code>train_solutions</code> which would allow access to the training validation and test sets.</p>
<p dir="auto"><code>loss</code> is a typical loss function used to train a large neural network. The current system is set up for supervised learning, with support for semi supervised learning coming soon. More information can be found in the documentation.</p>
<h2 dir="auto"><a id="user-content-for-process-based-parallelism---multi-node-parallelism" class="anchor" aria-hidden="true" href="#for-process-based-parallelism---multi-node-parallelism"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For process based parallelism - multi-node parallelism</h2>
<h3 dir="auto"><a id="user-content-basic-usage-1" class="anchor" aria-hidden="true" href="#basic-usage-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basic Usage</h3>
<ul dir="auto">
<li>In the <code>bin</code> directory, you would find <code>driver.jl</code> which has all the initial configuration and a high level function designed to start the training loop</li>
<li>One would notice that the function takes in several arguments and even more keyword arguments. Most of them have sensible defaults but it is worth mentioning a few ones of interest.</li>
<li>Start a <code>julia</code> command with more threads than the number of logical cores available</li>
<li>By default, <code>driver.jl</code> starts 4 workers, implying training would happen on 4 GPUs. This may be tweaked according to the number of GPUs available.</li>
<li>This demo is written with heterogenous file loading in mind, such that the data to be trained may be from a local filsystem or hosted remotely (such as on Amazon AWS S3 bucket).</li>
</ul>
<p dir="auto"><code>rcs</code> and <code>updated_grads_channel</code> are <code>RemoteChannel</code> between the first process and all the child processes. These are used to send gradients back and forth in order to synchronise them to perform data parallel training.</p>
<p dir="auto"><code>syncgrads</code> starts a task on the main process which continually monitors for gradients coming in from all the available processes and does a manual synchrnisation and sends the updated gradients back to the processes. These gradients are what ultimately trains sent to optimise the model.</p>
<h3 dir="auto"><a id="user-content-logging-support" class="anchor" aria-hidden="true" href="#logging-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Logging Support</h3>
<p dir="auto">In order to hook into your favourite MLOps backend, it is possible to set up logging with various backends. By default, a trace of the training metrics is printed to console. In order to replace that, it is possible to use Julia's logging infrastructure to set up a different logger.</p>
<h4 dir="auto"><a id="user-content-wandbjl" class="anchor" aria-hidden="true" href="#wandbjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Wandb.jl</h4>
<p dir="auto"><a href="https://github.com/avik-pal/Wandb.jl">Wandb.jl</a> has unofficial bindings for the wandb.ai platform.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using FluxDistributed, Flux, Metalhead
using Wandb, Logging

lg = WandbLogger(project = &quot;DDP&quot;,
                 name = &quot;Resnet-34,
                 config = Dict(:saveweights =&gt; false, :cycles =&gt; 100),
                 step_increment = 1)

with_logger(lg) do
  FluxDistributed.train(...)
end"><pre><span class="pl-k">using</span> FluxDistributed, Flux, Metalhead
<span class="pl-k">using</span> Wandb, Logging

lg <span class="pl-k">=</span> <span class="pl-c1">WandbLogger</span>(project <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>DDP<span class="pl-pds">"</span></span>,
                 name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Resnet-34,</span>
<span class="pl-s">                 config = Dict(:saveweights =&gt; false, :cycles =&gt; 100),</span>
<span class="pl-s">                 step_increment = 1)</span>
<span class="pl-s"></span>
<span class="pl-s">with_logger(lg) do</span>
<span class="pl-s">  FluxDistributed.train(...)</span>
<span class="pl-s">end</span></pre></div>
</article></div>