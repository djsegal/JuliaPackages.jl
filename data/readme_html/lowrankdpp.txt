<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><p><a href="https://travis-ci.org/cgartrel/LowRankDPP.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/cfc1907b472c1e3c96fc57c0eb50866881d50885/68747470733a2f2f7472617669732d63692e6f72672f636761727472656c2f4c6f7752616e6b4450502e6a6c2e7376673f6272616e63683d6d6173746572" alt="Travis" data-canonical-src="https://travis-ci.org/cgartrel/LowRankDPP.jl.svg?branch=master" style="max-width:100%;"></a></p>
<h1><a id="user-content-low-rank-dpp-learning-and-prediction" class="anchor" aria-hidden="true" href="#low-rank-dpp-learning-and-prediction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Low-rank DPP Learning and Prediction</h1>
<p>Julia implementation of low-rank determinantal point process (DPP) learning and
prediction algorithms.  Two learning algorithms are provided: the first is an
optimization-based algorithm that uses stochastic gradient ascent (SGA), and the
second is a Bayesian algorithm that uses stochastic gradient Hamiltonian Monte
Carlo (SGHMC).</p>
<p>For details on low-rank DPPs, including SGA-based learning and prediction, see
the <a href="http://ulrichpaquet.com/Papers/LowRankDPP2017.pdf" rel="nofollow">Low-Rank Factorization of Determinantal Point
Processes</a> paper (<a href="https://drive.google.com/open?id=0BzbRNlai9LOqZGNKV0s4WnF6U2s" rel="nofollow">slides</a>).  For more
on Bayesian low-rank DPPs, see the <a href="http://ulrichpaquet.com/Papers/RecSys-2016-DPP.pdf" rel="nofollow">Bayesian Low-Rank Determinantal Point
Processes</a> paper (<a href="https://drive.google.com/open?id=0BzbRNlai9LOqNFpZUVd1b3QyMDA" rel="nofollow">slides</a>).</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>Within Julia, use the package manager:</p>
<div class="highlight highlight-source-julia"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-c1">PackageSpec</span>(url<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>git://github.com/cgartrel/LowRankDPP.jl.git<span class="pl-pds">"</span></span>))</pre></div>
<h2><a id="user-content-data-files" class="anchor" aria-hidden="true" href="#data-files"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Files</h2>
<p>The Amazon baby registry dataset is included in the <code>data/</code> directory.  This
dataset is described in the <a href="https://arxiv.org/abs/1411.1088" rel="nofollow">Expectation-Maximization for Learning Determinantal
Point Processes</a> paper.</p>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Basic Usage</h2>
<p><code>DPPExamples.jl</code> contains a number of examples that show how to convert CSV data
files into the <a href="https://github.com/JuliaIO/JLD.jl">JLD</a> files required by this
low-rank DPP package, perform low-rank DPP learning using the SGA and SGHMC
learning algorithms, compute predictions using models generated by both types of
learning algorithms, and compute prediction metrics (mean percentile rank and
precision@N).</p>
<p>To run the examples for the full CSV data conversion, learning, prediction, and
prediction metrics pipeline for SGA-based models, use the following functions
from <code>DPPExamples.jl</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> LowRankDPP

<span class="pl-c1">convertCsvToBasketsExample</span>()
<span class="pl-c1">dppLearningExample</span>()
<span class="pl-c1">predictionExample</span>()
<span class="pl-c1">predictionMetricsExample</span>()</pre></div>
<p>To run the examples for the learning and prediction pipeline for SGHMC-based
models, use the following functions from <code>DPPExamples.jl</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> LowRankDPP

<span class="pl-c1">dppLearningBayesianExample</span>()
<span class="pl-c1">predictionForMCMCSamplesExample</span>()</pre></div>
<p>The provided hyperparameter settings of the learning algorithms should work for
most of the included Amazon baby registry data.  However, these hyperparameters
will likely need to be tuned for other datasets.  In particular, the <code>epsFixed</code>,
<code>epsInitialDecay</code>, and <code>numIterationsFixedEps</code> settings in
<code>doStochasticGradientAscent</code> (from <code>DPPLearning.jl</code>) will need to be tuned to
ensure proper convergence to a local maximum for SGA learning, while the
<code>stepSizeLarger</code>, <code>stepSizeIntermediate</code>, <code>stepSizeSmaller</code>,
<code>numIterationsLargerStepSize</code>, and <code>numIterationsIntermediateStepSize</code> settings
in <code>runStochasticGradientHamiltonianMonteCarloSampler</code> (from
<code>DPPLearningBayesian.jl</code>) will need to be tuned to ensure proper convergence to
a local mode for SGHMC learning.</p>
</article></div>