<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-discretevalueiteration" class="anchor" aria-hidden="true" href="#discretevalueiteration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DiscreteValueIteration</h1>
<p dir="auto"><a href="https://github.com/JuliaPOMDP/DiscreteValueIteration.jl/actions/workflows/CI.yml"><img src="https://github.com/JuliaPOMDP/DiscreteValueIteration.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://app.codecov.io/github/JuliaPOMDP/DiscreteValueIteration.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/865f6df1c8824af30a726a610155eb73b866e3c3f47de688ec8975a2bc4141e9/68747470733a2f2f636f6465636f762e696f2f6769746875622f4a756c6961504f4d44502f446973637265746556616c7565497465726174696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d51636d6b796536664230" alt="codecov" data-canonical-src="https://codecov.io/github/JuliaPOMDP/DiscreteValueIteration.jl/branch/master/graph/badge.svg?token=Qcmkye6fB0" style="max-width: 100%;"></a></p>
<p dir="auto">This package implements the discrete value iteration algorithm in Julia for solving Markov decision processes (MDPs).
The user should define the problem with <a href="https://github.com/JuliaPOMDP/QuickPOMDPs.jl">QuickPOMDPs.jl</a> or according to the API in <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a>. Examples of
problem definitions can be found in <a href="https://github.com/JuliaPOMDP/POMDPModels.jl">POMDPModels.jl</a>. For an extensive tutorial, see <a href="https://github.com/JuliaPOMDP/POMDPExamples.jl">these</a> notebooks.</p>
<p dir="auto">There are two solvers in the package. The "vanilla" <a href="src/vanilla.jl"><code>ValueIterationSolver</code></a> calls functions from the POMDPs.jl interface in every iteration, while the <a href="src/sparse.jl"><code>SparseValueIterationSolver</code></a> first creates sparse transition and reward matrices and then performs value iteration with the new matrix representation. While both solvers take advantage of sparsity, the <code>SparseValueIterationSolver</code> is generally faster because of low-level optimizations, while the <code>ValueIterationSolver</code> has the advantage that it does not require allocation of transition matrices (which could potentially be too large to fit in memory).</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg; Pkg.add(&quot;DiscreteValueIteration&quot;)"><pre><span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>DiscreteValueIteration<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Given an MDP <code>mdp</code> defined with <a href="https://github.com/JuliaPOMDP/QuickPOMDPs.jl">QuickPOMDPs.jl</a> or the POMDPs.jl interface, use</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using DiscreteValueIteration

solver = ValueIterationSolver(max_iterations=100, belres=1e-6, verbose=true) # creates the solver
policy = solve(solver, mdp) # runs value iterations"><pre><span class="pl-k">using</span> DiscreteValueIteration

solver <span class="pl-k">=</span> <span class="pl-c1">ValueIterationSolver</span>(max_iterations<span class="pl-k">=</span><span class="pl-c1">100</span>, belres<span class="pl-k">=</span><span class="pl-c1">1e-6</span>, verbose<span class="pl-k">=</span><span class="pl-c1">true</span>) <span class="pl-c"><span class="pl-c">#</span> creates the solver</span>
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, mdp) <span class="pl-c"><span class="pl-c">#</span> runs value iterations</span></pre></div>
<p dir="auto">To extract the policy for a given state, simply call the action function:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="a = action(policy, s) # returns the optimal action for state s"><pre>a <span class="pl-k">=</span> <span class="pl-c1">action</span>(policy, s) <span class="pl-c"><span class="pl-c">#</span> returns the optimal action for state s</span></pre></div>
<p dir="auto">Or to extract the value, use</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="value(policy, s) # returns the optimal value at state s"><pre><span class="pl-c1">value</span>(policy, s) <span class="pl-c"><span class="pl-c">#</span> returns the optimal value at state s</span></pre></div>
<h3 dir="auto"><a id="user-content-requirements-for-problems-defined-using-the-pomdpsjl-interface" class="anchor" aria-hidden="true" href="#requirements-for-problems-defined-using-the-pomdpsjl-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Requirements for problems defined using the POMDPs.jl interface</h3>
<p dir="auto">If you are using the POMDPs.jl interface instead of QuickPOMDPs.jl, you can see the requirements for using these solvers with</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using POMDPs
using DiscreteValueIteration
@requirements_info ValueIterationSolver() YourMDP()
@requirements_info SparseValueIterationSolver() YourMDP()"><pre><span class="pl-k">using</span> POMDPs
<span class="pl-k">using</span> DiscreteValueIteration
<span class="pl-c1">@requirements_info</span> <span class="pl-c1">ValueIterationSolver</span>() <span class="pl-c1">YourMDP</span>()
<span class="pl-c1">@requirements_info</span> <span class="pl-c1">SparseValueIterationSolver</span>() <span class="pl-c1">YourMDP</span>()</pre></div>
<p dir="auto">This should return a list of the following functions to be implemented for your MDP:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="discount(::MDP)
n_states(::MDP)
n_actions(::MDP)
transition(::MDP, ::State, ::Action)
reward(::MDP, ::State, ::Action, ::State)
stateindex(::MDP, ::State)
actionindex(::MDP, ::Action)
actions(::MDP, ::State)
support(::StateDistribution)
pdf(::StateDistribution, ::State)
states(::MDP)
actions(::MDP)"><pre><span class="pl-c1">discount</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>)
<span class="pl-c1">n_states</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>)
<span class="pl-c1">n_actions</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>)
<span class="pl-c1">transition</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>, <span class="pl-k">::</span><span class="pl-c1">State</span>, <span class="pl-k">::</span><span class="pl-c1">Action</span>)
<span class="pl-c1">reward</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>, <span class="pl-k">::</span><span class="pl-c1">State</span>, <span class="pl-k">::</span><span class="pl-c1">Action</span>, <span class="pl-k">::</span><span class="pl-c1">State</span>)
<span class="pl-c1">stateindex</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>, <span class="pl-k">::</span><span class="pl-c1">State</span>)
<span class="pl-c1">actionindex</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>, <span class="pl-k">::</span><span class="pl-c1">Action</span>)
<span class="pl-c1">actions</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>, <span class="pl-k">::</span><span class="pl-c1">State</span>)
<span class="pl-c1">support</span>(<span class="pl-k">::</span><span class="pl-c1">StateDistribution</span>)
<span class="pl-c1">pdf</span>(<span class="pl-k">::</span><span class="pl-c1">StateDistribution</span>, <span class="pl-k">::</span><span class="pl-c1">State</span>)
<span class="pl-c1">states</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>)
<span class="pl-c1">actions</span>(<span class="pl-k">::</span><span class="pl-c1">MDP</span>)</pre></div>
</article></div>