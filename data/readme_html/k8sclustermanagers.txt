<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-k8sclustermanagersjl" class="anchor" aria-hidden="true" href="#k8sclustermanagersjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>K8sClusterManagers.jl</h1>
<p><a href="https://github.com/beacon-biosignals/K8sClusterManagers.jl/actions/workflows/CI.yml"><img src="https://github.com/beacon-biosignals/K8sClusterManagers.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/beacon-biosignals/K8sClusterManagers.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/baf90c356e9395799c4acaf16c021507364132a669f7545c9144855ef70f50d8/68747470733a2f2f636f6465636f762e696f2f67682f626561636f6e2d62696f7369676e616c732f4b3873436c75737465724d616e61676572732e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d4d47385a4f3441504449" alt="codecov" data-canonical-src="https://codecov.io/gh/beacon-biosignals/K8sClusterManagers.jl/branch/main/graph/badge.svg?token=MG8ZO4APDI" style="max-width:100%;"></a>
<a href="https://beacon-biosignals.github.io/K8sClusterManagers.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Docs: stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://beacon-biosignals.github.io/K8sClusterManagers.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Docs: development" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></p>
<p>A Julia cluster manager for provisioning workers in a Kubernetes (K8s) cluster.</p>
<h2><a id="user-content-k8sclustermanager" class="anchor" aria-hidden="true" href="#k8sclustermanager"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>K8sClusterManager</h2>
<p>The <code>K8sClusterManager</code> is intended to be used from a Pod running inside a Kubernetes
cluster.</p>
<p>Assuming you have <code>kubectl</code> installed locally and configured to connect to a cluster, you
can easily create an interactive Julia REPL session running from within the cluster by
executing:</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="kubectl run -it example-manager-pod --image julia:1
"><pre>kubectl run -it example-manager-pod --image julia:1</pre></div>
<p>Or equivalently, using a K8s manifest named <code>example-manager-pod.yaml</code> containing:</p>
<div class="highlight highlight-source-yaml position-relative" data-snippet-clipboard-copy-content="apiVersion: v1
kind: Pod
metadata:
  name: example-manager-pod
spec:
  containers:
  - name: manager
    image: julia:1
    stdin: true
    tty: true
"><pre><span class="pl-ent">apiVersion</span>: <span class="pl-c1">v1</span>
<span class="pl-ent">kind</span>: <span class="pl-s">Pod</span>
<span class="pl-ent">metadata</span>:
  <span class="pl-ent">name</span>: <span class="pl-s">example-manager-pod</span>
<span class="pl-ent">spec</span>:
  <span class="pl-ent">containers</span>:
  - <span class="pl-ent">name</span>: <span class="pl-s">manager</span>
    <span class="pl-ent">image</span>: <span class="pl-s">julia:1</span>
    <span class="pl-ent">stdin</span>: <span class="pl-c1">true</span>
    <span class="pl-ent">tty</span>: <span class="pl-c1">true</span></pre></div>
<p>and running the following commands will also create a Julia REPL running inside a Kubernetes
Pod:</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="kubectl apply -f example-manager-pod.yaml

# Once the pod is running
kubectl attach -it pod/example-driver-pod
"><pre>kubectl apply -f example-manager-pod.yaml

<span class="pl-c"><span class="pl-c">#</span> Once the pod is running</span>
kubectl attach -it pod/example-driver-pod</pre></div>
<p>Now in this Julia REPL session, you can do add two workers via:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using Pkg; Pkg.add(&quot;K8sClusterManagers&quot;)

julia&gt; using K8sClusterManagers

julia&gt; addprocs(K8sClusterManager(2))
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>K8sClusterManagers<span class="pl-pds">"</span></span>)

julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> K8sClusterManagers

julia<span class="pl-k">&gt;</span> <span class="pl-c1">addprocs</span>(<span class="pl-c1">K8sClusterManager</span>(<span class="pl-c1">2</span>))</pre></div>
<h3><a id="user-content-advanced-configuration" class="anchor" aria-hidden="true" href="#advanced-configuration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Advanced configuration</h3>
<p><code>K8sClusterManager</code> exposes a <code>configure</code> keyword argument that can be used to make
modifications to the Pod manifest when defining workers.</p>
<p>When launching the cluster the function <code>configure(pod)</code> will be called where <code>pod</code> is an
dict-object representing the YAML/JSON Pod manifest. The function must return an object of
the same type. For example if you wanted to change the workers to require GPU resources you
could write the following:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function my_gpu_configurator(pod)
    worker_container = pod[&quot;spec&quot;][&quot;containers&quot;][1]
    worker_container[&quot;resources&quot;][&quot;limits&quot;][&quot;nvidia.com/gpu&quot;] = 1
    return pod
end
"><pre><span class="pl-k">function</span> <span class="pl-en">my_gpu_configurator</span>(pod)
    worker_container <span class="pl-k">=</span> pod[<span class="pl-s"><span class="pl-pds">"</span>spec<span class="pl-pds">"</span></span>][<span class="pl-s"><span class="pl-pds">"</span>containers<span class="pl-pds">"</span></span>][<span class="pl-c1">1</span>]
    worker_container[<span class="pl-s"><span class="pl-pds">"</span>resources<span class="pl-pds">"</span></span>][<span class="pl-s"><span class="pl-pds">"</span>limits<span class="pl-pds">"</span></span>][<span class="pl-s"><span class="pl-pds">"</span>nvidia.com/gpu<span class="pl-pds">"</span></span>] <span class="pl-k">=</span> <span class="pl-c1">1</span>
    <span class="pl-k">return</span> pod
<span class="pl-k">end</span></pre></div>
<p>To get an example instance of <code>pod</code> objects that might be passed into the <code>configure</code>, call</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using K8sClusterManagers, JSON
pod = K8sClusterManagers.worker_pod_spec(manager_name=&quot;example&quot;, image=&quot;julia&quot;, cmd=`julia`)
JSON.print(pod, 4)
"><pre><span class="pl-k">using</span> K8sClusterManagers, JSON
pod <span class="pl-k">=</span> K8sClusterManagers<span class="pl-k">.</span><span class="pl-c1">worker_pod_spec</span>(manager_name<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>example<span class="pl-pds">"</span></span>, image<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>julia<span class="pl-pds">"</span></span>, cmd<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">`</span>julia<span class="pl-pds">`</span></span>)
JSON<span class="pl-k">.</span><span class="pl-c1">print</span>(pod, <span class="pl-c1">4</span>)</pre></div>
<h2><a id="user-content-useful-commands" class="anchor" aria-hidden="true" href="#useful-commands"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Useful Commands</h2>
<p>Monitor the status of all your Pods</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="watch kubectl get pods,services
"><pre>watch kubectl get pods,services</pre></div>
<p>Stream the stdout of the worker "example-driver-pod-worker-9001":</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="kubectl logs -f pod/example-driver-pod-worker-9001
"><pre>kubectl logs -f pod/example-driver-pod-worker-9001</pre></div>
<p>Currently cleaning up after / killing all your pods can be slow / ineffective from a Julia
context, especially if the driver Julia session dies unexpectedly. It may be necessary to
kill your workers from the command line.</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="kubectl delete pod/example-driver-pod-worker-9001 --grace-period=0 --force=true
"><pre>kubectl delete pod/example-driver-pod-worker-9001 --grace-period=0 --force=true</pre></div>
<p>It may be convenient to set a common label in your worker podspecs, so that you can select
them all with <code>-l='...'</code> by label, and kill all the worker Pods in a single invocation.</p>
<p>Display info about a Pod -- this is especially useful to troubleshoot a Pod that is taking
longer than expected to get up and running.</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="kubectl describe pod/example-driver-pod
"><pre>kubectl describe pod/example-driver-pod</pre></div>
<h2><a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Troubleshooting</h2>
<p>If you get <code>deserialize</code> errors during interations between driver and worker processes, make
sure you are using the same version of Julia on the driver as on all the workers!</p>
<p>If you aren't sure what went wrong, check the logs! The syntax is</p>
<div class="highlight highlight-source-shell position-relative" data-snippet-clipboard-copy-content="kubectl logs -f pod/pod_name
"><pre>kubectl logs -f pod/pod_name</pre></div>
<p>where the Pod name <code>pod_name</code> you can get from <code>kubectl get pods</code>.</p>
<h2><a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Testing</h2>
<p>The K8sClusterManagers package includes tests that are expect to have access to a Kubernetes
cluster. The tests should be able to be run in any Kubernetes cluster but have only been
run with <a href="https://minikube.sigs.k8s.io/" rel="nofollow">minikube</a>.</p>
<h3><a id="user-content-minikube" class="anchor" aria-hidden="true" href="#minikube"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Minikube</h3>
<ol>
<li><a href="https://docs.docker.com/get-docker/" rel="nofollow">Install Docker or Docker Desktop</a></li>
<li>If using Docker Desktop: set the resources to a minimum of 3 CPUs and 2.25 GB Memory</li>
<li><a href="https://minikube.sigs.k8s.io/docs/start/" rel="nofollow">Install minikube</a></li>
<li>Start the Kubernetes cluster: <code>minikube start</code></li>
<li>Use the in-cluster Docker daemon for image builds: <code>eval $(minikube docker-env)</code>
(Note: only works with single-node clusters)</li>
<li>Run the K8sClusterManagers.jl tests</li>
</ol>
</article></div>