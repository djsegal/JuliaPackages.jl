<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-reinforce" class="anchor" aria-hidden="true" href="#reinforce"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reinforce</h1>
<p><a href="https://travis-ci.org/JuliaML/Reinforce.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/7bdd001b948e7b4cdbd23f825ba35c6c25016f36/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614d4c2f5265696e666f7263652e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaML/Reinforce.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://gitter.im/reinforcejl/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge" rel="nofollow"><img src="https://camo.githubusercontent.com/2d4bd919b0902b46fd54e85e52677f8873e2d889/68747470733a2f2f6261646765732e6769747465722e696d2f7265696e666f7263656a6c2f4c6f6262792e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/reinforcejl/Lobby.svg" style="max-width:100%;"></a></p>
<p>Reinforce.jl is an interface for Reinforcement Learning.  It is intended to connect modular environments, policies, and solvers with a simple interface.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/933338/17670982/8923a2f6-62e2-11e6-943f-bd0a2a7b5c1f.gif"><img src="https://cloud.githubusercontent.com/assets/933338/17670982/8923a2f6-62e2-11e6-943f-bd0a2a7b5c1f.gif" alt="" style="max-width:100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/933338/17703784/f3e18414-63a0-11e6-9f9e-f531278216f9.gif"><img src="https://cloud.githubusercontent.com/assets/933338/17703784/f3e18414-63a0-11e6-9f9e-f531278216f9.gif" alt="" style="max-width:100%;"></a></p>
<hr>
<p>Packages which build on Reinforce:</p>
<ul>
<li><a href="https://github.com/JuliaML/AtariAlgos.jl">AtariAlgos</a>: Environment which wraps Atari games using <a href="https://github.com/nowozin/ArcadeLearningEnvironment.jl">ArcadeLearningEnvironment</a></li>
<li><a href="https://github.com/JuliaML/OpenAIGym.jl">OpenAIGym</a>: Wrapper for OpenAI's python package: gym</li>
</ul>
<h2><a id="user-content-environment-interface" class="anchor" aria-hidden="true" href="#environment-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Environment Interface</h2>
<p>New environments are created by subtyping <code>AbstractEnvironment</code> and implementing
a few methods:</p>
<ul>
<li><code>reset!(env) -&gt; env</code></li>
<li><code>actions(env, s) -&gt; A</code></li>
<li><code>step!(env, s, a) -&gt; (r, s′)</code></li>
<li><code>finished(env, s′) -&gt; Bool</code></li>
</ul>
<p>and optional overrides:</p>
<ul>
<li><code>state(env) -&gt; s</code></li>
<li><code>reward(env) -&gt; r</code></li>
</ul>
<p>which map to <code>env.state</code> and <code>env.reward</code> respectively when unset.</p>
<ul>
<li><code>ismdp(env) -&gt; Bool</code></li>
</ul>
<p>An environment may be fully observable (MDP) or partially observable (POMDP).
In the case of a partially observable environment, the state <code>s</code> is really
an observation <code>o</code>.  To maintain consistency, we call everything a state,
and assume that an environment is free to maintain additional (unobserved)
internal state.  The <code>ismdp</code> query returns true when the environment is MDP,
and false otherwise.</p>
<ul>
<li><code>maxsteps(env) -&gt; Int</code></li>
</ul>
<p>The terminating condition of an episode is control by
<code>maxsteps() || finished()</code>.
It's default value is <code>0</code>, indicates unlimited.</p>
<hr>
<p>An minimal example for testing purpose is <code>test/foo.jl</code>.</p>
<p>TODO: more details and examples</p>
<h2><a id="user-content-policy-interface" class="anchor" aria-hidden="true" href="#policy-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Policy Interface</h2>
<p>Agents/policies are created by subtyping <code>AbstractPolicy</code> and implementing <code>action</code>.
The built-in random policy is a short example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">struct</span> RandomPolicy <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractPolicy</span> <span class="pl-k">end</span>
<span class="pl-en">action</span>(π<span class="pl-k">::</span><span class="pl-c1">RandomPolicy</span>, r, s, A) <span class="pl-k">=</span> <span class="pl-c1">rand</span>(A)</pre></div>
<p>Where <code>A</code> is the action space.
The <code>action</code> method maps the last reward and current state to the next chosen action:
<code>(r, s) -&gt; a</code>.</p>
<ul>
<li><code>reset!(π::AbstractPolicy) -&gt; π</code></li>
</ul>
<h2><a id="user-content-episode-iterator" class="anchor" aria-hidden="true" href="#episode-iterator"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Episode Iterator</h2>
<p>Iterate through episodes using the <code>Episode</code> iterator.
A 4-tuple <code>(s,a,r,s′)</code> is returned from each step of the episode:</p>
<div class="highlight highlight-source-julia"><pre>ep <span class="pl-k">=</span> <span class="pl-c1">Episode</span>(env, π)
<span class="pl-k">for</span> (s, a, r, s′) <span class="pl-k">in</span> ep
    <span class="pl-c"><span class="pl-c">#</span> do some custom processing of the sars-tuple</span>
<span class="pl-k">end</span>
R <span class="pl-k">=</span> ep<span class="pl-k">.</span>total_reward
T <span class="pl-k">=</span> ep<span class="pl-k">.</span>niter</pre></div>
<p>There is also a convenience method <code>run_episode</code>.
The following is an equivalent method to the last example:</p>
<div class="highlight highlight-source-julia"><pre>R <span class="pl-k">=</span> <span class="pl-c1">run_episode</span>(env, π) <span class="pl-k">do</span>
    <span class="pl-c"><span class="pl-c">#</span> anything you want... this section is called after each step</span>
<span class="pl-k">end</span></pre></div>
<hr>
<h2><a id="user-content-author-tom-breloff" class="anchor" aria-hidden="true" href="#author-tom-breloff"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Author: <a href="https://github.com/tbreloff">Tom Breloff</a></h2>
</article></div>