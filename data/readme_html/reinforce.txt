<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-deprecated" class="anchor" aria-hidden="true" href="#deprecated"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DEPRECATED</h1>
<p dir="auto">This package is discontinued. Please check <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a>, <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a> or <a href="https://github.com/jonathan-laurent/AlphaZero.jl">AlphaZero.jl</a> instead.</p>
<h1 dir="auto"><a id="user-content-reinforce" class="anchor" aria-hidden="true" href="#reinforce"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reinforce</h1>
<p dir="auto"><a href="https://travis-ci.org/JuliaML/Reinforce.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c7f59cbb909adb447d0965c991370cbfd9eafc8ea19c207bcb1910bbedd5d907/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614d4c2f5265696e666f7263652e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaML/Reinforce.jl.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://gitter.im/reinforcejl/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge" rel="nofollow"><img src="https://camo.githubusercontent.com/f569b600ff4bf907c91fe4f69649346100d7f6c948a1c91092782895c358bbef/68747470733a2f2f6261646765732e6769747465722e696d2f7265696e666f7263656a6c2f4c6f6262792e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/reinforcejl/Lobby.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Reinforce.jl is an interface for Reinforcement Learning.  It is intended to connect modular environments, policies, and solvers with a simple interface.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://cloud.githubusercontent.com/assets/933338/17670982/8923a2f6-62e2-11e6-943f-bd0a2a7b5c1f.gif"><img src="https://cloud.githubusercontent.com/assets/933338/17670982/8923a2f6-62e2-11e6-943f-bd0a2a7b5c1f.gif" alt="" data-animated-image="" style="max-width: 100%;"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://cloud.githubusercontent.com/assets/933338/17703784/f3e18414-63a0-11e6-9f9e-f531278216f9.gif"><img src="https://cloud.githubusercontent.com/assets/933338/17703784/f3e18414-63a0-11e6-9f9e-f531278216f9.gif" alt="" data-animated-image="" style="max-width: 100%;"></a></p>
<hr>
<p dir="auto">Packages which build on Reinforce:</p>
<ul dir="auto">
<li><a href="https://github.com/JuliaML/AtariAlgos.jl">AtariAlgos</a>: Environment which wraps Atari games using <a href="https://github.com/nowozin/ArcadeLearningEnvironment.jl">ArcadeLearningEnvironment</a></li>
<li><a href="https://github.com/JuliaML/OpenAIGym.jl">OpenAIGym</a>: Wrapper for OpenAI's python package: gym</li>
</ul>
<h2 dir="auto"><a id="user-content-environment-interface" class="anchor" aria-hidden="true" href="#environment-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Environment Interface</h2>
<p dir="auto">New environments are created by subtyping <code>AbstractEnvironment</code> and implementing
a few methods:</p>
<ul dir="auto">
<li><code>reset!(env) -&gt; env</code></li>
<li><code>actions(env, s) -&gt; A</code></li>
<li><code>step!(env, s, a) -&gt; (r, s′)</code></li>
<li><code>finished(env, s′) -&gt; Bool</code></li>
</ul>
<p dir="auto">and optional overrides:</p>
<ul dir="auto">
<li><code>state(env) -&gt; s</code></li>
<li><code>reward(env) -&gt; r</code></li>
</ul>
<p dir="auto">which map to <code>env.state</code> and <code>env.reward</code> respectively when unset.</p>
<ul dir="auto">
<li><code>ismdp(env) -&gt; Bool</code></li>
</ul>
<p dir="auto">An environment may be fully observable (MDP) or partially observable (POMDP).
In the case of a partially observable environment, the state <code>s</code> is really
an observation <code>o</code>.  To maintain consistency, we call everything a state,
and assume that an environment is free to maintain additional (unobserved)
internal state.  The <code>ismdp</code> query returns true when the environment is MDP,
and false otherwise.</p>
<ul dir="auto">
<li><code>maxsteps(env) -&gt; Int</code></li>
</ul>
<p dir="auto">The terminating condition of an episode is control by
<code>maxsteps() || finished()</code>.
It's default value is <code>0</code>, indicates unlimited.</p>
<hr>
<p dir="auto">An minimal example for testing purpose is <code>test/foo.jl</code>.</p>
<p dir="auto">TODO: more details and examples</p>
<h2 dir="auto"><a id="user-content-policy-interface" class="anchor" aria-hidden="true" href="#policy-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Policy Interface</h2>
<p dir="auto">Agents/policies are created by subtyping <code>AbstractPolicy</code> and implementing <code>action</code>.
The built-in random policy is a short example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="struct RandomPolicy &lt;: AbstractPolicy end
action(π::RandomPolicy, r, s, A) = rand(A)"><pre><span class="pl-k">struct</span> RandomPolicy <span class="pl-k">&lt;:</span> <span class="pl-c1">AbstractPolicy</span> <span class="pl-k">end</span>
<span class="pl-en">action</span>(π<span class="pl-k">::</span><span class="pl-c1">RandomPolicy</span>, r, s, A) <span class="pl-k">=</span> <span class="pl-c1">rand</span>(A)</pre></div>
<p dir="auto">Where <code>A</code> is the action space.
The <code>action</code> method maps the last reward and current state to the next chosen action:
<code>(r, s) -&gt; a</code>.</p>
<ul dir="auto">
<li><code>reset!(π::AbstractPolicy) -&gt; π</code></li>
</ul>
<h2 dir="auto"><a id="user-content-episode-iterator" class="anchor" aria-hidden="true" href="#episode-iterator"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Episode Iterator</h2>
<p dir="auto">Iterate through episodes using the <code>Episode</code> iterator.
A 4-tuple <code>(s,a,r,s′)</code> is returned from each step of the episode:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="ep = Episode(env, π)
for (s, a, r, s′) in ep
    # do some custom processing of the sars-tuple
end
R = ep.total_reward
T = ep.niter"><pre>ep <span class="pl-k">=</span> <span class="pl-c1">Episode</span>(env, π)
<span class="pl-k">for</span> (s, a, r, s′) <span class="pl-k">in</span> ep
    <span class="pl-c"><span class="pl-c">#</span> do some custom processing of the sars-tuple</span>
<span class="pl-k">end</span>
R <span class="pl-k">=</span> ep<span class="pl-k">.</span>total_reward
T <span class="pl-k">=</span> ep<span class="pl-k">.</span>niter</pre></div>
<p dir="auto">There is also a convenience method <code>run_episode</code>.
The following is an equivalent method to the last example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="R = run_episode(env, π) do
    # anything you want... this section is called after each step
end"><pre>R <span class="pl-k">=</span> <span class="pl-c1">run_episode</span>(env, π) <span class="pl-k">do</span>
    <span class="pl-c"><span class="pl-c">#</span> anything you want... this section is called after each step</span>
<span class="pl-k">end</span></pre></div>
<hr>
<h2 dir="auto"><a id="user-content-author-tom-breloff" class="anchor" aria-hidden="true" href="#author-tom-breloff"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Author: <a href="https://github.com/tbreloff">Tom Breloff</a></h2>
</article></div>