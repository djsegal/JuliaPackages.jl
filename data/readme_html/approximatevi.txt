<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-approximatevijl" class="anchor" aria-hidden="true" href="#approximatevijl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ApproximateVI.jl</h1>
<p>This package implements approximate variational inference as presented in<br>
<em>Approximate variational inference based on a finite sample of Gaussian latent variables,<br>
Pattern Analysis and Applications volume 19, pages 475‚Äì485, 2015</em> <a href="https://doi.org/10.1007/s10044-015-0496-9" rel="nofollow">[DOI]</a>, <a href="https://arxiv.org/pdf/1906.04507.pdf" rel="nofollow">[Arxiv]</a>.</p>
<p><strong>Documentation and more functionality will be added to this repository soon</strong></p>
<h2><a id="user-content-what-is-this-package-about" class="anchor" aria-hidden="true" href="#what-is-this-package-about"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is this package about</h2>
<p>This package implements variational inference using the re-parametrisation trick.
The work was independently developed and published <a href="https://doi.org/10.1007/s10044-015-0496-9" rel="nofollow">here</a>.
Of course, the method has been widely popularised by the works <a href="http://proceedings.mlr.press/v32/titsias14.pdf" rel="nofollow">Doubly Stochastic Variational Bayes for non-Conjugate Inference</a> and <a href="https://arxiv.org/abs/1312.6114" rel="nofollow">Auto-Encoding Variational Bayes</a>.
The method has indepedently appeared earlier in <a href="https://arxiv.org/abs/1206.6679" rel="nofollow">Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression</a> and later in <a href="https://link.springer.com/article/10.1007%2Fs00180-015-0638-y" rel="nofollow">A comparison of variational approximations for fast inference in mixed logit models</a> and very likely in other publications too.</p>
<h2><a id="user-content-what-does-the-package-do" class="anchor" aria-hidden="true" href="#what-does-the-package-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What does the package do</h2>
<p>The package offers function <code>VI</code>. This function approximates the posterior parameter distribution
with a Gaussian q(Œ∏) = ùú®(Œ∏|Œº,Œ£) by minimizing the expected lower bound:</p>
<p>‚à´ q(Œ∏) log p(x,Œ∏) dŒ∏ + ‚Ñã[q]</p>
<p>The above integral is approximated with a monte carlo average of S samples:</p>
<p>1/S ùúÆ‚Çõ log p(x,Œ∏‚Çõ) dŒ∏ + ‚Ñã[q]</p>
<p>Using the reparametrisation trick, we re-introduce the variational parameters that we need to optimise:</p>
<p>1/S ùúÆ‚Çõ log p(x,Œº + ‚àöŒ£ z‚Çõ) dŒ∏ + ‚Ñã[q], where ‚àöŒ£ is a matrix root of Œ£, i.e. ‚àöŒ£*‚àöŒ£' = Œ£, and z‚Çõ‚àºùú®(0,I).</p>
<p>Contrary to other flavours of the method, that repeatedly draw new samples z‚Çõ at each iteration of the optimiser, here a large number of samples z‚Çõ is drawn
instead and kept fixed throughout the execution of the algorithm (see <a href="https://arxiv.org/pdf/1906.04507.pdf" rel="nofollow">paper</a>, Algorithm 1).
This avoids the difficulty of working with a noisy gradient and allows the use of optimisers like <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="nofollow">LBFGS</a>. However, this comes at the expense of risking overfitting to the samples z‚Çõ that happened to be drawn. A mechanism for monitoring potential overfitting is described in the <a href="https://arxiv.org/pdf/1906.04507.pdf" rel="nofollow">paper</a>, section 2.3. Because of fixing the sample z‚Çõ, the algorithm doesn't not scale well to high number of parameters and is thus recommended for problems with relatively few parameters, e.g. 2-20 parameters. Future work may address this limitation. A method that attempts to address this limitation has been presented <a href="https://arxiv.org/abs/1901.04791" rel="nofollow">here</a>.</p>
<h2><a id="user-content-how-to-use-the-package" class="anchor" aria-hidden="true" href="#how-to-use-the-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to use the package</h2>
<p>The package is fairly easy to use. The only function of interest to the user is <code>VI</code>. At the very minimum, the user needs to provide a function that codes the joint log-likelihood function.</p>
<p>Consider, approximating a target density given by a three-component mixture model:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="using PyPlot # Necessary for this example

# Define means for three-component Gaussian mixture model
# All components are implicitly equally weighted and have unit covariance
Œº = [zeros(2), [2.5; 0.0], [-2.5; 0.0]]

# Define log-likelihood
logp(Œ∏) = log(exp(-0.5*sum((Œº[1].-Œ∏).^2)) + exp(-0.5*sum((Œº[1].-Œ∏).^2)) + exp(-0.5*sum((Œº[3].-Œ∏).^2)))
"><pre><code>using PyPlot # Necessary for this example

# Define means for three-component Gaussian mixture model
# All components are implicitly equally weighted and have unit covariance
Œº = [zeros(2), [2.5; 0.0], [-2.5; 0.0]]

# Define log-likelihood
logp(Œ∏) = log(exp(-0.5*sum((Œº[1].-Œ∏).^2)) + exp(-0.5*sum((Œº[1].-Œ∏).^2)) + exp(-0.5*sum((Œº[3].-Œ∏).^2)))
</code></pre></div>
<p>We will now approximate it with a Gaussian density. We need to pass to <code>VI</code> the log-likelihood function, a starting point for the mean of the approximate Gaussian posterior, as well as the number of fixed samples and the number of iterations we want to optimise the lower bound for:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="posterior, logevidence = VI(logp, randn(2); S = 100, iterations = 30)
"><pre><code>posterior, logevidence = VI(logp, randn(2); S = 100, iterations = 30)
</code></pre></div>
<p>This returns two outputs: the first one is the approximating posterior q(Œ∏) of type <code>MvNormal</code> (see <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>). The second output is the approximate lower bound of type <code>Float64</code>.</p>
<p>Below we plot as contour plot the target unnormalised posterior distribution.
We also plot the approximating posterior q(Œ∏) as a blue ellipse:</p>
<p><a target="_blank" rel="noopener noreferrer" href="docs/images/examplemixturemodel_ellipse.png"><img src="docs/images/examplemixturemodel_ellipse.png" alt="image" style="max-width:100%;"></a></p>
<h2><a id="user-content-further-examples" class="anchor" aria-hidden="true" href="#further-examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Further examples</h2>
<p>More examples can be found in the <a href="/src/Examples">/src/Examples</a> folder.</p>
</article></div>