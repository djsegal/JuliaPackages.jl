<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-qmdp" class="anchor" aria-hidden="true" href="#qmdp"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>QMDP</h1>
<p dir="auto"><a href="https://github.com/JuliaPOMDP/QMDP.jl/actions/workflows/CI.yml/"><img src="https://github.com/JuliaPOMDP/QMDP.jl/actions/workflows/CI.yml/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaPOMDP/QMDP.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dca0aac21374d4bac0c11909a59d946196e1a48a616a2c8a5f8ee72b5fba2855/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961504f4d44502f514d44502e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d65683647557851695167" alt="codecov" data-canonical-src="https://codecov.io/gh/JuliaPOMDP/QMDP.jl/branch/master/graph/badge.svg?token=eh6GUxQiQg" style="max-width: 100%;"></a></p>
<p dir="auto">This Julia package implements the QMDP approximate solver for POMDP/MDP planning. The QMDP solver is documented in:</p>
<ul dir="auto">
<li>Michael Littman, Anthony Cassandra, and Leslie Kaelbling. "<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.6374" rel="nofollow">Learning policies for partially observable environments: Scaling up</a>." In Proceedings of the Twelfth International Conference on Machine Learning, pages 362--370, San Francisco, CA, 1995.</li>
</ul>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Pkg
Pkg.add(&quot;QMDP&quot;)"><pre><span class="pl-k">import</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>QMDP<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using QMDP
pomdp = MyPOMDP() # initialize POMDP

# initialize the solver
# key-word args are the maximum number of iterations the solver will run for, and the Bellman tolerance
solver = QMDPSolver(max_iterations=20,
                    belres=1e-3,
                    verbose=true
                   ) 

# run the solver
policy = solve(solver, pomdp)"><pre><span class="pl-k">using</span> QMDP
pomdp <span class="pl-k">=</span> <span class="pl-c1">MyPOMDP</span>() <span class="pl-c"><span class="pl-c">#</span> initialize POMDP</span>

<span class="pl-c"><span class="pl-c">#</span> initialize the solver</span>
<span class="pl-c"><span class="pl-c">#</span> key-word args are the maximum number of iterations the solver will run for, and the Bellman tolerance</span>
solver <span class="pl-k">=</span> <span class="pl-c1">QMDPSolver</span>(max_iterations<span class="pl-k">=</span><span class="pl-c1">20</span>,
                    belres<span class="pl-k">=</span><span class="pl-c1">1e-3</span>,
                    verbose<span class="pl-k">=</span><span class="pl-c1">true</span>
                   ) 

<span class="pl-c"><span class="pl-c">#</span> run the solver</span>
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, pomdp)</pre></div>
<p dir="auto">To compute optimal action, define a belief with the <a href="http://juliapomdp.github.io/POMDPs.jl/latest/interfaces.html#Distributions-1" rel="nofollow">distribution interface</a>, or use the DiscreteBelief provided in <a href="http://juliapomdp.github.io/POMDPs.jl/latest/POMDPTools/beliefs/#Implemented-Belief-Updaters" rel="nofollow">POMDPTools</a>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using POMDPTools
b = uniform_belief(pomdp) # initialize to a uniform belief
a = action(policy, b)"><pre><span class="pl-k">using</span> POMDPTools
b <span class="pl-k">=</span> <span class="pl-c1">uniform_belief</span>(pomdp) <span class="pl-c"><span class="pl-c">#</span> initialize to a uniform belief</span>
a <span class="pl-k">=</span> <span class="pl-c1">action</span>(policy, b)</pre></div>
<p dir="auto">In order to use the efficient <code>SparseValueIterationSolver</code> from <a href="https://github.com/JuliaPOMDP/DiscreteValueIteration.jl">DiscreteValueIteration.jl</a>, you can directly pass the solver to the <code>QMDPSolver</code> constructor as follows:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using QMDP, DiscreteValueIteration
pomdp = MyPOMDP()

solver = QMDPSolver(SparseValueIterationSolver(max_iterations=20, verbose=true))

policy = solve(solver, pomdp)"><pre><span class="pl-k">using</span> QMDP, DiscreteValueIteration
pomdp <span class="pl-k">=</span> <span class="pl-c1">MyPOMDP</span>()

solver <span class="pl-k">=</span> <span class="pl-c1">QMDPSolver</span>(<span class="pl-c1">SparseValueIterationSolver</span>(max_iterations<span class="pl-k">=</span><span class="pl-c1">20</span>, verbose<span class="pl-k">=</span><span class="pl-c1">true</span>))

policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, pomdp)</pre></div>
</article></div>