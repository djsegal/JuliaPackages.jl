<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-qmdp" class="anchor" aria-hidden="true" href="#qmdp"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>QMDP</h1>
<p><a href="https://travis-ci.org/JuliaPOMDP/QMDP.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c56562213851d1eb6e6120764516a115e9fbb549/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961504f4d44502f514d44502e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaPOMDP/QMDP.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/r/JuliaPOMDP/QMDP.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/86efaef245acb0b5127bc0f3d63bdf44142b457e/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f4a756c6961504f4d44502f514d44502e6a6c2f62616467652e737667" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/JuliaPOMDP/QMDP.jl/badge.svg" style="max-width:100%;"></a></p>
<p>This Julia package implements the QMDP approximate solver for POMDP/MDP planning. The QMDP solver is documented in:</p>
<ul>
<li>Michael Littman, Anthony Cassandra, and Leslie Kaelbling. "<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.6374" rel="nofollow">Learning policies for partially observable environments: Scaling up</a>." In Proceedings of the Twelfth International Conference on Machine Learning, pages 362--370, San Francisco, CA, 1995.</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> POMDPs, Pkg
POMDPs<span class="pl-k">.</span><span class="pl-c1">add_registry</span>()
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>QMDP<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> QMDP
pomdp <span class="pl-k">=</span> <span class="pl-c1">MyPOMDP</span>() <span class="pl-c"><span class="pl-c">#</span> initialize POMDP</span>

<span class="pl-c"><span class="pl-c">#</span> initialize the solver</span>
<span class="pl-c"><span class="pl-c">#</span> key-word args are the maximum number of iterations the solver will run for, and the Bellman tolerance</span>
solver <span class="pl-k">=</span> <span class="pl-c1">QMDPSolver</span>(max_iterations<span class="pl-k">=</span><span class="pl-c1">20</span>,
                    belres<span class="pl-k">=</span><span class="pl-c1">1e-3</span>,
                    verbose<span class="pl-k">=</span><span class="pl-c1">true</span>
                   ) 

<span class="pl-c"><span class="pl-c">#</span> run the solver</span>
policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, pomdp)</pre></div>
<p>To compute optimal action, define a belief with the <a href="http://juliapomdp.github.io/POMDPs.jl/latest/interfaces.html#Distributions-1" rel="nofollow">distribution interface</a>, or use the DiscreteBelief provided in <a href="https://github.com/JuliaPOMDP/BeliefUpdaters.jl">BeliefUpdaters</a>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> BeliefUpdaters
b <span class="pl-k">=</span> <span class="pl-c1">uniform_belief</span>(pomdp) <span class="pl-c"><span class="pl-c">#</span> initialize to a uniform belief</span>
a <span class="pl-k">=</span> <span class="pl-c1">action</span>(policy, b)</pre></div>
<p>In order to use the efficient <code>SparseValueIterationSolver</code> from <a href="https://github.com/JuliaPOMDP/DiscreteValueIteration.jl">DiscreteValueIteration.jl</a>, you can directly pass the solver to the <code>QMDPSolver</code> constructor as follows:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> QMDP, DiscreteValueIteration
pomdp <span class="pl-k">=</span> <span class="pl-c1">MyPOMDP</span>()

solver <span class="pl-k">=</span> <span class="pl-c1">QMDPSolver</span>(<span class="pl-c1">SparseValueIterationSolver</span>(max_iterations<span class="pl-k">=</span><span class="pl-c1">20</span>, verbose<span class="pl-k">=</span><span class="pl-c1">true</span>))

policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(solver, pomdp)</pre></div>
</article></div>