<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://github.com/JuliaPOMDP/GlobalApproximationValueIteration.jl"><img src="https://github.com/JuliaPOMDP/GlobalApproximationValueIteration.jl/actions/workflows/CI.yml/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaPOMDP/GlobalApproximationValueIteration.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/b6a1188733433fffd0b969800b1a21e79533b55fc19e9f1b526bdc6c04a2c2a5/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961504f4d44502f476c6f62616c417070726f78696d6174696f6e56616c7565497465726174696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage Status" data-canonical-src="https://codecov.io/gh/JuliaPOMDP/GlobalApproximationValueIteration.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-globalapproximationvalueiterationjl" class="anchor" aria-hidden="true" href="#globalapproximationvalueiterationjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GlobalApproximationValueIteration.jl</h1>
<p dir="auto">This package implements the Global Approximation Value Iteration algorithm in Julia for solving
Markov Decision Processes (MDPs) with global function approximation.
It is functionally very similar to the previously released
<a href="https://github.com/JuliaPOMDP/LocalApproximationValueIteration.jl">LocalApproximationValueIteration.jl</a>
and interested users can refer to its README for more details.
The user should define the POMDP problem according to the API in
<a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a>. Examples of problem definitions can be found in
<a href="https://github.com/JuliaPOMDP/POMDPModels.jl">POMDPModels.jl</a>.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">You need to have <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a> already and the JuliaPOMDP registry added (see the README of POMDPs.jl).
Thereafter, you can add GlobalApproximationValueIteration from the package manager</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;GlobalApproximationValueIteration&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>GlobalApproximationValueIteration<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it Works</h2>
<p dir="auto">This solver is one example of <em>Approximate Dynamic Programming</em>, which tries to find approximately optimal
value functions and policies for large or continuous state spaces.
As the name suggests, global approximation value iteration tries to approximate the value function over the
entire state space using a compact representation. The quality of the approximation varies with the
kind of function approximation scheme used; this repository can accommodate both linear (with feature vectors) and nonlinear
schemes. Please see <strong>Section 4.5.1</strong> of the book <a href="https://dl.acm.org/citation.cfm?id=2815660" rel="nofollow">Decision Making Under Uncertainty : Theory and Application</a>
and <strong>Chapter 3</strong> of <a href="https://books.google.co.in/books?hl=en&amp;lr=&amp;id=2J8_-O4-ABIC&amp;oi=fnd&amp;pg=PT8&amp;dq=markov+decision+processes+in+AI&amp;ots=mcxpyqiv0X&amp;sig=w-gF6nzm3JxgutcslIbUDD0dAXY" rel="nofollow">Markov Decision Processes in Artificial Intelligence</a> for more.</p>
<h2 dir="auto"><a id="user-content-state-space-representation" class="anchor" aria-hidden="true" href="#state-space-representation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>State Space Representation</h2>
<p dir="auto">The Global Approximation solver needs two things in particular from the state space of the MDP. First, it should be able to sample a state
from the state space (whether discrete or continuous). During value iteration, in each step, the solver will sample several states, estimate the value
at them and try to fit the approximation scheme.
Second, a state instance should be representable as a <em>feature vector</em> which will be used for linear or non-linear function approximation.
In the default case, the <code>feature</code> can just be the vector encoding of the state (see <em>State Space Representation</em> in the <a href="https://github.com/JuliaPOMDP/LocalApproximationValueIteration.jl">README</a>
of LocalApproximationValueIteration.jl for more on this).</p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Please refer to the <a href="https://github.com/JuliaPOMDP/LocalApproximationValueIteration.jl">README</a>
of LocalApproximationValueIteration.jl as the usage of the global variant is very similar to that one.
A simple example is also provided in the <code>test/</code> folder for each of linear and nonliner function approximation.
<code>POMDPs.jl</code> has a macro <code>@requirements_info</code> that determines the functions necessary to use some solver on some specific MDP model.
Other than the typical methods required for approximate value iteration and state space representation mentioned above,
the solver also requires a <code>GlobalFunctionApproximator</code> object (see <code>src/global_function_approximation.jl</code> for details
on the interface). We have also implemented two examplar approximations, linear and non-linear.
The following code snippet from <code>test/test_with_linear_gfa.jl</code> is the most relevant chunk of code
for using the solver correctly.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Create the MDP for a typical grid world
mdp = SimpleGridWorld(size=(SIZE_X, SIZE_Y), rewards=rewards)

# Create the linear function approximation with 10 weight parameters, initialized to zero
lin_gfa = LinearGlobalFunctionApproximator(zeros(10))

# Initialize the global approximation solver with the linear approximator and solve the MDP to obtain the policy
gfa_solver = GlobalApproximationValueIterationSolver(lin_gfa, num_samples=NUM_SAMPLES, max_iterations=MAX_ITERS, verbose=true, fv_type=SVector{10, Float64})
gfa_policy = solve(gfa_solver, mdp)"><pre><span class="pl-c"><span class="pl-c">#</span> Create the MDP for a typical grid world</span>
mdp <span class="pl-k">=</span> <span class="pl-c1">SimpleGridWorld</span>(size<span class="pl-k">=</span>(SIZE_X, SIZE_Y), rewards<span class="pl-k">=</span>rewards)

<span class="pl-c"><span class="pl-c">#</span> Create the linear function approximation with 10 weight parameters, initialized to zero</span>
lin_gfa <span class="pl-k">=</span> <span class="pl-c1">LinearGlobalFunctionApproximator</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">10</span>))

<span class="pl-c"><span class="pl-c">#</span> Initialize the global approximation solver with the linear approximator and solve the MDP to obtain the policy</span>
gfa_solver <span class="pl-k">=</span> <span class="pl-c1">GlobalApproximationValueIterationSolver</span>(lin_gfa, num_samples<span class="pl-k">=</span>NUM_SAMPLES, max_iterations<span class="pl-k">=</span>MAX_ITERS, verbose<span class="pl-k">=</span><span class="pl-c1">true</span>, fv_type<span class="pl-k">=</span>SVector{<span class="pl-c1">10</span>, Float64})
gfa_policy <span class="pl-k">=</span> <span class="pl-c1">solve</span>(gfa_solver, mdp)</pre></div>
</article></div>