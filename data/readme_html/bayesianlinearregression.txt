<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-bayesianlinearregression" class="anchor" aria-hidden="true" href="#bayesianlinearregression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BayesianLinearRegression</h1>
<p dir="auto"><a href="https://travis-ci.com/cscherrer/BayesianLinearRegression.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/68bee3b835dd1e754eee95432a074ab615bbd06c628ae93ab267abfcfb7522cb/68747470733a2f2f7472617669732d63692e636f6d2f6373636865727265722f426179657369616e4c696e65617252656772657373696f6e2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/cscherrer/BayesianLinearRegression.jl.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/cscherrer/BayesianLinearRegression.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6ee1bbfb845fef04e37b91adeff1bc46e86950f5b63496548ea56711fa27a8ea/68747470733a2f2f636f6465636f762e696f2f67682f6373636865727265722f426179657369616e4c696e65617252656772657373696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/cscherrer/BayesianLinearRegression.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">This library implements Bayesian Linear Regression, as described in <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="nofollow">Pattern Recognition and Machine Learning</a> by <a href="https://www.microsoft.com/en-us/research/people/cmbishop/" rel="nofollow">Chris Bishop</a>.</p>
<p dir="auto">The idea is that if the weights are given iid normal prior distributions and the noise of the response vector is known (also iid normal), the posterior can be found easily in closed form. This leaves the question of how these values should be determined, and Bishop's suggestion is to use <a href="http://en.wikipedia.org/wiki/Marginal_likelihood" rel="nofollow">marginal likelihood</a>.</p>
<p dir="auto">The result of fitting such a model is a form that yields a posterior predictive distribution in closed form, and model evidence that can easily be used for model selection.</p>
<h2 dir="auto"><a id="user-content-background" class="anchor" aria-hidden="true" href="#background"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Background</h2>
<p dir="auto">This package fits the linear regression model</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1184449/79926227-34896d00-83f1-11ea-826f-b461530ffbd6.png"><img src="https://user-images.githubusercontent.com/1184449/79926227-34896d00-83f1-11ea-826f-b461530ffbd6.png" height="100" style="max-width: 100%;"></a></p>
<p dir="auto">Rather than finding “the” value for w, we take a Bayesian approach and find the posterior distribution, given by</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1184449/79926304-613d8480-83f1-11ea-9f33-644cea477a29.png"><img src="https://user-images.githubusercontent.com/1184449/79926304-613d8480-83f1-11ea-9f33-644cea477a29.png" height="40" style="max-width: 100%;"></a></p>
<p dir="auto">where</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1184449/79926327-76b2ae80-83f1-11ea-81a2-b66f7b3c6863.png"><img src="https://user-images.githubusercontent.com/1184449/79926327-76b2ae80-83f1-11ea-81a2-b66f7b3c6863.png" height="100" style="max-width: 100%;"></a></p>
<p dir="auto">All of the above depends on fixed values for α and β being specified in adviance. Alternatively, a “full Bayesian” approach would specify prior distributions over these, and work in terms of their posterior distribution for the final result.</p>
<p dir="auto">Marginal likelihood finds a middle ground between these two approaches, and determines values for the \alpha and \beta hyperparameters by maximizing</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1184449/79926398-acf02e00-83f1-11ea-9c3d-15c59fb33589.png"><img src="https://user-images.githubusercontent.com/1184449/79926398-acf02e00-83f1-11ea-9c3d-15c59fb33589.png" height="100" style="max-width: 100%;"></a></p>
<p dir="auto">This reduces to</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1184449/79926422-bed1d100-83f1-11ea-9e8f-02be03d8ed7a.png"><img src="https://user-images.githubusercontent.com/1184449/79926422-bed1d100-83f1-11ea-9e8f-02be03d8ed7a.png" style="max-width: 100%;"></a></p>
<p dir="auto">This package maximizes the marginal likelihood using the approach described in Bishop (2006), which cycles through</p>
<ol dir="auto">
<li>
<p dir="auto">Update α and β</p>
</li>
<li>
<p dir="auto">Update H</p>
</li>
<li>
<p dir="auto">Update μ</p>
</li>
</ol>
<p dir="auto">For details, see</p>
<p dir="auto">Bishop, C. M. (2006). <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/" rel="nofollow">Pattern Recognition and Machine Learning</a> (M. Jordan, J. Kleinberg, &amp; B. Schölkopf (eds.); Vol. 53, Issue 9). Springer. Available online: <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="nofollow">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<p dir="auto">First let's generate some fake data</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using BayesianLinearRegression
n = 20
p = 7;
X = randn(n, p);
β = randn(p);
y = X * β + randn(n);"><pre><span class="pl-k">using</span> BayesianLinearRegression
n <span class="pl-k">=</span> <span class="pl-c1">20</span>
p <span class="pl-k">=</span> <span class="pl-c1">7</span>;
X <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n, p);
β <span class="pl-k">=</span> <span class="pl-c1">randn</span>(p);
y <span class="pl-k">=</span> X <span class="pl-k">*</span> β <span class="pl-k">+</span> <span class="pl-c1">randn</span>(n);</pre></div>
<p dir="auto">Next we instantiate and fit the model. <code>fit!</code> takes a <code>callback</code> keyword, which can be used to control stopping criteria, output some function of the model, or even change the model at each iteration.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; m = fit!(BayesianLinReg(X,y);
           callback = 
               # fixedEvidence()
               # stopAfter(2)
               stopAtIteration(50)
       );"><pre>julia<span class="pl-k">&gt;</span> m <span class="pl-k">=</span> <span class="pl-c1">fit!</span>(<span class="pl-c1">BayesianLinReg</span>(X,y);
           callback <span class="pl-k">=</span> 
               <span class="pl-c"><span class="pl-c">#</span> fixedEvidence()</span>
               <span class="pl-c"><span class="pl-c">#</span> stopAfter(2)</span>
               <span class="pl-c1">stopAtIteration</span>(<span class="pl-c1">50</span>)
       );</pre></div>
<p dir="auto">Once the model is fit, we can ask for the log-evidence (same as the marginal likelihood):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; logEvidence(m)
-37.98301162730431"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">logEvidence</span>(m)
<span class="pl-k">-</span><span class="pl-c1">37.98301162730431</span></pre></div>
<p dir="auto">This can be used as a criterion for model selection; adding some junk features tends to reduce the log-evidence:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; logEvidence(fit!(BayesianLinReg([X randn(n,3)],y)))
-41.961261348171064"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">logEvidence</span>(<span class="pl-c1">fit!</span>(<span class="pl-c1">BayesianLinReg</span>([X <span class="pl-c1">randn</span>(n,<span class="pl-c1">3</span>)],y)))
<span class="pl-k">-</span><span class="pl-c1">41.961261348171064</span></pre></div>
<p dir="auto">We can also query the values determined for the prior and the noise scales:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; priorScale(m)
1.2784615163925537

julia&gt; noiseScale(m)
0.8520928089955583"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">priorScale</span>(m)
<span class="pl-c1">1.2784615163925537</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">noiseScale</span>(m)
<span class="pl-c1">0.8520928089955583</span></pre></div>
<p dir="auto">The <code>posteriorWeights</code> include uncertainty, thanks to Measurements.jl:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; posteriorWeights(m)
7-element Array{Measurements.Measurement{Float64},1}:
 -0.35 ± 0.19
  0.07 ± 0.21
 -2.21 ± 0.29
  0.57 ± 0.22
  1.08 ± 0.18
 -2.14 ± 0.24
 -0.15 ± 0.25"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">posteriorWeights</span>(m)
<span class="pl-c1">7</span><span class="pl-k">-</span>element Array{Measurements<span class="pl-k">.</span>Measurement{Float64},<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-k">-</span><span class="pl-c1">0.35</span> ± <span class="pl-c1">0.19</span>
  <span class="pl-c1">0.07</span> ± <span class="pl-c1">0.21</span>
 <span class="pl-k">-</span><span class="pl-c1">2.21</span> ± <span class="pl-c1">0.29</span>
  <span class="pl-c1">0.57</span> ± <span class="pl-c1">0.22</span>
  <span class="pl-c1">1.08</span> ± <span class="pl-c1">0.18</span>
 <span class="pl-k">-</span><span class="pl-c1">2.14</span> ± <span class="pl-c1">0.24</span>
 <span class="pl-k">-</span><span class="pl-c1">0.15</span> ± <span class="pl-c1">0.25</span></pre></div>
<p dir="auto">This uncertainty is propagated for prediciton:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; predict(m,X)
20-element Array{Measurements.Measurement{Float64},1}:
   5.6 ± 1.0
  3.06 ± 0.95
 -3.14 ± 0.98
   3.3 ± 1.0
   4.6 ± 1.1
  -0.2 ± 1.0
 -0.91 ± 0.96
 -3.71 ± 0.99
  -4.0 ± 1.0
 -0.86 ± 0.95
 -5.69 ± 0.95
  4.32 ± 0.94
 -3.49 ± 0.94
  -0.7 ± 1.0
   0.5 ± 1.1
 -0.49 ± 0.92
  0.67 ± 0.91
  0.39 ± 0.95
  -7.3 ± 1.1
  0.11 ± 0.98"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">predict</span>(m,X)
<span class="pl-c1">20</span><span class="pl-k">-</span>element Array{Measurements<span class="pl-k">.</span>Measurement{Float64},<span class="pl-c1">1</span>}<span class="pl-k">:</span>
   <span class="pl-c1">5.6</span> ± <span class="pl-c1">1.0</span>
  <span class="pl-c1">3.06</span> ± <span class="pl-c1">0.95</span>
 <span class="pl-k">-</span><span class="pl-c1">3.14</span> ± <span class="pl-c1">0.98</span>
   <span class="pl-c1">3.3</span> ± <span class="pl-c1">1.0</span>
   <span class="pl-c1">4.6</span> ± <span class="pl-c1">1.1</span>
  <span class="pl-k">-</span><span class="pl-c1">0.2</span> ± <span class="pl-c1">1.0</span>
 <span class="pl-k">-</span><span class="pl-c1">0.91</span> ± <span class="pl-c1">0.96</span>
 <span class="pl-k">-</span><span class="pl-c1">3.71</span> ± <span class="pl-c1">0.99</span>
  <span class="pl-k">-</span><span class="pl-c1">4.0</span> ± <span class="pl-c1">1.0</span>
 <span class="pl-k">-</span><span class="pl-c1">0.86</span> ± <span class="pl-c1">0.95</span>
 <span class="pl-k">-</span><span class="pl-c1">5.69</span> ± <span class="pl-c1">0.95</span>
  <span class="pl-c1">4.32</span> ± <span class="pl-c1">0.94</span>
 <span class="pl-k">-</span><span class="pl-c1">3.49</span> ± <span class="pl-c1">0.94</span>
  <span class="pl-k">-</span><span class="pl-c1">0.7</span> ± <span class="pl-c1">1.0</span>
   <span class="pl-c1">0.5</span> ± <span class="pl-c1">1.1</span>
 <span class="pl-k">-</span><span class="pl-c1">0.49</span> ± <span class="pl-c1">0.92</span>
  <span class="pl-c1">0.67</span> ± <span class="pl-c1">0.91</span>
  <span class="pl-c1">0.39</span> ± <span class="pl-c1">0.95</span>
  <span class="pl-k">-</span><span class="pl-c1">7.3</span> ± <span class="pl-c1">1.1</span>
  <span class="pl-c1">0.11</span> ± <span class="pl-c1">0.98</span></pre></div>
<p dir="auto">Finally, we can output the effective number of parameters, which is useful for some computations:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; effectiveNumParameters(m)
6.776655463465779"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">effectiveNumParameters</span>(m)
<span class="pl-c1">6.776655463465779</span></pre></div>
</article></div>