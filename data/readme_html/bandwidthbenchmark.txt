<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-bandwidthbenchmarkjl" class="anchor" aria-hidden="true" href="#bandwidthbenchmarkjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BandwidthBenchmark.jl</h1>
<p dir="auto"><a href="https://git.uni-paderborn.de/pc2-ci/julia/BandwidthBenchmark.jl/-/pipelines" rel="nofollow"><img src="https://camo.githubusercontent.com/c964cadd2d9e84f1939a1f0229fb01ae4a942d5041c3e888684d04ffedee7952/68747470733a2f2f6769742e756e692d7061646572626f726e2e64652f7063322d63692f6a756c69612f42616e64776964746842656e63686d61726b2e6a6c2f6261646765732f6d61696e2f706970656c696e652e7376673f6b65795f746578743d434940504332" alt="CI@PC2" data-canonical-src="https://git.uni-paderborn.de/pc2-ci/julia/BandwidthBenchmark.jl/badges/main/pipeline.svg?key_text=CI@PC2" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaPerf/BandwidthBenchmark.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c2bb0db61f41752da54ce1b3fc87cd9f3414b6a2ff8e1da69425172cdd6e6ad9/68747470733a2f2f636f6465636f762e696f2f67682f4a756c6961506572662f42616e64776964746842656e63686d61726b2e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d6b39684970737558656e" alt="codecov" data-canonical-src="https://codecov.io/gh/JuliaPerf/BandwidthBenchmark.jl/branch/main/graph/badge.svg?token=k9hIpsuXen" style="max-width: 100%;"></a></p>
<p dir="auto"><em>Measuring memory bandwidth using streaming kernels</em></p>
<p dir="auto"><a href="https://github.com/RRZE-HPC/TheBandwidthBenchmark">TheBandwidthBenchmark</a> as a Julia package.</p>
<p dir="auto">Citing from the <a href="https://github.com/RRZE-HPC/TheBandwidthBenchmark">original source</a>:</p>
<blockquote>
<p dir="auto">It contains the following streaming kernels with corresponding data access pattern (Notation: S - store, L - load, WA - write allocate). All variables are vectors, s is a scalar:</p>
<ul dir="auto">
<li>init (S1, WA): Initilize an array: <code>a = s</code>. Store only.</li>
<li>sum (L1): Vector reduction: <code>s += a</code>. Load only.</li>
<li>copy (L1, S1, WA): Classic memcopy: <code>a = b</code>.</li>
<li>update (L1, S1): Update vector: <code>a = a * scalar</code>. Also load + store but <strong>without write allocate</strong>.</li>
<li>triad (L2, S1, WA): Stream triad: <code>a = b + c * scalar</code>.</li>
<li>daxpy (L2, S1): Daxpy (<strong>without write allocate</strong>): <code>a = a + b * scalar</code>.</li>
<li>striad (L3, S1, WA): Schoenauer triad: <code>a = b + c * d</code>.</li>
<li>sdaxpy (L3, S1): Schoenauer triad <strong>without write allocate</strong>: <code>a = a + b * c</code>.</li>
</ul>
</blockquote>
<p dir="auto">Note, that we do not (yet) include the sum reduction in this Julia package.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto"><code>] add BandwidthBenchmark</code></p>
<h2 dir="auto"><a id="user-content-example-benchmarks" class="anchor" aria-hidden="true" href="#example-benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example Benchmarks</h2>
<p dir="auto">All benchmarks:</p>
<ul dir="auto">
<li><a href="https://hpc.fau.de/systems-services/systems-documentation-instructions/clusters/emmy-cluster/" rel="nofollow">Emmy</a> @ <a href="https://hpc.fau.de/" rel="nofollow">FAU</a>: <a href="https://github.com/carstenbauer/BandwidthBenchmark.jl/tree/main/benchmarks/emmy_fau">benchmarks/emmy_fau</a></li>
<li><a href="https://pc2.uni-paderborn.de/hpc-services/available-systems/noctua1/" rel="nofollow">Noctua1</a> @ <a href="https://pc2.uni-paderborn.de/" rel="nofollow">PC²</a>: <a href="https://github.com/carstenbauer/BandwidthBenchmark.jl/tree/main/benchmarks/noctua_pc2/bwbench/">benchmarks/noctua_pc2</a></li>
<li><a href="https://pc2.uni-paderborn.de/hpc-services/available-systems/noctua1/" rel="nofollow">Noctua2</a> @ <a href="https://pc2.uni-paderborn.de/" rel="nofollow">PC²</a>: <a href="https://github.com/carstenbauer/BandwidthBenchmark.jl/tree/main/benchmarks/noctua2_pc2/bwbench/">benchmarks/noctua2_pc2</a></li>
</ul>
<p dir="auto">The benchmarks showcased in this README have been conducted on the <a href="https://hpc.fau.de/systems-services/systems-documentation-instructions/clusters/emmy-cluster/" rel="nofollow">Emmy cluster at NHR@FAU</a> because we could fix the CPU frequencies on this system.</p>
<h2 dir="auto"><a id="user-content-bwbench" class="anchor" aria-hidden="true" href="#bwbench"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>bwbench</code></h2>
<p dir="auto"><strong>Keyword arguments:</strong></p>
<ul dir="auto">
<li><code>N</code> (default: <code>120_000_000</code>): length of vectors</li>
<li><code>nthreads</code> (default: <code>Threads.nthreads()</code>): number of Julia threads to use</li>
<li><code>niter</code> (default: <code>10</code>): # of times we repeat the measurement</li>
<li><code>alignment</code> (default: <code>64</code>): array alignment</li>
<li><code>verbose</code> (default: <code>false</code>): print result table + thread information etc.</li>
<li><code>write_allocate</code> (default: <code>false</code>): include write allocate compensation factors</li>
</ul>
<p dir="auto">It is highly recommended(!) to pin the Julia threads to specific cores (according to the architecture at hand). The simplest way is probably to set <code>JULIA_EXCLUSIVE=1</code>, which will pin Julia threads to the first <code>N</code> cores of the system. For more specific pinning, <a href="https://github.com/carstenbauer/ThreadPinning.jl">ThreadPinning.jl</a>, <a href="https://github.com/JuliaPerf/LIKWID.jl">LIKIWD.jl</a>, or other tools like <code>numactl</code> may be useful.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using BandwidthBenchmark

julia&gt; bwbench(; verbose=true);
Threading enabled, using 8 (of 8) Julia threads
Total allocated datasize: 3840.0 MB
	Thread 6 running on core 5.
	Thread 5 running on core 4.
	Thread 2 running on core 1.
	Thread 3 running on core 2.
	Thread 7 running on core 6.
	Thread 8 running on core 7.
	Thread 1 running on core 0.
	Thread 4 running on core 3.
┌──────────┬─────────────┬────────────────┬───────────┬───────────┬───────────┐
│ Function │ Rate (MB/s) │ Rate (MFlop/s) │  Avg time │  Min time │  Max time │
│   String │     Float64 │        Float64 │   Float64 │   Float64 │   Float64 │
├──────────┼─────────────┼────────────────┼───────────┼───────────┼───────────┤
│     Init │     18972.0 │            0.0 │ 0.0508247 │ 0.0506008 │ 0.0510763 │
│     Copy │     27137.6 │            0.0 │ 0.0710205 │ 0.0707507 │ 0.0715388 │
│   Update │     37939.9 │        2371.24 │ 0.0509081 │ 0.0506063 │ 0.0512734 │
│    Triad │     30753.6 │         2562.8 │ 0.0943514 │ 0.0936477 │  0.094945 │
│    Daxpy │     40548.7 │        3379.05 │  0.071467 │ 0.0710258 │ 0.0719743 │
│   STriad │     33344.3 │        2084.02 │  0.115823 │  0.115162 │   0.11737 │
│   SDaxpy │     41370.0 │        2585.62 │ 0.0935033 │ 0.0928209 │ 0.0941559 │
└──────────┴─────────────┴────────────────┴───────────┴───────────┴───────────┘"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BandwidthBenchmark

julia<span class="pl-k">&gt;</span> <span class="pl-c1">bwbench</span>(; verbose<span class="pl-k">=</span><span class="pl-c1">true</span>);
Threading enabled, <span class="pl-k">using</span> <span class="pl-c1">8</span> (of <span class="pl-c1">8</span>) Julia threads
Total allocated datasize<span class="pl-k">:</span> <span class="pl-c1">3840.0</span> MB
	Thread <span class="pl-c1">6</span> running on core <span class="pl-c1">5.</span>
	Thread <span class="pl-c1">5</span> running on core <span class="pl-c1">4.</span>
	Thread <span class="pl-c1">2</span> running on core <span class="pl-c1">1.</span>
	Thread <span class="pl-c1">3</span> running on core <span class="pl-c1">2.</span>
	Thread <span class="pl-c1">7</span> running on core <span class="pl-c1">6.</span>
	Thread <span class="pl-c1">8</span> running on core <span class="pl-c1">7.</span>
	Thread <span class="pl-c1">1</span> running on core <span class="pl-c1">0.</span>
	Thread <span class="pl-c1">4</span> running on core <span class="pl-c1">3.</span>
┌──────────┬─────────────┬────────────────┬───────────┬───────────┬───────────┐
│ Function │ Rate (MB<span class="pl-k">/</span>s) │ Rate (MFlop<span class="pl-k">/</span>s) │  Avg time │  Min time │  Max time │
│   String │     Float64 │        Float64 │   Float64 │   Float64 │   Float64 │
├──────────┼─────────────┼────────────────┼───────────┼───────────┼───────────┤
│     Init │     <span class="pl-c1">18972.0</span> │            <span class="pl-c1">0.0</span> │ <span class="pl-c1">0.0508247</span> │ <span class="pl-c1">0.0506008</span> │ <span class="pl-c1">0.0510763</span> │
│     Copy │     <span class="pl-c1">27137.6</span> │            <span class="pl-c1">0.0</span> │ <span class="pl-c1">0.0710205</span> │ <span class="pl-c1">0.0707507</span> │ <span class="pl-c1">0.0715388</span> │
│   Update │     <span class="pl-c1">37939.9</span> │        <span class="pl-c1">2371.24</span> │ <span class="pl-c1">0.0509081</span> │ <span class="pl-c1">0.0506063</span> │ <span class="pl-c1">0.0512734</span> │
│    Triad │     <span class="pl-c1">30753.6</span> │         <span class="pl-c1">2562.8</span> │ <span class="pl-c1">0.0943514</span> │ <span class="pl-c1">0.0936477</span> │  <span class="pl-c1">0.094945</span> │
│    Daxpy │     <span class="pl-c1">40548.7</span> │        <span class="pl-c1">3379.05</span> │  <span class="pl-c1">0.071467</span> │ <span class="pl-c1">0.0710258</span> │ <span class="pl-c1">0.0719743</span> │
│   STriad │     <span class="pl-c1">33344.3</span> │        <span class="pl-c1">2084.02</span> │  <span class="pl-c1">0.115823</span> │  <span class="pl-c1">0.115162</span> │   <span class="pl-c1">0.11737</span> │
│   SDaxpy │     <span class="pl-c1">41370.0</span> │        <span class="pl-c1">2585.62</span> │ <span class="pl-c1">0.0935033</span> │ <span class="pl-c1">0.0928209</span> │ <span class="pl-c1">0.0941559</span> │
└──────────┴─────────────┴────────────────┴───────────┴───────────┴───────────┘</pre></div>
<h3 dir="auto"><a id="user-content-likwid--write-allocate" class="anchor" aria-hidden="true" href="#likwid--write-allocate"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LIKWID &amp; Write Allocate</h3>
<p dir="auto">When LIKWID.jl is loaded, <code>bwbench</code> will automatically try to use LIKWIDs Marker API! Running e.g.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="JULIA_EXCLUSIVE=1 likwid-perfctr -c 0-7 -g MEM_DP -m julia --project=. --math-mode=fast -t8 bwbench_likwid.jl"><pre class="notranslate"><code>JULIA_EXCLUSIVE=1 likwid-perfctr -c 0-7 -g MEM_DP -m julia --project=. --math-mode=fast -t8 bwbench_likwid.jl
</code></pre></div>
<p dir="auto">one gets detailed information from hardware-performance counters: <a href="https://github.com/carstenbauer/BandwidthBenchmark.jl/blob/main/benchmarks/emmy_fau/likwid/run_bwbench_likwid.out">example output</a>. Among other things, we can use these values to check / prove that write allocates have happened. Inspecting the memory bandwith associated with read and write in the STRIAD region, extracted here for convenience,</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="Memory read bandwidth [MBytes/s]  | 31721.7327
Memory write bandwidth [MBytes/s] |  8014.1479"><pre class="notranslate"><code>Memory read bandwidth [MBytes/s]  | 31721.7327
Memory write bandwidth [MBytes/s] |  8014.1479
</code></pre></div>
<p dir="auto">we see that <code>31721.7327 / 8014.1479 ≈ 4</code> times more reads have happened than writes. Naively, one would expect 3 loads and 1 store, i.e. a factor of 3 instead of 4 for the Schönauer Triad <code>a[i] = b[i] + c[i] * d[i]</code>. The additional load is due to the write allocate (<code>a</code> must be loaded before it can be written to).</p>
<p dir="auto">If we know that write allocates happen (as is usually the case), we can pass <code>write_allocate=true</code> to <code>bwbench</code> to account for the extra loads. In this case, we obtain the following table</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="┌──────────┬─────────────┬────────────────┬───────────┬───────────┬───────────┐
│ Function │ Rate (MB/s) │ Rate (MFlop/s) │  Avg time │  Min time │  Max time │
│   String │     Float64 │        Float64 │   Float64 │   Float64 │   Float64 │
├──────────┼─────────────┼────────────────┼───────────┼───────────┼───────────┤
│     Init │     38150.7 │            0.0 │ 0.0506281 │ 0.0503267 │ 0.0508563 │
│     Copy │     40560.1 │            0.0 │ 0.0712144 │ 0.0710057 │ 0.0714202 │
│   Update │     37862.3 │        2366.39 │ 0.0512318 │ 0.0507101 │   0.05211 │
│    Triad │     40979.8 │        2561.24 │ 0.0945374 │ 0.0937046 │ 0.0949424 │
│    Daxpy │     40347.0 │        3362.25 │ 0.0719729 │ 0.0713808 │   0.07248 │
│   STriad │     41754.5 │        2087.73 │  0.115761 │  0.114958 │  0.117923 │
│   SDaxpy │     41276.1 │        2579.75 │  0.093708 │ 0.0930321 │ 0.0949028 │
└──────────┴─────────────┴────────────────┴───────────┴───────────┴───────────┘"><pre>┌──────────┬─────────────┬────────────────┬───────────┬───────────┬───────────┐
│ Function │ Rate (MB<span class="pl-k">/</span>s) │ Rate (MFlop<span class="pl-k">/</span>s) │  Avg time │  Min time │  Max time │
│   String │     Float64 │        Float64 │   Float64 │   Float64 │   Float64 │
├──────────┼─────────────┼────────────────┼───────────┼───────────┼───────────┤
│     Init │     <span class="pl-c1">38150.7</span> │            <span class="pl-c1">0.0</span> │ <span class="pl-c1">0.0506281</span> │ <span class="pl-c1">0.0503267</span> │ <span class="pl-c1">0.0508563</span> │
│     Copy │     <span class="pl-c1">40560.1</span> │            <span class="pl-c1">0.0</span> │ <span class="pl-c1">0.0712144</span> │ <span class="pl-c1">0.0710057</span> │ <span class="pl-c1">0.0714202</span> │
│   Update │     <span class="pl-c1">37862.3</span> │        <span class="pl-c1">2366.39</span> │ <span class="pl-c1">0.0512318</span> │ <span class="pl-c1">0.0507101</span> │   <span class="pl-c1">0.05211</span> │
│    Triad │     <span class="pl-c1">40979.8</span> │        <span class="pl-c1">2561.24</span> │ <span class="pl-c1">0.0945374</span> │ <span class="pl-c1">0.0937046</span> │ <span class="pl-c1">0.0949424</span> │
│    Daxpy │     <span class="pl-c1">40347.0</span> │        <span class="pl-c1">3362.25</span> │ <span class="pl-c1">0.0719729</span> │ <span class="pl-c1">0.0713808</span> │   <span class="pl-c1">0.07248</span> │
│   STriad │     <span class="pl-c1">41754.5</span> │        <span class="pl-c1">2087.73</span> │  <span class="pl-c1">0.115761</span> │  <span class="pl-c1">0.114958</span> │  <span class="pl-c1">0.117923</span> │
│   SDaxpy │     <span class="pl-c1">41276.1</span> │        <span class="pl-c1">2579.75</span> │  <span class="pl-c1">0.093708</span> │ <span class="pl-c1">0.0930321</span> │ <span class="pl-c1">0.0949028</span> │
└──────────┴─────────────┴────────────────┴───────────┴───────────┴───────────┘</pre></div>
<p dir="auto">Note that there are no write allocates for <code>Update</code>, <code>Daxpy</code>, and <code>SDaxpy</code>.</p>
<h2 dir="auto"><a id="user-content-bwscaling" class="anchor" aria-hidden="true" href="#bwscaling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>bwscaling</code></h2>
<p dir="auto">Use <code>bwscaling()</code> to measure the memory bandwidth for an increasing number of threads (<code>1:max_nthreads</code>).</p>
<p dir="auto"><strong>Keyword arguments:</strong></p>
<ul dir="auto">
<li><code>max_nthreads</code> (default: <code>Threads.nthreads()</code>): upper limit for the number of threads to be used</li>
</ul>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="1 Thread(s): SDaxpy Bandwidth (MB/s) is 13047.85
Threading enabled, using 2 (of 10) Julia threads
2 Thread(s): SDaxpy Bandwidth (MB/s) is 24023.27
Threading enabled, using 3 (of 10) Julia threads
3 Thread(s): SDaxpy Bandwidth (MB/s) is 31636.02
Threading enabled, using 4 (of 10) Julia threads
4 Thread(s): SDaxpy Bandwidth (MB/s) is 36304.15
Threading enabled, using 5 (of 10) Julia threads
5 Thread(s): SDaxpy Bandwidth (MB/s) is 38594.68
Threading enabled, using 6 (of 10) Julia threads
6 Thread(s): SDaxpy Bandwidth (MB/s) is 40556.33
Threading enabled, using 7 (of 10) Julia threads
7 Thread(s): SDaxpy Bandwidth (MB/s) is 40692.84
Threading enabled, using 8 (of 10) Julia threads
8 Thread(s): SDaxpy Bandwidth (MB/s) is 39040.98
Threading enabled, using 9 (of 10) Julia threads
9 Thread(s): SDaxpy Bandwidth (MB/s) is 39417.47
Threading enabled, using 10 (of 10) Julia threads
10 Thread(s): SDaxpy Bandwidth (MB/s) is 39913.76


Scaling results:
┌───────────┬─────────────────────────┐
│ # Threads │ SDaxpy Bandwidth (MB/s) │
├───────────┼─────────────────────────┤
│       1.0 │                 13047.8 │
│       2.0 │                 24023.3 │
│       3.0 │                 31636.0 │
│       4.0 │                 36304.1 │
│       5.0 │                 38594.7 │
│       6.0 │                 40556.3 │
│       7.0 │                 40692.8 │
│       8.0 │                 39041.0 │
│       9.0 │                 39417.5 │
│      10.0 │                 39913.8 │
└───────────┴─────────────────────────┘
                           Bandwidth Scaling             
              +----------------------------------------+ 
        50000 |                                        | 
              |                                        | 
              |                                        | 
              |                   ._.------__.   ._____| 
              |              ._-*&quot;'          '&quot;&quot;&quot;'     | 
              |           _*&quot;`                         | 
              |        .r/                             | 
   MB/s       |       ./                               | 
              |     ./`                                | 
              |    .'                                  | 
              |   .`                                   | 
              |  .`                                    | 
              | /`                                     | 
              |/                                       | 
        10000 |                                        | 
              +----------------------------------------+ 
               1                                     10  
                                # cores                  "><pre class="notranslate"><code>1 Thread(s): SDaxpy Bandwidth (MB/s) is 13047.85
Threading enabled, using 2 (of 10) Julia threads
2 Thread(s): SDaxpy Bandwidth (MB/s) is 24023.27
Threading enabled, using 3 (of 10) Julia threads
3 Thread(s): SDaxpy Bandwidth (MB/s) is 31636.02
Threading enabled, using 4 (of 10) Julia threads
4 Thread(s): SDaxpy Bandwidth (MB/s) is 36304.15
Threading enabled, using 5 (of 10) Julia threads
5 Thread(s): SDaxpy Bandwidth (MB/s) is 38594.68
Threading enabled, using 6 (of 10) Julia threads
6 Thread(s): SDaxpy Bandwidth (MB/s) is 40556.33
Threading enabled, using 7 (of 10) Julia threads
7 Thread(s): SDaxpy Bandwidth (MB/s) is 40692.84
Threading enabled, using 8 (of 10) Julia threads
8 Thread(s): SDaxpy Bandwidth (MB/s) is 39040.98
Threading enabled, using 9 (of 10) Julia threads
9 Thread(s): SDaxpy Bandwidth (MB/s) is 39417.47
Threading enabled, using 10 (of 10) Julia threads
10 Thread(s): SDaxpy Bandwidth (MB/s) is 39913.76


Scaling results:
┌───────────┬─────────────────────────┐
│ # Threads │ SDaxpy Bandwidth (MB/s) │
├───────────┼─────────────────────────┤
│       1.0 │                 13047.8 │
│       2.0 │                 24023.3 │
│       3.0 │                 31636.0 │
│       4.0 │                 36304.1 │
│       5.0 │                 38594.7 │
│       6.0 │                 40556.3 │
│       7.0 │                 40692.8 │
│       8.0 │                 39041.0 │
│       9.0 │                 39417.5 │
│      10.0 │                 39913.8 │
└───────────┴─────────────────────────┘
                           Bandwidth Scaling             
              +----------------------------------------+ 
        50000 |                                        | 
              |                                        | 
              |                                        | 
              |                   ._.------__.   ._____| 
              |              ._-*"'          '"""'     | 
              |           _*"`                         | 
              |        .r/                             | 
   MB/s       |       ./                               | 
              |     ./`                                | 
              |    .'                                  | 
              |   .`                                   | 
              |  .`                                    | 
              | /`                                     | 
              |/                                       | 
        10000 |                                        | 
              +----------------------------------------+ 
               1                                     10  
                                # cores                  
</code></pre></div>
<h2 dir="auto"><a id="user-content-bwscaling_memory_domains" class="anchor" aria-hidden="true" href="#bwscaling_memory_domains"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>bwscaling_memory_domains</code></h2>
<p dir="auto">Use <code>bwscaling_memory_domains()</code> (instead of <code>bwscaling</code> above) to measure the memory bandwidth across memory domains (NUMA). Returns a <code>DataFrame</code> in which each row contains the kernel name,
the number of threads per memory domain, the number of domains considered, and the
measured memory bandwidth (in MB/s).</p>
<p dir="auto"><strong>Keyword arguments:</strong></p>
<ul dir="auto">
<li><code>max_nnuma</code> (default: <code>ThreadPinning.nnuma()</code>): maximum number of memory domains to consider</li>
<li><code>max_nthreads</code> (default: <code>maximum(ThreadPinning.ncores_per_numa())</code>): maximum number of threads per memory domain to consider</li>
</ul>
<h2 dir="auto"><a id="user-content-flopsscaling" class="anchor" aria-hidden="true" href="#flopsscaling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>flopsscaling</code></h2>
<p dir="auto">Since we also estimate the MFlops/s for our streaming kernels, we can also investigate the scaling of the floating point performance for increasing number of threads (<code>1:max_nthreads</code>). The function <code>flopsscaling</code> does exactly that based on the Triad kernel.</p>
<p dir="auto"><strong>Keyword arguments:</strong></p>
<ul dir="auto">
<li><code>max_nthreads</code> (default: <code>Threads.nthreads()</code>): upper limit for the number of threads to be used</li>
</ul>
<h3 dir="auto"><a id="user-content-compact-vs-scattered-pinning" class="anchor" aria-hidden="true" href="#compact-vs-scattered-pinning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Compact vs Scattered Pinning:</h3>
<p dir="auto">Using <code>flopsscaling</code>, we can, for example, benchmark and compare the performance of different thread pinning scenarios:</p>
<ul dir="auto">
<li>Compact Pinning: fill physical cores chronologically, i.e. first socket first, second socket second.</li>
<li>Scattered Pinning: fill both sockets simultaneously, i.e. alternating between first and second socket</li>
</ul>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="                                Compact Pinning              
                  +----------------------------------------+ 
            10000 |                                        | 
                  |                                      ./| 
                  |                                    ./  | 
                  |                                ..-&quot;    | 
                  |                              .*'       | 
                  |                           .-/`         | 
                  |                         _*`            | 
   MFlops/s       |                      .r&quot;               | 
                  |           ..  .__._.-'                 | 
                  |        _-&quot;'&quot;&quot;&quot;`  `                     | 
                  |      ./                                | 
                  |     .`                                 | 
                  |    r`                                  | 
                  |   /                                    | 
             1000 |  /                                     | 
                  +----------------------------------------+ 
                   0                                     20  
                                    # cores            "><pre class="notranslate"><code>                                Compact Pinning              
                  +----------------------------------------+ 
            10000 |                                        | 
                  |                                      ./| 
                  |                                    ./  | 
                  |                                ..-"    | 
                  |                              .*'       | 
                  |                           .-/`         | 
                  |                         _*`            | 
   MFlops/s       |                      .r"               | 
                  |           ..  .__._.-'                 | 
                  |        _-"'"""`  `                     | 
                  |      ./                                | 
                  |     .`                                 | 
                  |    r`                                  | 
                  |   /                                    | 
             1000 |  /                                     | 
                  +----------------------------------------+ 
                   0                                     20  
                                    # cores            
</code></pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="                                Scattered Pinning              
                  +----------------------------------------+ 
            10000 |                                        | 
                  |                    .  .,  /\..*\. .\..r| 
                  |                   .'**`\..`     \/  &quot;' | 
                  |               ,\ .`     \`             | 
                  |               . \.                     | 
                  |            . ,`  `                     | 
                  |          ./'..                         | 
   MFlops/s       |         ./   `                         | 
                  |        ./                              | 
                  |       .`                               | 
                  |      .`                                | 
                  |     /`                                 | 
                  |    /                                   | 
                  |   /                                    | 
             1000 |  /                                     | 
                  +----------------------------------------+ 
                   0                                     20  
                                    # cores                  "><pre class="notranslate"><code>                                Scattered Pinning              
                  +----------------------------------------+ 
            10000 |                                        | 
                  |                    .  .,  /\..*\. .\..r| 
                  |                   .'**`\..`     \/  "' | 
                  |               ,\ .`     \`             | 
                  |               . \.                     | 
                  |            . ,`  `                     | 
                  |          ./'..                         | 
   MFlops/s       |         ./   `                         | 
                  |        ./                              | 
                  |       .`                               | 
                  |      .`                                | 
                  |     /`                                 | 
                  |    /                                   | 
                  |   /                                    | 
             1000 |  /                                     | 
                  +----------------------------------------+ 
                   0                                     20  
                                    # cores                  
</code></pre></div>
<p dir="auto">(For a discussion, see <a href="https://discourse.julialang.org/t/compact-vs-scattered-pinning/69722" rel="nofollow">https://discourse.julialang.org/t/compact-vs-scattered-pinning/69722</a>)</p>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<ul dir="auto">
<li><a href="https://github.com/RRZE-HPC/TheBandwidthBenchmark">TheBandwidthBenchmark</a> by RRZE-HPC Erlangen</li>
<li>Sister package <a href="https://github.com/JuliaPerf/STREAMBenchmark.jl">STREAMBenchmark.jl</a></li>
<li><a href="https://github.com/RRZE-HPC/likwid">LIKWID</a> and <a href="https://github.com/JuliaPerf/LIKWID.jl">LIKIWD.jl</a></li>
</ul>
</article></div>