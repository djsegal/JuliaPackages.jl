<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-adversarialjl" class="anchor" aria-hidden="true" href="#adversarialjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adversarial.jl</h1>
<p><a href="https://jaypmorgan.github.io/Adversarial.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></p>
<p>Adversarial attacks for Neural Networks written with FluxML.</p>
<p>Adversarial examples are inputs to Neural Networks (NNs) that result in a miss-classification. Through the exploitation that NNs are susceptible to slight changes (perturbations) of the input space, adversarial examples are often indistinguishable from the original input from the point of view of a human.</p>
<p>A common example of this phenomenon is from the use of the Fast Gradient Sign Method (FGSM) proposed by Goodfellow et al. 2014 <a href="https://arxiv.org/abs/1412.6572" rel="nofollow">https://arxiv.org/abs/1412.6572</a> where the gradient information of the network can be used to move the pixels of the image in the direction of gradient, and thereby increasing the loss for the resulting image. Despite the very small shift in pixels, this is enough for the NN to miss-classify the image: <a href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html" rel="nofollow">https://pytorch.org/tutorials/beginner/fgsm_tutorial.html</a></p>
<p>We have included some of the common methods to create adversarial examples, this includes:</p>
<ul>
<li>Fast Gradient Sign Method (FGSM)</li>
<li>Projected Gradient Descent (PGD)</li>
<li>Jacobian-based Saliency Map Attack (JSMA)</li>
<li>Carlini &amp; Wagner (CW)</li>
</ul>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>You can install this package through Julia's package manager in the REPL:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="] add Adversarial
"><pre>] add Adversarial</pre></div>
<p>or via a script:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg; Pkg.add(&quot;Adversarial&quot;)
"><pre><span class="pl-k">using</span> Pkg; Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Adversarial<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-quick-start-guide" class="anchor" aria-hidden="true" href="#quick-start-guide"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick Start Guide</h2>
<p>As an example, we can create an adversarial image using the FGSM method:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="x_adv = FGSM(model, loss, x, y; ϵ = 0.07)
"><pre>x_adv <span class="pl-k">=</span> <span class="pl-c1">FGSM</span>(model, loss, x, y; ϵ <span class="pl-k">=</span> <span class="pl-c1">0.07</span>)</pre></div>
<p>Where model is the FluxML model, loss is some loss function that uses a predict function, for example <code>crossentropy(model(x), y)</code>. x is the original input, y is the true class label, and \epsilon is a parameter that determines how much each pixel is changed by.</p>
<p>More indepth examples and uses of different methods can be found in the <a href="examples/markdown">examples folder</a></p>
</article></div>