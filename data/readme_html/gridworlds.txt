<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gridworlds" class="anchor" aria-hidden="true" href="#gridworlds"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GridWorlds</h1>
<p dir="auto">A package for creating grid world environments for reinforcement learning in Julia. This package is designed to be lightweight and fast.</p>
<p dir="auto">This package is inspired by <a href="https://github.com/maximecb/gym-minigrid">gym-minigrid</a>. In order to cite this package, please refer to the file <code>CITATION.bib</code>. Starring the repository on GitHub is also appreciated. For benchmarks, refer to <code>benchmarks/benchmarks.md</code>.</p>
<h2 dir="auto"><a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Table of contents:</h2>
<ul dir="auto">
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
<p dir="auto"><a href="#list-of-environments">List of Environments</a></p>
<ol dir="auto">
<li><a href="#singleroomundirected">SingleRoomUndirected</a></li>
<li><a href="#singleroomdirected">SingleRoomDirected</a></li>
<li><a href="#gridroomsundirected">GridRoomsUndirected</a></li>
<li><a href="#gridroomsdirected">GridRoomsDirected</a></li>
<li><a href="#sequentialroomsundirected">SequentialRoomsUndirected</a></li>
<li><a href="#sequentialroomsdirected">SequentialRoomsDirected</a></li>
<li><a href="#mazeundirected">MazeUndirected</a></li>
<li><a href="#mazedirected">MazeDirected</a></li>
<li><a href="#gototargetundirected">GoToTargetUndirected</a></li>
<li><a href="#gototargetdirected">GoToTargetDirected</a></li>
<li><a href="#doorkeyundirected">DoorKeyUndirected</a></li>
<li><a href="#doorkeydirected">DoorKeyDirected</a></li>
<li><a href="#collectgemsundirected">CollectGemsUndirected</a></li>
<li><a href="#collectgemsdirected">CollectGemsDirected</a></li>
<li><a href="#collectgemsmultiagentundirected">CollectGemsMultiAgentUndirected</a></li>
<li><a href="#dynamicobstaclesundirected">DynamicObstaclesUndirected</a></li>
<li><a href="#dynamicobstaclesdirected">DynamicObstaclesDirected</a></li>
<li><a href="#sokobanundirected">SokobanUndirected</a></li>
<li><a href="#sokobandirected">SokobanDirected</a></li>
<li><a href="#snake">Snake</a></li>
<li><a href="#catcher">Catcher</a></li>
<li><a href="#transportundirected">TransportUndirected</a></li>
<li><a href="#transportdirected">TransportDirected</a></li>
</ol>
<h2 dir="auto"><a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import GridWorlds as GW

# Each environment `Env` lives in its own module `EnvModule`
# For example, the `SingleRoomUndirected` environment lives inside the `SingleRoomUndirectedModule` module

env = GW.SingleRoomUndirectedModule.SingleRoomUndirected()

# reset the environment. All environments are randomized

GW.reset!(env)

# get names of actions that can be performed in this environment

GW.get_action_names(env)

# perform actions in the environment

GW.act!(env, 1) # move up
GW.act!(env, 2) # move down
GW.act!(env, 3) # move left
GW.act!(env, 4) # move right

# play an environment interactively inside the terminal

GW.play!(env)

# play and record the interaction in a file called recording.txt

GW.play!(env, file_name = &quot;recording.txt&quot;, frame_start_delimiter = &quot;FRAME_START_DELIMITER&quot;)

# manually step through the frames in the recording

GW.replay(file_name = &quot;recording.txt&quot;, frame_start_delimiter = &quot;FRAME_START_DELIMITER&quot;)

# replay the recording inside the terminal at a given frame rate

GW.replay(file_name = &quot;recording.txt&quot;, frame_start_delimiter = &quot;FRAME_START_DELIMITER&quot;, frame_rate = 2)

# use the RLBase API

import ReinforcementLearningBase as RLBase

# wrap a game instance from this package to create an RLBase compatible environment

rlbase_env = GW.RLBaseEnv(env)

# perform RLBase operations on the wrapped environment

RLBase.reset!(rlbase_env)
state = RLBase.state(rlbase_env)
action_space = RLBase.action_space(rlbase_env)
reward = RLBase.reward(rlbase_env)
done = RLBase.is_terminated(rlbase_env)

rlbase_env(1) # move up
rlbase_env(2) # move down
rlbase_env(3) # move left
rlbase_env(4) # move right"><pre><span class="pl-k">import</span> GridWorlds <span class="pl-k">as</span> GW

<span class="pl-c"><span class="pl-c">#</span> Each environment `Env` lives in its own module `EnvModule`</span>
<span class="pl-c"><span class="pl-c">#</span> For example, the `SingleRoomUndirected` environment lives inside the `SingleRoomUndirectedModule` module</span>

env <span class="pl-k">=</span> GW<span class="pl-k">.</span>SingleRoomUndirectedModule<span class="pl-k">.</span><span class="pl-c1">SingleRoomUndirected</span>()

<span class="pl-c"><span class="pl-c">#</span> reset the environment. All environments are randomized</span>

GW<span class="pl-k">.</span><span class="pl-c1">reset!</span>(env)

<span class="pl-c"><span class="pl-c">#</span> get names of actions that can be performed in this environment</span>

GW<span class="pl-k">.</span><span class="pl-c1">get_action_names</span>(env)

<span class="pl-c"><span class="pl-c">#</span> perform actions in the environment</span>

GW<span class="pl-k">.</span><span class="pl-c1">act!</span>(env, <span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> move up</span>
GW<span class="pl-k">.</span><span class="pl-c1">act!</span>(env, <span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> move down</span>
GW<span class="pl-k">.</span><span class="pl-c1">act!</span>(env, <span class="pl-c1">3</span>) <span class="pl-c"><span class="pl-c">#</span> move left</span>
GW<span class="pl-k">.</span><span class="pl-c1">act!</span>(env, <span class="pl-c1">4</span>) <span class="pl-c"><span class="pl-c">#</span> move right</span>

<span class="pl-c"><span class="pl-c">#</span> play an environment interactively inside the terminal</span>

GW<span class="pl-k">.</span><span class="pl-c1">play!</span>(env)

<span class="pl-c"><span class="pl-c">#</span> play and record the interaction in a file called recording.txt</span>

GW<span class="pl-k">.</span><span class="pl-c1">play!</span>(env, file_name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>recording.txt<span class="pl-pds">"</span></span>, frame_start_delimiter <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>FRAME_START_DELIMITER<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> manually step through the frames in the recording</span>

GW<span class="pl-k">.</span><span class="pl-c1">replay</span>(file_name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>recording.txt<span class="pl-pds">"</span></span>, frame_start_delimiter <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>FRAME_START_DELIMITER<span class="pl-pds">"</span></span>)

<span class="pl-c"><span class="pl-c">#</span> replay the recording inside the terminal at a given frame rate</span>

GW<span class="pl-k">.</span><span class="pl-c1">replay</span>(file_name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>recording.txt<span class="pl-pds">"</span></span>, frame_start_delimiter <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>FRAME_START_DELIMITER<span class="pl-pds">"</span></span>, frame_rate <span class="pl-k">=</span> <span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> use the RLBase API</span>

<span class="pl-k">import</span> ReinforcementLearningBase <span class="pl-k">as</span> RLBase

<span class="pl-c"><span class="pl-c">#</span> wrap a game instance from this package to create an RLBase compatible environment</span>

rlbase_env <span class="pl-k">=</span> GW<span class="pl-k">.</span><span class="pl-c1">RLBaseEnv</span>(env)

<span class="pl-c"><span class="pl-c">#</span> perform RLBase operations on the wrapped environment</span>

RLBase<span class="pl-k">.</span><span class="pl-c1">reset!</span>(rlbase_env)
state <span class="pl-k">=</span> RLBase<span class="pl-k">.</span><span class="pl-c1">state</span>(rlbase_env)
action_space <span class="pl-k">=</span> RLBase<span class="pl-k">.</span><span class="pl-c1">action_space</span>(rlbase_env)
reward <span class="pl-k">=</span> RLBase<span class="pl-k">.</span><span class="pl-c1">reward</span>(rlbase_env)
done <span class="pl-k">=</span> RLBase<span class="pl-k">.</span><span class="pl-c1">is_terminated</span>(rlbase_env)

<span class="pl-c1">rlbase_env</span>(<span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> move up</span>
<span class="pl-c1">rlbase_env</span>(<span class="pl-c1">2</span>) <span class="pl-c"><span class="pl-c">#</span> move down</span>
<span class="pl-c1">rlbase_env</span>(<span class="pl-c1">3</span>) <span class="pl-c"><span class="pl-c">#</span> move left</span>
<span class="pl-c1">rlbase_env</span>(<span class="pl-c1">4</span>) <span class="pl-c"><span class="pl-c">#</span> move right</span></pre></div>
<h2 dir="auto"><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notes</h2>
<h3 dir="auto"><a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reinforcement Learning</h3>
<p dir="auto">This package does not intend to reinvent a fully usable reinforcement learning API. Instead, all the games in this package provide the bare minimum of what is needed to for the game logic, which is the ability to reset an environment using <code>GW.reset!(env)</code> and to perform actions in the environment using <code>GW.act!(env, action)</code>. In order to utilize such a game for reinforcement learning, you would probably be using a higher level reinforcement learning API like the one offered by the <code>ReinforcementLearning.jl</code> package (<code>RLBase</code> API), for example. As of this writing, all the environments provide a default implementation for the <code>RLBase</code> API, which means that you can easily wrap a game from <code>GridWorlds.jl</code> and use it directly with the rest of the <code>ReinforcementLearning.jl</code> ecosystem.</p>
<ol dir="auto">
<li>
<h3 dir="auto"><a id="user-content-states" class="anchor" aria-hidden="true" href="#states"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>States</h3>
<p dir="auto">There are a few possible options for representing the state/observation for an environment. You can use the entire tile map. You can also augment that with other environment specific information like the agent's direction, target (in <code>GoToTargetUndirected</code>) etc. In several games, you can also use the <code>GW.get_sub_tile_map!</code> function to get a partial view of the tile map to be used as the observation.</p>
<p dir="auto">All environemnts provide a default implementation of the <code>RLBase.state</code> function. It is recommended that before performing reinforcement learning experiments using an environment, you carefully understand the information contained in the state representation for that environment.</p>
</li>
<li>
<h3 dir="auto"><a id="user-content-actions" class="anchor" aria-hidden="true" href="#actions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Actions</h3>
<p dir="auto">As of this writing, all actions in all environments are discrete. And so, to keep things simple and consistent, they are represented by elements of <code>Base.OneTo(NUM_ACTIONS)</code> (basically integers going from 1 to NUM_ACTIONS). In order to know which action does what, you can call <code>GW.get_action_names(env)</code> to get a list of names which gives a better description. For example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; env = GW.SingleRoomUndirectedModule.SingleRoomUndirected();

julia&gt; GW.get_action_names(env)
(:MOVE_UP, :MOVE_DOWN, :MOVE_LEFT, :MOVE_RIGHT)"><pre>julia<span class="pl-k">&gt;</span> env <span class="pl-k">=</span> GW<span class="pl-k">.</span>SingleRoomUndirectedModule<span class="pl-k">.</span><span class="pl-c1">SingleRoomUndirected</span>();

julia<span class="pl-k">&gt;</span> GW<span class="pl-k">.</span><span class="pl-c1">get_action_names</span>(env)
(<span class="pl-c1">:MOVE_UP</span>, <span class="pl-c1">:MOVE_DOWN</span>, <span class="pl-c1">:MOVE_LEFT</span>, <span class="pl-c1">:MOVE_RIGHT</span>)</pre></div>
<p dir="auto">The order of elements in this list corresponds to that of the actions.</p>
</li>
<li>
<h3 dir="auto"><a id="user-content-rewards-and-termination" class="anchor" aria-hidden="true" href="#rewards-and-termination"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Rewards and Termination</h3>
<p dir="auto">As mentioned before, in order to use these for reinforcement learning experiments, you would mostly be using a higher level API like <code>RLBase</code>, which should already provide a way to get these values. For example, in RLBase, rewards can be accessed using <code>RLBase.reward(env)</code> and checking whether an environment has terminated or not can by done by calling <code>RLBase.is_terminated(env)</code>. In case you are using some other API and need more direct control, it is better to take a look at the implementation for that environment to access things like reward and check for termination.</p>
</li>
</ol>
<h3 dir="auto"><a id="user-content-tile-map" class="anchor" aria-hidden="true" href="#tile-map"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tile Map</h3>
<p dir="auto">Each environment contains a tile map, which is a <code>BitArray{3}</code> that encodes information about the presence or absence of objects in the grid world. It is of size <code>(num_objects, height, width)</code>. The second and third dimensions correspond to positions along the height and width of the tile map. The first dimension corresponds to the presence or absence of objects at a particular position using a multi-hot encoding along the first dimension. You can get the name and ordering of objects along the first dimension of the tile map by using the following method:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; env = GW.SingleRoomUndirectedModule.SingleRoomUndirected();

julia&gt; GW.get_object_names(env)
(:AGENT, :WALL, :GOAL)"><pre>julia<span class="pl-k">&gt;</span> env <span class="pl-k">=</span> GW<span class="pl-k">.</span>SingleRoomUndirectedModule<span class="pl-k">.</span><span class="pl-c1">SingleRoomUndirected</span>();

julia<span class="pl-k">&gt;</span> GW<span class="pl-k">.</span><span class="pl-c1">get_object_names</span>(env)
(<span class="pl-c1">:AGENT</span>, <span class="pl-c1">:WALL</span>, <span class="pl-c1">:GOAL</span>)</pre></div>
<h3 dir="auto"><a id="user-content-navigation" class="anchor" aria-hidden="true" href="#navigation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Navigation</h3>
<p dir="auto">Several environments contain the word <code>Undirected</code> or <code>Directed</code> within their name. This refers to the navigation style of the agent. <code>Undirected</code> means that the agent has no direction associated with it, and navigates around by directly moving up, down, left, or right on the tile map. <code>Directed</code> means that the agent has a direction associated with it, and it navigates around by moving forward or backward along its current direction, or it could also turn left or right with respect to its current direction. There are 4 directions - <code>UP</code>, <code>DOWN</code>, <code>LEFT</code>, and <code>RIGHT</code>.</p>
<h3 dir="auto"><a id="user-content-interactive-playing-and-recording" class="anchor" aria-hidden="true" href="#interactive-playing-and-recording"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Interactive Playing and Recording</h3>
<p dir="auto">All the environments can be played directly inside the REPL. These interactive sessions can also be recorded in plain text files and replayed in the terminal. There are two ways to replay a recording:</p>
<ol dir="auto">
<li>The default way is to manually step through each recorded frame. This allows you to move through the frames one by one at your own pace using keyboard inputs.</li>
<li>The second way is to replay the frames at a given frame rate. This would loop through all the frames once and then (and only then) exit the replay.</li>
</ol>
<p dir="auto">Here is an example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126912986-83c112e4-feb2-4953-a4dc-06f7d67bb023.gif"><img src="https://user-images.githubusercontent.com/32610387/126912986-83c112e4-feb2-4953-a4dc-06f7d67bb023.gif" data-animated-image="" style="max-width: 100%;"></a></p>
<h3 dir="auto"><a id="user-content-programmatic-recording-of-agents-behavior" class="anchor" aria-hidden="true" href="#programmatic-recording-of-agents-behavior"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Programmatic Recording of Agent's Behavior</h3>
<p dir="auto">In order to programmatically record the behavior of an agent during an episode, you can simply log the string representation of the environment at each step prefixed with a delimiter. You can also log other arbitrary information if you want, like the total reward so far, for example. You can then use the <code>GW.replay</code> functiton to replay the recording inside the terminal. The string representation of an environment can be obtained using <code>repr(MIME"text/plain"(), env)</code>. Here is an example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import GridWorlds as GW
import ReinforcementLearningBase as RLBase

game = GW.SingleRoomUndirectedModule.SingleRoomUndirected()
env = GW.RLBaseEnv(game)

total_reward = zero(RLBase.reward(env))
frame_number = 1

str = &quot;&quot;

str = str * &quot;FRAME_START_DELIMITER&quot;
str = str * &quot;frame_number: $(frame_number)\n&quot;
str = str * repr(MIME&quot;text/plain&quot;(), env)
str = str * &quot;\ntotal_reward: $(total_reward)&quot;

while !RLBase.is_terminated(env)
    action = rand(RLBase.action_space(env))
    env(action)
    reward = RLBase.reward(env)

    global total_reward += reward
    global frame_number += 1

    global str = str * &quot;FRAME_START_DELIMITER&quot;
    global str = str * &quot;frame_number: $(frame_number)\n&quot;
    global str = str * repr(MIME&quot;text/plain&quot;(), env)
    global str = str * &quot;\ntotal_reward: $(total_reward)&quot;
end

write(&quot;recording.txt&quot;, str)

GW.replay(file_name = &quot;recording.txt&quot;, frame_start_delimiter = &quot;FRAME_START_DELIMITER&quot;)"><pre><span class="pl-k">import</span> GridWorlds <span class="pl-k">as</span> GW
<span class="pl-k">import</span> ReinforcementLearningBase <span class="pl-k">as</span> RLBase

game <span class="pl-k">=</span> GW<span class="pl-k">.</span>SingleRoomUndirectedModule<span class="pl-k">.</span><span class="pl-c1">SingleRoomUndirected</span>()
env <span class="pl-k">=</span> GW<span class="pl-k">.</span><span class="pl-c1">RLBaseEnv</span>(game)

total_reward <span class="pl-k">=</span> <span class="pl-c1">zero</span>(RLBase<span class="pl-k">.</span><span class="pl-c1">reward</span>(env))
frame_number <span class="pl-k">=</span> <span class="pl-c1">1</span>

str <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>

str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-s"><span class="pl-pds">"</span>FRAME_START_DELIMITER<span class="pl-pds">"</span></span>
str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-s"><span class="pl-pds">"</span>frame_number: <span class="pl-v">$(frame_number)</span><span class="pl-cce">\n</span><span class="pl-pds">"</span></span>
str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-c1">repr</span>(<span class="pl-s"><span class="pl-pds"><span class="pl-c1">MIME</span>"</span>text/plain<span class="pl-pds">"</span></span>(), env)
str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\n</span>total_reward: <span class="pl-v">$(total_reward)</span><span class="pl-pds">"</span></span>

<span class="pl-k">while</span> <span class="pl-k">!</span>RLBase<span class="pl-k">.</span><span class="pl-c1">is_terminated</span>(env)
    action <span class="pl-k">=</span> <span class="pl-c1">rand</span>(RLBase<span class="pl-k">.</span><span class="pl-c1">action_space</span>(env))
    <span class="pl-c1">env</span>(action)
    reward <span class="pl-k">=</span> RLBase<span class="pl-k">.</span><span class="pl-c1">reward</span>(env)

    <span class="pl-k">global</span> total_reward <span class="pl-k">+=</span> reward
    <span class="pl-k">global</span> frame_number <span class="pl-k">+=</span> <span class="pl-c1">1</span>

    <span class="pl-k">global</span> str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-s"><span class="pl-pds">"</span>FRAME_START_DELIMITER<span class="pl-pds">"</span></span>
    <span class="pl-k">global</span> str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-s"><span class="pl-pds">"</span>frame_number: <span class="pl-v">$(frame_number)</span><span class="pl-cce">\n</span><span class="pl-pds">"</span></span>
    <span class="pl-k">global</span> str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-c1">repr</span>(<span class="pl-s"><span class="pl-pds"><span class="pl-c1">MIME</span>"</span>text/plain<span class="pl-pds">"</span></span>(), env)
    <span class="pl-k">global</span> str <span class="pl-k">=</span> str <span class="pl-k">*</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\n</span>total_reward: <span class="pl-v">$(total_reward)</span><span class="pl-pds">"</span></span>
<span class="pl-k">end</span>

<span class="pl-c1">write</span>(<span class="pl-s"><span class="pl-pds">"</span>recording.txt<span class="pl-pds">"</span></span>, str)

GW<span class="pl-k">.</span><span class="pl-c1">replay</span>(file_name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>recording.txt<span class="pl-pds">"</span></span>, frame_start_delimiter <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>FRAME_START_DELIMITER<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">In <code>ReinforcementLearning.jl</code>, you can create a <a href="https://juliareinforcementlearning.org/docs/How_to_use_hooks/" rel="nofollow">hook</a> for recording the agent's behavior at any point during training.</p>
<h2 dir="auto"><a id="user-content-list-of-environments" class="anchor" aria-hidden="true" href="#list-of-environments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>List of Environments</h2>
<ol dir="auto">
<li>
<h3 dir="auto"><a id="user-content-singleroomundirected" class="anchor" aria-hidden="true" href="#singleroomundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SingleRoomUndirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909935-d6a1d303-9925-4fc9-9a9f-025fc133c64c.png"><img src="https://user-images.githubusercontent.com/32610387/126909935-d6a1d303-9925-4fc9-9a9f-025fc133c64c.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909804-b53f43a7-98d0-4d53-874e-d8988494ae53.gif"><img src="https://user-images.githubusercontent.com/32610387/126909804-b53f43a7-98d0-4d53-874e-d8988494ae53.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-singleroomdirected" class="anchor" aria-hidden="true" href="#singleroomdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SingleRoomDirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909946-b0b00a56-c48a-4e17-9aaf-08c054e72f9a.png"><img src="https://user-images.githubusercontent.com/32610387/126909946-b0b00a56-c48a-4e17-9aaf-08c054e72f9a.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909809-6b6b37ad-3ccb-41c5-b263-5709e6a322db.gif"><img src="https://user-images.githubusercontent.com/32610387/126909809-6b6b37ad-3ccb-41c5-b263-5709e6a322db.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-gridroomsundirected" class="anchor" aria-hidden="true" href="#gridroomsundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GridRoomsUndirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909952-765ed7d4-f91c-4d31-8bfa-6f42fea791b7.png"><img src="https://user-images.githubusercontent.com/32610387/126909952-765ed7d4-f91c-4d31-8bfa-6f42fea791b7.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909820-037aa2fb-e332-46d2-8f6e-cea9a138db82.gif"><img src="https://user-images.githubusercontent.com/32610387/126909820-037aa2fb-e332-46d2-8f6e-cea9a138db82.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-gridroomsdirected" class="anchor" aria-hidden="true" href="#gridroomsdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GridRoomsDirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909955-81c9f724-45bb-40cb-ba91-ab124f27fe20.png"><img src="https://user-images.githubusercontent.com/32610387/126909955-81c9f724-45bb-40cb-ba91-ab124f27fe20.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909822-301db2eb-1bc3-42ce-9fa9-1c305cadb9c3.gif"><img src="https://user-images.githubusercontent.com/32610387/126909822-301db2eb-1bc3-42ce-9fa9-1c305cadb9c3.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-sequentialroomsundirected" class="anchor" aria-hidden="true" href="#sequentialroomsundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SequentialRoomsUndirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909957-eaf2340a-f985-4b93-b677-1ad28a9fc671.png"><img src="https://user-images.githubusercontent.com/32610387/126909957-eaf2340a-f985-4b93-b677-1ad28a9fc671.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909834-c88338b7-06d4-4a78-a48a-84774f2450ff.gif"><img src="https://user-images.githubusercontent.com/32610387/126909834-c88338b7-06d4-4a78-a48a-84774f2450ff.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-sequentialroomsdirected" class="anchor" aria-hidden="true" href="#sequentialroomsdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SequentialRoomsDirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909964-6c011d1a-59a9-4083-952a-ba46a016496f.png"><img src="https://user-images.githubusercontent.com/32610387/126909964-6c011d1a-59a9-4083-952a-ba46a016496f.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909842-f6007d93-59ba-4c2f-ab84-e77bff09ab71.gif"><img src="https://user-images.githubusercontent.com/32610387/126909842-f6007d93-59ba-4c2f-ab84-e77bff09ab71.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-mazeundirected" class="anchor" aria-hidden="true" href="#mazeundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MazeUndirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909969-9b42380a-b96f-4448-be2c-bfe4c8a4a6ae.png"><img src="https://user-images.githubusercontent.com/32610387/126909969-9b42380a-b96f-4448-be2c-bfe4c8a4a6ae.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909846-c6a86cd3-a80b-4519-a786-0e42beaa34ac.gif"><img src="https://user-images.githubusercontent.com/32610387/126909846-c6a86cd3-a80b-4519-a786-0e42beaa34ac.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-mazedirected" class="anchor" aria-hidden="true" href="#mazedirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MazeDirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909973-b06a42eb-cf87-40fa-b627-09e08716847f.png"><img src="https://user-images.githubusercontent.com/32610387/126909973-b06a42eb-cf87-40fa-b627-09e08716847f.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909852-4f096c83-11b2-406f-a6fe-64d87e6c1e8d.gif"><img src="https://user-images.githubusercontent.com/32610387/126909852-4f096c83-11b2-406f-a6fe-64d87e6c1e8d.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-gototargetundirected" class="anchor" aria-hidden="true" href="#gototargetundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GoToTargetUndirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the desired target. When the agent reaches the desired target, it receives a reward of 1. When the agent reaches the other target, it receives a reward of -1. In either case, the environment terminates upon reaching a target.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909978-63bed874-f994-4278-b6ca-06a5f42e85db.png"><img src="https://user-images.githubusercontent.com/32610387/126909978-63bed874-f994-4278-b6ca-06a5f42e85db.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909857-1b1159c7-df54-4b29-9c59-2bde48e667de.gif"><img src="https://user-images.githubusercontent.com/32610387/126909857-1b1159c7-df54-4b29-9c59-2bde48e667de.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-gototargetdirected" class="anchor" aria-hidden="true" href="#gototargetdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GoToTargetDirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the desired target. When the agent reaches the desired target, it receives a reward of 1. When the agent reaches the other target, it receives a reward of -1. In either case, the environment terminates upon reaching a target.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910005-b226bd16-a5b3-46a4-8ed0-4e60e9b8142f.png"><img src="https://user-images.githubusercontent.com/32610387/126910005-b226bd16-a5b3-46a4-8ed0-4e60e9b8142f.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909863-5d49fb4a-690a-4394-a391-81a5f068210e.gif"><img src="https://user-images.githubusercontent.com/32610387/126909863-5d49fb4a-690a-4394-a391-81a5f068210e.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-doorkeyundirected" class="anchor" aria-hidden="true" href="#doorkeyundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DoorKeyUndirected</h3>
<p dir="auto">The objective of the agent is to collect the key and navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates. Without picking up the key, the agent will not be able to pass through the door that separtes the agent and goal.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910012-2c802689-0112-4c2b-8f08-cfa33792bece.png"><img src="https://user-images.githubusercontent.com/32610387/126910012-2c802689-0112-4c2b-8f08-cfa33792bece.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909869-4fff6bf7-7f4b-41c2-9710-ab6b76c5d184.gif"><img src="https://user-images.githubusercontent.com/32610387/126909869-4fff6bf7-7f4b-41c2-9710-ab6b76c5d184.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-doorkeydirected" class="anchor" aria-hidden="true" href="#doorkeydirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DoorKeyDirected</h3>
<p dir="auto">The objective of the agent is to collect the key and navigate its way to the goal. When the agent reaches the goal, it receives a reward of 1 and the environment terminates. Without picking up the key, the agent will not be able to pass through the door that separtes the agent and goal.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910014-75f94b0a-88c5-462e-b9a8-fe97d537d8e6.png"><img src="https://user-images.githubusercontent.com/32610387/126910014-75f94b0a-88c5-462e-b9a8-fe97d537d8e6.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909871-c5951db4-6f2e-469e-a5bc-256038892200.gif"><img src="https://user-images.githubusercontent.com/32610387/126909871-c5951db4-6f2e-469e-a5bc-256038892200.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-collectgemsundirected" class="anchor" aria-hidden="true" href="#collectgemsundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CollectGemsUndirected</h3>
<p dir="auto">The objective of the agent is to collect all the randomly scattered gems. When the agent collects a gem, it receives a reward of 1. The environment terminates when the agent has collected all the gems.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910020-3722de31-4fa4-4013-9e4e-b42e77061a84.png"><img src="https://user-images.githubusercontent.com/32610387/126910020-3722de31-4fa4-4013-9e4e-b42e77061a84.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909874-15138b3f-25ce-4437-8e7a-eb31844dd228.gif"><img src="https://user-images.githubusercontent.com/32610387/126909874-15138b3f-25ce-4437-8e7a-eb31844dd228.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-collectgemsdirected" class="anchor" aria-hidden="true" href="#collectgemsdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CollectGemsDirected</h3>
<p dir="auto">The objective of the agent is to collect all the randomly scattered gems. When the agent collects a gem, it receives a reward of 1. The environment terminates when the agent has collected all the gems.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910021-56c5cf39-4638-456d-ab67-cf442de3f341.png"><img src="https://user-images.githubusercontent.com/32610387/126910021-56c5cf39-4638-456d-ab67-cf442de3f341.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909881-d3aeed2e-0487-43c4-be58-ab43001e45c9.gif"><img src="https://user-images.githubusercontent.com/32610387/126909881-d3aeed2e-0487-43c4-be58-ab43001e45c9.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-collectgemsmultiagentundirected" class="anchor" aria-hidden="true" href="#collectgemsmultiagentundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CollectGemsMultiAgentUndirected</h3>
<p dir="auto">The objective of the agents is to collect all the randomly scattered gems. The agents take turns for performing actions. When an agent collects a gem, the environment gives a reward of 1. The environment terminates when the agents have collected all the gems.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910026-fcff97b1-f498-4b23-acfe-ffc393fe851e.png"><img src="https://user-images.githubusercontent.com/32610387/126910026-fcff97b1-f498-4b23-acfe-ffc393fe851e.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909885-fbc10e9c-3247-45ee-a354-946ebc77b513.gif"><img src="https://user-images.githubusercontent.com/32610387/126909885-fbc10e9c-3247-45ee-a354-946ebc77b513.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-dynamicobstaclesundirected" class="anchor" aria-hidden="true" href="#dynamicobstaclesundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DynamicObstaclesUndirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal while avoiding collision with obstacles. When the agent reaches the goal, it receives a reward of 1 and the environment terminates. If the agent collides with an obstacle, the agent receives a reward of -1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910030-d93a714d-10b7-4117-887c-773afe78c625.png"><img src="https://user-images.githubusercontent.com/32610387/126910030-d93a714d-10b7-4117-887c-773afe78c625.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909888-8fa8473f-deb6-4562-9004-419fa8080693.gif"><img src="https://user-images.githubusercontent.com/32610387/126909888-8fa8473f-deb6-4562-9004-419fa8080693.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-dynamicobstaclesdirected" class="anchor" aria-hidden="true" href="#dynamicobstaclesdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DynamicObstaclesDirected</h3>
<p dir="auto">The objective of the agent is to navigate its way to the goal while avoiding collision with obstacles. When the agent reaches the goal, it receives a reward of 1 and the environment terminates. If the agent collides with an obstacle, the agent receives a reward of -1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910033-ee68bd19-06f3-42eb-8606-1042633bbe9b.png"><img src="https://user-images.githubusercontent.com/32610387/126910033-ee68bd19-06f3-42eb-8606-1042633bbe9b.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909890-9bb101aa-7fc8-4854-a9ad-f87ff81e7f73.gif"><img src="https://user-images.githubusercontent.com/32610387/126909890-9bb101aa-7fc8-4854-a9ad-f87ff81e7f73.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-sokobanundirected" class="anchor" aria-hidden="true" href="#sokobanundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SokobanUndirected</h3>
<p dir="auto">The agent needs to push the boxes onto the target positions. The levels are taken from <a href="https://github.com/deepmind/boxoban-levels">https://github.com/deepmind/boxoban-levels</a>. Upon each reset, a level is randomly selected from <a href="https://github.com/deepmind/boxoban-levels/blob/master/medium/train/000.txt">https://github.com/deepmind/boxoban-levels/blob/master/medium/train/000.txt</a>. The level dataset can be dynamically swapped during runtime in case more levels are needed. One way to achieve this while using <code>ReinforcementLearning.jl</code> is with the help of <a href="https://juliareinforcementlearning.org/docs/How_to_use_hooks/" rel="nofollow">hooks</a>.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910035-5746bba6-7692-4ad9-b081-5db34e66f0a5.png"><img src="https://user-images.githubusercontent.com/32610387/126910035-5746bba6-7692-4ad9-b081-5db34e66f0a5.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909897-621ee9eb-69e8-4be2-a5a2-e15929bd9acd.gif"><img src="https://user-images.githubusercontent.com/32610387/126909897-621ee9eb-69e8-4be2-a5a2-e15929bd9acd.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-sokobandirected" class="anchor" aria-hidden="true" href="#sokobandirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SokobanDirected</h3>
<p dir="auto">The agent needs to push the boxes onto the target positions. The levels are taken from <a href="https://github.com/deepmind/boxoban-levels">https://github.com/deepmind/boxoban-levels</a>. Upon each reset, a level is randomly selected from <a href="https://github.com/deepmind/boxoban-levels/blob/master/medium/train/000.txt">https://github.com/deepmind/boxoban-levels/blob/master/medium/train/000.txt</a>. The level dataset can be dynamically swapped during runtime in case more levels are needed. One way to achieve this while using <code>ReinforcementLearning.jl</code> is with the help of <a href="https://juliareinforcementlearning.org/docs/How_to_use_hooks/" rel="nofollow">hooks</a>.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910036-b8f5754d-abf6-40b7-b40a-495f528b89b2.png"><img src="https://user-images.githubusercontent.com/32610387/126910036-b8f5754d-abf6-40b7-b40a-495f528b89b2.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909900-b932420f-176c-4b95-8fff-db5efdf8fc9f.gif"><img src="https://user-images.githubusercontent.com/32610387/126909900-b932420f-176c-4b95-8fff-db5efdf8fc9f.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-snake" class="anchor" aria-hidden="true" href="#snake"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Snake</h3>
<p dir="auto">The objective of the agent is to eat as many food pellets as possible. As soon as the agent eats a food pellet, the length of its body incrases by one and it receives a reward of 1. When the agent tries to move into a wall or into its body, it receives a reward of <code>- tile_map_height * tile_map_width</code> and the environment terminates. When the agent collects all the food pellets possible, it receives a reward of <code>tile_map_height * tile_map_width</code> + 1 (for the last food pellet it ate).</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910039-aa960aa8-a1f6-4329-851f-1ebec350a7eb.png"><img src="https://user-images.githubusercontent.com/32610387/126910039-aa960aa8-a1f6-4329-851f-1ebec350a7eb.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909904-63160435-d75b-4510-b439-58f3f419d961.gif"><img src="https://user-images.githubusercontent.com/32610387/126909904-63160435-d75b-4510-b439-58f3f419d961.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-catcher" class="anchor" aria-hidden="true" href="#catcher"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Catcher</h3>
<p dir="auto">The objective of the agent is to keep catching the falling gems for as long as possible. It receives a reward of 1 when it catches a gem and a new gem gets spawned in the next step. When the agent misses catching a gem, it receives a reward of -1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910040-e8e55c02-cb74-4089-b2c1-e5666143687e.png"><img src="https://user-images.githubusercontent.com/32610387/126910040-e8e55c02-cb74-4089-b2c1-e5666143687e.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909911-236987ce-12f2-49c8-9b93-2147e7c9ea01.gif"><img src="https://user-images.githubusercontent.com/32610387/126909911-236987ce-12f2-49c8-9b93-2147e7c9ea01.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-transportundirected" class="anchor" aria-hidden="true" href="#transportundirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TransportUndirected</h3>
<p dir="auto">The objective of the agent is to pick up the gem and drop it to the target location. When the agent drops the gem at the target location, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910044-1e4896f3-8fa9-421d-9a5b-ee90f1b68d81.png"><img src="https://user-images.githubusercontent.com/32610387/126910044-1e4896f3-8fa9-421d-9a5b-ee90f1b68d81.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909918-87c80ac8-8015-4d96-a2f7-f775692118ac.gif"><img src="https://user-images.githubusercontent.com/32610387/126909918-87c80ac8-8015-4d96-a2f7-f775692118ac.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
<li>
<h3 dir="auto"><a id="user-content-transportdirected" class="anchor" aria-hidden="true" href="#transportdirected"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TransportDirected</h3>
<p dir="auto">The objective of the agent is to pick up the gem and drop it to the target location. When the agent drops the gem at the target location, it receives a reward of 1 and the environment terminates.</p>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126910050-723e100c-c5c7-4703-8eab-5ab86a15e41f.png"><img src="https://user-images.githubusercontent.com/32610387/126910050-723e100c-c5c7-4703-8eab-5ab86a15e41f.png" style="max-width: 100%;"></a>
 <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/32610387/126909921-fdb3c853-4cac-4e6a-b20c-604caf5632e0.gif"><img src="https://user-images.githubusercontent.com/32610387/126909921-fdb3c853-4cac-4e6a-b20c-604caf5632e0.gif" data-animated-image="" style="max-width: 100%;"></a>
</li>
</ol>
</article></div>