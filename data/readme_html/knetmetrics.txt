<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-knetmetrics" class="anchor" aria-hidden="true" href="#knetmetrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>KnetMetrics</h1>
<p>A standalone machine learning metrics library implemented in pure <a href="http://docs.julialang.org" rel="nofollow">Julia</a> by <a href="https://github.com/ekurtulus">Emirhan Kurtuluş</a>. A vast collection of classification, regression and pairwise metrics and related visualizations are included. This package is created as a part of the <a href="https://github.com/denizyuret/Knet.jl">Knet</a> ecosystem; however, built-in Julia arrays and all other types that support the same set of operations are compatible.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; Pkg.add(&quot;KnetMetrics&quot;); using KnetMetrics
# dummy data
julia&gt; y_true = [2, 3, 2, 2, 2, 1, 3, 3, 2, 1, 2, 3, 2, 3, 2, 3, 1,1];
julia&gt; y_pred = [1, 3, 1, 1, 3, 2, 2, 1, 3, 2, 3, 3, 2, 3, 2, 1, 1, 3];
# creating a confusion matrix
julia&gt; c = confusion_matrix( y_true, y_pred, labels = [1,2,3]) #labels are truly optional

            Expected

      1      2      3
_____________________
      1      2      1   │1
      3      2      3   │2
      2      1      3   │3      Predicted

# testing some metrics
julia&gt; f1_score(c) # f1_score(y_true, y_pred)
0.32307692307692304

julia&gt; f1_score(c, average = &quot;binary&quot;)
3-element Array{Float64,1}:
 0.2
 0.3076923076923077
 0.4615384615384615

julia&gt; f1_score(c, average = &quot;binary&quot;, normalize=true)
3-element Array{Float64,1}:
 0.33918173268560714
 0.5218180502855494
 0.7827270754283241

julia&gt; f1_score(c, class_name = 3)
0.4615384615384615

julia&gt; matthews_correlation_coeff(c, average = &quot;micro&quot;)
0.20396752553080869

julia&gt; matthews_correlation_coeff(c, average = &quot;weighted&quot;)
0.07138375997792953

julia&gt; matthews_correlation_coeff(c, average = &quot;sample-weights&quot;, weights = [3,2,1])
0.03263110671272045

julia&gt; minkowski_distance(y_true, y_pred)
21

julia&gt; mean_absolute_error(y_true, y_pred)
0.8333333333333334
"><pre>julia<span class="pl-k">&gt;</span> Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>KnetMetrics<span class="pl-pds">"</span></span>); <span class="pl-k">using</span> KnetMetrics
<span class="pl-c"><span class="pl-c">#</span> dummy data</span>
julia<span class="pl-k">&gt;</span> y_true <span class="pl-k">=</span> [<span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>,<span class="pl-c1">1</span>];
julia<span class="pl-k">&gt;</span> y_pred <span class="pl-k">=</span> [<span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">1</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">3</span>, <span class="pl-c1">2</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">3</span>];
<span class="pl-c"><span class="pl-c">#</span> creating a confusion matrix</span>
julia<span class="pl-k">&gt;</span> c <span class="pl-k">=</span> <span class="pl-c1">confusion_matrix</span>( y_true, y_pred, labels <span class="pl-k">=</span> [<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>]) <span class="pl-c"><span class="pl-c">#</span>labels are truly optional</span>

            Expected

      <span class="pl-c1">1</span>      <span class="pl-c1">2</span>      <span class="pl-c1">3</span>
_____________________
      <span class="pl-c1">1</span>      <span class="pl-c1">2</span>      <span class="pl-c1">1</span>   │1
      <span class="pl-c1">3</span>      <span class="pl-c1">2</span>      <span class="pl-c1">3</span>   │2
      <span class="pl-c1">2</span>      <span class="pl-c1">1</span>      <span class="pl-c1">3</span>   │3      Predicted

<span class="pl-c"><span class="pl-c">#</span> testing some metrics</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">f1_score</span>(c) <span class="pl-c"><span class="pl-c">#</span> f1_score(y_true, y_pred)</span>
<span class="pl-c1">0.32307692307692304</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">f1_score</span>(c, average <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>binary<span class="pl-pds">"</span></span>)
<span class="pl-c1">3</span><span class="pl-k">-</span>element Array{Float64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.2</span>
 <span class="pl-c1">0.3076923076923077</span>
 <span class="pl-c1">0.4615384615384615</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">f1_score</span>(c, average <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>binary<span class="pl-pds">"</span></span>, normalize<span class="pl-k">=</span><span class="pl-c1">true</span>)
<span class="pl-c1">3</span><span class="pl-k">-</span>element Array{Float64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-c1">0.33918173268560714</span>
 <span class="pl-c1">0.5218180502855494</span>
 <span class="pl-c1">0.7827270754283241</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">f1_score</span>(c, class_name <span class="pl-k">=</span> <span class="pl-c1">3</span>)
<span class="pl-c1">0.4615384615384615</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matthews_correlation_coeff</span>(c, average <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>micro<span class="pl-pds">"</span></span>)
<span class="pl-c1">0.20396752553080869</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matthews_correlation_coeff</span>(c, average <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>weighted<span class="pl-pds">"</span></span>)
<span class="pl-c1">0.07138375997792953</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matthews_correlation_coeff</span>(c, average <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>sample-weights<span class="pl-pds">"</span></span>, weights <span class="pl-k">=</span> [<span class="pl-c1">3</span>,<span class="pl-c1">2</span>,<span class="pl-c1">1</span>])
<span class="pl-c1">0.03263110671272045</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">minkowski_distance</span>(y_true, y_pred)
<span class="pl-c1">21</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">mean_absolute_error</span>(y_true, y_pred)
<span class="pl-c1">0.8333333333333334</span></pre></div>
<h2><a id="user-content-currently-supported-metrics" class="anchor" aria-hidden="true" href="#currently-supported-metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Currently Supported Metrics</h2>
<p>Note: (*) symbol denotes that the function has a built-in visualization through <code>visualize</code> function.</p>
<h4><a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Classification</h4>
<ul>
<li>Confusion Matrix *</li>
<li>Condition Positive and Negative *</li>
<li>Predicted Positive and Negative *</li>
<li>Correctly and Incorrectly Classified</li>
<li>True Positive Rate (Sensitivity Score, Recall Score) *</li>
<li>True Negative Rate (Specificity Score) *</li>
<li>Positive Predictive Value (Precision Score) *</li>
<li>Accuracy Score *</li>
<li>Balanced Accuracy Score *</li>
<li>Negative Predictive Value *</li>
<li>False Negative Rate *</li>
<li>False Positive Rate *</li>
<li>False Discovery Rate *</li>
<li>False Omission Rate *</li>
<li>Fbeta Score (F1 Score) *</li>
<li>Prevalence Threshold *</li>
<li>Threat Score *</li>
<li>Matthews Correlation Coefficient *</li>
<li>Fowlkes Mallows Index *</li>
<li>Informedness *</li>
<li>Markedness *</li>
<li>Cohen Kappa Score</li>
<li>Hamming Loss</li>
<li>Jaccard Score *</li>
</ul>
<h4><a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Regression</h4>
<ul>
<li>Maximum Residual Error</li>
<li>Mean Absolute Error</li>
<li>Mean Squared Error</li>
<li>Mean Squared Log Error</li>
<li>Median Absolute Error</li>
<li>Mean Absolute Percentage Error</li>
</ul>
<h4><a id="user-content-pairwise" class="anchor" aria-hidden="true" href="#pairwise"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pairwise</h4>
<ul>
<li>Minkowski Distance</li>
<li>Euclidian Distance</li>
<li>Manhattan Distance</li>
<li>Chebyshev Distance</li>
<li>Braycurtis Distance</li>
<li>Canberra Distance</li>
<li>Cityblock Distance</li>
<li>Mahalanobis Distance</li>
<li>Correlation</li>
<li>Cosine Distance</li>
<li>Cosine Similarity</li>
</ul>
<h2><a id="user-content-to-do" class="anchor" aria-hidden="true" href="#to-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TO-DO</h2>
<ol>
<li>A greater range of Roc Curve related functions</li>
<li>A greater range of regression functions</li>
<li>A greater range of pairwise functions and kernels</li>
<li>Clustering metrics</li>
</ol>
</article></div>