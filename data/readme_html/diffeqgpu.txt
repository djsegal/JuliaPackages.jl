<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-diffeqgpu" class="anchor" aria-hidden="true" href="#diffeqgpu"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DiffEqGPU</h1>
<table>
<thead>
<tr>
<th>Julia</th>
<th>GPU CI</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.x</td>
<td><a href="https://buildkite.com/julialang/diffeqgpu-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/3f29071d9190aa6d8cc7364ea18023ed8f4323e28366f48eea87b01e01960274/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f34303961623464383835303330303632363831613434343332383836386432653861643131376361646330613765313432342e7376673f6272616e63683d6d617374657226737465703d4a756c6961253230312e35" alt="Build status" data-canonical-src="https://badge.buildkite.com/409ab4d885030062681a444328868d2e8ad117cadc0a7e1424.svg?branch=master&amp;step=Julia%201.5" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>This library is a component package of the DifferentialEquations.jl ecosystem. It includes functionality for making
use of GPUs in the differential equation solvers.</p>
<h2><a id="user-content-within-method-gpu-parallelism-with-direct-cuarray-usage" class="anchor" aria-hidden="true" href="#within-method-gpu-parallelism-with-direct-cuarray-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Within-Method GPU Parallelism with Direct CuArray Usage</h2>
<p>The native Julia libraries, including (but not limited to) OrdinaryDiffEq, StochasticDiffEq, and DelayDiffEq, are
compatible with <code>u0</code> being a <code>CuArray</code>. When this occurs, all array operations take place on the GPU, including
any implicit solves. This is independent of the DiffEqGPU library. These speedup the solution of a differential
equation which is sufficiently large or expensive. This does not require DiffEqGPU.jl.</p>
<p>For example, the following is a GPU-accelerated solve with <code>Tsit5</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using OrdinaryDiffEq, CUDA, LinearAlgebra
u0 = cu(rand(1000))
A  = cu(randn(1000,1000))
f(du,u,p,t)  = mul!(du,A,u)
prob = ODEProblem(f,u0,(0.0f0,1.0f0)) # Float32 is better on GPUs!
sol = solve(prob,Tsit5())
"><pre><span class="pl-k">using</span> OrdinaryDiffEq, CUDA, LinearAlgebra
u0 <span class="pl-k">=</span> <span class="pl-c1">cu</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">1000</span>))
A  <span class="pl-k">=</span> <span class="pl-c1">cu</span>(<span class="pl-c1">randn</span>(<span class="pl-c1">1000</span>,<span class="pl-c1">1000</span>))
<span class="pl-en">f</span>(du,u,p,t)  <span class="pl-k">=</span> <span class="pl-c1">mul!</span>(du,A,u)
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(f,u0,(<span class="pl-c1">0.0f0</span>,<span class="pl-c1">1.0f0</span>)) <span class="pl-c"><span class="pl-c">#</span> Float32 is better on GPUs!</span>
sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(prob,<span class="pl-c1">Tsit5</span>())</pre></div>
<h2><a id="user-content-parameter-parallelism-with-gpu-ensemble-methods" class="anchor" aria-hidden="true" href="#parameter-parallelism-with-gpu-ensemble-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Parameter-Parallelism with GPU Ensemble Methods</h2>
<p>Parameter-parallel GPU methods are provided for the case where a single solve is too cheap to benefit from
within-method parallelism, but the solution of the same structure (same <code>f</code>) is required for very many
different choices of <code>u0</code> or <code>p</code>. For these cases, DiffEqGPU exports the following ensemble algorithms:</p>
<ul>
<li><code>EnsembleGPUArray</code>: Utilizes the CuArray setup to parallelize ODE solves across the GPU.</li>
<li><code>EnsembleCPUArray</code>: A test version for analyzing the overhead of the array-based parallelism setup.</li>
</ul>
<p>For more information on using the ensemble interface, see
<a href="http://docs.juliadiffeq.org/dev/features/ensemble" rel="nofollow">the DiffEqDocs page on ensembles</a></p>
<p>For example, the following solves the Lorenz equation with 10,000 separate random parameters on the GPU:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function lorenz(du,u,p,t)
    du[1] = p[1]*(u[2]-u[1])
    du[2] = u[1]*(p[2]-u[3]) - u[2]
    du[3] = u[1]*u[2] - p[3]*u[3]
end

u0 = Float32[1.0;0.0;0.0]
tspan = (0.0f0,100.0f0)
p = [10.0f0,28.0f0,8/3f0]
prob = ODEProblem(lorenz,u0,tspan,p)
prob_func = (prob,i,repeat) -&gt; remake(prob,p=rand(Float32,3).*p)
monteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy=false)
@time sol = solve(monteprob,Tsit5(),EnsembleGPUArray(),trajectories=10_000,saveat=1.0f0)
"><pre><span class="pl-k">function</span> <span class="pl-en">lorenz</span>(du,u,p,t)
    du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> p[<span class="pl-c1">1</span>]<span class="pl-k">*</span>(u[<span class="pl-c1">2</span>]<span class="pl-k">-</span>u[<span class="pl-c1">1</span>])
    du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]<span class="pl-k">*</span>(p[<span class="pl-c1">2</span>]<span class="pl-k">-</span>u[<span class="pl-c1">3</span>]) <span class="pl-k">-</span> u[<span class="pl-c1">2</span>]
    du[<span class="pl-c1">3</span>] <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]<span class="pl-k">*</span>u[<span class="pl-c1">2</span>] <span class="pl-k">-</span> p[<span class="pl-c1">3</span>]<span class="pl-k">*</span>u[<span class="pl-c1">3</span>]
<span class="pl-k">end</span>

u0 <span class="pl-k">=</span> Float32[<span class="pl-c1">1.0</span>;<span class="pl-c1">0.0</span>;<span class="pl-c1">0.0</span>]
tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0f0</span>,<span class="pl-c1">100.0f0</span>)
p <span class="pl-k">=</span> [<span class="pl-c1">10.0f0</span>,<span class="pl-c1">28.0f0</span>,<span class="pl-c1">8</span><span class="pl-k">/</span><span class="pl-c1">3f0</span>]
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(lorenz,u0,tspan,p)
prob_func <span class="pl-k">=</span> (prob,i,repeat) <span class="pl-k">-&gt;</span> <span class="pl-c1">remake</span>(prob,p<span class="pl-k">=</span><span class="pl-c1">rand</span>(Float32,<span class="pl-c1">3</span>)<span class="pl-k">.*</span>p)
monteprob <span class="pl-k">=</span> <span class="pl-c1">EnsembleProblem</span>(prob, prob_func <span class="pl-k">=</span> prob_func, safetycopy<span class="pl-k">=</span><span class="pl-c1">false</span>)
<span class="pl-c1">@time</span> sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(monteprob,<span class="pl-c1">Tsit5</span>(),<span class="pl-c1">EnsembleGPUArray</span>(),trajectories<span class="pl-k">=</span><span class="pl-c1">10_000</span>,saveat<span class="pl-k">=</span><span class="pl-c1">1.0f0</span>)</pre></div>
<h4><a id="user-content-current-support" class="anchor" aria-hidden="true" href="#current-support"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Current Support</h4>
<p>Automated GPU parameter parallelism support is continuing to be improved, so there are currently a few limitations.
Not everything is supported yet, but most of the standard features have support, including:</p>
<ul>
<li>Explicit Runge-Kutta methods</li>
<li>Implicit Runge-Kutta methods</li>
<li>Rosenbrock methods</li>
<li>DiscreteCallbacks and ContinuousCallbacks</li>
<li>Multiple GPUs over clusters</li>
</ul>
<h4><a id="user-content-current-limitations" class="anchor" aria-hidden="true" href="#current-limitations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Current Limitations</h4>
<p>Stiff ODEs require the analytical solution of every derivative function it requires. For example,
Rosenbrock methods require the Jacobian and the gradient with respect to time, and so these two functions are required to
be given. Note that they can be generated by the
<a href="https://docs.juliadiffeq.org/latest/tutorials/advanced_ode_example/#Automatic-Derivation-of-Jacobian-Functions-1" rel="nofollow">modelingtoolkitize</a>
approach. For example, 10,000 trajectories solved with Rodas5 and TRBDF2 is done via:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function lorenz_jac(J,u,p,t)
    σ = p[1]
    ρ = p[2]
    β = p[3]
    x = u[1]
    y = u[2]
    z = u[3]
    J[1,1] = -σ
    J[2,1] = ρ - z
    J[3,1] = y
    J[1,2] = σ
    J[2,2] = -1
    J[3,2] = x
    J[1,3] = 0
    J[2,3] = -x
    J[3,3] = -β
end

function lorenz_tgrad(J,u,p,t)
    nothing
end

func = ODEFunction(lorenz,jac=lorenz_jac,tgrad=lorenz_tgrad)
prob_jac = ODEProblem(func,u0,tspan,p)
monteprob_jac = EnsembleProblem(prob_jac, prob_func = prob_func)

@time solve(monteprob_jac,Rodas5(),EnsembleGPUArray(),dt=0.1,trajectories=10_000,saveat=1.0f0)
@time solve(monteprob_jac,TRBDF2(),EnsembleGPUArray(),dt=0.1,trajectories=10_000,saveat=1.0f0)
"><pre><span class="pl-k">function</span> <span class="pl-en">lorenz_jac</span>(J,u,p,t)
    σ <span class="pl-k">=</span> p[<span class="pl-c1">1</span>]
    ρ <span class="pl-k">=</span> p[<span class="pl-c1">2</span>]
    β <span class="pl-k">=</span> p[<span class="pl-c1">3</span>]
    x <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]
    y <span class="pl-k">=</span> u[<span class="pl-c1">2</span>]
    z <span class="pl-k">=</span> u[<span class="pl-c1">3</span>]
    J[<span class="pl-c1">1</span>,<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-k">-</span>σ
    J[<span class="pl-c1">2</span>,<span class="pl-c1">1</span>] <span class="pl-k">=</span> ρ <span class="pl-k">-</span> z
    J[<span class="pl-c1">3</span>,<span class="pl-c1">1</span>] <span class="pl-k">=</span> y
    J[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>] <span class="pl-k">=</span> σ
    J[<span class="pl-c1">2</span>,<span class="pl-c1">2</span>] <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">1</span>
    J[<span class="pl-c1">3</span>,<span class="pl-c1">2</span>] <span class="pl-k">=</span> x
    J[<span class="pl-c1">1</span>,<span class="pl-c1">3</span>] <span class="pl-k">=</span> <span class="pl-c1">0</span>
    J[<span class="pl-c1">2</span>,<span class="pl-c1">3</span>] <span class="pl-k">=</span> <span class="pl-k">-</span>x
    J[<span class="pl-c1">3</span>,<span class="pl-c1">3</span>] <span class="pl-k">=</span> <span class="pl-k">-</span>β
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">lorenz_tgrad</span>(J,u,p,t)
    <span class="pl-c1">nothing</span>
<span class="pl-k">end</span>

func <span class="pl-k">=</span> <span class="pl-c1">ODEFunction</span>(lorenz,jac<span class="pl-k">=</span>lorenz_jac,tgrad<span class="pl-k">=</span>lorenz_tgrad)
prob_jac <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(func,u0,tspan,p)
monteprob_jac <span class="pl-k">=</span> <span class="pl-c1">EnsembleProblem</span>(prob_jac, prob_func <span class="pl-k">=</span> prob_func)

<span class="pl-c1">@time</span> <span class="pl-c1">solve</span>(monteprob_jac,<span class="pl-c1">Rodas5</span>(),<span class="pl-c1">EnsembleGPUArray</span>(),dt<span class="pl-k">=</span><span class="pl-c1">0.1</span>,trajectories<span class="pl-k">=</span><span class="pl-c1">10_000</span>,saveat<span class="pl-k">=</span><span class="pl-c1">1.0f0</span>)
<span class="pl-c1">@time</span> <span class="pl-c1">solve</span>(monteprob_jac,<span class="pl-c1">TRBDF2</span>(),<span class="pl-c1">EnsembleGPUArray</span>(),dt<span class="pl-k">=</span><span class="pl-c1">0.1</span>,trajectories<span class="pl-k">=</span><span class="pl-c1">10_000</span>,saveat<span class="pl-k">=</span><span class="pl-c1">1.0f0</span>)</pre></div>
<p>These limitations are not fundamental and will be eased over time.</p>
<h4><a id="user-content-setting-up-multi-gpu" class="anchor" aria-hidden="true" href="#setting-up-multi-gpu"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Setting Up Multi-GPU</h4>
<p>To setup a multi-GPU environment, first setup a processes such that every process
has a different GPU. For example:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Setup processes with different CUDA devices
using Distributed
addprocs(numgpus)
import CUDAdrv, CUDAnative

let gpuworkers = asyncmap(collect(zip(workers(), CUDAdrv.devices()))) do (p, d)
  remotecall_wait(CUDAnative.device!, p, d)
  p
end
"><pre><span class="pl-c"><span class="pl-c">#</span> Setup processes with different CUDA devices</span>
<span class="pl-k">using</span> Distributed
<span class="pl-c1">addprocs</span>(numgpus)
<span class="pl-k">import</span> CUDAdrv, CUDAnative

<span class="pl-k">let</span> gpuworkers <span class="pl-k">=</span> <span class="pl-c1">asyncmap</span>(<span class="pl-c1">collect</span>(<span class="pl-c1">zip</span>(<span class="pl-c1">workers</span>(), CUDAdrv<span class="pl-k">.</span><span class="pl-c1">devices</span>()))) <span class="pl-k">do</span> (p, d)
  <span class="pl-c1">remotecall_wait</span>(CUDAnative<span class="pl-k">.</span>device!, p, d)
  p
<span class="pl-k">end</span></pre></div>
<p>Then setup the calls to work with distributed processes:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="@everywhere using DiffEqGPU, CuArrays, OrdinaryDiffEq, Test, Random

@everywhere begin
    function lorenz_distributed(du,u,p,t)
        du[1] = p[1]*(u[2]-u[1])
        du[2] = u[1]*(p[2]-u[3]) - u[2]
        du[3] = u[1]*u[2] - p[3]*u[3]
    end
    CuArrays.allowscalar(false)
    u0 = Float32[1.0;0.0;0.0]
    tspan = (0.0f0,100.0f0)
    p = [10.0f0,28.0f0,8/3f0]
    Random.seed!(1)
    function prob_func_distributed(prob,i,repeat)
        remake(prob,p=rand(3).*p)
    end
end
"><pre><span class="pl-c1">@everywhere</span> <span class="pl-k">using</span> DiffEqGPU, CuArrays, OrdinaryDiffEq, Test, Random

<span class="pl-c1">@everywhere</span> <span class="pl-k">begin</span>
    <span class="pl-k">function</span> <span class="pl-en">lorenz_distributed</span>(du,u,p,t)
        du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> p[<span class="pl-c1">1</span>]<span class="pl-k">*</span>(u[<span class="pl-c1">2</span>]<span class="pl-k">-</span>u[<span class="pl-c1">1</span>])
        du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]<span class="pl-k">*</span>(p[<span class="pl-c1">2</span>]<span class="pl-k">-</span>u[<span class="pl-c1">3</span>]) <span class="pl-k">-</span> u[<span class="pl-c1">2</span>]
        du[<span class="pl-c1">3</span>] <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]<span class="pl-k">*</span>u[<span class="pl-c1">2</span>] <span class="pl-k">-</span> p[<span class="pl-c1">3</span>]<span class="pl-k">*</span>u[<span class="pl-c1">3</span>]
    <span class="pl-k">end</span>
    CuArrays<span class="pl-k">.</span><span class="pl-c1">allowscalar</span>(<span class="pl-c1">false</span>)
    u0 <span class="pl-k">=</span> Float32[<span class="pl-c1">1.0</span>;<span class="pl-c1">0.0</span>;<span class="pl-c1">0.0</span>]
    tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0f0</span>,<span class="pl-c1">100.0f0</span>)
    p <span class="pl-k">=</span> [<span class="pl-c1">10.0f0</span>,<span class="pl-c1">28.0f0</span>,<span class="pl-c1">8</span><span class="pl-k">/</span><span class="pl-c1">3f0</span>]
    Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">1</span>)
    <span class="pl-k">function</span> <span class="pl-en">prob_func_distributed</span>(prob,i,repeat)
        <span class="pl-c1">remake</span>(prob,p<span class="pl-k">=</span><span class="pl-c1">rand</span>(<span class="pl-c1">3</span>)<span class="pl-k">.*</span>p)
    <span class="pl-k">end</span>
<span class="pl-k">end</span></pre></div>
<p>Now each batch will run on separate GPUs. Thus we need to use the <code>batch_size</code>
keyword argument from the Ensemble interface to ensure there are multiple batches.
Let's solve 40,000 trajectories, batching 10,000 trajectories at a time:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="prob = ODEProblem(lorenz_distributed,u0,tspan,p)
monteprob = EnsembleProblem(prob, prob_func = prob_func_distributed)

@time sol2 = solve(monteprob,Tsit5(),EnsembleGPUArray(),trajectories=40_000,
                                                 batch_size=10_000,saveat=1.0f0)
"><pre>prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(lorenz_distributed,u0,tspan,p)
monteprob <span class="pl-k">=</span> <span class="pl-c1">EnsembleProblem</span>(prob, prob_func <span class="pl-k">=</span> prob_func_distributed)

<span class="pl-c1">@time</span> sol2 <span class="pl-k">=</span> <span class="pl-c1">solve</span>(monteprob,<span class="pl-c1">Tsit5</span>(),<span class="pl-c1">EnsembleGPUArray</span>(),trajectories<span class="pl-k">=</span><span class="pl-c1">40_000</span>,
                                                 batch_size<span class="pl-k">=</span><span class="pl-c1">10_000</span>,saveat<span class="pl-k">=</span><span class="pl-c1">1.0f0</span>)</pre></div>
<p>This will <code>pmap</code> over the batches, and thus if you have 4 processes each with
a GPU, each batch of 10,000 trajectories will be run simultaneously. If you have
two processes with two GPUs, this will do two sets of 10,000 at a time.</p>
<h4><a id="user-content-example-multi-gpu-script" class="anchor" aria-hidden="true" href="#example-multi-gpu-script"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example Multi-GPU Script</h4>
<p>In this example we know we have a 2-GPU system (1 eGPU), and we split the work
across the two by directly defining the devices on the two worker processes:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using DiffEqGPU, CuArrays, OrdinaryDiffEq, Test
CuArrays.device!(0)

using Distributed
addprocs(2)
@everywhere using DiffEqGPU, CuArrays, OrdinaryDiffEq, Test, Random

@everywhere begin
    function lorenz_distributed(du,u,p,t)
        du[1] = p[1]*(u[2]-u[1])
        du[2] = u[1]*(p[2]-u[3]) - u[2]
        du[3] = u[1]*u[2] - p[3]*u[3]
    end
    CuArrays.allowscalar(false)
    u0 = Float32[1.0;0.0;0.0]
    tspan = (0.0f0,100.0f0)
    p = [10.0f0,28.0f0,8/3f0]
    Random.seed!(1)
    pre_p_distributed = [rand(Float32,3) for i in 1:100_000]
    function prob_func_distributed(prob,i,repeat)
        remake(prob,p=pre_p_distributed[i].*p)
    end
end

@sync begin
    @spawnat 2 begin
        CuArrays.allowscalar(false)
        CuArrays.device!(0)
    end
    @spawnat 3 begin
        CuArrays.allowscalar(false)
        CuArrays.device!(1)
    end
end

CuArrays.allowscalar(false)
prob = ODEProblem(lorenz_distributed,u0,tspan,p)
monteprob = EnsembleProblem(prob, prob_func = prob_func_distributed)

@time sol = solve(monteprob,Tsit5(),EnsembleGPUArray(),trajectories=100_000,batch_size=50_000,saveat=1.0f0)
"><pre><span class="pl-k">using</span> DiffEqGPU, CuArrays, OrdinaryDiffEq, Test
CuArrays<span class="pl-k">.</span><span class="pl-c1">device!</span>(<span class="pl-c1">0</span>)

<span class="pl-k">using</span> Distributed
<span class="pl-c1">addprocs</span>(<span class="pl-c1">2</span>)
<span class="pl-c1">@everywhere</span> <span class="pl-k">using</span> DiffEqGPU, CuArrays, OrdinaryDiffEq, Test, Random

<span class="pl-c1">@everywhere</span> <span class="pl-k">begin</span>
    <span class="pl-k">function</span> <span class="pl-en">lorenz_distributed</span>(du,u,p,t)
        du[<span class="pl-c1">1</span>] <span class="pl-k">=</span> p[<span class="pl-c1">1</span>]<span class="pl-k">*</span>(u[<span class="pl-c1">2</span>]<span class="pl-k">-</span>u[<span class="pl-c1">1</span>])
        du[<span class="pl-c1">2</span>] <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]<span class="pl-k">*</span>(p[<span class="pl-c1">2</span>]<span class="pl-k">-</span>u[<span class="pl-c1">3</span>]) <span class="pl-k">-</span> u[<span class="pl-c1">2</span>]
        du[<span class="pl-c1">3</span>] <span class="pl-k">=</span> u[<span class="pl-c1">1</span>]<span class="pl-k">*</span>u[<span class="pl-c1">2</span>] <span class="pl-k">-</span> p[<span class="pl-c1">3</span>]<span class="pl-k">*</span>u[<span class="pl-c1">3</span>]
    <span class="pl-k">end</span>
    CuArrays<span class="pl-k">.</span><span class="pl-c1">allowscalar</span>(<span class="pl-c1">false</span>)
    u0 <span class="pl-k">=</span> Float32[<span class="pl-c1">1.0</span>;<span class="pl-c1">0.0</span>;<span class="pl-c1">0.0</span>]
    tspan <span class="pl-k">=</span> (<span class="pl-c1">0.0f0</span>,<span class="pl-c1">100.0f0</span>)
    p <span class="pl-k">=</span> [<span class="pl-c1">10.0f0</span>,<span class="pl-c1">28.0f0</span>,<span class="pl-c1">8</span><span class="pl-k">/</span><span class="pl-c1">3f0</span>]
    Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">1</span>)
    pre_p_distributed <span class="pl-k">=</span> [<span class="pl-c1">rand</span>(Float32,<span class="pl-c1">3</span>) <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">100_000</span>]
    <span class="pl-k">function</span> <span class="pl-en">prob_func_distributed</span>(prob,i,repeat)
        <span class="pl-c1">remake</span>(prob,p<span class="pl-k">=</span>pre_p_distributed[i]<span class="pl-k">.</span><span class="pl-k">*</span>p)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-c1">@sync</span> <span class="pl-k">begin</span>
    <span class="pl-c1">@spawnat</span> <span class="pl-c1">2</span> <span class="pl-k">begin</span>
        CuArrays<span class="pl-k">.</span><span class="pl-c1">allowscalar</span>(<span class="pl-c1">false</span>)
        CuArrays<span class="pl-k">.</span><span class="pl-c1">device!</span>(<span class="pl-c1">0</span>)
    <span class="pl-k">end</span>
    <span class="pl-c1">@spawnat</span> <span class="pl-c1">3</span> <span class="pl-k">begin</span>
        CuArrays<span class="pl-k">.</span><span class="pl-c1">allowscalar</span>(<span class="pl-c1">false</span>)
        CuArrays<span class="pl-k">.</span><span class="pl-c1">device!</span>(<span class="pl-c1">1</span>)
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

CuArrays<span class="pl-k">.</span><span class="pl-c1">allowscalar</span>(<span class="pl-c1">false</span>)
prob <span class="pl-k">=</span> <span class="pl-c1">ODEProblem</span>(lorenz_distributed,u0,tspan,p)
monteprob <span class="pl-k">=</span> <span class="pl-c1">EnsembleProblem</span>(prob, prob_func <span class="pl-k">=</span> prob_func_distributed)

<span class="pl-c1">@time</span> sol <span class="pl-k">=</span> <span class="pl-c1">solve</span>(monteprob,<span class="pl-c1">Tsit5</span>(),<span class="pl-c1">EnsembleGPUArray</span>(),trajectories<span class="pl-k">=</span><span class="pl-c1">100_000</span>,batch_size<span class="pl-k">=</span><span class="pl-c1">50_000</span>,saveat<span class="pl-k">=</span><span class="pl-c1">1.0f0</span>)</pre></div>
<h4><a id="user-content-optimal-numbers-of-trajectories" class="anchor" aria-hidden="true" href="#optimal-numbers-of-trajectories"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Optimal Numbers of Trajectories</h4>
<p>There is a balance between two things for choosing the number of trajectories:</p>
<ul>
<li>The number of trajectories needs to be high enough that the work per kernel
is sufficient to overcome the kernel call cost.</li>
<li>More trajectories means that every trajectory will need more time steps since
the adaptivity syncs all solves.</li>
</ul>
<p>From our testing, the balance is found at around 10,000 trajectories being optimal.
Thus for larger sets of trajectories, use a batch size of 10,000. Of course,
benchmark for yourself on your own setup!</p>
</article></div>