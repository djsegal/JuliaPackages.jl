<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-dianoiaml" class="anchor" aria-hidden="true" href="#dianoiaml"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DianoiaML</h1>
<p dir="auto">DianoiaML is an experimental Keras-like deep learning framework.</p>
<p dir="auto">The user guide and the To-Do list can be found <a href="https://github.com/SkyWorld117/YisyAIFramework.jl/wiki">here</a>.</p>
<p dir="auto">Environment: Julia 1.6.1</p>
<p dir="auto"><strong>Features</strong>:</p>
<details>
 
<p dir="auto">
</p><ul dir="auto">
<li>Network
<ul dir="auto">
<li>Sequential</li>
<li>GAN</li>
<li>ResNet</li>
</ul>
</li>
<li>Layer
<ul dir="auto">
<li>Flatten</li>
<li>Constructive</li>
<li>Dense</li>
<li>Convolutional2D</li>
<li>MaxPooling2D</li>
<li>UpSampling2D</li>
<li>Transposed Convolutional2D</li>
<li>Dropout</li>
<li>Residual</li>
</ul>
</li>
<li>Activation Function
<ul dir="auto">
<li>ReLU</li>
<li>Sigmoid</li>
<li>Softmax</li>
<li>tanh</li>
</ul>
</li>
<li>Loss Function
<ul dir="auto">
<li>Quadratic Loss</li>
<li>Categorical Cross Entropy Loss</li>
<li>Binary Cross Entropy Loss</li>
<li>Mean Squared Error</li>
</ul>
</li>
<li>Monitor
<ul dir="auto">
<li>Absolute</li>
<li>Classification</li>
</ul>
</li>
<li>Optimizer
<ul dir="auto">
<li>Minibatch Gradient Descent</li>
<li>Stochastic Gradient Descent</li>
<li>Adam</li>
<li>AdaBelief</li>
<li>Genetic Algorithm</li>
</ul>
</li>
<li>Tools
<ul dir="auto">
<li>Model Management</li>
<li>One Hot</li>
</ul>
</li>
</ul>
<p dir="auto"></p>
</details>
<p dir="auto"><strong>An example of Sequential network</strong>:</p>
<details>
 
<p dir="auto">
</p><div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLDatasets, DianoiaML

train_x, train_y = MNIST.traindata()
test_x, test_y = MNIST.testdata()
dict = Dict{Int64, Int64}(1=&gt;1, 2=&gt;2, 3=&gt;3, 4=&gt;4, 5=&gt;5, 6=&gt;6, 7=&gt;7, 8=&gt;8, 9=&gt;9, 0=&gt;10)

model = Sequential()
model.add_layer(model, Conv2D; filter=32, input_shape=(28,28,1), kernel_size=(3,3), activation_function=ReLU)
model.add_layer(model, MaxPooling2D; pool_size=(2,2))
model.add_layer(model, Conv2D; filter=64, kernel_size=(3,3), activation_function=ReLU)
model.add_layer(model, MaxPooling2D; pool_size=(2,2))
model.add_layer(model, Flatten;)
model.add_layer(model, Dense; layer_size=128, activation_function=ReLU)
model.add_layer(model, Dense; layer_size=10, activation_function=Softmax_CEL)

Adam.fit(model=model, input_data=Array{Float32}(reshape(train_x, 28,28,1,60000)), output_data=oneHot(train_y, 10, dict),
        loss_function=Categorical_Cross_Entropy_Loss, monitor=Classification, epochs=10, batch=128)"><pre><span class="pl-k">using</span> MLDatasets, DianoiaML

train_x, train_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>()
test_x, test_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">testdata</span>()
dict <span class="pl-k">=</span> <span class="pl-c1">Dict</span><span class="pl-c1">{Int64, Int64}</span>(<span class="pl-c1">1</span><span class="pl-k">=&gt;</span><span class="pl-c1">1</span>, <span class="pl-c1">2</span><span class="pl-k">=&gt;</span><span class="pl-c1">2</span>, <span class="pl-c1">3</span><span class="pl-k">=&gt;</span><span class="pl-c1">3</span>, <span class="pl-c1">4</span><span class="pl-k">=&gt;</span><span class="pl-c1">4</span>, <span class="pl-c1">5</span><span class="pl-k">=&gt;</span><span class="pl-c1">5</span>, <span class="pl-c1">6</span><span class="pl-k">=&gt;</span><span class="pl-c1">6</span>, <span class="pl-c1">7</span><span class="pl-k">=&gt;</span><span class="pl-c1">7</span>, <span class="pl-c1">8</span><span class="pl-k">=&gt;</span><span class="pl-c1">8</span>, <span class="pl-c1">9</span><span class="pl-k">=&gt;</span><span class="pl-c1">9</span>, <span class="pl-c1">0</span><span class="pl-k">=&gt;</span><span class="pl-c1">10</span>)

model <span class="pl-k">=</span> <span class="pl-c1">Sequential</span>()
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Conv2D; filter<span class="pl-k">=</span><span class="pl-c1">32</span>, input_shape<span class="pl-k">=</span>(<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>), kernel_size<span class="pl-k">=</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, MaxPooling2D; pool_size<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>))
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Conv2D; filter<span class="pl-k">=</span><span class="pl-c1">64</span>, kernel_size<span class="pl-k">=</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, MaxPooling2D; pool_size<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>))
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Flatten;)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">128</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">10</span>, activation_function<span class="pl-k">=</span>Softmax_CEL)

Adam<span class="pl-k">.</span><span class="pl-c1">fit</span>(model<span class="pl-k">=</span>model, input_data<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float32}</span>(<span class="pl-c1">reshape</span>(train_x, <span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,<span class="pl-c1">60000</span>)), output_data<span class="pl-k">=</span><span class="pl-c1">oneHot</span>(train_y, <span class="pl-c1">10</span>, dict),
        loss_function<span class="pl-k">=</span>Categorical_Cross_Entropy_Loss, monitor<span class="pl-k">=</span>Classification, epochs<span class="pl-k">=</span><span class="pl-c1">10</span>, batch<span class="pl-k">=</span><span class="pl-c1">128</span>)</pre></div>
<p dir="auto"></p>
</details>
<p dir="auto"><strong>An example of GAN</strong>:</p>
<details>
 
<p dir="auto">
</p><div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLDatasets, DianoiaML

train_x, train_y = MNIST.traindata()
test_x, test_y = MNIST.testdata()
dict = Dict{Int64, Int64}(1=&gt;1, 2=&gt;2, 3=&gt;3, 4=&gt;4, 5=&gt;5, 6=&gt;6, 7=&gt;7, 8=&gt;8, 9=&gt;9, 0=&gt;10)

function noise()
    return reshape([rand(1.0f0:1.0f0:10.0f0), rand(Float32)], (2,1))
end

model = GAN(noise)

model.add_Glayer(model, Dense; input_shape=(2,), layer_size=16, activation_function=ReLU)
model.add_Glayer(model, Dense; layer_size=64, activation_function=ReLU)
model.add_Glayer(model, Constructive; shape=(8,8,1))
model.add_Glayer(model, UpSampling2D; size=(2,2), activation_function=None)
model.add_Glayer(model, Flatten;)
model.add_Glayer(model, Dense; layer_size=256, activation_function=ReLU)
model.add_Glayer(model, Dense; layer_size=784, activation_function=ReLU)
model.add_Glayer(model, Constructive; shape=(28,28,1))

model.add_Dlayer(model, Conv2D; filter=16, kernel_size=(3,3), activation_function=ReLU)
model.add_Dlayer(model, Conv2D; filter=32, kernel_size=(3,3), activation_function=ReLU)
model.add_Dlayer(model, MaxPooling2D; kernel_size=(2,2), activation_function=None)
model.add_Dlayer(model, Flatten;)
model.add_Dlayer(model, Dense; layer_size=128, activation_function=ReLU)
model.add_Dlayer(model, Dense; layer_size=64, activation_function=ReLU)
model.add_Dlayer(model, Dense; layer_size=2, activation_function=Sigmoid)

SGD.fit(model=model, input_data=Array{Float32}(reshape(train_x, 28,28,1,60000)), output_data=oneHot(train_y, 10, dict),
        loss_function=Binary_Cross_Entropy_Loss, monitor=Classification, epochs=50, batch=128)"><pre><span class="pl-k">using</span> MLDatasets, DianoiaML

train_x, train_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>()
test_x, test_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">testdata</span>()
dict <span class="pl-k">=</span> <span class="pl-c1">Dict</span><span class="pl-c1">{Int64, Int64}</span>(<span class="pl-c1">1</span><span class="pl-k">=&gt;</span><span class="pl-c1">1</span>, <span class="pl-c1">2</span><span class="pl-k">=&gt;</span><span class="pl-c1">2</span>, <span class="pl-c1">3</span><span class="pl-k">=&gt;</span><span class="pl-c1">3</span>, <span class="pl-c1">4</span><span class="pl-k">=&gt;</span><span class="pl-c1">4</span>, <span class="pl-c1">5</span><span class="pl-k">=&gt;</span><span class="pl-c1">5</span>, <span class="pl-c1">6</span><span class="pl-k">=&gt;</span><span class="pl-c1">6</span>, <span class="pl-c1">7</span><span class="pl-k">=&gt;</span><span class="pl-c1">7</span>, <span class="pl-c1">8</span><span class="pl-k">=&gt;</span><span class="pl-c1">8</span>, <span class="pl-c1">9</span><span class="pl-k">=&gt;</span><span class="pl-c1">9</span>, <span class="pl-c1">0</span><span class="pl-k">=&gt;</span><span class="pl-c1">10</span>)

<span class="pl-k">function</span> <span class="pl-en">noise</span>()
    <span class="pl-k">return</span> <span class="pl-c1">reshape</span>([<span class="pl-c1">rand</span>(<span class="pl-c1">1.0f0</span><span class="pl-k">:</span><span class="pl-c1">1.0f0</span><span class="pl-k">:</span><span class="pl-c1">10.0f0</span>), <span class="pl-c1">rand</span>(Float32)], (<span class="pl-c1">2</span>,<span class="pl-c1">1</span>))
<span class="pl-k">end</span>

model <span class="pl-k">=</span> <span class="pl-c1">GAN</span>(noise)

model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Dense; input_shape<span class="pl-k">=</span>(<span class="pl-c1">2</span>,), layer_size<span class="pl-k">=</span><span class="pl-c1">16</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">64</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Constructive; shape<span class="pl-k">=</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>,<span class="pl-c1">1</span>))
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, UpSampling2D; size<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>), activation_function<span class="pl-k">=</span>None)
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Flatten;)
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">256</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">784</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Glayer</span>(model, Constructive; shape<span class="pl-k">=</span>(<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>))

model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, Conv2D; filter<span class="pl-k">=</span><span class="pl-c1">16</span>, kernel_size<span class="pl-k">=</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, Conv2D; filter<span class="pl-k">=</span><span class="pl-c1">32</span>, kernel_size<span class="pl-k">=</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, MaxPooling2D; kernel_size<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>), activation_function<span class="pl-k">=</span>None)
model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, Flatten;)
model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">128</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">64</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_Dlayer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">2</span>, activation_function<span class="pl-k">=</span>Sigmoid)

SGD<span class="pl-k">.</span><span class="pl-c1">fit</span>(model<span class="pl-k">=</span>model, input_data<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float32}</span>(<span class="pl-c1">reshape</span>(train_x, <span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,<span class="pl-c1">60000</span>)), output_data<span class="pl-k">=</span><span class="pl-c1">oneHot</span>(train_y, <span class="pl-c1">10</span>, dict),
        loss_function<span class="pl-k">=</span>Binary_Cross_Entropy_Loss, monitor<span class="pl-k">=</span>Classification, epochs<span class="pl-k">=</span><span class="pl-c1">50</span>, batch<span class="pl-k">=</span><span class="pl-c1">128</span>)</pre></div>
<p dir="auto"></p>
</details>
<p dir="auto"><strong>An example of ResNet</strong>:</p>
<details>
 
<p dir="auto">
</p><div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLDatasets, DianoiaML

train_x, train_y = MNIST.traindata()
test_x, test_y = MNIST.testdata()
dict = Dict{Int64, Int64}(1=&gt;1, 2=&gt;2, 3=&gt;3, 4=&gt;4, 5=&gt;5, 6=&gt;6, 7=&gt;7, 8=&gt;8, 9=&gt;9, 0=&gt;10)


resnet = ResNet()
resnet.add_layer(resnet, Dense; input_shape=(128,), layer_size=64, activation_function=ReLU)
resnet.add_layer(resnet, Dense; layer_size=64, activation_function=ReLU)
resnet.add_layer(resnet, Dense; layer_size=128, activation_function=ReLU)

model = Sequential()
model.add_layer(model, Conv2D; filter=32, input_shape=(28,28,1), kernel_size=(3,3), activation_function=ReLU)
model.add_layer(model, MaxPooling2D; pool_size=(2,2))
model.add_layer(model, Conv2D; filter=64, kernel_size=(3,3), activation_function=ReLU)
model.add_layer(model, MaxPooling2D; pool_size=(2,2))
model.add_layer(model, Flatten;)
model.add_layer(model, Dropout; rate=0.5)
model.add_layer(model, Dense; layer_size=128, activation_function=ReLU)
model.add_layer(model, Residual; resnet=resnet)
model.add_layer(model, Dense; layer_size=10, activation_function=Softmax_CEL)

Adam.fit(model=model, input_data=Array{Float32}(reshape(train_x, 28,28,1,60000)), output_data=oneHot(train_y, 10, dict),
        loss_function=Categorical_Cross_Entropy_Loss, monitor=Classification, epochs=10, batch=128)"><pre><span class="pl-k">using</span> MLDatasets, DianoiaML

train_x, train_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">traindata</span>()
test_x, test_y <span class="pl-k">=</span> MNIST<span class="pl-k">.</span><span class="pl-c1">testdata</span>()
dict <span class="pl-k">=</span> <span class="pl-c1">Dict</span><span class="pl-c1">{Int64, Int64}</span>(<span class="pl-c1">1</span><span class="pl-k">=&gt;</span><span class="pl-c1">1</span>, <span class="pl-c1">2</span><span class="pl-k">=&gt;</span><span class="pl-c1">2</span>, <span class="pl-c1">3</span><span class="pl-k">=&gt;</span><span class="pl-c1">3</span>, <span class="pl-c1">4</span><span class="pl-k">=&gt;</span><span class="pl-c1">4</span>, <span class="pl-c1">5</span><span class="pl-k">=&gt;</span><span class="pl-c1">5</span>, <span class="pl-c1">6</span><span class="pl-k">=&gt;</span><span class="pl-c1">6</span>, <span class="pl-c1">7</span><span class="pl-k">=&gt;</span><span class="pl-c1">7</span>, <span class="pl-c1">8</span><span class="pl-k">=&gt;</span><span class="pl-c1">8</span>, <span class="pl-c1">9</span><span class="pl-k">=&gt;</span><span class="pl-c1">9</span>, <span class="pl-c1">0</span><span class="pl-k">=&gt;</span><span class="pl-c1">10</span>)


resnet <span class="pl-k">=</span> <span class="pl-c1">ResNet</span>()
resnet<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(resnet, Dense; input_shape<span class="pl-k">=</span>(<span class="pl-c1">128</span>,), layer_size<span class="pl-k">=</span><span class="pl-c1">64</span>, activation_function<span class="pl-k">=</span>ReLU)
resnet<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(resnet, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">64</span>, activation_function<span class="pl-k">=</span>ReLU)
resnet<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(resnet, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">128</span>, activation_function<span class="pl-k">=</span>ReLU)

model <span class="pl-k">=</span> <span class="pl-c1">Sequential</span>()
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Conv2D; filter<span class="pl-k">=</span><span class="pl-c1">32</span>, input_shape<span class="pl-k">=</span>(<span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>), kernel_size<span class="pl-k">=</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, MaxPooling2D; pool_size<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>))
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Conv2D; filter<span class="pl-k">=</span><span class="pl-c1">64</span>, kernel_size<span class="pl-k">=</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>), activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, MaxPooling2D; pool_size<span class="pl-k">=</span>(<span class="pl-c1">2</span>,<span class="pl-c1">2</span>))
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Flatten;)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Dropout; rate<span class="pl-k">=</span><span class="pl-c1">0.5</span>)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">128</span>, activation_function<span class="pl-k">=</span>ReLU)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Residual; resnet<span class="pl-k">=</span>resnet)
model<span class="pl-k">.</span><span class="pl-c1">add_layer</span>(model, Dense; layer_size<span class="pl-k">=</span><span class="pl-c1">10</span>, activation_function<span class="pl-k">=</span>Softmax_CEL)

Adam<span class="pl-k">.</span><span class="pl-c1">fit</span>(model<span class="pl-k">=</span>model, input_data<span class="pl-k">=</span><span class="pl-c1">Array</span><span class="pl-c1">{Float32}</span>(<span class="pl-c1">reshape</span>(train_x, <span class="pl-c1">28</span>,<span class="pl-c1">28</span>,<span class="pl-c1">1</span>,<span class="pl-c1">60000</span>)), output_data<span class="pl-k">=</span><span class="pl-c1">oneHot</span>(train_y, <span class="pl-c1">10</span>, dict),
        loss_function<span class="pl-k">=</span>Categorical_Cross_Entropy_Loss, monitor<span class="pl-k">=</span>Classification, epochs<span class="pl-k">=</span><span class="pl-c1">10</span>, batch<span class="pl-k">=</span><span class="pl-c1">128</span>)</pre></div>
<p dir="auto"></p>
</details>
<p dir="auto">Please feel free to leave comments, trouble-shootings or advice (which are very valuable for me).</p>
</article></div>