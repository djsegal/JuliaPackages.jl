<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-tensoroperationsjl" class="anchor" aria-hidden="true" href="#tensoroperationsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TensorOperations.jl</h1>
<p>Fast tensor operations using a convenient Einstein index notation.</p>
<table>
<thead>
<tr>
<th align="center"><strong>Documentation</strong></th>
<th align="center"><strong>Build Status</strong></th>
<th align="center"><strong>Digital Object Identifier</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://jutho.github.io/TensorOperations.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a> <a href="https://jutho.github.io/TensorOperations.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/Jutho/TensorOperations.jl/actions?query=workflow%3ACI"><img src="https://github.com/Jutho/TensorOperations.jl/workflows/CI/badge.svg" alt="CI" style="max-width:100%;"></a> <a href="https://github.com/Jutho/TensorOperations.jl/actions?query=workflow%3A%22CI+%28Julia+nightly%29%22"><img src="https://github.com/Jutho/TensorOperations.jl/workflows/CI%20(Julia%20nightly)/badge.svg" alt="CI (Julia nightly)" style="max-width:100%;"></a> <a href="https://codecov.io/gh/Jutho/TensorOperations.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4b437d76010ca0a653f651a3095452fb5e1073d2314e7065cdf0535f86cda946/68747470733a2f2f636f6465636f762e696f2f67682f4a7574686f2f54656e736f724f7065726174696f6e732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/Jutho/TensorOperations.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://doi.org/10.5281/zenodo.3245497" rel="nofollow"><img src="https://camo.githubusercontent.com/4b1ed2f8e1ac3d358630c167ffa848f32e3379daa9fac359be820e39ff90b86f/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333234353439372e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.3245497.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h2><a id="user-content-whats-new-in-v3" class="anchor" aria-hidden="true" href="#whats-new-in-v3"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What's new in v3</h2>
<ul>
<li>
<p>Switched to CUDA.jl instead of CuArrays.jl, which effectively restricts support to
Julia 1.4 and higher.</p>
</li>
<li>
<p>The default cache size for intermediate results is now the minimum of either 4GB or one
quarter of your total memory (obtained via <code>Sys.total_memory()</code>). Furthermore, the
structure (i.e. <code>size</code>) and <code>eltype</code> of the temporaries is now also used as lookup key
in the LRU cache, such that you can run the same code on different objects with
different sizes or element types, without constantly having to reallocate the
temporaries. Finally, the task rather than <code>threadid</code> is used to make the cache
compatible with concurrency at any level.</p>
<p>As a consequence, different objects for the same temporary location can now be cached,
such that the cache can grow out of size quickly. Once the cache is not able to hold all
the temporary objects needed for your simulation, it might actually deteriorate
perfomance, and you might be better off disabling the cache alltogether with
<code>TensorOperations.disable_cache()</code>.</p>
</li>
</ul>
<blockquote>
<p><strong>WARNING:</strong> TensorOperations 3.0 contains breaking changes if you did implement support
for custom array / tensor types by overloading <code>checked_similar_from_indices</code> etc.</p>
</blockquote>
<h3><a id="user-content-code-example" class="anchor" aria-hidden="true" href="#code-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code example</h3>
<p>TensorOperations.jl is mostly used through the <code>@tensor</code> macro which allows one to express
a given operation in terms of
<a href="https://en.wikipedia.org/wiki/Abstract_index_notation" rel="nofollow">index notation</a> format, a.k.a.
<a href="https://en.wikipedia.org/wiki/Einstein_notation" rel="nofollow">Einstein notation</a>
(using Einstein's summation convention).</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using TensorOperations
α=randn()
A=randn(5,5,5,5,5,5)
B=randn(5,5,5)
C=randn(5,5,5)
D=zeros(5,5,5)
@tensor begin
    D[a,b,c] = A[a,e,f,c,f,g]*B[g,b,e] + α*C[c,a,b]
    E[a,b,c] := A[a,e,f,c,f,g]*B[g,b,e] + α*C[c,a,b]
end
"><pre><span class="pl-k">using</span> TensorOperations
α<span class="pl-k">=</span><span class="pl-c1">randn</span>()
A<span class="pl-k">=</span><span class="pl-c1">randn</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>)
B<span class="pl-k">=</span><span class="pl-c1">randn</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>)
C<span class="pl-k">=</span><span class="pl-c1">randn</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>)
D<span class="pl-k">=</span><span class="pl-c1">zeros</span>(<span class="pl-c1">5</span>,<span class="pl-c1">5</span>,<span class="pl-c1">5</span>)
<span class="pl-c1">@tensor</span> <span class="pl-k">begin</span>
    D[a,b,c] <span class="pl-k">=</span> A[a,e,f,c,f,g]<span class="pl-k">*</span>B[g,b,e] <span class="pl-k">+</span> α<span class="pl-k">*</span>C[c,a,b]
    E[a,b,c] <span class="pl-k">:=</span> A[a,e,f,c,f,g]<span class="pl-k">*</span>B[g,b,e] <span class="pl-k">+</span> α<span class="pl-k">*</span>C[c,a,b]
<span class="pl-k">end</span></pre></div>
<p>In the second to last line, the result of the operation will be stored in the preallocated
array <code>D</code>, whereas the last line uses a different assignment operator <code>:=</code> in order to
define and allocate a new array <code>E</code> of the correct size. The contents of <code>D</code> and <code>E</code> will
be equal.</p>
<p>For more information, please see the documentation.</p>
</article></div>