<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-svddjl" class="anchor" aria-hidden="true" href="#svddjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SVDD.jl</h1>
<p dir="auto"><em>A Julia package for Support Vector Data Description.</em></p>
<p dir="auto"><a href="https://englhardt.github.io/SVDD.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/4fbf1a0e9af715d83da2c6cb134932756a9f0a25d715cecf4c83b1437b7996eb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6d61737465722d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-master-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/englhardt/SVDD.jl/actions"><img src="https://github.com/englhardt/SVDD.jl/actions/workflows/ci.yml/badge.svg" alt="Actions Status: test" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/englhardt/SVDD.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/bbbc7ae397ead1ff9804f90d3dd346beb8b9c79bb69831c1f80e72252cbc8e0b/68747470733a2f2f636f6465636f762e696f2f67682f656e676c68617264742f535644442e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d5148384a515245454948" alt="codecov" data-canonical-src="https://codecov.io/gh/englhardt/SVDD.jl/branch/master/graph/badge.svg?token=QH8JQREEIH" style="max-width: 100%;"></a></p>
<p dir="auto">This package implements one-class classifiers and based on support vector data description.
The package has been developed as part of a benchmark suite for <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" rel="nofollow">active-learning</a> strategies for one-class classification. For more information about this research project, see the <a href="https://www.ipd.kit.edu/ocal/" rel="nofollow">OCAL project</a> website, and the companion paper.</p>
<blockquote>
<p dir="auto">Holger Trittenbach, Adrian Englhardt, Klemens Böhm, "An overview and a benchmark of active learning for outlier detection with one-class classifiers", DOI: <a href="https://doi.org/10.1016/j.eswa.2020.114372" rel="nofollow">10.1016/j.eswa.2020.114372</a>, Expert Systems with Applications, 2021.</p>
</blockquote>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">This package works with Julia 1.0 or newer.
This package is not registered yet. Please use the following command to add the package with Pkg3.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;https://github.com/englhardt/SVDD.jl.git&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/englhardt/SVDD.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<p dir="auto"><a href="https://en.wikipedia.org/wiki/One-class_classification" rel="nofollow">One-class classifiers</a> learn to identify if objects belong to a specific class, often used for outlier detection.
The package implements several one-class classifiers, and strategies to initialize parameters.
We other visualizations in our example notebooks, see <a href="#examples">Examples</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="examples/SVDD_example.png"><img src="examples/SVDD_example.png" alt="SVDD" style="max-width: 100%;"></a></p>
<h3 dir="auto"><a id="user-content-classifiers" class="anchor" aria-hidden="true" href="#classifiers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Classifiers</h3>
<p dir="auto">Currently, the classifiers have been implemented as optimization problems based on <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a>.
The package includes:</p>
<ul dir="auto">
<li>Vanilla Support Vector Data Description (VanillaSVDD) [1]</li>
<li>SVDD with negative examples (SVDDNeg) [1]</li>
<li>Semi-supervised Anomaly Detection (SSAD) [2]</li>
<li>Subspace SVDD (SubSVDD) [3]</li>
</ul>
<h3 dir="auto"><a id="user-content-parameter-initialization" class="anchor" aria-hidden="true" href="#parameter-initialization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Parameter Initialization</h3>
<p dir="auto">There are two types of parameters to estimate for the classifiers: cost parameters and a kernel function.
The packages includes the following strategies to initialize parameters.</p>
<ul dir="auto">
<li><em>Gauss Kernel gamma</em>
<ul dir="auto">
<li>Rule of Scott [4]</li>
<li>Rule of Silverman [5]</li>
<li>Mean criterion [6]</li>
<li>Modified mean criterion [7]</li>
<li>Wang data shifting [8]</li>
<li>Fixed Gamma</li>
</ul>
</li>
<li><em>Cost parameters C</em>
<ul dir="auto">
<li>Rule of Tax [1]</li>
<li>Binary Search</li>
<li>Fixed C</li>
</ul>
</li>
</ul>
<h3 dir="auto"><a id="user-content-conventions" class="anchor" aria-hidden="true" href="#conventions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Conventions</h3>
<ul dir="auto">
<li><em>Classification scores:</em> The classifiers return scores by the following convention:
<ul dir="auto">
<li>score &gt; 0 for outliers</li>
<li>score &lt;= 0 for inliers</li>
</ul>
</li>
<li><em>Data Format:</em> The data is expected to be in column major order, i.e., first array dimension is the attribute, second is the observation.
<ul dir="auto">
<li><code>[1 2 3 4; 5 6 7 8]</code> is a 2x4 Array with 2 attributes and 4 observations</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">There are two notebooks that show to train a SVDD (<a href="examples/svdd_training.ipynb">here</a>) and how to use the parametrization methods (<a href="examples/svdd_parametrization.ipynb">here</a>).
Execute the following commands to run the example notebooks:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="git clone https://github.com/englhardt/SVDD.jl
cd SVDD/examples
julia -e &quot;using Pkg; Pkg.instantiate()&quot;
julia -e &quot;using IJulia; notebook()&quot;"><pre class="notranslate"><code>git clone https://github.com/englhardt/SVDD.jl
cd SVDD/examples
julia -e "using Pkg; Pkg.instantiate()"
julia -e "using IJulia; notebook()"
</code></pre></div>
<p dir="auto">You can then access the jupyter notebook server at <a href="http://localhost:8888/" rel="nofollow">http://localhost:8888/</a> and run the notebooks.</p>
<h2 dir="auto"><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Authors</h2>
<p dir="auto">We welcome contributions and bug reports.</p>
<p dir="auto">This package is developed and maintained by <a href="https://github.com/holtri/">Holger Trittenbach</a> and <a href="https://github.com/englhardt">Adrian Englhardt</a>.</p>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<p dir="auto">[1] Tax, David MJ, and Robert PW Duin. "Support vector data description." Machine learning 54.1 (2004): 45-66.<br>
[2] Görnitz, Nico, et al. "Toward supervised anomaly detection." Journal of Artificial Intelligence Research 46 (2013): 235-262.<br>
[4] Scott, David W. Multivariate density estimation: theory, practice, and visualization. John Wiley &amp; Sons, 2015.<br>
[3] Trittenbach, Holger, and Klemens Böhm. "One-Class Active Learning for Outlier Detection with Multiple Subspaces." ACM International Conference on Information and Knowledge Management (CIKM), 2019.<br>
[5] Silverman, Bernard W. Density estimation for statistics and data analysis. Routledge, 2018.<br>
[6] Chaudhuri, Arin, et al. "The mean and median criteria for kernel bandwidth selection for support vector data description." 2017 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE, 2017.<br>
[7] Liao, Yuwei, et al. "A new bandwidth selection criterion for using SVDD to analyze hyperspectral data." Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIV. Vol. 10644. International Society for Optics and Photonics, 2018.<br>
[8] Wang, Siqi, et al. "Hyperparameter selection of one-class support vector machine by self-adaptive data shifting." Pattern Recognition 74 (2018): 198-211.</p>
</article></div>