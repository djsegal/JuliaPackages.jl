<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-svddjl" class="anchor" aria-hidden="true" href="#svddjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SVDD.jl</h1>
<p><em>A Julia package for Support Vector Data Description.</em></p>
<p><a href="https://englhardt.github.io/SVDD.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/4fbf1a0e9af715d83da2c6cb134932756a9f0a25d715cecf4c83b1437b7996eb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6d61737465722d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-master-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.com/englhardt/SVDD.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/b82569c2858aa7308277d9c973462bdf5a43ef7d8b901c06ed7800431a6c9693/68747470733a2f2f7472617669732d63692e636f6d2f656e676c68617264742f535644442e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/englhardt/SVDD.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/englhardt/SVDD.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/5a1c12bb782890b5acebab808355f062a26ee1addeaa220a114fcb7feb3a8683/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f656e676c68617264742f535644442e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/englhardt/SVDD.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package implements one-class classifiers and based on support vector data description.
The package has been developed as part of a benchmark suite for <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" rel="nofollow">active-learning</a> strategies for one-class classification. For more information about this research project, see the <a href="https://www.ipd.kit.edu/ocal/" rel="nofollow">OCAL project</a> website, and the companion paper.</p>
<blockquote>
<p>Holger Trittenbach, Adrian Englhardt, Klemens Böhm, "An overview and a benchmark of active learning for outlier detection with one-class classifiers", DOI: <a href="https://doi.org/10.1016/j.eswa.2020.114372" rel="nofollow">10.1016/j.eswa.2020.114372</a>, Expert Systems with Applications, 2021.</p>
</blockquote>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>This package works with Julia 1.0 or newer (latest tested version is 1.5).
This package is not registered yet. Please use the following command to add the package with Pkg3.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;https://github.com/englhardt/SVDD.jl.git&quot;)
"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>https://github.com/englhardt/SVDD.jl.git<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview</h2>
<p><a href="https://en.wikipedia.org/wiki/One-class_classification" rel="nofollow">One-class classifiers</a> learn to identify if objects belong to a specific class, often used for outlier detection.
The package implements several one-class classifiers, and strategies to initialize parameters.
We other visualizations in our example notebooks, see <a href="#examples">Examples</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="examples/SVDD_example.png"><img src="examples/SVDD_example.png" alt="SVDD" style="max-width:100%;"></a></p>
<h3><a id="user-content-classifiers" class="anchor" aria-hidden="true" href="#classifiers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Classifiers</h3>
<p>Currently, the classifiers have been implemented as optimization problems based on <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a>.
The package includes:</p>
<ul>
<li>Vanilla Support Vector Data Description (VanillaSVDD) [1]</li>
<li>SVDD with negative examples (SVDDNeg) [1]</li>
<li>Semi-supervised Anomaly Detection (SSAD) [2]</li>
<li>Subspace SVDD (SubSVDD) [3]</li>
</ul>
<h3><a id="user-content-parameter-initialization" class="anchor" aria-hidden="true" href="#parameter-initialization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Parameter Initialization</h3>
<p>There are two types of parameters to estimate for the classifiers: cost parameters and a kernel function.
The packages includes the following strategies to initialize parameters.</p>
<ul>
<li><em>Gauss Kernel gamma</em>
<ul>
<li>Rule of Scott [4]</li>
<li>Rule of Silverman [5]</li>
<li>Mean criterion [6]</li>
<li>Modified mean criterion [7]</li>
<li>Wang data shifting [8]</li>
<li>Fixed Gamma</li>
</ul>
</li>
<li><em>Cost parameters C</em>
<ul>
<li>Rule of Tax [1]</li>
<li>Binary Search</li>
<li>Fixed C</li>
</ul>
</li>
</ul>
<h3><a id="user-content-conventions" class="anchor" aria-hidden="true" href="#conventions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conventions</h3>
<ul>
<li><em>Classification scores:</em> The classifiers return scores by the following convention:
<ul>
<li>score &gt; 0 for outliers</li>
<li>score &lt;= 0 for inliers</li>
</ul>
</li>
<li><em>Data Format:</em> The data is expected to be in column major order, i.e., first array dimension is the attribute, second is the observation.
<ul>
<li><code>[1 2 3 4; 5 6 7 8]</code> is a 2x4 Array with 2 attributes and 4 observations</li>
</ul>
</li>
</ul>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<p>There are two notebooks that show to train a SVDD (<a href="examples/svdd_training.ipynb">here</a>) and how to use the parametrization methods (<a href="examples/svdd_parametrization.ipynb">here</a>).
Execute the following commands to run the example notebooks:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="git clone https://github.com/englhardt/SVDD.jl
cd SVDD/examples
julia -e &quot;using Pkg; Pkg.instantiate()&quot;
julia -e &quot;using IJulia; notebook()&quot;
"><pre><code>git clone https://github.com/englhardt/SVDD.jl
cd SVDD/examples
julia -e "using Pkg; Pkg.instantiate()"
julia -e "using IJulia; notebook()"
</code></pre></div>
<p>You can then access the jupyter notebook server at <a href="http://localhost:8888/" rel="nofollow">http://localhost:8888/</a> and run the notebooks.</p>
<h2><a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Authors</h2>
<p>We welcome contributions and bug reports.</p>
<p>This package is developed and maintained by <a href="https://github.com/holtri/">Holger Trittenbach</a> and <a href="https://github.com/englhardt">Adrian Englhardt</a>.</p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<p>[1] Tax, David MJ, and Robert PW Duin. "Support vector data description." Machine learning 54.1 (2004): 45-66.<br>
[2] Görnitz, Nico, et al. "Toward supervised anomaly detection." Journal of Artificial Intelligence Research 46 (2013): 235-262.<br>
[4] Scott, David W. Multivariate density estimation: theory, practice, and visualization. John Wiley &amp; Sons, 2015.<br>
[3] Trittenbach, Holger, and Klemens Böhm. "One-Class Active Learning for Outlier Detection with Multiple Subspaces." ACM International Conference on Information and Knowledge Management (CIKM), 2019.<br>
[5] Silverman, Bernard W. Density estimation for statistics and data analysis. Routledge, 2018.<br>
[6] Chaudhuri, Arin, et al. "The mean and median criteria for kernel bandwidth selection for support vector data description." 2017 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE, 2017.<br>
[7] Liao, Yuwei, et al. "A new bandwidth selection criterion for using SVDD to analyze hyperspectral data." Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIV. Vol. 10644. International Society for Optics and Photonics, 2018.<br>
[8] Wang, Siqi, et al. "Hyperparameter selection of one-class support vector machine by self-adaptive data shifting." Pattern Recognition 74 (2018): 198-211.</p>
</article></div>