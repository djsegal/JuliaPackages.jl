<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-bayesian-linear-regression-in-julia" class="anchor" aria-hidden="true" href="#bayesian-linear-regression-in-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bayesian Linear Regression in Julia</h1>
<p><a href="https://github.com/willtebbutt/BayesianLinearRegressors.jl/actions"><img src="https://github.com/willtebbutt/BayesianLinearRegressors.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/willtebbutt/BayesianLinearRegressors.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/36707f4dc7a9c07ea39a193c6fc81bbedc5c994c319622d7fa17088df488b320/68747470733a2f2f636f6465636f762e696f2f67682f77696c6c746562627574742f426179657369616e4c696e656172526567726573736f72732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/willtebbutt/BayesianLinearRegressors.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>This is a simple package that does one thing, Bayesian Linear Regression, in around 100 lines of code.</p>
<p>It <em>is</em> actively maintained, but it might appear inactive as it's one of those packages which requires very little maintenence because it's very simple.</p>
<h2><a id="user-content-intended-use-and-functionality" class="anchor" aria-hidden="true" href="#intended-use-and-functionality"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Intended Use and Functionality</h2>
<p>The interface sits at roughly the same level as that of <a href="https://github.com/JuliaStats/Distributions.jl/">Distributions.jl</a>. This means that while you won't find a scikit-learn-style <code>fit</code> function, you will find all of the primitives that you need to construct such a function to suit your particular problem. In particular, one can:</p>
<ul>
<li>Construct a <code>BayesianLinearRegressor</code> (BLR) object by providing a mean-vector and precision matrix for the weights of said regressor. This object represents a distribution over (linear) functions.</li>
<li>A <code>BayesianLinearRegressor</code> is a <code>AbstractGP</code>, and implements the primary AbstractGP API.</li>
<li>Think of an instance of <code>BayesianLinearRegressor</code> as a very restricted GP, where the time complexity of inference scales linearly in the number of observations <code>N</code>.</li>
</ul>
<h2><a id="user-content-conventions" class="anchor" aria-hidden="true" href="#conventions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conventions</h2>
<p>A <code>BayesianLinearRegressor</code> in <code>D</code> dimensions works with data where:</p>
<ul>
<li>inputs <code>X</code> should be a <code>D x N</code> matrix of <code>Real</code>s where each column is from one data point.</li>
<li>outputs <code>y</code> should be an <code>N</code>-vector of <code>Real</code>s, where each element is from one data point.</li>
</ul>
<h2><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example Usage</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Install the packages if you don't already have them installed
] add AbstractGPs, BayesianLinearRegressors LinearAlgebra Random Optim Plots Zygote
using AbstractGPs, BayesianLinearRegressors, LinearAlgebra, Random, Optim, Plots, Zygote

# Fix seed for re-producibility.
rng = MersenneTwister(123456)

# Construct a BayesianLinearRegressor prior over linear functions of `X`.
mw, Λw = zeros(2), Diagonal(ones(2))
f = BayesianLinearRegressor(mw, Λw)

# Index into the regressor and assume heterscedastic observation noise `Σ_noise`.
N = 10
X = collect(hcat(collect(range(-5.0, 5.0, length=N)), ones(N))')
Σ_noise = Diagonal(exp.(randn(N)))
fX = f(X, Σ_noise)

# Generate some toy data by sampling from the prior.
y = rand(rng, fX)

# Compute the adjoint of `rand` w.r.t. everything given random sensitivities of y′.
_, back_rand = Zygote.pullback(
    (X, Σ_noise, mw, Λw)-&gt;rand(rng, BayesianLinearRegressor(mw, Λw)(X, Σ_noise), 5),
    X, Σ_noise, mw, Λw,
)
back_rand(randn(N, 5))

# Compute the `logpdf`. Read as `the log probability of observing `y` at `X` under `f`, and
# Gaussian observation noise with zero-mean and covariance `Σ_noise`.
logpdf(fX, y)

# Compute the gradient of the `logpdf` w.r.t. everything.
Zygote.gradient(
    (X, Σ_noise, y, mw, Λw)-&gt;logpdf(BayesianLinearRegressor(mw, Λw)(X, Σ_noise), y),
    X, Σ_noise, y, mw, Λw,
)

# Perform posterior inference. Note that `f′` has the same type as `f`.
f′ = posterior(fX, y)

# Compute `logpdf` of the observations under the posterior predictive.
logpdf(f′(X, Σ_noise), y)

# Sample from the posterior predictive distribution.
N_plt = 1000
X_plt = hcat(collect(range(-6.0, 6.0, length=N_plt)), ones(N_plt))'
f′X_plt = rand(rng, f′(X_plt, eps()), 100) # Samples with machine-epsilon noise for stability

# Compute some posterior marginal statisics.
normals = marginals(f′(X_plt, eps()))
m′X_plt = mean.(normals)
σ′X_plt = std.(normals)

# Plot the posterior. This uses the default AbstractGPs plotting recipes.
posterior_plot = plot();
plot!(posterior_plot, X_plt[1, :], f′(X_plt, eps()); color=:blue, ribbon_scale=3);
sampleplot!(posterior_plot, X_plt[1, :], f′(X_plt, eps()); color=:blue, samples=10);
scatter!(posterior_plot, X[1, :], y; # Observations.
    markercolor=:red,
    markershape=:circle,
    markerstrokewidth=0.0,
    markersize=4,
    markeralpha=0.7,
    label=&quot;&quot;,
);
display(posterior_plot);
"><pre><span class="pl-c"><span class="pl-c">#</span> Install the packages if you don't already have them installed</span>
] add AbstractGPs, BayesianLinearRegressors LinearAlgebra Random Optim Plots Zygote
<span class="pl-k">using</span> AbstractGPs, BayesianLinearRegressors, LinearAlgebra, Random, Optim, Plots, Zygote

<span class="pl-c"><span class="pl-c">#</span> Fix seed for re-producibility.</span>
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>)

<span class="pl-c"><span class="pl-c">#</span> Construct a BayesianLinearRegressor prior over linear functions of `X`.</span>
mw, Λw <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">Diagonal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>))
f <span class="pl-k">=</span> <span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)

<span class="pl-c"><span class="pl-c">#</span> Index into the regressor and assume heterscedastic observation noise `Σ_noise`.</span>
N <span class="pl-k">=</span> <span class="pl-c1">10</span>
X <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">collect</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>, length<span class="pl-k">=</span>N)), <span class="pl-c1">ones</span>(N))<span class="pl-k">'</span>)
Σ_noise <span class="pl-k">=</span> <span class="pl-c1">Diagonal</span>(<span class="pl-c1">exp</span>.(<span class="pl-c1">randn</span>(N)))
fX <span class="pl-k">=</span> <span class="pl-c1">f</span>(X, Σ_noise)

<span class="pl-c"><span class="pl-c">#</span> Generate some toy data by sampling from the prior.</span>
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, fX)

<span class="pl-c"><span class="pl-c">#</span> Compute the adjoint of `rand` w.r.t. everything given random sensitivities of y′.</span>
_, back_rand <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">pullback</span>(
    (X, Σ_noise, mw, Λw)<span class="pl-k">-&gt;</span><span class="pl-c1">rand</span>(rng, <span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)(X, Σ_noise), <span class="pl-c1">5</span>),
    X, Σ_noise, mw, Λw,
)
<span class="pl-c1">back_rand</span>(<span class="pl-c1">randn</span>(N, <span class="pl-c1">5</span>))

<span class="pl-c"><span class="pl-c">#</span> Compute the `logpdf`. Read as `the log probability of observing `y` at `X` under `f`, and</span>
<span class="pl-c"><span class="pl-c">#</span> Gaussian observation noise with zero-mean and covariance `Σ_noise`.</span>
<span class="pl-c1">logpdf</span>(fX, y)

<span class="pl-c"><span class="pl-c">#</span> Compute the gradient of the `logpdf` w.r.t. everything.</span>
Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(
    (X, Σ_noise, y, mw, Λw)<span class="pl-k">-&gt;</span><span class="pl-c1">logpdf</span>(<span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)(X, Σ_noise), y),
    X, Σ_noise, y, mw, Λw,
)

<span class="pl-c"><span class="pl-c">#</span> Perform posterior inference. Note that `f′` has the same type as `f`.</span>
f′ <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(fX, y)

<span class="pl-c"><span class="pl-c">#</span> Compute `logpdf` of the observations under the posterior predictive.</span>
<span class="pl-c1">logpdf</span>(<span class="pl-c1">f′</span>(X, Σ_noise), y)

<span class="pl-c"><span class="pl-c">#</span> Sample from the posterior predictive distribution.</span>
N_plt <span class="pl-k">=</span> <span class="pl-c1">1000</span>
X_plt <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">collect</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">6.0</span>, <span class="pl-c1">6.0</span>, length<span class="pl-k">=</span>N_plt)), <span class="pl-c1">ones</span>(N_plt))<span class="pl-k">'</span>
f′X_plt <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, <span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()), <span class="pl-c1">100</span>) <span class="pl-c"><span class="pl-c">#</span> Samples with machine-epsilon noise for stability</span>

<span class="pl-c"><span class="pl-c">#</span> Compute some posterior marginal statisics.</span>
normals <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()))
m′X_plt <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(normals)
σ′X_plt <span class="pl-k">=</span> <span class="pl-c1">std</span>.(normals)

<span class="pl-c"><span class="pl-c">#</span> Plot the posterior. This uses the default AbstractGPs plotting recipes.</span>
posterior_plot <span class="pl-k">=</span> <span class="pl-c1">plot</span>();
<span class="pl-c1">plot!</span>(posterior_plot, X_plt[<span class="pl-c1">1</span>, :], <span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()); color<span class="pl-k">=</span><span class="pl-c1">:blue</span>, ribbon_scale<span class="pl-k">=</span><span class="pl-c1">3</span>);
<span class="pl-c1">sampleplot!</span>(posterior_plot, X_plt[<span class="pl-c1">1</span>, :], <span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()); color<span class="pl-k">=</span><span class="pl-c1">:blue</span>, samples<span class="pl-k">=</span><span class="pl-c1">10</span>);
<span class="pl-c1">scatter!</span>(posterior_plot, X[<span class="pl-c1">1</span>, :], y; <span class="pl-c"><span class="pl-c">#</span> Observations.</span>
    markercolor<span class="pl-k">=</span><span class="pl-c1">:red</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>,
);
<span class="pl-c1">display</span>(posterior_plot);</pre></div>
<h2><a id="user-content-up-for-grabs" class="anchor" aria-hidden="true" href="#up-for-grabs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Up For Grabs</h2>
<ul>
<li>Scikit-learn style interface: it wouldn't be too hard to implement a scikit-learn - style interface to handle basic regression tasks, so please feel free to make a PR that implements this.</li>
<li>Monte Carlo VI (MCVI): i.e. variational inference using the reparametrisation trick. This could be very useful when working with large data sets and applying big non-linear transformations, such as neural networks, to the inputs as it would enable mini-batching. I would envise at least supporting both a dense approximate posterior covariance and diagonal (i.e. mean-field), where the latter is for small-moderate dimensionalities and the latter for very high-dimensional problems.</li>
</ul>
<h2><a id="user-content-bugs-issues-and-prs" class="anchor" aria-hidden="true" href="#bugs-issues-and-prs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bugs, Issues, and PRs</h2>
<p>Please do report and bugs you find by raising an issue. Please also feel free to raise PRs, especially if for one of the above <code>Up For Grabs</code> items. Raise an issue to discuss the extension in detail before opening a PR if you prefer though.</p>
<h2><a id="user-content-related-work" class="anchor" aria-hidden="true" href="#related-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Related Work</h2>
<p><a href="https://github.com/cscherrer/BayesianLinearRegression.jl">BayesianLinearRegression.jl</a> is closely related, but appears to be a WIP and hasn't been touched in around a year or so (as of 27-03-2019).</p>
</article></div>