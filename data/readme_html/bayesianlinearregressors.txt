<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-bayesian-linear-regression-in-julia" class="anchor" aria-hidden="true" href="#bayesian-linear-regression-in-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Bayesian Linear Regression in Julia</h1>
<p><a href="https://willtebbutt.github.io/BayesianLinearRegressors.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://willtebbutt.github.io/BayesianLinearRegressors.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/3e353c26ddfe819150acbc732248f4f2a37f5175/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.com/willtebbutt/BayesianLinearRegressors.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ef68161268c4f4e1846e22ac3e890fbead145264/68747470733a2f2f7472617669732d63692e636f6d2f77696c6c746562627574742f426179657369616e4c696e656172526567726573736f72732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/willtebbutt/BayesianLinearRegressors.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/willtebbutt/BayesianLinearRegressors.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/5ac76f8acab23510dafdc5c177155d9d8e958304/68747470733a2f2f636f6465636f762e696f2f67682f77696c6c746562627574742f426179657369616e4c696e656172526567726573736f72732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/willtebbutt/BayesianLinearRegressors.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>This is a simple package that does one thing, Bayesian Linear Regression, in around 100 lines of code.</p>
<h2><a id="user-content-intended-use-and-functionality" class="anchor" aria-hidden="true" href="#intended-use-and-functionality"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Intended Use and Functionality</h2>
<p>The interface sits at roughly the same level as that of <a href="https://github.com/JuliaStats/Distributions.jl/">Distributions.jl</a>. This means that while you won't find a scikit-learn-style <code>fit</code> function, you will find all of the primitives that you need to construct such a function to suit your particular problem. In particular, one can:</p>
<ul>
<li>Construct a <code>BayesianLinearRegressor</code> (BLR) object by providing a mean-vector and precision matrix for the weights of said regressor. This object represents a distribution over (linear) functions.</li>
<li>"Index into" said distribution over functions to construct an <code>IndexedBLR</code> object, which represents a finite-dimensional marginal of a <code>BayesianLinearRegressor</code>.</li>
<li>Compute the log marginal likelihood of a vector of observations.</li>
<li>Sample from the finite-dimensional marginals of a BLR.</li>
<li>Perform posterior inference to produce a new BLR with an update mean and precision.</li>
<li>All operations are fully compatible with <a href="https://github.com/FluxML/Zygote.jl/">Zygote.jl</a> (hopefully), so you can use gradient-based optimisation to tune the hyperparameters of your regressor etc.</li>
</ul>
<p>For examples of how to use this package in conjunction with Flux and Zygote, see the examples directory.</p>
<h2><a id="user-content-conventions" class="anchor" aria-hidden="true" href="#conventions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conventions</h2>
<p>A <code>BayesianLinearRegressor</code> in <code>D</code> dimensions works with data where:</p>
<ul>
<li>inputs <code>X</code> should be a <code>D x N</code> matrix of <code>Real</code>s where each column is from one data point.</li>
<li>outputs <code>y</code> should be an <code>N</code>-vector of <code>Real</code>s, where each element is from one data point.</li>
</ul>
<h2><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example Usage</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> Install the packages if you don't already have them installed</span>
] add BayesianLinearRegressors LinearAlgebra Random Optim Plots Zygote
<span class="pl-k">using</span> BayesianLinearRegressors, LinearAlgebra, Random, Optim, Plots, Zygote

<span class="pl-c"><span class="pl-c">#</span> Fix seed for re-producibility.</span>
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>)

<span class="pl-c"><span class="pl-c">#</span> We don't export anything, so you need to explicitly import the stuff that you need.</span>
<span class="pl-k">using</span> BayesianLinearRegressors<span class="pl-k">:</span> BayesianLinearRegressor, logpdf, rand, posterior, marginals,
    cov

<span class="pl-c"><span class="pl-c">#</span> Construct a BayesianLinearRegressor prior over linear functions of `X`.</span>
mw, Λw <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">Diagonal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>))
f <span class="pl-k">=</span> <span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)

<span class="pl-c"><span class="pl-c">#</span> Index into the regressor and assume heterscedastic observation noise `Σ_noise`.</span>
N <span class="pl-k">=</span> <span class="pl-c1">10</span>
X <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">collect</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>, length<span class="pl-k">=</span>N)), <span class="pl-c1">ones</span>(N))<span class="pl-k">'</span>)
Σ_noise <span class="pl-k">=</span> <span class="pl-c1">Diagonal</span>(<span class="pl-c1">exp</span>.(<span class="pl-c1">randn</span>(N)))
fX <span class="pl-k">=</span> <span class="pl-c1">f</span>(X, Σ_noise)

<span class="pl-c"><span class="pl-c">#</span> Generate some toy data by sampling from the prior.</span>
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, fX)

<span class="pl-c"><span class="pl-c">#</span> Compute the adjoint of `rand` w.r.t. everything given random sensitivities of y′.</span>
_, back_rand <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">pullback</span>(
    (X, Σ_noise, mw, Λw)<span class="pl-k">-&gt;</span><span class="pl-c1">rand</span>(rng, <span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)(X, Σ_noise), <span class="pl-c1">5</span>),
    X, Σ_noise, mw, Λw,
)
<span class="pl-c1">back_rand</span>(<span class="pl-c1">randn</span>(N, <span class="pl-c1">5</span>))

<span class="pl-c"><span class="pl-c">#</span> Compute the `logpdf`. Read as `the log probability of observing `y` at `X` under `f`, and</span>
<span class="pl-c"><span class="pl-c">#</span> Gaussian observation noise with zero-mean and covariance `Σ_noise`.</span>
<span class="pl-c1">logpdf</span>(fX, y)

<span class="pl-c"><span class="pl-c">#</span> Compute the gradient of the `logpdf` w.r.t. everything.</span>
Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(
    (X, Σ_noise, y, mw, Λw)<span class="pl-k">-&gt;</span><span class="pl-c1">logpdf</span>(<span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)(X, Σ_noise), y),
    X, Σ_noise, y, mw, Λw,
)

<span class="pl-c"><span class="pl-c">#</span> Perform posterior inference. Note that `f′` has the same type as `f`.</span>
f′ <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(fX, y)

<span class="pl-c"><span class="pl-c">#</span> Compute `logpdf` of the observations under the posterior predictive (because why not?)</span>
<span class="pl-c1">logpdf</span>(<span class="pl-c1">f′</span>(X, Σ_noise), y)

<span class="pl-c"><span class="pl-c">#</span> Sample from the posterior predictive distribution.</span>
N_plt <span class="pl-k">=</span> <span class="pl-c1">1000</span>
X_plt <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">collect</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">6.0</span>, <span class="pl-c1">6.0</span>, length<span class="pl-k">=</span>N_plt)), <span class="pl-c1">ones</span>(N_plt))<span class="pl-k">'</span>
f′X_plt <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, <span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()), <span class="pl-c1">100</span>) <span class="pl-c"><span class="pl-c">#</span> Samples with machine-epsilon noise for stability</span>

<span class="pl-c"><span class="pl-c">#</span> Compute some posterior marginal statisics.</span>
normals <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()))
m′X_plt <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(normals)
σ′X_plt <span class="pl-k">=</span> <span class="pl-c1">std</span>.(normals)

<span class="pl-c"><span class="pl-c">#</span> Plot the posterior marginals.</span>
<span class="pl-c1">plotly</span>(); <span class="pl-c"><span class="pl-c">#</span> My prefered backend. Use a different one if you prefer / this doesn't work.</span>
posterior_plot <span class="pl-k">=</span> <span class="pl-c1">plot</span>();
<span class="pl-c1">plot!</span>(posterior_plot, X_plt[<span class="pl-c1">1</span>, :], f′X_plt; <span class="pl-c"><span class="pl-c">#</span> Posterior samples.</span>
    linecolor<span class="pl-k">=</span><span class="pl-c1">:blue</span>,
    linealpha<span class="pl-k">=</span><span class="pl-c1">0.2</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);
<span class="pl-c1">plot!</span>(posterior_plot, X_plt[<span class="pl-c1">1</span>, :], [m′X_plt m′X_plt]; <span class="pl-c"><span class="pl-c">#</span> Posterior credible intervals.</span>
    linewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    fillrange<span class="pl-k">=</span>[m′X_plt <span class="pl-k">.-</span> <span class="pl-c1">3</span> <span class="pl-k">.*</span> σ′X_plt, m′X_plt <span class="pl-k">.+</span> <span class="pl-c1">3</span> <span class="pl-k">*</span> σ′X_plt],
    fillalpha<span class="pl-k">=</span><span class="pl-c1">0.3</span>,
    fillcolor<span class="pl-k">=</span><span class="pl-c1">:blue</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);
<span class="pl-c1">plot!</span>(posterior_plot, X_plt[<span class="pl-c1">1</span>, :], m′X_plt; <span class="pl-c"><span class="pl-c">#</span> Posterior mean.</span>
    linecolor<span class="pl-k">=</span><span class="pl-c1">:blue</span>,
    linewidth<span class="pl-k">=</span><span class="pl-c1">2.0</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>);
<span class="pl-c1">scatter!</span>(posterior_plot, X[<span class="pl-c1">1</span>, :], y; <span class="pl-c"><span class="pl-c">#</span> Observations.</span>
    markercolor<span class="pl-k">=</span><span class="pl-c1">:red</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>,
);
<span class="pl-c1">display</span>(posterior_plot);</pre></div>
<h2><a id="user-content-up-for-grabs" class="anchor" aria-hidden="true" href="#up-for-grabs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Up For Grabs</h2>
<ul>
<li>Scikit-learn style interface: it wouldn't be too hard to implement a scikit-learn - style interface to handle basic regression tasks, so please feel free to make a PR that implements this.</li>
<li>Monte Carlo VI (MCVI): i.e. variational inference using the reparametrisation trick. This could be very useful when working with large data sets and applying big non-linear transformations, such as neural networks, to the inputs as it would enable mini-batching. I would envise at least supporting both a dense approximate posterior covariance and diagonal (i.e. mean-field), where the latter is for small-moderate dimensionalities and the latter for very high-dimensional problems.</li>
</ul>
<h2><a id="user-content-bugs-issues-and-prs" class="anchor" aria-hidden="true" href="#bugs-issues-and-prs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Bugs, Issues, and PRs</h2>
<p>Please do report and bugs you find by raising an issue. Please also feel free to raise PRs, especially if for one of the above <code>Up For Grabs</code> items. Raise an issue to discuss the extension in detail before opening a PR if you prefer though.</p>
<h2><a id="user-content-related-work" class="anchor" aria-hidden="true" href="#related-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Related Work</h2>
<p><a href="https://github.com/cscherrer/BayesianLinearRegression.jl">BayesianLinearRegression.jl</a> is closely related, but appears to be a WIP and hasn't been touched in around a year or so (as of 27-03-2019).</p>
</article></div>