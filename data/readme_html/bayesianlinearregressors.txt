<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-bayesian-linear-regression-in-julia" class="anchor" aria-hidden="true" href="#bayesian-linear-regression-in-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Bayesian Linear Regression in Julia</h1>
<p dir="auto"><a href="https://github.com/willtebbutt/BayesianLinearRegressors.jl/actions"><img src="https://github.com/willtebbutt/BayesianLinearRegressors.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaGaussianProcesses/BayesianLinearRegressors.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/36707f4dc7a9c07ea39a193c6fc81bbedc5c994c319622d7fa17088df488b320/68747470733a2f2f636f6465636f762e696f2f67682f77696c6c746562627574742f426179657369616e4c696e656172526567726573736f72732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/willtebbutt/BayesianLinearRegressors.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">This is a simple package that does one thing, Bayesian Linear Regression, in around 100 lines of code.</p>
<p dir="auto">It <em>is</em> actively maintained, but it might appear inactive as it's one of those packages which requires very little maintenance because it's very simple.</p>
<h2 dir="auto"><a id="user-content-intended-use-and-functionality" class="anchor" aria-hidden="true" href="#intended-use-and-functionality"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Intended Use and Functionality</h2>
<p dir="auto">The interface sits at roughly the same level as that of <a href="https://github.com/JuliaStats/Distributions.jl/">Distributions.jl</a>. This means that while you won't find a scikit-learn-style <code>fit</code> function, you will find all of the primitives that you need to construct such a function to suit your particular problem. In particular, one can:</p>
<ul dir="auto">
<li>Construct a <code>BayesianLinearRegressor</code> (BLR) object by providing a mean-vector and precision matrix for the weights of said regressor. This object represents a distribution over (linear) functions.</li>
<li>Use a <code>BayesianLinearRegressor</code> as an <code>AbstractGP</code>, as it implements the primary AbstractGP API.</li>
<li>Think of an instance of <code>BayesianLinearRegressor</code> as a very restricted GP, where the time complexity of inference scales linearly in the number of observations <code>N</code>.</li>
<li>Draw function samples from a <code>BayesianLinearRegressor</code> using <code>rand</code>.</li>
<li>Construct a <code>BasisFunctionRegressor</code> object which is a thin wrapper around a <code>BayesianLinearRegressor</code> to allow a non-linear feature mapping <code>ϕ</code> to act on the input.</li>
</ul>
<h2 dir="auto"><a id="user-content-conventions" class="anchor" aria-hidden="true" href="#conventions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Conventions</h2>
<p dir="auto"><code>BayesianLinearRegressors</code> is consistent with <code>AbstractGPs</code>.
Consequently, a <code>BayesianLinearRegressor</code> in <code>D</code> dimensions can work with the following input types:</p>
<ol dir="auto">
<li><code>ColVecs</code> -- a wrapper around an <code>D x N</code> matrix of <code>Real</code>s saying that each column should be interpreted as an input.</li>
<li><code>RowVecs</code>s -- a wrapper around an <code>N x D</code> matrix of <code>Real</code>s, saying that each row should be interpreted as an input.</li>
<li><code>Matrix{&lt;:Real}</code> -- must be <code>D x N</code>. Prefer using <code>ColVecs</code> or <code>RowVecs</code> for the sake of being explicit.</li>
</ol>
<p dir="auto">Consult the <code>Design</code> section of the <a href="https://juliagaussianprocesses.github.io/KernelFunctions.jl/dev/design/" rel="nofollow">KernelFunctions.jl</a> docs for more info on these conventions.</p>
<p dir="auto">Outputs for a BayesianLinearRegressor should be an <code>AbstractVector{&lt;:Real}</code> of length <code>N</code>.</p>
<h2 dir="auto"><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example Usage</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Install the packages if you don't already have them installed
] add AbstractGPs BayesianLinearRegressors LinearAlgebra Random Plots Zygote
using AbstractGPs, BayesianLinearRegressors, LinearAlgebra, Random, Plots, Zygote

# Fix seed for re-producibility.
rng = MersenneTwister(123456)

# Construct a BayesianLinearRegressor prior over linear functions of `X`.
mw, Λw = zeros(2), Diagonal(ones(2))
f = BayesianLinearRegressor(mw, Λw)

# Index into the regressor and assume heteroscedastic observation noise `Σ_noise`.
N = 10
X = ColVecs(hcat(range(-5.0, 5.0, length=N), ones(N))')
Σ_noise = Diagonal(exp.(randn(N)))
fX = f(X, Σ_noise)

# Generate some toy data by sampling from the prior.
y = rand(rng, fX)

# Compute the adjoint of `rand` w.r.t. everything given random sensitivities of y′.
_, back_rand = Zygote.pullback(
    (X, Σ_noise, mw, Λw)-&gt;rand(rng, BayesianLinearRegressor(mw, Λw)(X, Σ_noise), 5),
    X, Σ_noise, mw, Λw,
)
back_rand(randn(N, 5))

# Compute the `logpdf`. Read as `the log probability of observing `y` at `X` under `f`, and
# Gaussian observation noise with zero-mean and covariance `Σ_noise`.
logpdf(fX, y)

# Compute the gradient of the `logpdf` w.r.t. everything.
Zygote.gradient(
    (X, Σ_noise, y, mw, Λw)-&gt;logpdf(BayesianLinearRegressor(mw, Λw)(X, Σ_noise), y),
    X, Σ_noise, y, mw, Λw,
)

# Perform posterior inference. Note that `f′` has the same type as `f`.
f′ = posterior(fX, y)

# Compute `logpdf` of the observations under the posterior predictive.
logpdf(f′(X, Σ_noise), y)

# Sample from the posterior predictive distribution.
N_plt = 1000
X_plt = ColVecs(hcat(range(-6.0, 6.0, length=N_plt), ones(N_plt))')

# Compute some posterior marginal statisics.
normals = marginals(f′(X_plt, eps()))
m′X_plt = mean.(normals)
σ′X_plt = std.(normals)

# Plot the posterior. This uses the default AbstractGPs plotting recipes.
posterior_plot = plot();
plot!(posterior_plot, X_plt.X[1, :], f′(X_plt, eps()); color=:blue, ribbon_scale=3);
sampleplot!(posterior_plot, X_plt.X[1, :], f′(X_plt, eps()); color=:blue, samples=10);
scatter!(posterior_plot, X.X[1, :], y; # Observations.
    markercolor=:red,
    markershape=:circle,
    markerstrokewidth=0.0,
    markersize=4,
    markeralpha=0.7,
    label=&quot;&quot;,
);
display(posterior_plot);"><pre><span class="pl-c"><span class="pl-c">#</span> Install the packages if you don't already have them installed</span>
] add AbstractGPs BayesianLinearRegressors LinearAlgebra Random Plots Zygote
<span class="pl-k">using</span> AbstractGPs, BayesianLinearRegressors, LinearAlgebra, Random, Plots, Zygote

<span class="pl-c"><span class="pl-c">#</span> Fix seed for re-producibility.</span>
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>)

<span class="pl-c"><span class="pl-c">#</span> Construct a BayesianLinearRegressor prior over linear functions of `X`.</span>
mw, Λw <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">Diagonal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>))
f <span class="pl-k">=</span> <span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)

<span class="pl-c"><span class="pl-c">#</span> Index into the regressor and assume heteroscedastic observation noise `Σ_noise`.</span>
N <span class="pl-k">=</span> <span class="pl-c1">10</span>
X <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>, length<span class="pl-k">=</span>N), <span class="pl-c1">ones</span>(N))<span class="pl-k">'</span>)
Σ_noise <span class="pl-k">=</span> <span class="pl-c1">Diagonal</span>(<span class="pl-c1">exp</span>.(<span class="pl-c1">randn</span>(N)))
fX <span class="pl-k">=</span> <span class="pl-c1">f</span>(X, Σ_noise)

<span class="pl-c"><span class="pl-c">#</span> Generate some toy data by sampling from the prior.</span>
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, fX)

<span class="pl-c"><span class="pl-c">#</span> Compute the adjoint of `rand` w.r.t. everything given random sensitivities of y′.</span>
_, back_rand <span class="pl-k">=</span> Zygote<span class="pl-k">.</span><span class="pl-c1">pullback</span>(
    (X, Σ_noise, mw, Λw)<span class="pl-k">-&gt;</span><span class="pl-c1">rand</span>(rng, <span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)(X, Σ_noise), <span class="pl-c1">5</span>),
    X, Σ_noise, mw, Λw,
)
<span class="pl-c1">back_rand</span>(<span class="pl-c1">randn</span>(N, <span class="pl-c1">5</span>))

<span class="pl-c"><span class="pl-c">#</span> Compute the `logpdf`. Read as `the log probability of observing `y` at `X` under `f`, and</span>
<span class="pl-c"><span class="pl-c">#</span> Gaussian observation noise with zero-mean and covariance `Σ_noise`.</span>
<span class="pl-c1">logpdf</span>(fX, y)

<span class="pl-c"><span class="pl-c">#</span> Compute the gradient of the `logpdf` w.r.t. everything.</span>
Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(
    (X, Σ_noise, y, mw, Λw)<span class="pl-k">-&gt;</span><span class="pl-c1">logpdf</span>(<span class="pl-c1">BayesianLinearRegressor</span>(mw, Λw)(X, Σ_noise), y),
    X, Σ_noise, y, mw, Λw,
)

<span class="pl-c"><span class="pl-c">#</span> Perform posterior inference. Note that `f′` has the same type as `f`.</span>
f′ <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(fX, y)

<span class="pl-c"><span class="pl-c">#</span> Compute `logpdf` of the observations under the posterior predictive.</span>
<span class="pl-c1">logpdf</span>(<span class="pl-c1">f′</span>(X, Σ_noise), y)

<span class="pl-c"><span class="pl-c">#</span> Sample from the posterior predictive distribution.</span>
N_plt <span class="pl-k">=</span> <span class="pl-c1">1000</span>
X_plt <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">6.0</span>, <span class="pl-c1">6.0</span>, length<span class="pl-k">=</span>N_plt), <span class="pl-c1">ones</span>(N_plt))<span class="pl-k">'</span>)

<span class="pl-c"><span class="pl-c">#</span> Compute some posterior marginal statisics.</span>
normals <span class="pl-k">=</span> <span class="pl-c1">marginals</span>(<span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()))
m′X_plt <span class="pl-k">=</span> <span class="pl-c1">mean</span>.(normals)
σ′X_plt <span class="pl-k">=</span> <span class="pl-c1">std</span>.(normals)

<span class="pl-c"><span class="pl-c">#</span> Plot the posterior. This uses the default AbstractGPs plotting recipes.</span>
posterior_plot <span class="pl-k">=</span> <span class="pl-c1">plot</span>();
<span class="pl-c1">plot!</span>(posterior_plot, X_plt<span class="pl-k">.</span>X[<span class="pl-c1">1</span>, :], <span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()); color<span class="pl-k">=</span><span class="pl-c1">:blue</span>, ribbon_scale<span class="pl-k">=</span><span class="pl-c1">3</span>);
<span class="pl-c1">sampleplot!</span>(posterior_plot, X_plt<span class="pl-k">.</span>X[<span class="pl-c1">1</span>, :], <span class="pl-c1">f′</span>(X_plt, <span class="pl-c1">eps</span>()); color<span class="pl-k">=</span><span class="pl-c1">:blue</span>, samples<span class="pl-k">=</span><span class="pl-c1">10</span>);
<span class="pl-c1">scatter!</span>(posterior_plot, X<span class="pl-k">.</span>X[<span class="pl-c1">1</span>, :], y; <span class="pl-c"><span class="pl-c">#</span> Observations.</span>
    markercolor<span class="pl-k">=</span><span class="pl-c1">:red</span>,
    markershape<span class="pl-k">=</span><span class="pl-c1">:circle</span>,
    markerstrokewidth<span class="pl-k">=</span><span class="pl-c1">0.0</span>,
    markersize<span class="pl-k">=</span><span class="pl-c1">4</span>,
    markeralpha<span class="pl-k">=</span><span class="pl-c1">0.7</span>,
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>,
);
<span class="pl-c1">display</span>(posterior_plot);</pre></div>
<h2 dir="auto"><a id="user-content-basis-function-regression" class="anchor" aria-hidden="true" href="#basis-function-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basis Function Regression</h2>
<p dir="auto">Any instance of a <code>BayesianLinearRegressor</code> can be replaced by a <code>BasisFunctionRegressor</code> (BFR). A <code>BasisFunctionRegressor</code> is a thin wrapper around a <code>BayesianLinearRegressor</code>, but includes a potentially non-linear feature mapping <code>ϕ</code> which is applied to the input before it is passed to the underlying BLR. It is essentially defined as <code>bfr(X) = blr(ϕ(X))</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using AbstractGPs, BayesianLinearRegressors, LinearAlgebra

X = RowVecs(hcat(range(-1.0, 1.0, length=5)))
blr = BayesianLinearRegressor(zeros(2), Diagonal(ones(2)))

# N.B. ϕ must accept one of the allowed input types and
# must return the same type (in this case RowVecs)
ϕ(x::RowVecs) = RowVecs(hcat(ones(length(x)), prod.(x)))

bfr = BasisFunctionRegressor(blr, ϕ)

# These are equivalent
var(bfr(X)) == var(blr(ϕ(X)))"><pre><span class="pl-k">using</span> AbstractGPs, BayesianLinearRegressors, LinearAlgebra

X <span class="pl-k">=</span> <span class="pl-c1">RowVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>, length<span class="pl-k">=</span><span class="pl-c1">5</span>)))
blr <span class="pl-k">=</span> <span class="pl-c1">BayesianLinearRegressor</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">Diagonal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>)))

<span class="pl-c"><span class="pl-c">#</span> N.B. ϕ must accept one of the allowed input types and</span>
<span class="pl-c"><span class="pl-c">#</span> must return the same type (in this case RowVecs)</span>
<span class="pl-en">ϕ</span>(x<span class="pl-k">::</span><span class="pl-c1">RowVecs</span>) <span class="pl-k">=</span> <span class="pl-c1">RowVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">length</span>(x)), <span class="pl-c1">prod</span>.(x)))

bfr <span class="pl-k">=</span> <span class="pl-c1">BasisFunctionRegressor</span>(blr, ϕ)

<span class="pl-c"><span class="pl-c">#</span> These are equivalent</span>
<span class="pl-c1">var</span>(<span class="pl-c1">bfr</span>(X)) <span class="pl-k">==</span> <span class="pl-c1">var</span>(<span class="pl-c1">blr</span>(<span class="pl-c1">ϕ</span>(X)))</pre></div>
<h2 dir="auto"><a id="user-content-drawing-function-samples" class="anchor" aria-hidden="true" href="#drawing-function-samples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Drawing Function Samples</h2>
<p dir="auto">There are two ways of drawing samples from <code>f::Union{BayesianLinearRegressor,BasisFunctionRegressor}</code>. The first is by using the <code>AbstractGPs</code> API (as in the above examples) where a <code>FiniteBLR</code> projection is first produced at a fixed set of input locations <code>X</code> with some specified observation noise <code>Σy</code>: <code>fx = f(X, Σy)::FiniteBLR</code>. Then, observations (including noise) at <code>X</code> can be sampled directly with <code>rand(rng, fx)</code>.
The other way is to draw a sample of an entire function from <code>f</code> using <code>g = rand(rng, f)</code>. This samples a value for the weights <code>w ~ N(mw, Λw)</code> and produces a function <code>g(X) = w'X</code> which can be evaluated at any input locations. Note that this method corresponds to drawing samples of noiseless observations.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using AbstractGPs, BayesianLinearRegressors, LinearAlgebra, Random, Plots

# Fix seed for re-producibility.
rng = MersenneTwister(123456)

X = RowVecs(hcat(range(-1.0, 1.0, length=5)))
f = BayesianLinearRegressor(zeros(2), Diagonal(ones(2)))

## The first method of drawing samples - using the AbstractGPs API:
# Index into the regressor at fixed inputs X and assume homoscedastic observation noise `Σ_noise`.
N = 10
X = ColVecs(hcat(range(-5.0, 5.0, length=N), ones(N))')
Σ_noise = 0.1
fX = f(X, Σ_noise)

rand(rng, fX)

## The second method - sampling an entire function:
g = rand(rng, f)

# This sample can now be evaulated at any input locations
g(X)

X′ = ColVecs(hcat(range(10.0, 15.0, length=N), ones(N))')
g(X′)

# Sample multiple functions for plotting
gs = rand(rng, f, 100)

N_plt = 1000
X_plt = ColVecs(hcat(range(-6.0, 6.0, length=N_plt), ones(N_plt))')
y_plt = reduce(hcat, [g(X_plt) for g in gs])

sample_plot = plot();
plot!(sample_plot, X_plt.X[1,:], y_plt;
    label=&quot;&quot;,
    color=:black,
    linealpha=0.4,
);
display(sample_plot)"><pre><span class="pl-k">using</span> AbstractGPs, BayesianLinearRegressors, LinearAlgebra, Random, Plots

<span class="pl-c"><span class="pl-c">#</span> Fix seed for re-producibility.</span>
rng <span class="pl-k">=</span> <span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">123456</span>)

X <span class="pl-k">=</span> <span class="pl-c1">RowVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">1.0</span>, <span class="pl-c1">1.0</span>, length<span class="pl-k">=</span><span class="pl-c1">5</span>)))
f <span class="pl-k">=</span> <span class="pl-c1">BayesianLinearRegressor</span>(<span class="pl-c1">zeros</span>(<span class="pl-c1">2</span>), <span class="pl-c1">Diagonal</span>(<span class="pl-c1">ones</span>(<span class="pl-c1">2</span>)))

<span class="pl-c"><span class="pl-c">#</span># The first method of drawing samples - using the AbstractGPs API:</span>
<span class="pl-c"><span class="pl-c">#</span> Index into the regressor at fixed inputs X and assume homoscedastic observation noise `Σ_noise`.</span>
N <span class="pl-k">=</span> <span class="pl-c1">10</span>
X <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">5.0</span>, <span class="pl-c1">5.0</span>, length<span class="pl-k">=</span>N), <span class="pl-c1">ones</span>(N))<span class="pl-k">'</span>)
Σ_noise <span class="pl-k">=</span> <span class="pl-c1">0.1</span>
fX <span class="pl-k">=</span> <span class="pl-c1">f</span>(X, Σ_noise)

<span class="pl-c1">rand</span>(rng, fX)

<span class="pl-c"><span class="pl-c">#</span># The second method - sampling an entire function:</span>
g <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, f)

<span class="pl-c"><span class="pl-c">#</span> This sample can now be evaulated at any input locations</span>
<span class="pl-c1">g</span>(X)

X′ <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-c1">10.0</span>, <span class="pl-c1">15.0</span>, length<span class="pl-k">=</span>N), <span class="pl-c1">ones</span>(N))<span class="pl-k">'</span>)
<span class="pl-c1">g</span>(X′)

<span class="pl-c"><span class="pl-c">#</span> Sample multiple functions for plotting</span>
gs <span class="pl-k">=</span> <span class="pl-c1">rand</span>(rng, f, <span class="pl-c1">100</span>)

N_plt <span class="pl-k">=</span> <span class="pl-c1">1000</span>
X_plt <span class="pl-k">=</span> <span class="pl-c1">ColVecs</span>(<span class="pl-c1">hcat</span>(<span class="pl-c1">range</span>(<span class="pl-k">-</span><span class="pl-c1">6.0</span>, <span class="pl-c1">6.0</span>, length<span class="pl-k">=</span>N_plt), <span class="pl-c1">ones</span>(N_plt))<span class="pl-k">'</span>)
y_plt <span class="pl-k">=</span> <span class="pl-c1">reduce</span>(hcat, [<span class="pl-c1">g</span>(X_plt) <span class="pl-k">for</span> g <span class="pl-k">in</span> gs])

sample_plot <span class="pl-k">=</span> <span class="pl-c1">plot</span>();
<span class="pl-c1">plot!</span>(sample_plot, X_plt<span class="pl-k">.</span>X[<span class="pl-c1">1</span>,:], y_plt;
    label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>,
    color<span class="pl-k">=</span><span class="pl-c1">:black</span>,
    linealpha<span class="pl-k">=</span><span class="pl-c1">0.4</span>,
);
<span class="pl-c1">display</span>(sample_plot)</pre></div>
<h2 dir="auto"><a id="user-content-up-for-grabs" class="anchor" aria-hidden="true" href="#up-for-grabs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Up For Grabs</h2>
<ul dir="auto">
<li>Scikit-learn style interface: it wouldn't be too hard to implement a scikit-learn - style interface to handle basic regression tasks, so please feel free to make a PR that implements this.</li>
<li>Monte Carlo VI (MCVI): i.e. variational inference using the reparametrisation trick. This could be very useful when working with large data sets and applying big non-linear transformations, such as neural networks, to the inputs as it would enable mini-batching. I would envisage at least supporting both a dense approximate posterior covariance and diagonal (i.e. mean-field), where the former is for small-moderate dimensionalities and the latter for very high-dimensional problems.</li>
</ul>
<h2 dir="auto"><a id="user-content-bugs-issues-and-prs" class="anchor" aria-hidden="true" href="#bugs-issues-and-prs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Bugs, Issues, and PRs</h2>
<p dir="auto">Please do report any bugs you find by raising an issue. Please also feel free to raise PRs, especially if for one of the above <code>Up For Grabs</code> items. Raise an issue to discuss the extension in detail before opening a PR if you prefer, though.</p>
<h2 dir="auto"><a id="user-content-related-work" class="anchor" aria-hidden="true" href="#related-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Related Work</h2>
<p dir="auto"><a href="https://github.com/cscherrer/BayesianLinearRegression.jl">BayesianLinearRegression.jl</a> is closely related, but appears to be a WIP and hasn't been touched in around a year or so (as of 27-03-2019).</p>
</article></div>