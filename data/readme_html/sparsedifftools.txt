<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-sparsedifftoolsjl" class="anchor" aria-hidden="true" href="#sparsedifftoolsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SparseDiffTools.jl</h1>
<p><a href="https://gitter.im/JuliaDiff/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge" rel="nofollow"><img src="https://camo.githubusercontent.com/702026b68c0f63c2ef9509283ee0755296f94a48/68747470733a2f2f6261646765732e6769747465722e696d2f4a756c6961446966662f4c6f6262792e737667" alt="Join the chat at https://gitter.im/JuliaDiff/Lobby" data-canonical-src="https://badges.gitter.im/JuliaDiff/Lobby.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.org/JuliaDiff/SparseDiffTools.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/d3c259feee8284cd11ab2f24e3f2caf3aeb10bf3/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961446966662f53706172736544696666546f6f6c732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaDiff/SparseDiffTools.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://gitlab.com/JuliaDiff/SparseDiffTools-jl/pipelines" rel="nofollow"><img src="https://camo.githubusercontent.com/3d22dce3a93dc5d3619b90eb580afbccbdcb9cb4/68747470733a2f2f6769746c61622e636f6d2f4a756c6961446966662f53706172736544696666546f6f6c732d6a6c2f6261646765732f6d61737465722f706970656c696e652e737667" alt="GitlabCI" data-canonical-src="https://gitlab.com/JuliaDiff/SparseDiffTools-jl/badges/master/pipeline.svg" style="max-width:100%;"></a></p>
<p>This package is for exploiting sparsity in Jacobians and Hessians to accelerate
computations. Matrix-free Jacobian-vector product and Hessian-vector product
operators are provided that are compatible with AbstractMatrix-based libraries
like IterativeSolvers.jl for easy and efficient Newton-Krylov implementation. It is
possible to perform matrix coloring, and utilize coloring in Jacobian and Hessian
construction.</p>
<p>Optionally, automatic and numerical differentiation are utilized.</p>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example</h2>
<p>Suppose we had the function</p>
<div class="highlight highlight-source-julia"><pre>fcalls <span class="pl-k">=</span> <span class="pl-c1">0</span>
<span class="pl-k">function</span> <span class="pl-en">f</span>(dx,x) <span class="pl-c"><span class="pl-c">#</span> in-place</span>
  <span class="pl-k">global</span> fcalls <span class="pl-k">+=</span> <span class="pl-c1">1</span>
  <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">length</span>(x)<span class="pl-k">-</span><span class="pl-c1">1</span>
    dx[i] <span class="pl-k">=</span> x[i<span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">-</span> <span class="pl-c1">2</span>x[i] <span class="pl-k">+</span> x[i<span class="pl-k">+</span><span class="pl-c1">1</span>]
  <span class="pl-k">end</span>
  dx[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">2</span>x[<span class="pl-c1">1</span>] <span class="pl-k">+</span> x[<span class="pl-c1">2</span>]
  dx[<span class="pl-c1">end</span>] <span class="pl-k">=</span> x[<span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">-</span> <span class="pl-c1">2</span>x[<span class="pl-c1">end</span>]
  <span class="pl-c1">nothing</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">g</span>(x) <span class="pl-c"><span class="pl-c">#</span> out-of-place</span>
  <span class="pl-k">global</span> fcalls <span class="pl-k">+=</span> <span class="pl-c1">1</span>
  dx <span class="pl-k">=</span> <span class="pl-c1">zero</span>(x)
  <span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">length</span>(x)<span class="pl-k">-</span><span class="pl-c1">1</span>
    dx[i] <span class="pl-k">=</span> x[i<span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">-</span> <span class="pl-c1">2</span>x[i] <span class="pl-k">+</span> x[i<span class="pl-k">+</span><span class="pl-c1">1</span>]
  <span class="pl-k">end</span>
  dx[<span class="pl-c1">1</span>] <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">2</span>x[<span class="pl-c1">1</span>] <span class="pl-k">+</span> x[<span class="pl-c1">2</span>]
  dx[<span class="pl-c1">end</span>] <span class="pl-k">=</span> x[<span class="pl-c1">end</span><span class="pl-k">-</span><span class="pl-c1">1</span>] <span class="pl-k">-</span> <span class="pl-c1">2</span>x[<span class="pl-c1">end</span>]
  dx
<span class="pl-k">end</span></pre></div>
<p>For this function, we know that the sparsity pattern of the Jacobian is a
<code>Tridiagonal</code> matrix. However, if we didn't know the sparsity pattern for
the Jacobian, we could use the <code>jacobian_sparsity</code> function to automatically
detect the sparsity pattern. This function is only available if you
load SparsityDetection.jl as well. We declare that the function <code>f</code> outputs a
vector of length 30 and takes in a vector of length 30, and <code>jacobian_sparsity</code> spits
out a <code>Sparsity</code> object which we can turn into a <code>SparseMatrixCSC</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> SparsityDetection, SparseArrays
input <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">30</span>)
output <span class="pl-k">=</span> <span class="pl-c1">similar</span>(input)
sparsity_pattern <span class="pl-k">=</span> <span class="pl-c1">jacobian_sparsity</span>(f,output,input)
jac <span class="pl-k">=</span> <span class="pl-c1">Float64</span>.(<span class="pl-c1">sparse</span>(sparsity_pattern))</pre></div>
<p>Now we call <code>matrix_colors</code> to get the colorvec vector for that matrix:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> SparseDiffTools
colors <span class="pl-k">=</span> <span class="pl-c1">matrix_colors</span>(jac)</pre></div>
<p>Since <code>maximum(colors)</code> is 3, this means that finite differencing can now
compute the Jacobian in just 4 <code>f</code>-evaluations. Generating the sparsity
pattern used 1 (pseudo) <code>f</code>-evaluation, so the total number of times that
<code>f</code> is called to compute the sparsity pattern plus the entire 30x30 Jacobian
is 5 times:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> FiniteDiff
FiniteDiff<span class="pl-k">.</span><span class="pl-c1">finite_difference_jacobian!</span>(jac, f, <span class="pl-c1">rand</span>(<span class="pl-c1">30</span>), colorvec<span class="pl-k">=</span>colors)
<span class="pl-c1">@show</span> fcalls <span class="pl-c"><span class="pl-c">#</span> 5</span></pre></div>
<p>In addition, a faster forward-mode autodiff call can be utilized as well:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">forwarddiff_color_jacobian!</span>(jac, f, x, colorvec <span class="pl-k">=</span> colors)</pre></div>
<p>If one only needs to compute products, one can use the operators. For example,</p>
<div class="highlight highlight-source-julia"><pre>u <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">30</span>)
J <span class="pl-k">=</span> <span class="pl-c1">JacVec</span>(f,u)</pre></div>
<p>makes <code>J</code> into a matrix-free operator which calculates <code>J*v</code> products. For
example:</p>
<div class="highlight highlight-source-julia"><pre>v <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">30</span>)
res <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v)
<span class="pl-c1">mul!</span>(res,J,v) <span class="pl-c"><span class="pl-c">#</span> Does 1 f evaluation</span></pre></div>
<p>makes <code>res = J*v</code>. Additional operators for <code>HesVec</code> exists, including
<code>HesVecGrad</code> which allows one to utilize a gradient function. These operators
are compatible with iterative solver libraries like IterativeSolvers.jl, meaning
the following performs the Newton-Krylov update iteration:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> IterativeSolvers
<span class="pl-c1">gmres!</span>(res,J,v)</pre></div>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Documentation</h2>
<h3><a id="user-content-matrix-coloring" class="anchor" aria-hidden="true" href="#matrix-coloring"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Matrix Coloring</h3>
<p>This library extends the common <code>ArrayInterface.matrix_colors</code> function to allow
for coloring sparse matrices using graphical techniques.</p>
<p>Matrix coloring allows you to reduce the number of times finite differencing
requires an <code>f</code> call to <code>maximum(colors)+1</code>, or reduces automatic differentiation
to using <code>maximum(colors)</code> partials. Since normally these values are <code>length(x)</code>,
this can be significant savings.</p>
<p>The API for computing the colorvec vector is:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">matrix_colors</span>(A<span class="pl-k">::</span><span class="pl-c1">AbstractMatrix</span>,alg<span class="pl-k">::</span><span class="pl-c1">ColoringAlgorithm</span> <span class="pl-k">=</span> <span class="pl-c1">GreedyD1Color</span>();
              partition_by_rows<span class="pl-k">::</span><span class="pl-c1">Bool</span> <span class="pl-k">=</span> <span class="pl-c1">false</span>)</pre></div>
<p>The first argument is the abstract matrix which represents the sparsity pattern
of the Jacobian. The second argument is the optional choice of coloring algorithm.
It will default to a greedy distance 1 coloring, though if your special matrix
type has more information, like is a <code>Tridiagonal</code> or <code>BlockBandedMatrix</code>, the
colorvec vector will be analytically calculated instead. The keyword argument
<code>partition_by_rows</code> allows you to partition the Jacobian on the basis of rows instead
of columns and generate a corresponding coloring vector which can be used for
reverse-mode AD. Default value is false.</p>
<p>The result is a vector which assigns a colorvec to each column (or row) of the matrix.</p>
<h3><a id="user-content-colorvec-assisted-differentiation" class="anchor" aria-hidden="true" href="#colorvec-assisted-differentiation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Colorvec-Assisted Differentiation</h3>
<p>Colorvec-assisted differentiation for numerical differentiation is provided by
FiniteDiff.jl and for automatic differentiation is provided by
ForwardDiff.jl.</p>
<p>For FiniteDiff.jl, one simply has to use the provided <code>colorvec</code> keyword
argument. See
<a href="https://github.com/JuliaDiff/FiniteDiff.jl#jacobians">the FiniteDiff Jacobian documentation</a>
for more details.</p>
<p>For forward-mode automatic differentiation, use of a colorvec vector is provided
by the following function:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">forwarddiff_color_jacobian!</span>(J<span class="pl-k">::</span><span class="pl-c1">AbstractMatrix{&lt;:Number}</span>,
                            f,
                            x<span class="pl-k">::</span><span class="pl-c1">AbstractArray{&lt;:Number}</span>;
                            dx <span class="pl-k">=</span> <span class="pl-c1">nothing</span>,
                            colorvec <span class="pl-k">=</span> <span class="pl-c1">eachindex</span>(x),
                            sparsity <span class="pl-k">=</span> <span class="pl-c1">nothing</span>)</pre></div>
<p>Notice that if a sparsity pattern is not supplied then the built Jacobian will
be the compressed Jacobian: <code>sparsity</code> must be a sparse matrix or a structured matrix
(<code>Tridiagonal</code>, <code>Banded</code>, etc. conforming to the ArrayInterface.jl specs) with the
appropriate sparsity pattern to allow for decompression.</p>
<p>This call will allocate the cache variables each time. To avoid allocating the
cache, construct the cache in advance:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">ForwardColorJacCache</span>(f,x,_chunksize <span class="pl-k">=</span> <span class="pl-c1">nothing</span>;
                              dx <span class="pl-k">=</span> <span class="pl-c1">nothing</span>,
                              colorvec<span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(x),
                              sparsity <span class="pl-k">=</span> <span class="pl-c1">nothing</span>)</pre></div>
<p>and utilize the following signature:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">forwarddiff_color_jacobian!</span>(J<span class="pl-k">::</span><span class="pl-c1">AbstractMatrix{&lt;:Number}</span>,
                            f,
                            x<span class="pl-k">::</span><span class="pl-c1">AbstractArray{&lt;:Number}</span>,
                            jac_cache<span class="pl-k">::</span><span class="pl-c1">ForwardColorJacCache</span>)</pre></div>
<p><code>dx</code> is a pre-allocated output vector which is used to declare the output size,
and thus allows for specifying a non-square Jacobian.</p>
<p>If one is using an out-of-place function <code>f(x)</code>, then the alternative form
ca be used:</p>
<div class="highlight highlight-source-julia"><pre>jacout <span class="pl-k">=</span> <span class="pl-c1">forwarddiff_color_jacobian</span>(g, x,
                                    dx <span class="pl-k">=</span> <span class="pl-c1">similar</span>(x),
                                    colorvec <span class="pl-k">=</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(x),
                                    sparsity <span class="pl-k">=</span> <span class="pl-c1">nothing</span>,
                                    jac_prototype <span class="pl-k">=</span> <span class="pl-c1">nothing</span>)</pre></div>
<p>Note that the out-of-place form is efficient and compatible with StaticArrays.
One can specify the type and shape of the output Jacobian by giving an
additional <code>jac_prototype</code> to the out-of place <code>forwarddiff_color_jacobian</code>
function, otherwise it will become a dense matrix. If <code>jac_prototype</code> and
<code>sparsity</code> are not specified, then the oop Jacobian will assume that the
function has a <em>square</em> Jacobian matrix. If it is not the case, please specify
the shape of output by giving <code>dx</code>.</p>
<h3><a id="user-content-jacobian-vector-and-hessian-vector-products" class="anchor" aria-hidden="true" href="#jacobian-vector-and-hessian-vector-products"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jacobian-Vector and Hessian-Vector Products</h3>
<p>Matrix-free implementations of Jacobian-Vector and Hessian-Vector products is
provided in both an operator and function form. For the functions, each choice
has the choice of being in-place and out-of-place, and the in-place versions
have the ability to pass in cache vectors to be non-allocating. When in-place
the function signature for Jacobians is <code>f!(du,u)</code>, while out-of-place has
<code>du=f(u)</code>. For Hessians, all functions must be <code>f(u)</code> which returns a scalar</p>
<p>The functions for Jacobians are:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">auto_jacvec!</span>(du, f, x, v,
                      cache1 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v),
                      cache2 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v))

<span class="pl-c1">auto_jacvec</span>(f, x, v)

<span class="pl-c"><span class="pl-c">#</span> If compute_f0 is false, then `f(cache1,x)` will be computed</span>
<span class="pl-c1">num_jacvec!</span>(du,f,x,v,cache1 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
                     cache2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v);
                     compute_f0 <span class="pl-k">=</span> <span class="pl-c1">true</span>)
<span class="pl-c1">num_jacvec</span>(f,x,v,f0<span class="pl-k">=</span><span class="pl-c1">nothing</span>)</pre></div>
<p>For Hessians, the following are provided:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">num_hesvec!</span>(du,f,x,v,
             cache1 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
             cache2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
             cache3 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v))

<span class="pl-c1">num_hesvec</span>(f,x,v)

<span class="pl-c1">numauto_hesvec!</span>(du,f,x,v,
                 cache <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">GradientConfig</span>(f,v),
                 cache1 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
                 cache2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v))

<span class="pl-c1">numauto_hesvec</span>(f,x,v)

<span class="pl-c1">autonum_hesvec!</span>(du,f,x,v,
                 cache1 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
                 cache2 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v),
                 cache3 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v))

<span class="pl-c1">autonum_hesvec</span>(f,x,v)</pre></div>
<p>In addition,
the following forms allow you to provide a gradient function <code>g(dx,x)</code> or <code>dx=g(x)</code>
respectively:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">num_hesvecgrad!</span>(du,g,x,v,
                     cache2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
                     cache3 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v))

<span class="pl-c1">num_hesvecgrad</span>(g,x,v)

<span class="pl-c1">auto_hesvecgrad!</span>(du,g,x,v,
                     cache2 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v),
                     cache3 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v))

<span class="pl-c1">auto_hesvecgrad</span>(g,x,v)</pre></div>
<p>The <code>numauto</code> and <code>autonum</code> methods both mix numerical and automatic differentiation, with
the former almost always being more efficient and thus being recommended.</p>
<p>Optionally, if you load Zygote.jl, the following <code>numback</code>
and <code>autoback</code> methods are available and allow numerical/ForwardDiff over reverse mode
automatic differentiation respectively, where the reverse-mode AD is provided by Zygote.jl.
Currently these methods are not competitive against <code>numauto</code>, but as Zygote.jl gets
optimized these will likely be the fastest.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Zygote <span class="pl-c"><span class="pl-c">#</span> Required</span>

<span class="pl-c1">numback_hesvec!</span>(du,f,x,v,
                     cache1 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v),
                     cache2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(v))

<span class="pl-c1">numback_hesvec</span>(f,x,v)

<span class="pl-c"><span class="pl-c">#</span> Currently errors! See https://github.com/FluxML/Zygote.jl/issues/241</span>
<span class="pl-c1">autoback_hesvec!</span>(du,f,x,v,
                     cache2 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v),
                     cache3 <span class="pl-k">=</span> ForwardDiff<span class="pl-k">.</span><span class="pl-c1">Dual</span><span class="pl-c1">{DeivVecTag}</span>.(x, v))

<span class="pl-c1">autoback_hesvec</span>(f,x,v)</pre></div>
<h4><a id="user-content-jv-and-hv-operators" class="anchor" aria-hidden="true" href="#jv-and-hv-operators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>J<em>v and H</em>v Operators</h4>
<p>The following produce matrix-free operators which are used for calculating
Jacobian-vector and Hessian-vector products where the differentiation takes
place at the vector <code>u</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">JacVec</span>(f,u<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>;autodiff<span class="pl-k">=</span><span class="pl-c1">true</span>)
<span class="pl-c1">HesVec</span>(f,u<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>;autodiff<span class="pl-k">=</span><span class="pl-c1">true</span>)
<span class="pl-c1">HesVecGrad</span>(g,u<span class="pl-k">::</span><span class="pl-c1">AbstractArray</span>;autodiff<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p>These all have the same interface, where <code>J*v</code> utilizes the out-of-place
Jacobian-vector or Hessian-vector function, whereas <code>mul!(res,J,v)</code> utilizes
the appropriate in-place versions. To update the location of differentiation
in the operator, simply mutate the vector <code>u</code>: <code>J.u .= ...</code>.</p>
</article></div>