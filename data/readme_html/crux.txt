<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-cruxjl" class="anchor" aria-hidden="true" href="#cruxjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Crux.jl</h1>
<p dir="auto"><a href="https://github.com/ancorso/Crux.jl/actions/workflows/CI.yml"><img src="https://github.com/ancorso/Crux.jl/actions/workflows/CI.yml/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/ancorso/Crux.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c201c3d973446792839d1997756103a03e2b57d13038c2fe027a01e7bd8918fc/68747470733a2f2f636f6465636f762e696f2f67682f616e636f72736f2f437275782e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Code Coverage" data-canonical-src="https://codecov.io/gh/ancorso/Crux.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Deep RL library with concise implementations of popular algorithms. Implemented using <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> and fits into the <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a> interface.</p>
<p dir="auto">Supports CPU and GPU computation and implements the following algorithms:</p>
<h3 dir="auto"><a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reinforcement Learning</h3>
<ul dir="auto">
<li><a href="./src/model_free/rl/dqn.jl">Deep Q-Learning</a>
<ul dir="auto">
<li>Prioritized Experience Replay</li>
</ul>
</li>
<li><a href="./src/model_free/rl/softq.jl">Soft Q-Learning</a></li>
<li><a href="./src/model_free/rl/reinforce.jl">REINFORCE</a></li>
<li><a href="./src/model_free/rl/ppo.jl">Proximal Policy Optimization (PPO)</a></li>
<li>Lagrange-Constrained PPO</li>
<li><a href="./src/model_free/rl/a2c.jl">Advantage Actor Critic</a></li>
<li><a href="./src/model_free/rl/ddpg.jl">Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./src/model_free/rl/td3.jl">Twin Delayed DDPG (TD3)</a></li>
<li><a href="./src/model_free/rl/sac.jl">Soft Actor Critic (SAC)</a></li>
</ul>
<h3 dir="auto"><a id="user-content-imitation-learning" class="anchor" aria-hidden="true" href="#imitation-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Imitation Learning</h3>
<ul dir="auto">
<li><a href="./src/model_free/il/bc.jl"> Behavioral Cloning </a></li>
<li><a href="./src/model_free/il/gail.jl">Generative Adversarial Imitation Learning (GAIL) w/ On-Policy and Off Policy Versions</a></li>
<li><a href="./src/model_free/il/AdVIL.jl">Adversarial value moment imitation learning (AdVIL)</a></li>
<li><a href="./src/model_free/il/AdRIL.jl">(AdRIL)</a></li>
<li><a href="./src/model_free/il/sqil.jl">(SQIL)</a></li>
<li><a href="./src/model_free/il/asaf.jl">Adversarial Soft Advantage Fitting (ASAF)</a></li>
<li><a href="./src/model_free/il/iqlearn.jl">Inverse Q-Learning (IQLearn)</a></li>
</ul>
<h3 dir="auto"><a id="user-content-batch-rl" class="anchor" aria-hidden="true" href="#batch-rl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Batch RL</h3>
<ul dir="auto">
<li><a href="./src/model_free/batch/sac.jl">Batch Soft Actor Critic (SAC)</a></li>
<li><a href="./src/model_free/batch/cql.jl">Conservative Q-Learning (CQL)</a></li>
</ul>
<h3 dir="auto"><a id="user-content-adversarial-rl" class="anchor" aria-hidden="true" href="#adversarial-rl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Adversarial RL</h3>
<ul dir="auto">
<li><a href="./src/model_free/adversarial/rarl.jl">Robust Adversarial RL (RARL)</a></li>
</ul>
<h3 dir="auto"><a id="user-content-continual-learning" class="anchor" aria-hidden="true" href="#continual-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Continual Learning</h3>
<ul dir="auto">
<li>Experience Replay</li>
</ul>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<ul dir="auto">
<li>Install <a href="https://github.com/ancorso/POMDPGym">POMDPGym</a></li>
<li>Install by opening julia and running <code>]add git@github.com:ancorso/Crux.git</code></li>
</ul>
<p dir="auto">To edit or contribute use <code>]dev Crux</code> and the repo will be cloned to <code>~/.julia/dev/Crux</code></p>
<p dir="auto">Maintained by Anthony Corso (<a href="mailto:acorso@stanford.edu">acorso@stanford.edu</a>)</p>
</article></div>