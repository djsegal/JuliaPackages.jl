<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-transformations" class="anchor" aria-hidden="true" href="#transformations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Transformations</h1>
<p><a href="https://travis-ci.org/JuliaML/Transformations.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/350e214ea5b3c1e9182e9df841c18feac384187d286ca05a67bfc0c96a9c73fd/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614d4c2f5472616e73666f726d6174696f6e732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaML/Transformations.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>Static transforms, activation functions, learnable transformations, neural nets, and more.</p>
<hr>
<p>A Transformation is an abstraction which represents a (possibly differentiable, possibly parameterized) mapping from input(s) to output(s).  In a classic computational graph framework, nodes of the graph are primitives: "variables", "constants", or "operators". They are connected together by edges which define a tree-like definition of computation.  Complex operations and automatic differentiation can be applied at the primitive-level, and the full connectivity of a graph must be considered during a "compilation" stage.</p>
<p>Transformations takes an alternative view in which each Transformation is a sub-graph from input node(s) to output node(s).  There may be parameter nodes and operations embedded inside, but from the outside it can be treated as a black box function: <code>output = f(input, Î¸)</code>.  The output of one Transformation can be "linked" to the input of another, which binds the underlying array storage and connects them in the computation pipeline.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/933338/20273883/edfaa236-aa60-11e6-9c6c-9e8c8945201b.png"><img src="https://cloud.githubusercontent.com/assets/933338/20273883/edfaa236-aa60-11e6-9c6c-9e8c8945201b.png" alt="" style="max-width:100%;"></a></p>
<p>The end goal is one of specialization and consolidation.  Instead of expanding out a massive graph into primitives, we can maintain modular building blocks of our choosing and make it simple (and fast) to dynamically add and remove transformations in a larger graph, without recompiling.</p>
<p>For more on the design, see <a href="http://www.breloff.com/transformations/" rel="nofollow">my blog post</a>.</p>
<p>Implemented:</p>
<ul>
<li>Linear (y = wx)</li>
<li>Affine (y = wx + b)</li>
<li>Activations:
<ul>
<li>logistic (sigmoid)</li>
<li>tanh</li>
<li>softsign</li>
<li>ReLU</li>
<li>softplus</li>
<li>sinusoid</li>
<li>gaussian</li>
</ul>
</li>
<li>Multivariate Normal</li>
<li>Online/Incremental Layer Normalization</li>
<li>N-Differentiable Functions</li>
<li>Convolution/Pooling (WIP)</li>
<li>Online/Incremental Whitening:
<ul>
<li>PCA</li>
<li>Whitened PCA</li>
<li>ZCA</li>
</ul>
</li>
<li>Feed-Forward ANNs</li>
<li>Aggregators:
<ul>
<li>Sum</li>
<li>Gate (Product)</li>
<li>Concat</li>
</ul>
</li>
</ul>
<h3><a id="user-content-primary-author-tom-breloff-tbreloff" class="anchor" aria-hidden="true" href="#primary-author-tom-breloff-tbreloff"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Primary author: Tom Breloff (@tbreloff)</h3>
</article></div>