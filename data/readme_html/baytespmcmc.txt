<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-baytespmcmc" class="anchor" aria-hidden="true" href="#baytespmcmc"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BaytesPMCMC</h1>

<p dir="auto"><a href="https://paschermayr.github.io/BaytesPMCMC.jl/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Documentation, Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/paschermayr/BaytesPMCMC.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/paschermayr/BaytesPMCMC.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/paschermayr/BaytesPMCMC.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9d4f6578ac1804b489615967c0a16d7fc7a0d11ef25935721683736600c989d0/68747470733a2f2f636f6465636f762e696f2f67682f706173636865726d6179722f426179746573504d434d432e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/paschermayr/BaytesPMCMC.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://github.com/SciML/ColPrac"><img src="https://camo.githubusercontent.com/a6c1efcb19a957860ecb25966a730260b03d6e05380d0c27992ee7f9e3b1feb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6c507261632d436f6e7472696275746f72277325323047756964652d626c756576696f6c6574" alt="ColPrac: Contributor's Guide on Collaborative Practices for Community Packages" data-canonical-src="https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet" style="max-width: 100%;"></a></p>
<p dir="auto">BaytesPMCMC.jl is a library to perform particle MCMC proposal steps for parameter in a <code>ModelWrapper</code> struct, see <a href="https://github.com/paschermayr/ModelWrappers.jl">ModelWrappers.jl</a>.</p>
<h2 dir="auto"><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto">BaytesPMCMC.jl implements a Particle Gibbs as well as an Particle Metropolis sampler. Note that the latter does not need a specified log objective function for the acceptance rate, but instead uses an approximation from a particle filter. It is recommended to use a fixed stepsize for this sampler, or, if possible, use Particle Gibbs instead.</p>
<p dir="auto">Let us start with creating a univariate normal Mixture model with two states via ModelWrappers.jl:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using ModelWrappers, BaytesMCMC, BaytesFilters, BaytesPMCMC
using Distributions, Random, UnPack
_rng = Random.MersenneTwister(1)
N = 10^3
# Parameter
μ = [-2., 2.]
σ = [1., 1.]
p = [.05, .95]
# Latent data
latent = rand(_rng, Categorical(p), N)
data = [rand(_rng, Normal(μ[iter], σ[iter])) for iter in latent]

# Create ModelWrapper struct, assuming we do not know latent
latent_init = rand(_rng, Categorical(p), N)
myparameter = (;
    μ = Param([Normal(-2., 5), Normal(2., 5)], μ, ),
    σ = Param([Gamma(2.,2.), Gamma(2.,2.)], σ, ),
    p = Param(Dirichlet(2, 2), p, ),
    latent = Param([Categorical(p) for _ in Base.OneTo(N)], latent_init, ),
)
mymodel = ModelWrapper(myparameter)
myobjective = Objective(mymodel, data)"><pre><span class="pl-k">using</span> ModelWrappers, BaytesMCMC, BaytesFilters, BaytesPMCMC
<span class="pl-k">using</span> Distributions, Random, UnPack
_rng <span class="pl-k">=</span> Random<span class="pl-k">.</span><span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">1</span>)
N <span class="pl-k">=</span> <span class="pl-c1">10</span><span class="pl-k">^</span><span class="pl-c1">3</span>
<span class="pl-c"><span class="pl-c">#</span> Parameter</span>
μ <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">2.</span>, <span class="pl-c1">2.</span>]
σ <span class="pl-k">=</span> [<span class="pl-c1">1.</span>, <span class="pl-c1">1.</span>]
p <span class="pl-k">=</span> [.<span class="pl-c1">05</span>, .<span class="pl-c1">95</span>]
<span class="pl-c"><span class="pl-c">#</span> Latent data</span>
latent <span class="pl-k">=</span> <span class="pl-c1">rand</span>(_rng, <span class="pl-c1">Categorical</span>(p), N)
data <span class="pl-k">=</span> [<span class="pl-c1">rand</span>(_rng, <span class="pl-c1">Normal</span>(μ[iter], σ[iter])) <span class="pl-k">for</span> iter <span class="pl-k">in</span> latent]

<span class="pl-c"><span class="pl-c">#</span> Create ModelWrapper struct, assuming we do not know latent</span>
latent_init <span class="pl-k">=</span> <span class="pl-c1">rand</span>(_rng, <span class="pl-c1">Categorical</span>(p), N)
myparameter <span class="pl-k">=</span> (;
    μ <span class="pl-k">=</span> <span class="pl-c1">Param</span>([<span class="pl-c1">Normal</span>(<span class="pl-k">-</span><span class="pl-c1">2.</span>, <span class="pl-c1">5</span>), <span class="pl-c1">Normal</span>(<span class="pl-c1">2.</span>, <span class="pl-c1">5</span>)], μ, ),
    σ <span class="pl-k">=</span> <span class="pl-c1">Param</span>([<span class="pl-c1">Gamma</span>(<span class="pl-c1">2.</span>,<span class="pl-c1">2.</span>), <span class="pl-c1">Gamma</span>(<span class="pl-c1">2.</span>,<span class="pl-c1">2.</span>)], σ, ),
    p <span class="pl-k">=</span> <span class="pl-c1">Param</span>(<span class="pl-c1">Dirichlet</span>(<span class="pl-c1">2</span>, <span class="pl-c1">2</span>), p, ),
    latent <span class="pl-k">=</span> <span class="pl-c1">Param</span>([<span class="pl-c1">Categorical</span>(p) <span class="pl-k">for</span> _ <span class="pl-k">in</span> Base<span class="pl-k">.</span><span class="pl-c1">OneTo</span>(N)], latent_init, ),
)
mymodel <span class="pl-k">=</span> <span class="pl-c1">ModelWrapper</span>(myparameter)
myobjective <span class="pl-k">=</span> <span class="pl-c1">Objective</span>(mymodel, data)</pre></div>
<h2 dir="auto"><a id="user-content-particle-metropolis" class="anchor" aria-hidden="true" href="#particle-metropolis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Particle Metropolis</h2>
<p dir="auto">Particle Metropolis uses a particle filter to estimate the parameter latent, and an MCMC kernel to estimate all other parameter iteratively. This method is likelihood-free and uses an estimate from the particle filter for the acceptance ratio. As such, one does not need to state the log objective function at all, but gradient based mcmc kernels cannot be used either in this case. To assign a Particle Metropolis sampler, we only have to assign the particle filter dynamics as in BaytesFilters.jl:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Assign Model dynamics
function BaytesFilters.dynamics(objective::Objective{&lt;:ModelWrapper{BaseModel}})
    @unpack model, data = objective
    @unpack μ, σ, p = model.val

    initial_latent = Categorical(p)
    transition_latent(particles, iter) = initial_latent
    transition_data(particles, iter) = Normal(μ[particles[iter]], σ[particles[iter]])

    return Markov(initial_latent, transition_latent, transition_data)
end
dynamics(myobjective)

# Assign an objective for both a particle filter and an mcmc kernel:
myobjective_pf = Objective(mymodel, data, :latent)
myobjective_mcmc = Objective(mymodel, data, (:μ, :σ, :p))

# Assign Particle Metropolis algorithm
mcmcdefault = MCMCDefault(;
	stepsize = ConfigStepsize(; ϵ = 1.0, stepsizeadaption = UpdateFalse()),
)
pmetropolis = ParticleMetropolis(
    #Particle filter
    ParticleFilter(_rng, myobjective_pf),
    #MCMC kernel
    MCMC(_rng, Metropolis, myobjective_mcmc, mcmcdefault)
)

# Proposal steps work exactly as in BaytesFilters.jl and BaytesMCMC.jl
_val, _diagnostics = propose!(_rng, pmetropolis, mymodel, data)"><pre><span class="pl-c"><span class="pl-c">#</span> Assign Model dynamics</span>
<span class="pl-k">function</span> BaytesFilters<span class="pl-k">.</span><span class="pl-en">dynamics</span>(objective<span class="pl-k">::</span><span class="pl-c1">Objective{&lt;:ModelWrapper{BaseModel}}</span>)
    <span class="pl-c1">@unpack</span> model, data <span class="pl-k">=</span> objective
    <span class="pl-c1">@unpack</span> μ, σ, p <span class="pl-k">=</span> model<span class="pl-k">.</span>val

    initial_latent <span class="pl-k">=</span> <span class="pl-c1">Categorical</span>(p)
    <span class="pl-en">transition_latent</span>(particles, iter) <span class="pl-k">=</span> initial_latent
    <span class="pl-en">transition_data</span>(particles, iter) <span class="pl-k">=</span> <span class="pl-c1">Normal</span>(μ[particles[iter]], σ[particles[iter]])

    <span class="pl-k">return</span> <span class="pl-c1">Markov</span>(initial_latent, transition_latent, transition_data)
<span class="pl-k">end</span>
<span class="pl-c1">dynamics</span>(myobjective)

<span class="pl-c"><span class="pl-c">#</span> Assign an objective for both a particle filter and an mcmc kernel:</span>
myobjective_pf <span class="pl-k">=</span> <span class="pl-c1">Objective</span>(mymodel, data, <span class="pl-c1">:latent</span>)
myobjective_mcmc <span class="pl-k">=</span> <span class="pl-c1">Objective</span>(mymodel, data, (<span class="pl-c1">:μ</span>, <span class="pl-c1">:σ</span>, <span class="pl-c1">:p</span>))

<span class="pl-c"><span class="pl-c">#</span> Assign Particle Metropolis algorithm</span>
mcmcdefault <span class="pl-k">=</span> <span class="pl-c1">MCMCDefault</span>(;
	stepsize <span class="pl-k">=</span> <span class="pl-c1">ConfigStepsize</span>(; ϵ <span class="pl-k">=</span> <span class="pl-c1">1.0</span>, stepsizeadaption <span class="pl-k">=</span> <span class="pl-c1">UpdateFalse</span>()),
)
pmetropolis <span class="pl-k">=</span> <span class="pl-c1">ParticleMetropolis</span>(
    <span class="pl-c"><span class="pl-c">#</span>Particle filter</span>
    <span class="pl-c1">ParticleFilter</span>(_rng, myobjective_pf),
    <span class="pl-c"><span class="pl-c">#</span>MCMC kernel</span>
    <span class="pl-c1">MCMC</span>(_rng, Metropolis, myobjective_mcmc, mcmcdefault)
)

<span class="pl-c"><span class="pl-c">#</span> Proposal steps work exactly as in BaytesFilters.jl and BaytesMCMC.jl</span>
_val, _diagnostics <span class="pl-k">=</span> <span class="pl-c1">propose!</span>(_rng, pmetropolis, mymodel, data)</pre></div>
<h2 dir="auto"><a id="user-content-particle-gibbs" class="anchor" aria-hidden="true" href="#particle-gibbs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Particle Gibbs</h2>
<p dir="auto">Particle Gibbs uses a conditional particle filter along with an MCMC kernel. In order to use this sampler, one has to define an objective function. However, we can condition the target function on the
latent sequence, which results usually in a much easier and faster form than the (marginal) likelihood,
where latent variables have to be integrated out. Once defined, we can also use more advanced mcmc kernels for estimation.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="function (objective::Objective{&lt;:ModelWrapper{BaseModel}})(θ::NamedTuple)
    @unpack model, data, tagged = objective
    @unpack μ, σ, p, latent = θ
## Prior -&gt; a faster shortcut without initializing the priors again
    lprior = log_prior(tagged.info.transform.constraint, ModelWrappers.subset(θ, tagged.parameter) )
##Likelihood
    dynamicsᵉ = [Normal(μ[iter], σ[iter]) for iter in eachindex(μ)]
    dynamicsˢ = Categorical(p)
    ll = 0.0
#FOR PMCMC ~ target p(θ ∣ latent_1:t, data_1:t)
    for iter in eachindex(data)
        ll += logpdf(dynamicsᵉ[latent[iter]], data[iter])
        ll += logpdf(dynamicsˢ, latent[iter] )
    end
#=
# FOR MCMC ~ target p(θ ∣ data_1:t) by integrating out latent_1:t
    for time in eachindex(data)
        ll += logsumexp(logpdf(dynamicsˢ, iter) + logpdf(dynamicsᵉ[iter], grab(data, time)) for iter in eachindex(dynamicsᵉ))
    end
=#
    return ll + lprior
end
myobjective_mcmc(myobjective_mcmc.model.val)
# Note - It is good to benchmark this function, as it will allocate &gt;98% of the mcmc kernel time
using BenchmarkTools
$myobjective_mcmc($myobjective_mcmc.model.val) #13.600 μs (2 allocations: 176 bytes)"><pre><span class="pl-k">function</span> (objective<span class="pl-k">::</span><span class="pl-c1">Objective{&lt;:ModelWrapper{BaseModel}}</span>)(θ<span class="pl-k">::</span><span class="pl-c1">NamedTuple</span>)
    <span class="pl-c1">@unpack</span> model, data, tagged <span class="pl-k">=</span> objective
    <span class="pl-c1">@unpack</span> μ, σ, p, latent <span class="pl-k">=</span> θ
<span class="pl-c"><span class="pl-c">#</span># Prior -&gt; a faster shortcut without initializing the priors again</span>
    lprior <span class="pl-k">=</span> <span class="pl-c1">log_prior</span>(tagged<span class="pl-k">.</span>info<span class="pl-k">.</span>transform<span class="pl-k">.</span>constraint, ModelWrappers<span class="pl-k">.</span><span class="pl-c1">subset</span>(θ, tagged<span class="pl-k">.</span>parameter) )
<span class="pl-c"><span class="pl-c">#</span>#Likelihood</span>
    dynamicsᵉ <span class="pl-k">=</span> [<span class="pl-c1">Normal</span>(μ[iter], σ[iter]) <span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(μ)]
    dynamicsˢ <span class="pl-k">=</span> <span class="pl-c1">Categorical</span>(p)
    ll <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
<span class="pl-c"><span class="pl-c">#</span>FOR PMCMC ~ target p(θ ∣ latent_1:t, data_1:t)</span>
    <span class="pl-k">for</span> iter <span class="pl-k">in</span> <span class="pl-c1">eachindex</span>(data)
        ll <span class="pl-k">+=</span> <span class="pl-c1">logpdf</span>(dynamicsᵉ[latent[iter]], data[iter])
        ll <span class="pl-k">+=</span> <span class="pl-c1">logpdf</span>(dynamicsˢ, latent[iter] )
    <span class="pl-k">end</span>
<span class="pl-c"><span class="pl-c">#=</span></span>
<span class="pl-c"># FOR MCMC ~ target p(θ ∣ data_1:t) by integrating out latent_1:t</span>
<span class="pl-c">    for time in eachindex(data)</span>
<span class="pl-c">        ll += logsumexp(logpdf(dynamicsˢ, iter) + logpdf(dynamicsᵉ[iter], grab(data, time)) for iter in eachindex(dynamicsᵉ))</span>
<span class="pl-c">    end</span>
<span class="pl-c"><span class="pl-c">=#</span></span>
    <span class="pl-k">return</span> ll <span class="pl-k">+</span> lprior
<span class="pl-k">end</span>
<span class="pl-c1">myobjective_mcmc</span>(myobjective_mcmc<span class="pl-k">.</span>model<span class="pl-k">.</span>val)
<span class="pl-c"><span class="pl-c">#</span> Note - It is good to benchmark this function, as it will allocate &gt;98% of the mcmc kernel time</span>
<span class="pl-k">using</span> BenchmarkTools
<span class="pl-k">$</span><span class="pl-c1">myobjective_mcmc</span>(<span class="pl-k">$</span>myobjective_mcmc<span class="pl-k">.</span>model<span class="pl-k">.</span>val) <span class="pl-c"><span class="pl-c">#</span>13.600 μs (2 allocations: 176 bytes)</span></pre></div>
<p dir="auto">As we can analytically compute the marginal likelihood of a univariate mixture, I could also write down (and comment out) the corresponding objective function in the MCMC case. This should help understanding my comments above. Once our objective is defined, we can intialize a <code>ParticleGibbs</code> struct and sample with it:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# Assign an objective for both a particle filter and an mcmc kernel:
myobjective_pf = Objective(mymodel, data, :latent)
myobjective_mcmc = Objective(mymodel, data, (:μ, :σ, :p))

# Assign Particle Gibbs sampler
pfdefault = ParticleFilterDefault(referencing = Conditional())
pgibbs = ParticleGibbs(
    #Conditional Particle filter
    ParticleFilter(_rng, myobjective_pf, pfdefault
    ),
    #MCMC kernel -&gt; can use more advanced kernels
    MCMC(_rng, NUTS, myobjective_mcmc)
)

# Proposal steps work exactly as in BaytesFilters.jl and BaytesMCMC.jl
_val, _diagnostics = propose!(_rng, pgibbs, mymodel, data)"><pre><span class="pl-c"><span class="pl-c">#</span> Assign an objective for both a particle filter and an mcmc kernel:</span>
myobjective_pf <span class="pl-k">=</span> <span class="pl-c1">Objective</span>(mymodel, data, <span class="pl-c1">:latent</span>)
myobjective_mcmc <span class="pl-k">=</span> <span class="pl-c1">Objective</span>(mymodel, data, (<span class="pl-c1">:μ</span>, <span class="pl-c1">:σ</span>, <span class="pl-c1">:p</span>))

<span class="pl-c"><span class="pl-c">#</span> Assign Particle Gibbs sampler</span>
pfdefault <span class="pl-k">=</span> <span class="pl-c1">ParticleFilterDefault</span>(referencing <span class="pl-k">=</span> <span class="pl-c1">Conditional</span>())
pgibbs <span class="pl-k">=</span> <span class="pl-c1">ParticleGibbs</span>(
    <span class="pl-c"><span class="pl-c">#</span>Conditional Particle filter</span>
    <span class="pl-c1">ParticleFilter</span>(_rng, myobjective_pf, pfdefault
    ),
    <span class="pl-c"><span class="pl-c">#</span>MCMC kernel -&gt; can use more advanced kernels</span>
    <span class="pl-c1">MCMC</span>(_rng, NUTS, myobjective_mcmc)
)

<span class="pl-c"><span class="pl-c">#</span> Proposal steps work exactly as in BaytesFilters.jl and BaytesMCMC.jl</span>
_val, _diagnostics <span class="pl-k">=</span> <span class="pl-c1">propose!</span>(_rng, pgibbs, mymodel, data)</pre></div>
<h2 dir="auto"><a id="user-content-going-forward" class="anchor" aria-hidden="true" href="#going-forward"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Going Forward</h2>
<p dir="auto">This package is still highly experimental - suggestions and comments are always welcome!</p>

</article></div>