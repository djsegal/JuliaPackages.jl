<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-jlboostmljjl" class="anchor" aria-hidden="true" href="#jlboostmljjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>JLBoostMLJ.jl</h1>
<p>The <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ.jl</a> interface to <a href="https://github.com/xiaodaigh/JLBoost.jl">JLBoost.jl</a>, a hackable implementation of Gradient Boosting Regression Trees.</p>
<h2><a id="user-content-usage-example" class="anchor" aria-hidden="true" href="#usage-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage Example</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> RDatasets;
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>);
iris[<span class="pl-k">!</span>, <span class="pl-c1">:is_setosa</span>] <span class="pl-k">.=</span> iris<span class="pl-k">.</span>Species <span class="pl-k">.==</span> <span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span>;

<span class="pl-k">using</span> MLJ, JLBoostMLJ;
X, y <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(iris, x<span class="pl-k">-&gt;</span><span class="pl-k">!</span>(x <span class="pl-k">in</span> [<span class="pl-c1">:is_setosa</span>, <span class="pl-c1">:Species</span>]), <span class="pl-k">==</span>(<span class="pl-c1">:is_setosa</span>));

<span class="pl-k">using</span> JLBoostMLJ<span class="pl-k">:</span>JLBoostClassifier;
model <span class="pl-k">=</span> <span class="pl-c1">JLBoostClassifier</span>()</pre></div>
<pre><code>JLBoostClassifier(
    loss = JLBoost.LogitLogLoss(),
    nrounds = 1,
    subsample = 1.0,
    eta = 1.0,
    max_depth = 6,
    min_child_weight = 1.0,
    lambda = 0.0,
    gamma = 0.0,
    colsample_bytree = 1) @ 4…85
</code></pre>
<h3><a id="user-content-using-mlj-machines" class="anchor" aria-hidden="true" href="#using-mlj-machines"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using MLJ machines</h3>
<p>Put the model and data in a machine</p>
<div class="highlight highlight-source-julia"><pre>mljmachine  <span class="pl-k">=</span> <span class="pl-c1">machine</span>(model, X, y)</pre></div>
<pre><code>Machine{JLBoostClassifier} @ 5…33
</code></pre>
<p>Fit model using machine</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fit!</span>(mljmachine)</pre></div>
<pre><code>Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
(feature = :PetalLength, split_at = 1.9, cutpt = 50, gain = 133.33333333333
334, lweight = 2.0, rweight = -2.0)
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
Machine{JLBoostClassifier} @ 5…33
</code></pre>
<p>Predict using machine</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">predict</span>(mljmachine, X)</pre></div>
<pre><code>150-element Array{MLJBase.UnivariateFinite{Bool,UInt32,Float64},1}:
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 ⋮                                          
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
</code></pre>
<p>Feature importance using machine</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">feature_importance</span>(<span class="pl-c1">fitted_params</span>(mljmachine)<span class="pl-k">.</span>fitresult, X, y)</pre></div>
<pre><code>1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │
</code></pre>
<h4><a id="user-content-hyperparameter-tuning" class="anchor" aria-hidden="true" href="#hyperparameter-tuning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hyperparameter tuning</h4>
<p>Data preparation: need to convert <code>y</code> to categorical</p>
<div class="highlight highlight-source-julia"><pre>y_cate <span class="pl-k">=</span> <span class="pl-c1">categorical</span>(y)</pre></div>
<pre><code>150-element CategoricalArrays.CategoricalArray{Bool,1,UInt32}:
 true 
 true 
 true 
 true 
 true 
 true 
 true 
 true 
 true 
 true 
 ⋮    
 false
 false
 false
 false
 false
 false
 false
 false
 false
</code></pre>
<p>Set up some hyperparameter ranges</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> JLBoost, JLBoostMLJ, MLJ
jlb <span class="pl-k">=</span> <span class="pl-c1">JLBoostClassifier</span>()
r1 <span class="pl-k">=</span> <span class="pl-c1">range</span>(jlb, <span class="pl-c1">:nrounds</span>, lower<span class="pl-k">=</span><span class="pl-c1">1</span>, upper <span class="pl-k">=</span> <span class="pl-c1">6</span>)
r2 <span class="pl-k">=</span> <span class="pl-c1">range</span>(jlb, <span class="pl-c1">:max_depth</span>, lower<span class="pl-k">=</span><span class="pl-c1">1</span>, upper <span class="pl-k">=</span> <span class="pl-c1">6</span>)
r3 <span class="pl-k">=</span> <span class="pl-c1">range</span>(jlb, <span class="pl-c1">:eta</span>, lower<span class="pl-k">=</span><span class="pl-c1">0.1</span>, upper<span class="pl-k">=</span><span class="pl-c1">1.0</span>)</pre></div>
<pre><code>MLJBase.NumericRange(Float64, :eta, ... )
</code></pre>
<p>Set up the machine</p>
<div class="highlight highlight-source-julia"><pre>tm <span class="pl-k">=</span> <span class="pl-c1">TunedModel</span>(model <span class="pl-k">=</span> jlb, ranges <span class="pl-k">=</span> [r1, r2, r3], measure <span class="pl-k">=</span> cross_entropy)
m <span class="pl-k">=</span> <span class="pl-c1">machine</span>(tm, X, y_cate)</pre></div>
<pre><code>Machine{ProbabilisticTunedModel{Grid,…}} @ 1…10
</code></pre>
<p>Fit it!</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fit!</span>(m)</pre></div>
<pre><code>Machine{ProbabilisticTunedModel{Grid,…}} @ 1…10
</code></pre>
<p>Inspected the tuned parameters</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fitted_params</span>(m)<span class="pl-k">.</span>best_model<span class="pl-k">.</span>max_depth
<span class="pl-c1">fitted_params</span>(m)<span class="pl-k">.</span>best_model<span class="pl-k">.</span>nrounds
<span class="pl-c1">fitted_params</span>(m)<span class="pl-k">.</span>best_model<span class="pl-k">.</span>eta</pre></div>
<pre><code>0.1
</code></pre>
<h3><a id="user-content-simple-fitting" class="anchor" aria-hidden="true" href="#simple-fitting"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Simple Fitting</h3>
<p>Fit the model with <code>verbosity = 1</code></p>
<div class="highlight highlight-source-julia"><pre>mljmodel <span class="pl-k">=</span> <span class="pl-c1">fit</span>(model, <span class="pl-c1">1</span>, X, y)</pre></div>
<pre><code>Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
(feature = :PetalLength, split_at = 1.9, cutpt = 50, gain = 133.33333333333
334, lweight = 2.0, rweight = -2.0)
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
(fitresult = (treemodel = JLBoost.JLBoostTrees.JLBoostTreeModel(JLBoost.JLB
oostTrees.AbstractJLBoostTree[eta = 1.0 (tree weight)

   -- PetalLength &lt;= 1.9
     ---- weight = 2.0

   -- PetalLength &gt; 1.9
     ---- weight = -2.0
], JLBoost.LogitLogLoss(), :__y__),
              target_levels = Bool[0, 1],),
 cache = nothing,
 report = (AUC = 0.16666666666666669,
           feature_importance = 1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │,),)
</code></pre>
<p>Predicting using the model</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">predict</span>(model, mljmodel<span class="pl-k">.</span>fitresult, X)</pre></div>
<pre><code>150-element Array{MLJBase.UnivariateFinite{Bool,UInt32,Float64},1}:
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite(false=&gt;0.881, true=&gt;0.119)
 ⋮                                          
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite(false=&gt;0.119, true=&gt;0.881)
</code></pre>
<p>Feature Importance for simple fitting
One can obtain the feature importance using the <code>feature_importance</code> function</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">feature_importance</span>(mljmodel<span class="pl-k">.</span>fitresult<span class="pl-k">.</span>treemodel, X, y)</pre></div>
<pre><code>1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │
</code></pre>
</article></div>