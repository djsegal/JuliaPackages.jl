<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-jlboostmljjl" class="anchor" aria-hidden="true" href="#jlboostmljjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>JLBoostMLJ.jl</h1>
<p>The <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ.jl</a> interface to <a href="https://github.com/xiaodaigh/JLBoost.jl">JLBoost.jl</a>, a hackable implementation of Gradient Boosting Regression Trees.</p>
<h2><a id="user-content-usage-example" class="anchor" aria-hidden="true" href="#usage-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage Example</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> RDatasets;
iris <span class="pl-k">=</span> <span class="pl-c1">dataset</span>(<span class="pl-s"><span class="pl-pds">"</span>datasets<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>);
iris[<span class="pl-k">!</span>, <span class="pl-c1">:is_setosa</span>] <span class="pl-k">=</span> iris<span class="pl-k">.</span>Species <span class="pl-k">.==</span> <span class="pl-s"><span class="pl-pds">"</span>setosa<span class="pl-pds">"</span></span>;

<span class="pl-k">using</span> MLJ, JLBoostMLJ;
X, y <span class="pl-k">=</span> <span class="pl-c1">unpack</span>(iris, x<span class="pl-k">-&gt;</span><span class="pl-k">!</span>(x <span class="pl-k">in</span> [<span class="pl-c1">:is_setosa</span>, <span class="pl-c1">:Species</span>]), <span class="pl-k">==</span>(<span class="pl-c1">:is_setosa</span>));

<span class="pl-k">using</span> JLBoostMLJ<span class="pl-k">:</span>JLBoostClassifier;
model <span class="pl-k">=</span> <span class="pl-c1">JLBoostClassifier</span>()</pre></div>
<pre><code>JLBoostClassifier(
    loss = LogitLogLoss(),
    nrounds = 1,
    subsample = 1.0,
    eta = 1.0,
    max_depth = 6,
    min_child_weight = 1.0,
    lambda = 0.0,
    gamma = 0.0,
    colsample_bytree = 1) @ 1…36
</code></pre>
<h3><a id="user-content-using-mlj-machines" class="anchor" aria-hidden="true" href="#using-mlj-machines"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using MLJ machines</h3>
<p>Put the model and data in a machine</p>
<div class="highlight highlight-source-julia"><pre>mljmachine  <span class="pl-k">=</span> <span class="pl-c1">machine</span>(model, X, y)</pre></div>
<pre><code>Machine{JLBoostClassifier} @ 5…37
</code></pre>
<p>Fit model using machine</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fit!</span>(mljmachine)</pre></div>
<pre><code>Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
(feature = :PetalLength, split_at = 1.9, cutpt = 50, gain = 133.33333333333
334, lweight = 2.0, rweight = -2.0)
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
Machine{JLBoostClassifier} @ 5…37
</code></pre>
<p>Predict using machine</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">predict</span>(mljmachine, X)</pre></div>
<pre><code>150-element Array{MLJBase.UnivariateFinite{ScientificTypes.Multiclass{2},Bo
ol,UInt32,Float64},1}:
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 ⋮
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
</code></pre>
<p>Feature importance using machine</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">feature_importance</span>(<span class="pl-c1">fitted_params</span>(mljmachine)<span class="pl-k">.</span>fitresult, X, y)</pre></div>
<pre><code>1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │
</code></pre>
<h4><a id="user-content-hyperparameter-tuning" class="anchor" aria-hidden="true" href="#hyperparameter-tuning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Hyperparameter tuning</h4>
<p>Data preparation: need to convert <code>y</code> to categorical</p>
<div class="highlight highlight-source-julia"><pre>y_cate <span class="pl-k">=</span> <span class="pl-c1">categorical</span>(y)</pre></div>
<pre><code>150-element CategoricalArrays.CategoricalArray{Bool,1,UInt32}:
 true
 true
 true
 true
 true
 true
 true
 true
 true
 true
 ⋮
 false
 false
 false
 false
 false
 false
 false
 false
 false
</code></pre>
<p>Set up some hyperparameter ranges</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> JLBoost, JLBoostMLJ, MLJ
jlb <span class="pl-k">=</span> <span class="pl-c1">JLBoostClassifier</span>()
r1 <span class="pl-k">=</span> <span class="pl-c1">range</span>(jlb, <span class="pl-c1">:nrounds</span>, lower<span class="pl-k">=</span><span class="pl-c1">1</span>, upper <span class="pl-k">=</span> <span class="pl-c1">6</span>)
r2 <span class="pl-k">=</span> <span class="pl-c1">range</span>(jlb, <span class="pl-c1">:max_depth</span>, lower<span class="pl-k">=</span><span class="pl-c1">1</span>, upper <span class="pl-k">=</span> <span class="pl-c1">6</span>)
r3 <span class="pl-k">=</span> <span class="pl-c1">range</span>(jlb, <span class="pl-c1">:eta</span>, lower<span class="pl-k">=</span><span class="pl-c1">0.1</span>, upper<span class="pl-k">=</span><span class="pl-c1">1.0</span>)</pre></div>
<pre><code>MLJBase.NumericRange(Float64, :eta, ... )
</code></pre>
<p>Set up the machine</p>
<div class="highlight highlight-source-julia"><pre>tm <span class="pl-k">=</span> <span class="pl-c1">TunedModel</span>(model <span class="pl-k">=</span> jlb, ranges <span class="pl-k">=</span> [r1, r2, r3], measure <span class="pl-k">=</span> cross_entropy)
m <span class="pl-k">=</span> <span class="pl-c1">machine</span>(tm, X, y_cate)</pre></div>
<pre><code>Machine{ProbabilisticTunedModel{Grid,…}} @ 6…55
</code></pre>
<p>Fit it!</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fit!</span>(m)</pre></div>
<pre><code>Machine{ProbabilisticTunedModel{Grid,…}} @ 6…55
</code></pre>
<p>Inspected the tuned parameters</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fitted_params</span>(m)<span class="pl-k">.</span>best_model<span class="pl-k">.</span>max_depth
<span class="pl-c1">fitted_params</span>(m)<span class="pl-k">.</span>best_model<span class="pl-k">.</span>nrounds
<span class="pl-c1">fitted_params</span>(m)<span class="pl-k">.</span>best_model<span class="pl-k">.</span>eta</pre></div>
<pre><code>0.9
</code></pre>
<h3><a id="user-content-simple-fitting" class="anchor" aria-hidden="true" href="#simple-fitting"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Simple Fitting</h3>
<p>Fit the model with <code>verbosity = 1</code></p>
<div class="highlight highlight-source-julia"><pre>mljmodel <span class="pl-k">=</span> <span class="pl-c1">fit</span>(model, <span class="pl-c1">1</span>, X, y)</pre></div>
<pre><code>Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
(feature = :PetalLength, split_at = 1.9, cutpt = 50, gain = 133.33333333333
334, lweight = 2.0, rweight = -2.0)
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
Choosing a split on SepalLength
Choosing a split on SepalWidth
Choosing a split on PetalLength
Choosing a split on PetalWidth
(fitresult = (treemodel = JLBoostTreeModel(AbstractJLBoostTree[eta = 1.0 (t
ree weight)

   -- PetalLength &lt;= 1.9
     ---- weight = 2.0

   -- PetalLength &gt; 1.9
     ---- weight = -2.0
], LogitLogLoss(), :__y__),
              target_levels = Bool[0, 1],),
 cache = nothing,
 report = (AUC = 0.16666666666666669,
           feature_importance = 1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │,),)
</code></pre>
<p>Predicting using the model</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">predict</span>(model, mljmodel<span class="pl-k">.</span>fitresult, X)</pre></div>
<pre><code>150-element Array{MLJBase.UnivariateFinite{ScientificTypes.Multiclass{2},Bo
ol,UInt32,Float64},1}:
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.881, true=&gt;0.119)
 ⋮
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
 UnivariateFinite{ScientificTypes.Multiclass{2}}(false=&gt;0.119, true=&gt;0.881)
</code></pre>
<p>Feature Importance for simple fitting
One can obtain the feature importance using the <code>feature_importance</code> function</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">feature_importance</span>(mljmodel<span class="pl-k">.</span>fitresult<span class="pl-k">.</span>treemodel, X, y)</pre></div>
<pre><code>1×4 DataFrames.DataFrame
│ Row │ feature     │ Quality_Gain │ Coverage │ Frequency │
│     │ Symbol      │ Float64      │ Float64  │ Float64   │
├─────┼─────────────┼──────────────┼──────────┼───────────┤
│ 1   │ PetalLength │ 1.0          │ 1.0      │ 1.0       │
</code></pre>
</article></div>