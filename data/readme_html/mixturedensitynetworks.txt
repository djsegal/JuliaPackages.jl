<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-mixturedensitynetworks" class="anchor" aria-hidden="true" href="#mixturedensitynetworks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MixtureDensityNetworks</h1>
<p dir="auto"><a href="https://JoshuaBillson.github.io/MixtureDensityNetworks.jl/stable/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://JoshuaBillson.github.io/MixtureDensityNetworks.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/JoshuaBillson/MixtureDensityNetworks.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/JoshuaBillson/MixtureDensityNetworks.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JoshuaBillson/MixtureDensityNetworks.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ecb06dfe8c60f5d42f980fd3febd02c09df27cd36ab2ad9198994fb608d644b5/68747470733a2f2f636f6465636f762e696f2f67682f4a6f7368756142696c6c736f6e2f4d69787475726544656e736974794e6574776f726b732e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/JoshuaBillson/MixtureDensityNetworks.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto">This package provides a simple interface for defining, training, and deploying Mixture Density Networks (MDNs). MDNs were first proposed by <a href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" rel="nofollow">Bishop (1994)</a>. We can think of an MDN as a specialized type of Artificial Neural Network (ANN), which takes some features <code>X</code> and returns a distribution over the labels <code>Y</code> under a Gaussian Mixture Model (GMM). Unlike an ANN, MDNs maintain the full conditional distribution P(Y|X). This makes them particularly well-suited for situations where we want to maintain some measure of the uncertainty in our predictions. Moreover, because GMMs can represent multimodal distributions, MDNs are capable of modelling one-to-many relationships, which occurs when each input <code>X</code> can be associated with more than one output <code>Y</code>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JoshuaBillson/MixtureDensityNetworks.jl/blob/main/docs/src/figures/PredictedDistribution.png?raw=true"><img src="https://github.com/JoshuaBillson/MixtureDensityNetworks.jl/raw/main/docs/src/figures/PredictedDistribution.png?raw=true" alt="" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-mlj-compatibility" class="anchor" aria-hidden="true" href="#mlj-compatibility"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MLJ Compatibility</h1>
<p dir="auto">This package implements the interface specified by <a href="https://github.com/JuliaAI/MLJModelInterface.jl">MLJModelInterface</a> and is thus fully compatible
with the MLJ ecosystem. Below is an example demonstrating the use of this package in conjunction with MLJ.</p>
<h1 dir="auto"><a id="user-content-example-native-interface" class="anchor" aria-hidden="true" href="#example-native-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example (Native Interface)</h1>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Flux, MixtureDensityNetworks, Distributions, CairoMakie, Logging, TerminalLoggers

const n_samples = 1000
const epochs = 1000
const batchsize = 128
const mixtures = 8
const layers = [128, 128]

function main()
    # Generate Data
    X, Y = generate_data(n_samples)

    # Create Model
    model = MixtureDensityNetwork(1, 1, layers, mixtures)

    # Fit Model
    model, report = with_logger(TerminalLogger()) do 
        MixtureDensityNetworks.fit!(model, X, Y; epochs=epochs, opt=Flux.Adam(1e-3), batchsize=batchsize)
    end

    # Plot Learning Curve
    fig, _, _ = lines(1:epochs, report.learning_curve, axis=(;xlabel=&quot;Epochs&quot;, ylabel=&quot;Loss&quot;))
    save(&quot;LearningCurve.png&quot;, fig)

    # Plot Learned Distribution
    Ŷ = model(X)
    fig, ax, plt = scatter(X[1,:], rand.(Ŷ), markersize=4, label=&quot;Predicted Distribution&quot;)
    scatter!(ax, X[1,:], Y[1,:], markersize=3, label=&quot;True Distribution&quot;)
    axislegend(ax, position=:lt)
    save(&quot;PredictedDistribution.png&quot;, fig)

    # Plot Conditional Distribution
    cond = model(reshape([-2.1], (1,1)))[1]
    fig = Figure(resolution=(1000, 500))
    density(fig[1,1], rand(cond, 10000), npoints=10000)
    save(&quot;ConditionalDistribution.png&quot;, fig)
end

main()"><pre><span class="pl-k">using</span> Flux, MixtureDensityNetworks, Distributions, CairoMakie, Logging, TerminalLoggers

<span class="pl-k">const</span> n_samples <span class="pl-k">=</span> <span class="pl-c1">1000</span>
<span class="pl-k">const</span> epochs <span class="pl-k">=</span> <span class="pl-c1">1000</span>
<span class="pl-k">const</span> batchsize <span class="pl-k">=</span> <span class="pl-c1">128</span>
<span class="pl-k">const</span> mixtures <span class="pl-k">=</span> <span class="pl-c1">8</span>
<span class="pl-k">const</span> layers <span class="pl-k">=</span> [<span class="pl-c1">128</span>, <span class="pl-c1">128</span>]

<span class="pl-k">function</span> <span class="pl-en">main</span>()
    <span class="pl-c"><span class="pl-c">#</span> Generate Data</span>
    X, Y <span class="pl-k">=</span> <span class="pl-c1">generate_data</span>(n_samples)

    <span class="pl-c"><span class="pl-c">#</span> Create Model</span>
    model <span class="pl-k">=</span> <span class="pl-c1">MixtureDensityNetwork</span>(<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, layers, mixtures)

    <span class="pl-c"><span class="pl-c">#</span> Fit Model</span>
    model, report <span class="pl-k">=</span> <span class="pl-c1">with_logger</span>(<span class="pl-c1">TerminalLogger</span>()) <span class="pl-k">do</span> 
        MixtureDensityNetworks<span class="pl-k">.</span><span class="pl-c1">fit!</span>(model, X, Y; epochs<span class="pl-k">=</span>epochs, opt<span class="pl-k">=</span>Flux<span class="pl-k">.</span><span class="pl-c1">Adam</span>(<span class="pl-c1">1e-3</span>), batchsize<span class="pl-k">=</span>batchsize)
    <span class="pl-k">end</span>

    <span class="pl-c"><span class="pl-c">#</span> Plot Learning Curve</span>
    fig, _, _ <span class="pl-k">=</span> <span class="pl-c1">lines</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>epochs, report<span class="pl-k">.</span>learning_curve, axis<span class="pl-k">=</span>(;xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Epochs<span class="pl-pds">"</span></span>, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Loss<span class="pl-pds">"</span></span>))
    <span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>LearningCurve.png<span class="pl-pds">"</span></span>, fig)

    <span class="pl-c"><span class="pl-c">#</span> Plot Learned Distribution</span>
    Ŷ <span class="pl-k">=</span> <span class="pl-c1">model</span>(X)
    fig, ax, plt <span class="pl-k">=</span> <span class="pl-c1">scatter</span>(X[<span class="pl-c1">1</span>,:], <span class="pl-c1">rand</span>.(Ŷ), markersize<span class="pl-k">=</span><span class="pl-c1">4</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Predicted Distribution<span class="pl-pds">"</span></span>)
    <span class="pl-c1">scatter!</span>(ax, X[<span class="pl-c1">1</span>,:], Y[<span class="pl-c1">1</span>,:], markersize<span class="pl-k">=</span><span class="pl-c1">3</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>True Distribution<span class="pl-pds">"</span></span>)
    <span class="pl-c1">axislegend</span>(ax, position<span class="pl-k">=</span><span class="pl-c1">:lt</span>)
    <span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>PredictedDistribution.png<span class="pl-pds">"</span></span>, fig)

    <span class="pl-c"><span class="pl-c">#</span> Plot Conditional Distribution</span>
    cond <span class="pl-k">=</span> <span class="pl-c1">model</span>(<span class="pl-c1">reshape</span>([<span class="pl-k">-</span><span class="pl-c1">2.1</span>], (<span class="pl-c1">1</span>,<span class="pl-c1">1</span>)))[<span class="pl-c1">1</span>]
    fig <span class="pl-k">=</span> <span class="pl-c1">Figure</span>(resolution<span class="pl-k">=</span>(<span class="pl-c1">1000</span>, <span class="pl-c1">500</span>))
    <span class="pl-c1">density</span>(fig[<span class="pl-c1">1</span>,<span class="pl-c1">1</span>], <span class="pl-c1">rand</span>(cond, <span class="pl-c1">10000</span>), npoints<span class="pl-k">=</span><span class="pl-c1">10000</span>)
    <span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>ConditionalDistribution.png<span class="pl-pds">"</span></span>, fig)
<span class="pl-k">end</span>

<span class="pl-c1">main</span>()</pre></div>
<h1 dir="auto"><a id="user-content-example-mlj-interface" class="anchor" aria-hidden="true" href="#example-mlj-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example (MLJ Interface)</h1>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MixtureDensityNetworks, Distributions, Logging, TerminalLoggers, CairoMakie, MLJ, Random

const n_samples = 1000
const epochs = 500
const batchsize = 128
const mixtures = 8
const layers = [128, 128]

function main()
    # Generate Data
    X, Y = generate_data(n_samples)

    # Create Model
    mach = MLJ.machine(MDN(epochs=epochs, mixtures=mixtures, layers=layers, batchsize=batchsize), MLJ.table(X'), Y[1,:])

    # Fit Model on Training Data, Then Evaluate on Test
    with_logger(TerminalLogger()) do 
        @info &quot;Evaluating...&quot;
        evaluation = MLJ.evaluate!(
            mach, 
            resampling=Holdout(shuffle=true), 
            measure=[rsq, rmse, mae, mape], 
            operation=MLJ.predict_mean
        )
        names = [&quot;R²&quot;, &quot;RMSE&quot;, &quot;MAE&quot;, &quot;MAPE&quot;]
        metrics = round.(evaluation.measurement, digits=3)
        @info &quot;Metrics: &quot; * join([&quot;$name: $metric&quot; for (name, metric) in zip(names, metrics)], &quot;, &quot;)
    end

    # Fit Model on Entire Dataset
    with_logger(TerminalLogger()) do 
        @info &quot;Training...&quot;
        MLJ.fit!(mach)
    end

    # Plot Learning Curve
    fig, _, _ = lines(1:epochs, MLJ.training_losses(mach), axis=(;xlabel=&quot;Epochs&quot;, ylabel=&quot;Loss&quot;))
    save(&quot;LearningCurve.png&quot;, fig)

    # Plot Learned Distribution
    Ŷ = MLJ.predict(mach) .|&gt; rand
    fig, ax, plt = scatter(X[1,:], Ŷ, markersize=4, label=&quot;Predicted Distribution&quot;)
    scatter!(ax, X[1,:], Y[1,:], markersize=3, label=&quot;True Distribution&quot;)
    axislegend(ax, position=:lt)
    save(&quot;PredictedDistribution.png&quot;, fig)

    # Plot Conditional Distribution
    cond = MLJ.predict(mach, MLJ.table(reshape([-2.1], (1,1))))[1]
    fig = Figure(resolution=(1000, 500))
    density(fig[1,1], rand(cond, 10000), npoints=10000)
    save(&quot;ConditionalDistribution.png&quot;, fig)
end

main()"><pre><span class="pl-k">using</span> MixtureDensityNetworks, Distributions, Logging, TerminalLoggers, CairoMakie, MLJ, Random

<span class="pl-k">const</span> n_samples <span class="pl-k">=</span> <span class="pl-c1">1000</span>
<span class="pl-k">const</span> epochs <span class="pl-k">=</span> <span class="pl-c1">500</span>
<span class="pl-k">const</span> batchsize <span class="pl-k">=</span> <span class="pl-c1">128</span>
<span class="pl-k">const</span> mixtures <span class="pl-k">=</span> <span class="pl-c1">8</span>
<span class="pl-k">const</span> layers <span class="pl-k">=</span> [<span class="pl-c1">128</span>, <span class="pl-c1">128</span>]

<span class="pl-k">function</span> <span class="pl-en">main</span>()
    <span class="pl-c"><span class="pl-c">#</span> Generate Data</span>
    X, Y <span class="pl-k">=</span> <span class="pl-c1">generate_data</span>(n_samples)

    <span class="pl-c"><span class="pl-c">#</span> Create Model</span>
    mach <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">machine</span>(<span class="pl-c1">MDN</span>(epochs<span class="pl-k">=</span>epochs, mixtures<span class="pl-k">=</span>mixtures, layers<span class="pl-k">=</span>layers, batchsize<span class="pl-k">=</span>batchsize), MLJ<span class="pl-k">.</span><span class="pl-c1">table</span>(X<span class="pl-k">'</span>), Y[<span class="pl-c1">1</span>,:])

    <span class="pl-c"><span class="pl-c">#</span> Fit Model on Training Data, Then Evaluate on Test</span>
    <span class="pl-c1">with_logger</span>(<span class="pl-c1">TerminalLogger</span>()) <span class="pl-k">do</span> 
        <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Evaluating...<span class="pl-pds">"</span></span>
        evaluation <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">evaluate!</span>(
            mach, 
            resampling<span class="pl-k">=</span><span class="pl-c1">Holdout</span>(shuffle<span class="pl-k">=</span><span class="pl-c1">true</span>), 
            measure<span class="pl-k">=</span>[rsq, rmse, mae, mape], 
            operation<span class="pl-k">=</span>MLJ<span class="pl-k">.</span>predict_mean
        )
        names <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">"</span>R²<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>RMSE<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>MAE<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>MAPE<span class="pl-pds">"</span></span>]
        metrics <span class="pl-k">=</span> <span class="pl-c1">round</span>.(evaluation<span class="pl-k">.</span>measurement, digits<span class="pl-k">=</span><span class="pl-c1">3</span>)
        <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Metrics: <span class="pl-pds">"</span></span> <span class="pl-k">*</span> <span class="pl-c1">join</span>([<span class="pl-s"><span class="pl-pds">"</span><span class="pl-v">$name</span>: <span class="pl-v">$metric</span><span class="pl-pds">"</span></span> <span class="pl-k">for</span> (name, metric) <span class="pl-k">in</span> <span class="pl-c1">zip</span>(names, metrics)], <span class="pl-s"><span class="pl-pds">"</span>, <span class="pl-pds">"</span></span>)
    <span class="pl-k">end</span>

    <span class="pl-c"><span class="pl-c">#</span> Fit Model on Entire Dataset</span>
    <span class="pl-c1">with_logger</span>(<span class="pl-c1">TerminalLogger</span>()) <span class="pl-k">do</span> 
        <span class="pl-c1">@info</span> <span class="pl-s"><span class="pl-pds">"</span>Training...<span class="pl-pds">"</span></span>
        MLJ<span class="pl-k">.</span><span class="pl-c1">fit!</span>(mach)
    <span class="pl-k">end</span>

    <span class="pl-c"><span class="pl-c">#</span> Plot Learning Curve</span>
    fig, _, _ <span class="pl-k">=</span> <span class="pl-c1">lines</span>(<span class="pl-c1">1</span><span class="pl-k">:</span>epochs, MLJ<span class="pl-k">.</span><span class="pl-c1">training_losses</span>(mach), axis<span class="pl-k">=</span>(;xlabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Epochs<span class="pl-pds">"</span></span>, ylabel<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Loss<span class="pl-pds">"</span></span>))
    <span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>LearningCurve.png<span class="pl-pds">"</span></span>, fig)

    <span class="pl-c"><span class="pl-c">#</span> Plot Learned Distribution</span>
    Ŷ <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">predict</span>(mach) <span class="pl-k">.|</span><span class="pl-k">&gt;</span> rand
    fig, ax, plt <span class="pl-k">=</span> <span class="pl-c1">scatter</span>(X[<span class="pl-c1">1</span>,:], Ŷ, markersize<span class="pl-k">=</span><span class="pl-c1">4</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Predicted Distribution<span class="pl-pds">"</span></span>)
    <span class="pl-c1">scatter!</span>(ax, X[<span class="pl-c1">1</span>,:], Y[<span class="pl-c1">1</span>,:], markersize<span class="pl-k">=</span><span class="pl-c1">3</span>, label<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>True Distribution<span class="pl-pds">"</span></span>)
    <span class="pl-c1">axislegend</span>(ax, position<span class="pl-k">=</span><span class="pl-c1">:lt</span>)
    <span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>PredictedDistribution.png<span class="pl-pds">"</span></span>, fig)

    <span class="pl-c"><span class="pl-c">#</span> Plot Conditional Distribution</span>
    cond <span class="pl-k">=</span> MLJ<span class="pl-k">.</span><span class="pl-c1">predict</span>(mach, MLJ<span class="pl-k">.</span><span class="pl-c1">table</span>(<span class="pl-c1">reshape</span>([<span class="pl-k">-</span><span class="pl-c1">2.1</span>], (<span class="pl-c1">1</span>,<span class="pl-c1">1</span>))))[<span class="pl-c1">1</span>]
    fig <span class="pl-k">=</span> <span class="pl-c1">Figure</span>(resolution<span class="pl-k">=</span>(<span class="pl-c1">1000</span>, <span class="pl-c1">500</span>))
    <span class="pl-c1">density</span>(fig[<span class="pl-c1">1</span>,<span class="pl-c1">1</span>], <span class="pl-c1">rand</span>(cond, <span class="pl-c1">10000</span>), npoints<span class="pl-k">=</span><span class="pl-c1">10000</span>)
    <span class="pl-c1">save</span>(<span class="pl-s"><span class="pl-pds">"</span>ConditionalDistribution.png<span class="pl-pds">"</span></span>, fig)
<span class="pl-k">end</span>

<span class="pl-c1">main</span>()</pre></div>
</article></div>