<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://jbytecode.github.io/ErrorsInVariables.jl/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Doc" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/jbytecode/eive.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/56cb027644f28ee9f8cab343333753acac5f3c951e071f5d5b338ffbd1e28cd9/68747470733a2f2f636f6465636f762e696f2f67682f6a62797465636f64652f656976652e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d4b4d4637483144533031" alt="codecov" data-canonical-src="https://codecov.io/gh/jbytecode/eive.jl/branch/main/graph/badge.svg?token=KMF7H1DS01" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-errorsinvariablesjl" class="anchor" aria-hidden="true" href="#errorsinvariablesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ErrorsInVariables.jl</h1>
<p dir="auto">Error-in-variables estimation using Compact Genetic Algorithms in Julia</p>
<h1 dir="auto"><a id="user-content-the-problem" class="anchor" aria-hidden="true" href="#the-problem"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The Problem</h1>
<p dir="auto">Suppose the linear regression model is</p>
<p dir="auto"><math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$$
y = \beta_0 + \beta_1 x^* + \varepsilon
$$</math-renderer></p>
<p dir="auto">where <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$y$</math-renderer>Â is n-vector of the response variable, <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$\beta_0$</math-renderer> and <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$\beta_1$</math-renderer> are unknown regression parameteres, <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$\varepsilon$</math-renderer> is the iid. error term, <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$x^*$</math-renderer> is the unknown n-vector of the independent variable, and <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$n$</math-renderer> is the number of observations.</p>
<p dir="auto">We call <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$x^*$</math-renderer> unknown because in some situations the true values of the variable cannot be visible or directly observable, or observable with some measurement error. Now suppose that <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$x$</math-renderer> is the observable version of the true values and it is defined as</p>
<p dir="auto"><math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$$
x = x^* + \delta
$$</math-renderer></p>
<p dir="auto">where <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$\delta$</math-renderer> is the measurement error and <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$x$</math-renderer> is the erroneous version of the true <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$x^*$</math-renderer>. If the estimated model is</p>
<p dir="auto"><math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x
$$</math-renderer></p>
<p dir="auto">then the ordinary least squares (OLS) estimates are no longer unbiased and even consistent.</p>
<p dir="auto">Eive-cga is an estimator devised for this problem. The aim is to reduce the errors-in-variable bias with some cost of increasing the variance. At the end, the estimator obtains lower Mean Square Error (MSE) values defined as</p>
<p dir="auto"><math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$$
MSE(\hat{\beta_1}) = Var(\hat{\beta_1}) + Bias^2(\hat{\beta_1})
$$</math-renderer></p>
<p dir="auto">for the Eive-cga estimator. For more detailed comparisons, see the original paper given in the Citation part.</p>
<h1 dir="auto">
<a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h1>
<p dir="auto">For the single variable case</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; eive(dirtyx = dirtyx, y = y, otherx = nothing) "><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">eive</span>(dirtyx <span class="pl-k">=</span> dirtyx, y <span class="pl-k">=</span> y, otherx <span class="pl-k">=</span> <span class="pl-c1">nothing</span>) </pre></div>
<p dir="auto">and for the multiple regression</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; eive(dirtyx = dirtyx, y = y, otherx = matrixofotherx) "><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">eive</span>(dirtyx <span class="pl-k">=</span> dirtyx, y <span class="pl-k">=</span> y, otherx <span class="pl-k">=</span> matrixofotherx) </pre></div>
<p dir="auto">Note that the method assumes there is only one erroneous variable in the set of independent variables.</p>
<h1 dir="auto">
<a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h1>
<p dir="auto">Lets generate data from the model <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$y = 20 + 10x^* + \varepsilon$</math-renderer></p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import Random
using ErrorsInVariables

rng = Random.MersenneTwister(1234)
n = 30
deltax = randn(rng, n) * sqrt(3.0)
cleanx = randn(rng, n) * sqrt(7.0)
e = randn(rng, n) * sqrt(5.0)
y = 20.0 .+ 10.0 .* cleanx .+ e
dirtyx = cleanx"><pre><span class="pl-k">import</span> Random
<span class="pl-k">using</span> ErrorsInVariables

rng <span class="pl-k">=</span> Random<span class="pl-k">.</span><span class="pl-c1">MersenneTwister</span>(<span class="pl-c1">1234</span>)
n <span class="pl-k">=</span> <span class="pl-c1">30</span>
deltax <span class="pl-k">=</span> <span class="pl-c1">randn</span>(rng, n) <span class="pl-k">*</span> <span class="pl-c1">sqrt</span>(<span class="pl-c1">3.0</span>)
cleanx <span class="pl-k">=</span> <span class="pl-c1">randn</span>(rng, n) <span class="pl-k">*</span> <span class="pl-c1">sqrt</span>(<span class="pl-c1">7.0</span>)
e <span class="pl-k">=</span> <span class="pl-c1">randn</span>(rng, n) <span class="pl-k">*</span> <span class="pl-c1">sqrt</span>(<span class="pl-c1">5.0</span>)
y <span class="pl-k">=</span> <span class="pl-c1">20.0</span> <span class="pl-k">.+</span> <span class="pl-c1">10.0</span> <span class="pl-k">.*</span> cleanx <span class="pl-k">.+</span> e
dirtyx <span class="pl-k">=</span> cleanx</pre></div>
<p dir="auto">and assume that</p>
<p dir="auto"><math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$$
x^*
$$</math-renderer></p>
<p dir="auto">is unobservable and it is observed as</p>
<p dir="auto"><math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="2aca1c9f2b0ee6db3220e83205d1c689">$$
x = x^* + \delta$$</math-renderer></p>
<p dir="auto">. We can calculate an unbiased estimate of the slope parameter using</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="eive(dirtyx = dirtyx, y = y, otherx = nothing) "><pre><span class="pl-c1">eive</span>(dirtyx <span class="pl-k">=</span> dirtyx, y <span class="pl-k">=</span> y, otherx <span class="pl-k">=</span> <span class="pl-c1">nothing</span>) </pre></div>
<p dir="auto">The result is</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="EiveResult([20.28458307772922, 9.456757289676714])"><pre><span class="pl-c1">EiveResult</span>([<span class="pl-c1">20.28458307772922</span>, <span class="pl-c1">9.456757289676714</span>])</pre></div>
<p dir="auto">whereas OLS estimates are</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; X = hcat(ones(n), dirtyx);

julia&gt; X \ y
2-element Vector{Float64}:
 17.94867860059858
  5.8099584879737876"><pre>julia<span class="pl-k">&gt;</span> X <span class="pl-k">=</span> <span class="pl-c1">hcat</span>(<span class="pl-c1">ones</span>(n), dirtyx);

julia<span class="pl-k">&gt;</span> X <span class="pl-k">\</span> y
<span class="pl-c1">2</span><span class="pl-k">-</span>element Vector{Float64}<span class="pl-k">:</span>
 <span class="pl-c1">17.94867860059858</span>
  <span class="pl-c1">5.8099584879737876</span></pre></div>
<p dir="auto">and clearly biased towards to zero.</p>
<h1 dir="auto">
<a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h1>
<div class="highlight highlight-text-tex-latex notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@article{satman2015reducing,
  title={Reducing errors-in-variables bias in linear regression using compact genetic algorithms},
  author={Satman, M Hakan and Diyarbakirlioglu, Erkin},
  journal={Journal of Statistical Computation and Simulation},
  volume={85},
  number={16},
  pages={3216--3235},
  year={2015},
  publisher={Taylor \&amp; Francis}
}"><pre>@article{satman2015reducing,
  title={Reducing errors-in-variables bias in linear regression using compact genetic algorithms},
  author={Satman, M Hakan and Diyarbakirlioglu, Erkin},
  journal={Journal of Statistical Computation and Simulation},
  volume={85},
  number={16},
  pages={3216--3235},
  year={2015},
  publisher={Taylor <span class="pl-cce">\&amp;</span> Francis}
}</pre></div>
</article></div>