<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-rlinterface" class="anchor" aria-hidden="true" href="#rlinterface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RLInterface</h1>
<p><a href="https://travis-ci.org/JuliaPOMDP/RLInterface.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/ad5c7ff5cacaa11aef85b669d2153f74e6e01fe4/68747470733a2f2f7472617669732d63692e6f72672f4a756c6961504f4d44502f524c496e746572666163652e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaPOMDP/RLInterface.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://coveralls.io/github/JuliaPOMDP/RLInterface.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/33c96afe0e30bb86bc5ca7717348c4522e0b7bcd/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4a756c6961504f4d44502f524c496e746572666163652e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/JuliaPOMDP/RLInterface.jl/badge.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package provides an interface for working with deep reinfrocement learning problems in Julia.
It is closely integrated with <a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a> to easily wrap problems defined in those formats.
While the focus of this interface is on partially observable Markov decision process (POMDP) reinforcement learning, it
is flexible and can easily handle problems that are fully observable as well.
The interface is very similar to that of <a href="https://gym.openai.com/" rel="nofollow">OpenAI Gym</a>. This allows algorithms that work with Gym to be used with problems that
are defined in this interface and vice versa.
A shared interface between POMDPs.jl allows easy comparison of reinforcement learning solutions to approximate dynamic
programming solutions when a complete model of the problem is defined.</p>
<h2><a id="user-content-simulation" class="anchor" aria-hidden="true" href="#simulation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Simulation</h2>
<p>Running a simulation can be done like so, we use a problem from
<a href="https://github.com/JuliaPOMDP/POMDPModels.jl">POMDPModels</a> as an example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> POMDPModels <span class="pl-c"><span class="pl-c">#</span> for TigerPOMDP</span>
<span class="pl-k">using</span> RLInterface

env <span class="pl-k">=</span> <span class="pl-c1">POMDPEnvironment</span>(<span class="pl-c1">TigerPOMDP</span>())

<span class="pl-k">function</span> <span class="pl-en">simulate</span>(env<span class="pl-k">::</span><span class="pl-c1">AbstractEnvironment</span>, nsteps<span class="pl-k">::</span><span class="pl-c1">Int</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>)
    done <span class="pl-k">=</span> <span class="pl-c1">false</span>
    r_tot <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
    step <span class="pl-k">=</span> <span class="pl-c1">1</span>
    o <span class="pl-k">=</span> <span class="pl-c1">reset!</span>(env)
    <span class="pl-k">while</span> <span class="pl-k">!</span>done <span class="pl-k">&amp;&amp;</span> step <span class="pl-k">&lt;=</span> nsteps
        action <span class="pl-k">=</span> <span class="pl-c1">sample_action</span>(env) <span class="pl-c"><span class="pl-c">#</span> take random action </span>
        obs, rew, done, info <span class="pl-k">=</span> <span class="pl-c1">step!</span>(env, action)
        <span class="pl-c1">@show</span> obs, rew, done, info
        r_tot <span class="pl-k">+=</span> rew
        step <span class="pl-k">+=</span> <span class="pl-c1">1</span>
    <span class="pl-k">end</span>
    <span class="pl-k">return</span> r_tot
<span class="pl-k">end</span>

<span class="pl-c1">@show</span> <span class="pl-c1">simulate</span>(env)</pre></div>
<h2><a id="user-content-interface" class="anchor" aria-hidden="true" href="#interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Interface</h2>
<p>The interface provides an <code>AbstractEnvironment</code> type from which all custom environments
should inherit. For an example see how this is done with <a href="https://github.com/sisl/Gym.jl">OpenAI Gym</a>.</p>
<p>Currently, the following functions make up the interface. See the docstrings for more information</p>
<div class="highlight highlight-source-julia"><pre>reset!
step!
actions
sample_action
render</pre></div>
<h2><a id="user-content-requirements-for-pomdpsjl-models" class="anchor" aria-hidden="true" href="#requirements-for-pomdpsjl-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Requirements for POMDPs.jl models</h2>
<p>To use POMDPs.jl models, the <a href="https://juliapomdp.github.io/POMDPs.jl/latest/generative/" rel="nofollow">generative interface</a>, including <a href="https://juliapomdp.github.io/POMDPs.jl/latest/api/#POMDPs.initialstate" rel="nofollow"><code>initialstate</code></a> must be implemented. In addition, the function</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">convert_s</span>(<span class="pl-k">::</span><span class="pl-c1">Type{Vector{Float32}}</span>, s<span class="pl-k">::</span><span class="pl-c1">S</span>, m<span class="pl-k">::</span><span class="pl-c1">M</span>)</pre></div>
<p>where <code>M</code> is the <code>MDP</code> type with states of type <code>S</code>, or</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">convert_o</span>(<span class="pl-k">::</span><span class="pl-c1">Type{Vector{Float32}}</span>, o<span class="pl-k">::</span><span class="pl-c1">O</span>, m<span class="pl-k">::</span><span class="pl-c1">M</span>)</pre></div>
<p>where <code>M</code> is a <code>POMDP</code> with observation type <code>O</code>, will be used to convert the observation into a vector (Sometimes <code>Vector{Float32}</code> will be replaced with a different <code>AbstractVector</code> type if the environment is configured differently).</p>
<p>The type of the observation vector returned by an environment may be specified as an argument to the environment constructor.
The default for a particular MDP or POMDP may be specified by implementing <code>obsvector_type</code>. This function default to <code>Vector{Float32}</code>.
It will be called when initializing any of the wrappers to determine the type to give as input to the <code>convert_s</code> function.</p>
</article></div>