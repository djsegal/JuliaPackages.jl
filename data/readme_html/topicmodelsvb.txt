<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-topicmodelsvbjl" class="anchor" aria-hidden="true" href="#topicmodelsvbjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TopicModelsVB.jl</h1>
<p><strong>v1.x compatible.</strong></p>
<p>A Julia package for variational Bayesian topic modeling.</p>
<p>Topic models are Bayesian hierarchical models designed to discover the latent low-dimensional thematic structure within corpora. Like most probabilistic graphical models, topic models are fit using either <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="nofollow">Markov chain Monte Carlo</a> (MCMC), or <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="nofollow">variational inference</a> (VI).</p>
<p>Markov chain Monte Carlo is slow but consistent, given infinite time MCMC will fit the desired model exactly. Unfortunately, the lack of an objective metric for assessing convergence means that it's difficult to state unequivocally that MCMC has reached an optimal steady-state.</p>
<p>Contrarily, variational inference is fast but inconsistent, since one must approximate distributions in order to ensure tractability. Fortunately, VI may be characterized in the language of numerical optimization, and its performance evaluated objectively via the assessment of convergence to local optima.</p>
<p>This package takes the latter approach to topic modeling.</p>
<h2><a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dependencies</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="DelimitedFiles
SpecialFunctions
LinearAlgebra
Random
Distributions
OpenCL
Crayons
"><pre>DelimitedFiles
SpecialFunctions
LinearAlgebra
Random
Distributions
OpenCL
Crayons</pre></div>
<h2><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="(@v1.6) pkg&gt; add TopicModelsVB
"><pre>(<span class="pl-c1">@v1</span>.<span class="pl-c1">6</span>) pkg<span class="pl-k">&gt;</span> add TopicModelsVB</pre></div>
<h2><a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Datasets</h2>
<p>Included in TopicModelsVB.jl are two datasets:</p>
<ol>
<li>National Science Foundation Abstracts 1989 - 2003:</li>
</ol>
<ul>
<li>128804 documents</li>
<li>25319 vocabulary</li>
</ul>
<ol start="2">
<li>CiteULike Science Article Database:</li>
</ol>
<ul>
<li>16980 documents</li>
<li>8000 vocabulary</li>
<li>5551 users</li>
</ul>
<h2><a id="user-content-corpus" class="anchor" aria-hidden="true" href="#corpus"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Corpus</h2>
<p>Let's begin with the Corpus data structure. The Corpus data structure has been designed for maximum ease-of-use. Datasets must still be cleaned and put into the appropriate format, but once a dataset is in the proper format and read into a corpus, it can easily be modified to meet the user's needs.</p>
<p>There are four plaintext files that make up a corpus:</p>
<ul>
<li>docfile</li>
<li>vocabfile</li>
<li>userfile</li>
<li>titlefile</li>
</ul>
<p>None of these files are mandatory to read a corpus, and in fact reading no files will result in an empty corpus. However in order to train a model a docfile will be necessary, since it contains all quantitative data known about the documents. On the other hand, the vocab, user and title files are used solely for interpreting output.</p>
<p>The docfile should be a plaintext file containing lines of delimited numerical values. Each document is a block of lines, the number of which depends on what information is known about the documents. Since a document is at its essence a list of terms, each document <em>must</em> contain at least one line containing a nonempty list of delimited positive integer values corresponding to the terms of which it is composed. Any further lines in a document block are optional, however if they are present they must be present for all documents and must come in the following order:</p>
<h5><a id="user-content-terms---a-line-of-delimited-positive-integers-corresponding-to-the-terms-which-make-up-the-document-this-line-is-mandatory" class="anchor" aria-hidden="true" href="#terms---a-line-of-delimited-positive-integers-corresponding-to-the-terms-which-make-up-the-document-this-line-is-mandatory"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>terms - A line of delimited positive integers corresponding to the terms which make up the document (this line is mandatory).</h5>
<h5><a id="user-content-counts---a-line-of-delimited-positive-integers-equal-in-length-to-terms-corresponding-to-the-number-of-times-a-term-appears-in-a-document" class="anchor" aria-hidden="true" href="#counts---a-line-of-delimited-positive-integers-equal-in-length-to-terms-corresponding-to-the-number-of-times-a-term-appears-in-a-document"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>counts - A line of delimited positive integers, equal in length to terms, corresponding to the number of times a term appears in a document.</h5>
<h5><a id="user-content-readers---a-line-of-delimited-positive-integers-corresponding-to-those-users-which-have-read-the-document" class="anchor" aria-hidden="true" href="#readers---a-line-of-delimited-positive-integers-corresponding-to-those-users-which-have-read-the-document"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>readers - A line of delimited positive integers corresponding to those users which have read the document.</h5>
<h5><a id="user-content-ratings---a-line-of-delimited-positive-integers-equal-in-length-to-readers-corresponding-to-the-rating-each-reader-gave-the-document" class="anchor" aria-hidden="true" href="#ratings---a-line-of-delimited-positive-integers-equal-in-length-to-readers-corresponding-to-the-rating-each-reader-gave-the-document"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ratings - A line of delimited positive integers, equal in length to readers, corresponding to the rating each reader gave the document.</h5>
<p>An example of a single doc block from a docfile with all possible lines included,</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="...
4,10,3,100,57
1,1,2,1,3
1,9,10
1,1,5
...
"><pre><code>...
4,10,3,100,57
1,1,2,1,3
1,9,10
1,1,5
...
</code></pre></div>
<p>The vocab and user files are tab delimited dictionaries mapping positive integers to terms and usernames (resp.). For example,</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="1    this
2    is
3    a
4    vocab
5    file
"><pre><code>1    this
2    is
3    a
4    vocab
5    file
</code></pre></div>
<p>A userfile is identitcal to a vocabfile, except usernames will appear in place of vocabulary terms.</p>
<p>Finally, a titlefile is simply a list of titles, not a dictionary, and is of the form,</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="title1
title2
title3
title4
title5
"><pre><code>title1
title2
title3
title4
title5
</code></pre></div>
<p>The order of these titles correspond to the order of document blocks in the associated docfile.</p>
<p>To read a corpus into Julia, use the following function,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="readcorp(;docfile=&quot;&quot;, vocabfile=&quot;&quot;, userfile=&quot;&quot;, titlefile=&quot;&quot;, delim=',', counts=false, readers=false, ratings=false)
"><pre><span class="pl-c1">readcorp</span>(;docfile<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, vocabfile<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, userfile<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, titlefile<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, delim<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>, counts<span class="pl-k">=</span><span class="pl-c1">false</span>, readers<span class="pl-k">=</span><span class="pl-c1">false</span>, ratings<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p>The <code>file</code> keyword arguments indicate the path where the respective file is located.</p>
<p>It is often the case that even once files are correctly formatted and read, the corpus will still contain formatting defects which prevent it from being loaded into a model. Therefore, before loading a corpus into a model, it is <strong>important</strong> that one of the following is run,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="fixcorp!(corp)
"><pre><span class="pl-c1">fixcorp!</span>(corp)</pre></div>
<p>or</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="fixcorp!(corp, pad=true)
"><pre><span class="pl-c1">fixcorp!</span>(corp, pad<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<p>Padding a corpus will ensure that any documents which contain vocab or user keys not in the vocab or user dictionaries are not removed. Instead, generic vocab and user keys will be added as necessary to the vocab and user dictionaries (resp.).</p>
<p>The <code>fixcorp!</code> function allows for significant customization of the corpus object.</p>
<p>For example, let's begin by loading the CiteULike corpus,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="corp = readcorp(:citeu)
"><pre>corp <span class="pl-k">=</span> <span class="pl-c1">readcorp</span>(<span class="pl-c1">:citeu</span>)</pre></div>
<p>A standard preprocessing step might involve removing stop words, removing terms which appear less than 200 times, and alphabetizing our corpus.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="fixcorp!(corp, stop=true, abridge=200, alphabetize=true, trim=true)
### Generally you will also want to trim your corpus.
### Setting trim=true will remove leftover terms from the corpus vocabulary.
"><pre><span class="pl-c1">fixcorp!</span>(corp, stop<span class="pl-k">=</span><span class="pl-c1">true</span>, abridge<span class="pl-k">=</span><span class="pl-c1">200</span>, alphabetize<span class="pl-k">=</span><span class="pl-c1">true</span>, trim<span class="pl-k">=</span><span class="pl-c1">true</span>)
<span class="pl-c"><span class="pl-c">#</span>## Generally you will also want to trim your corpus.</span>
<span class="pl-c"><span class="pl-c">#</span>## Setting trim=true will remove leftover terms from the corpus vocabulary.</span></pre></div>
<p>After removing stop words and abridging our corpus, the vocabulary size has gone from 8000 to 1692.</p>
<p>A consequence of removing so many terms from our corpus is that some documents may now by empty. We can remove these documents from our corpus with the following command,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="fixcorp!(corp, remove_empty_docs=true)
"><pre><span class="pl-c1">fixcorp!</span>(corp, remove_empty_docs<span class="pl-k">=</span><span class="pl-c1">true</span>)</pre></div>
<p>In addition, if you would like to preserve term order in your documents, then you should refrain from condesing your corpus.</p>
<p>For example,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="corp = Corpus(Document(1:9), vocab=split(&quot;the quick brown fox jumped over the lazy dog&quot;))
showdocs(corp)
"><pre>corp <span class="pl-k">=</span> <span class="pl-c1">Corpus</span>(<span class="pl-c1">Document</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">9</span>), vocab<span class="pl-k">=</span><span class="pl-c1">split</span>(<span class="pl-s"><span class="pl-pds">"</span>the quick brown fox jumped over the lazy dog<span class="pl-pds">"</span></span>))
<span class="pl-c1">showdocs</span>(corp)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● Document 1
jumped fox over the quick dog lazy brown the
"><pre><code> ●●● Document 1
jumped fox over the quick dog lazy brown the
</code></pre></div>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="fixcorp!(corp, condense=true)
showdocs(corp)
"><pre><span class="pl-c1">fixcorp!</span>(corp, condense<span class="pl-k">=</span><span class="pl-c1">true</span>)
<span class="pl-c1">showdocs</span>(corp)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● Document 1
the fox dog quick brown jumped lazy over the
"><pre><code> ●●● Document 1
the fox dog quick brown jumped lazy over the
</code></pre></div>
<p><strong>Important.</strong> A corpus is only a container for documents.</p>
<p>Whenever you load a corpus into a model, a copy of that corpus is made, such that if you modify the original corpus at corpus-level (remove documents, re-order vocab keys, etc.), this will not affect any corpus attached to a model. However! Since corpora are containers for their documents, modifying an individual document will affect it in all corpora which contain it. Therefore,</p>
<ol>
<li>
<p>Using <code>fixcorp!</code> to modify the documents of a corpus will not result in corpus defects, but will cause them also to be changed in all other corpora which contain them.</p>
</li>
<li>
<p>If you would like to make a copy of a corpus with independent documents, use <code>deepcopy(corp)</code>.</p>
</li>
<li>
<p>Manually modifying documents is dangerous, and can result in corpus defects which cannot be fixed by <code>fixcorp!</code>. It is advised that you don't do this without good reason.</p>
</li>
</ol>
<h2><a id="user-content-models" class="anchor" aria-hidden="true" href="#models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Models</h2>
<p>The available models are as follows:</p>
<h3><a id="user-content-cpu-models" class="anchor" aria-hidden="true" href="#cpu-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CPU Models</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="LDA(corp, K)
Latent Dirichlet allocation model with K topics.

fLDA(corp, K)
Filtered latent Dirichlet allocation model with K topics.

CTM(corp, K)
Correlated topic model with K topics.

fCTM(corp, K)
Filtered correlated topic model with K topics.

CTPF(corp, K)
Collaborative topic Poisson factorization model with K topics.
"><pre><span class="pl-c1">LDA</span>(corp, K)
Latent Dirichlet allocation model with K topics.

<span class="pl-c1">fLDA</span>(corp, K)
Filtered latent Dirichlet allocation model with K topics.

<span class="pl-c1">CTM</span>(corp, K)
Correlated topic model with K topics.

<span class="pl-c1">fCTM</span>(corp, K)
Filtered correlated topic model with K topics.

<span class="pl-c1">CTPF</span>(corp, K)
Collaborative topic Poisson factorization model with K topics.</pre></div>
<h3><a id="user-content-gpu-models" class="anchor" aria-hidden="true" href="#gpu-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPU Models</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="gpuLDA(corp, K)
GPU accelerated latent Dirichlet allocation model with K topics.

gpuCTM(corp, K)
GPU accelerated correlated topic model with K topics.

gpuCTPF(corp, K)
GPU accelerated collaborative topic Poisson factorization model with K topics.
"><pre><span class="pl-c1">gpuLDA</span>(corp, K)
GPU accelerated latent Dirichlet allocation model with K topics.

<span class="pl-c1">gpuCTM</span>(corp, K)
GPU accelerated correlated topic model with K topics.

<span class="pl-c1">gpuCTPF</span>(corp, K)
GPU accelerated collaborative topic Poisson factorization model with K topics.</pre></div>
<h2><a id="user-content-tutorial" class="anchor" aria-hidden="true" href="#tutorial"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorial</h2>
<h3><a id="user-content-latent-dirichlet-allocation" class="anchor" aria-hidden="true" href="#latent-dirichlet-allocation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Latent Dirichlet Allocation</h3>
<p>Let's begin our tutorial with a simple latent Dirichlet allocation (LDA) model with 9 topics, trained on the first 5000 documents from the NSF corpus.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using TopicModelsVB
using Random
using Distributions

Random.seed!(10);

corp = readcorp(:nsf) 

corp.docs = corp[1:5000];
fixcorp!(corp, trim=true)
### It's strongly recommended that you trim your corpus when reducing its size in order to remove excess vocabulary. 

### Notice that the post-fix vocabulary is smaller after removing all but the first 5000 docs.

model = LDA(corp, 9)

train!(model, iter=150, tol=0)
### Setting tol=0 will ensure that all 150 iterations are completed.
### If you don't want to compute the ∆elbo, set checkelbo=Inf.

### training...

showtopics(model, cols=9, 20)
"><pre><span class="pl-k">using</span> TopicModelsVB
<span class="pl-k">using</span> Random
<span class="pl-k">using</span> Distributions

Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">10</span>);

corp <span class="pl-k">=</span> <span class="pl-c1">readcorp</span>(<span class="pl-c1">:nsf</span>) 

corp<span class="pl-k">.</span>docs <span class="pl-k">=</span> corp[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5000</span>];
<span class="pl-c1">fixcorp!</span>(corp, trim<span class="pl-k">=</span><span class="pl-c1">true</span>)
<span class="pl-c"><span class="pl-c">#</span>## It's strongly recommended that you trim your corpus when reducing its size in order to remove excess vocabulary. </span>

<span class="pl-c"><span class="pl-c">#</span>## Notice that the post-fix vocabulary is smaller after removing all but the first 5000 docs.</span>

model <span class="pl-k">=</span> <span class="pl-c1">LDA</span>(corp, <span class="pl-c1">9</span>)

<span class="pl-c1">train!</span>(model, iter<span class="pl-k">=</span><span class="pl-c1">150</span>, tol<span class="pl-k">=</span><span class="pl-c1">0</span>)
<span class="pl-c"><span class="pl-c">#</span>## Setting tol=0 will ensure that all 150 iterations are completed.</span>
<span class="pl-c"><span class="pl-c">#</span>## If you don't want to compute the ∆elbo, set checkelbo=Inf.</span>

<span class="pl-c"><span class="pl-c">#</span>## training...</span>

<span class="pl-c1">showtopics</span>(model, cols<span class="pl-k">=</span><span class="pl-c1">9</span>, <span class="pl-c1">20</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="topic 1          topic 2        topic 3        topic 4          topic 5          topic 6        topic 7          topic 8          topic 9
plant            research       models         research         data             research       research         research         theory
cell             chemistry      research       project          research         system         dr               students         problems
protein          study          study          study            species          systems        university       science          study
cells            high           data           data             study            design         support          program          research
genetic          chemical       model          social           project          data           award            university       equations
gene             studies        numerical      theory           important        project        program          conference       work
molecular        surface        theoretical    economic         provide          earthquake     sciences         support          geometry
studies          materials      methods        understanding    studies          performance    project          scientists       project
proteins         metal          problems       important        time             control        months           provide          groups
dna              reactions      theory         work             field            based          mathematical     engineering      algebraic
plants           properties     physics        information      ocean            computer       professor        workshop         differential
genes            organic        work           development      water            analysis       year             faculty          investigator
research         program        systems        policy           analysis         algorithms     science          graduate         space
study            electron       flow           models           understanding    parallel       equipment        national         principal
specific         phase          analysis       behavior         determine        developed      scientists       scientific       mathematical
system           structure      time           provide          results          techniques     institute        international    systems
important        temperature    processes      analysis         climate          information    scientific       undergraduate    analysis
function         molecular      solar          political        patterns         time           collaboration    held             spaces
understanding    systems        large          model            large            network        projects         projects         problem
development      project        project        public           processes        structures     national         project          solutions
"><pre><code>topic 1          topic 2        topic 3        topic 4          topic 5          topic 6        topic 7          topic 8          topic 9
plant            research       models         research         data             research       research         research         theory
cell             chemistry      research       project          research         system         dr               students         problems
protein          study          study          study            species          systems        university       science          study
cells            high           data           data             study            design         support          program          research
genetic          chemical       model          social           project          data           award            university       equations
gene             studies        numerical      theory           important        project        program          conference       work
molecular        surface        theoretical    economic         provide          earthquake     sciences         support          geometry
studies          materials      methods        understanding    studies          performance    project          scientists       project
proteins         metal          problems       important        time             control        months           provide          groups
dna              reactions      theory         work             field            based          mathematical     engineering      algebraic
plants           properties     physics        information      ocean            computer       professor        workshop         differential
genes            organic        work           development      water            analysis       year             faculty          investigator
research         program        systems        policy           analysis         algorithms     science          graduate         space
study            electron       flow           models           understanding    parallel       equipment        national         principal
specific         phase          analysis       behavior         determine        developed      scientists       scientific       mathematical
system           structure      time           provide          results          techniques     institute        international    systems
important        temperature    processes      analysis         climate          information    scientific       undergraduate    analysis
function         molecular      solar          political        patterns         time           collaboration    held             spaces
understanding    systems        large          model            large            network        projects         projects         problem
development      project        project        public           processes        structures     national         project          solutions
</code></pre></div>
<p>If you are interested in the raw topic distributions. For LDA and CTM models, you may access them via the matrix,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model.beta
### K x V matrix
### K = number of topics.
### V = number of vocabulary terms, ordered identically to the keys in model.corp.vocab.
"><pre>model<span class="pl-k">.</span>beta
<span class="pl-c"><span class="pl-c">#</span>## K x V matrix</span>
<span class="pl-c"><span class="pl-c">#</span>## K = number of topics.</span>
<span class="pl-c"><span class="pl-c">#</span>## V = number of vocabulary terms, ordered identically to the keys in model.corp.vocab.</span></pre></div>
<p>Now that we've trained our LDA model we can, if we want, take a look at the topic proportions for individual documents.</p>
<p>For instance, document 1 has topic breakdown,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="println(round.(topicdist(model, 1), digits=3))
### = [0.161, 0.0, 0.0, 0.063, 0.774, 0.0, 0.0, 0.0, 0.0]
"><pre><span class="pl-c1">println</span>(<span class="pl-c1">round</span>.(<span class="pl-c1">topicdist</span>(model, <span class="pl-c1">1</span>), digits<span class="pl-k">=</span><span class="pl-c1">3</span>))
<span class="pl-c"><span class="pl-c">#</span>## = [0.161, 0.0, 0.0, 0.063, 0.774, 0.0, 0.0, 0.0, 0.0]</span></pre></div>
<p>This vector of topic weights suggests that document 1 is mostly about biology, and in fact looking at the document text confirms this observation,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="showdocs(model, 1)
### Could also have done showdocs(corp, 1).
"><pre><span class="pl-c1">showdocs</span>(model, <span class="pl-c1">1</span>)
<span class="pl-c"><span class="pl-c">#</span>## Could also have done showdocs(corp, 1).</span></pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● Document 1
 ●●● CRB: Genetic Diversity of Endangered Populations of Mysticete Whales: Mitochondrial DNA and Historical Demography
commercial exploitation past hundred years great extinction variation sizes
populations prior minimal population size current permit analyses effects 
differing levels species distributions life history...
"><pre><code> ●●● Document 1
 ●●● CRB: Genetic Diversity of Endangered Populations of Mysticete Whales: Mitochondrial DNA and Historical Demography
commercial exploitation past hundred years great extinction variation sizes
populations prior minimal population size current permit analyses effects 
differing levels species distributions life history...
</code></pre></div>
<p>Just for fun, let's consider one more document (document 25),</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="println(round.(topicdist(model, 25), digits=3))
### = [0.0, 0.0, 0.583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.415]

showdocs(model, 25)
"><pre><span class="pl-c1">println</span>(<span class="pl-c1">round</span>.(<span class="pl-c1">topicdist</span>(model, <span class="pl-c1">25</span>), digits<span class="pl-k">=</span><span class="pl-c1">3</span>))
<span class="pl-c"><span class="pl-c">#</span>## = [0.0, 0.0, 0.583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.415]</span>

<span class="pl-c1">showdocs</span>(model, <span class="pl-c1">25</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● Document 25
 ●●● Mathematical Sciences: Nonlinear Partial Differential Equations from Hydrodynamics
work project continues mathematical research nonlinear elliptic problems arising perfect
fluid hydrodynamics emphasis analytical study propagation waves stratified media techniques
analysis partial differential equations form basis studies primary goals understand nature 
internal presence vortex rings arise density stratification due salinity temperature...
"><pre><code> ●●● Document 25
 ●●● Mathematical Sciences: Nonlinear Partial Differential Equations from Hydrodynamics
work project continues mathematical research nonlinear elliptic problems arising perfect
fluid hydrodynamics emphasis analytical study propagation waves stratified media techniques
analysis partial differential equations form basis studies primary goals understand nature 
internal presence vortex rings arise density stratification due salinity temperature...
</code></pre></div>
<p>We see that in this case document 25 appears to be about mathematical physics, which corresponds precisely to topics 3 and 9.</p>
<p>Furthermore, if we want to, we can also generate artificial corpora by using the <code>gencorp</code> function.</p>
<p>Generating artificial corpora will in turn run the underlying probabilistic graphical model as a generative process in order to produce entirely new collections of documents, let's try it out,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Random.seed!(10);

artificial_corp = gencorp(model, 5000, laplace_smooth=1e-5)
### The laplace_smooth argument governs the amount of Laplace smoothing (defaults to 0).

artificial_model = LDA(artificial_corp, 9)
train!(artificial_model, iter=150, tol=0, checkelbo=10)

### training...

showtopics(artificial_model, cols=9)
"><pre>Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">10</span>);

artificial_corp <span class="pl-k">=</span> <span class="pl-c1">gencorp</span>(model, <span class="pl-c1">5000</span>, laplace_smooth<span class="pl-k">=</span><span class="pl-c1">1e-5</span>)
<span class="pl-c"><span class="pl-c">#</span>## The laplace_smooth argument governs the amount of Laplace smoothing (defaults to 0).</span>

artificial_model <span class="pl-k">=</span> <span class="pl-c1">LDA</span>(artificial_corp, <span class="pl-c1">9</span>)
<span class="pl-c1">train!</span>(artificial_model, iter<span class="pl-k">=</span><span class="pl-c1">150</span>, tol<span class="pl-k">=</span><span class="pl-c1">0</span>, checkelbo<span class="pl-k">=</span><span class="pl-c1">10</span>)

<span class="pl-c"><span class="pl-c">#</span>## training...</span>

<span class="pl-c1">showtopics</span>(artificial_model, cols<span class="pl-k">=</span><span class="pl-c1">9</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="topic 1        topic 2        topic 3       topic 4      topic 5        topic 6         topic 7      topic 8        topic 9
research       models         research      protein      research       research        data         research       theory
project        study          study         plant        system         dr              research     students       problems
data           research       chemistry     cell         systems        university      species      program        study
system         data           surface       cells        design         support         project      science        equations
design         methods        high          dna          data           award           study        conference     research
systems        theoretical    materials     genetic      earthquake     program         provide      university     work
study          model          chemical      gene         project        project         time         support        project
information    numerical      metal         proteins     program        sciences        important    scientists     groups
earthquake     problems       electron      molecular    developed      months          studies      engineering    geometry
theory         theory         studies       plants       control        mathematical    analysis     provide        differential
models         physics        properties    studies      based          professor       processes    workshop       algebraic
analysis       analysis       organic       research     techniques     equipment       climate      faculty        investigator
control        work           program       genes        performance    science         results      graduate       mathematical
work           systems        reactions     important    time           scientists      field        national       systems
performance    flow           phase         system       high           year            water        scientific     principal
"><pre><code>topic 1        topic 2        topic 3       topic 4      topic 5        topic 6         topic 7      topic 8        topic 9
research       models         research      protein      research       research        data         research       theory
project        study          study         plant        system         dr              research     students       problems
data           research       chemistry     cell         systems        university      species      program        study
system         data           surface       cells        design         support         project      science        equations
design         methods        high          dna          data           award           study        conference     research
systems        theoretical    materials     genetic      earthquake     program         provide      university     work
study          model          chemical      gene         project        project         time         support        project
information    numerical      metal         proteins     program        sciences        important    scientists     groups
earthquake     problems       electron      molecular    developed      months          studies      engineering    geometry
theory         theory         studies       plants       control        mathematical    analysis     provide        differential
models         physics        properties    studies      based          professor       processes    workshop       algebraic
analysis       analysis       organic       research     techniques     equipment       climate      faculty        investigator
control        work           program       genes        performance    science         results      graduate       mathematical
work           systems        reactions     important    time           scientists      field        national       systems
performance    flow           phase         system       high           year            water        scientific     principal
</code></pre></div>
<h3><a id="user-content-correlated-topic-model" class="anchor" aria-hidden="true" href="#correlated-topic-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Correlated Topic Model</h3>
<p>For our next model, let's upgrade to a (filtered) correlated topic model (fCTM).</p>
<p>Filtering the correlated topic model will dynamically identify and suppress stop words which would otherwise clutter up the topic distribution output.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Random.seed!(10);

model = fCTM(corp, 9)
train!(model, tol=0, checkelbo=Inf)

### training...

showtopics(model, 20, cols=9)
"><pre>Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">10</span>);

model <span class="pl-k">=</span> <span class="pl-c1">fCTM</span>(corp, <span class="pl-c1">9</span>)
<span class="pl-c1">train!</span>(model, tol<span class="pl-k">=</span><span class="pl-c1">0</span>, checkelbo<span class="pl-k">=</span><span class="pl-c1">Inf</span>)

<span class="pl-c"><span class="pl-c">#</span>## training...</span>

<span class="pl-c1">showtopics</span>(model, <span class="pl-c1">20</span>, cols<span class="pl-k">=</span><span class="pl-c1">9</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="topic 1         topic 2         topic 3        topic 4          topic 5        topic 6          topic 7         topic 8         topic 9
design          materials       economic       species          earthquake     students         chemistry       theory          cell
system          flow            social         ocean            data           university       reactions       problems        protein
systems         temperature     theory         populations      seismic        science          university      equations       cells
algorithms      surface         policy         water            soil           support          metal           geometry        gene
parallel        phase           political      data             damage         program          organic         investigator    plant
performance     high            public         climate          university     scientists       molecular       algebraic       proteins
based           optical         decision       marine           stars          sciences         chemical        groups          genes
networks        laser           labor          sea              buildings      conference       compounds       principal       dna
network         properties      market         plant            ground         scientific       molecules       mathematical    molecular
control         liquid          data           population       response       national         professor       differential    plants
computer        measurements    children       patterns         solar          year             reaction        space           genetic
processing      experimental    science        evolutionary     equipment      engineering      synthesis       problem         regulation
problems        heat            change         plants           nsf            faculty          program         solutions       expression
software        growth          people         genetic          national       workshop         electron        mathematics     growth
programming     electron        women          north            california     mathematical     complexes       spaces          specific
distributed     films           human          pacific          san            months           department      nonlinear       function
neural          gas             factors        change           program        graduate         energy          finite          binding
applications    fluid           groups         samples          hazard         projects         species         manifolds       cellular
efficient       quantum         individuals    environmental    earthquakes    academic         spectroscopy    functions       membrane
problem         solid           case           history          october        international    carbon          dimensional     sequence
"><pre><code>topic 1         topic 2         topic 3        topic 4          topic 5        topic 6          topic 7         topic 8         topic 9
design          materials       economic       species          earthquake     students         chemistry       theory          cell
system          flow            social         ocean            data           university       reactions       problems        protein
systems         temperature     theory         populations      seismic        science          university      equations       cells
algorithms      surface         policy         water            soil           support          metal           geometry        gene
parallel        phase           political      data             damage         program          organic         investigator    plant
performance     high            public         climate          university     scientists       molecular       algebraic       proteins
based           optical         decision       marine           stars          sciences         chemical        groups          genes
networks        laser           labor          sea              buildings      conference       compounds       principal       dna
network         properties      market         plant            ground         scientific       molecules       mathematical    molecular
control         liquid          data           population       response       national         professor       differential    plants
computer        measurements    children       patterns         solar          year             reaction        space           genetic
processing      experimental    science        evolutionary     equipment      engineering      synthesis       problem         regulation
problems        heat            change         plants           nsf            faculty          program         solutions       expression
software        growth          people         genetic          national       workshop         electron        mathematics     growth
programming     electron        women          north            california     mathematical     complexes       spaces          specific
distributed     films           human          pacific          san            months           department      nonlinear       function
neural          gas             factors        change           program        graduate         energy          finite          binding
applications    fluid           groups         samples          hazard         projects         species         manifolds       cellular
efficient       quantum         individuals    environmental    earthquakes    academic         spectroscopy    functions       membrane
problem         solid           case           history          october        international    carbon          dimensional     sequence
</code></pre></div>
<p>Based on the top 20 terms in each topic, we might tentatively assign the following topic labels:</p>
<ul>
<li>topic 1: <em>Computer Science</em></li>
<li>topic 2: <em>Physics</em></li>
<li>topic 3: <em>Economics</em></li>
<li>topic 4: <em>Ecology</em></li>
<li>topic 5: <em>Earthquakes</em></li>
<li>topic 6: <em>Academia</em></li>
<li>topic 7: <em>Chemistry</em></li>
<li>topic 8: <em>Mathematics</em></li>
<li>topic 9: <em>Molecular Biology</em></li>
</ul>
<p>Now let's take a look at the topic-covariance matrix,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model.sigma

### Top two off-diagonal positive entries:
model.sigma[4,9] # = 11.219
model.sigma[1,8] # = 4.639

### Top two negative entries:
model.sigma[4,8] # = -34.815
model.sigma[8,9] # = -13.546
"><pre>model<span class="pl-k">.</span>sigma

<span class="pl-c"><span class="pl-c">#</span>## Top two off-diagonal positive entries:</span>
model<span class="pl-k">.</span>sigma[<span class="pl-c1">4</span>,<span class="pl-c1">9</span>] <span class="pl-c"><span class="pl-c">#</span> = 11.219</span>
model<span class="pl-k">.</span>sigma[<span class="pl-c1">1</span>,<span class="pl-c1">8</span>] <span class="pl-c"><span class="pl-c">#</span> = 4.639</span>

<span class="pl-c"><span class="pl-c">#</span>## Top two negative entries:</span>
model<span class="pl-k">.</span>sigma[<span class="pl-c1">4</span>,<span class="pl-c1">8</span>] <span class="pl-c"><span class="pl-c">#</span> = -34.815</span>
model<span class="pl-k">.</span>sigma[<span class="pl-c1">8</span>,<span class="pl-c1">9</span>] <span class="pl-c"><span class="pl-c">#</span> = -13.546</span></pre></div>
<p>According to the list above, the most closely related topics are topics 4 and 9, which correspond to the <em>Ecology</em> and <em>Molecular Biology</em> topics, followed by 1 and 8, corresponding to <em>Computer Science</em> and <em>Mathematics</em>.</p>
<p>As for the most unlikely topic pairings, most strongly negatively correlated are topics 4 and 8, corresponding to <em>Ecology</em> and <em>Mathematics</em>, followed by topics 8 and 9, corresponding to <em>Mathematics</em> and <em>Molecular Biology</em>.</p>
<h3><a id="user-content-topic-prediction" class="anchor" aria-hidden="true" href="#topic-prediction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Topic Prediction</h3>
<p>The topic models so far discussed can also be used to train a classification algorithm designed to predict the topic distribution of new, unseen documents.</p>
<p>Let's take our 5,000 document NSF corpus, and partition it into training and test corpora,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="train_corp = copy(corp)
train_corp.docs = train_corp[1:4995];

test_corp = copy(corp)
test_corp.docs = test_corp[4996:5000];
"><pre>train_corp <span class="pl-k">=</span> <span class="pl-c1">copy</span>(corp)
train_corp<span class="pl-k">.</span>docs <span class="pl-k">=</span> train_corp[<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">4995</span>];

test_corp <span class="pl-k">=</span> <span class="pl-c1">copy</span>(corp)
test_corp<span class="pl-k">.</span>docs <span class="pl-k">=</span> test_corp[<span class="pl-c1">4996</span><span class="pl-k">:</span><span class="pl-c1">5000</span>];</pre></div>
<p>Now we can train our LDA model on just the training corpus, and then use that trained model to predict the topic distributions of the five documents in our test corpus,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Random.seed!(10);

train_model = LDA(train_corp, 9)
train!(train_model, checkelbo=Inf)

test_model = predict(test_corp, train_model)
"><pre>Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">10</span>);

train_model <span class="pl-k">=</span> <span class="pl-c1">LDA</span>(train_corp, <span class="pl-c1">9</span>)
<span class="pl-c1">train!</span>(train_model, checkelbo<span class="pl-k">=</span><span class="pl-c1">Inf</span>)

test_model <span class="pl-k">=</span> <span class="pl-c1">predict</span>(test_corp, train_model)</pre></div>
<p>The <code>predict</code> function works by taking in a corpus of new, unseen documents, and a trained model, and returning a new model of the same type. This new model can then be inspected directly, or using <code>topicdist</code>, in order to see the topic distribution for the documents in the test corpus.</p>
<p>Let's first take a look at both the topics for the trained model and the documents in our test corpus,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="showtopics(train_model, cols=9, 20)
"><pre><span class="pl-c1">showtopics</span>(train_model, cols<span class="pl-k">=</span><span class="pl-c1">9</span>, <span class="pl-c1">20</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="topic 1          topic 2         topic 3        topic 4          topic 5          topic 6        topic 7          topic 8          topic 9
plant            research        models         research         data             research       research         research         theory
cell             chemistry       research       project          research         system         dr               students         problems
protein          study           study          study            species          systems        university       science          study
cells            high            data           data             study            design         support          program          research
genetic          chemical        model          theory           project          data           award            university       equations
gene             studies         numerical      social           important        project        program          conference       work
molecular        surface         methods        economic         provide          earthquake     sciences         support          geometry
studies          materials       theoretical    understanding    studies          performance    project          scientists       groups
proteins         metal           problems       important        time             control        months           provide          algebraic
dna              reactions       theory         work             field            based          mathematical     engineering      project
plants           properties      work           information      ocean            computer       professor        workshop         differential
genes            organic         physics        development      analysis         analysis       year             faculty          investigator
research         program         systems        policy           water            algorithms     science          graduate         space
study            electron        analysis       models           understanding    parallel       equipment        national         mathematical
specific         phase           flow           provide          determine        information    scientists       scientific       principal
system           structure       time           behavior         results          techniques     institute        international    systems
important        temperature     large          analysis         climate          developed      scientific       undergraduate    spaces
function         molecular       processes      political        patterns         time           collaboration    held             analysis
understanding    systems         solar          model            large            network        projects         project          solutions
development      measurements    project        public           processes        structures     national         projects         mathematics
"><pre><code>topic 1          topic 2         topic 3        topic 4          topic 5          topic 6        topic 7          topic 8          topic 9
plant            research        models         research         data             research       research         research         theory
cell             chemistry       research       project          research         system         dr               students         problems
protein          study           study          study            species          systems        university       science          study
cells            high            data           data             study            design         support          program          research
genetic          chemical        model          theory           project          data           award            university       equations
gene             studies         numerical      social           important        project        program          conference       work
molecular        surface         methods        economic         provide          earthquake     sciences         support          geometry
studies          materials       theoretical    understanding    studies          performance    project          scientists       groups
proteins         metal           problems       important        time             control        months           provide          algebraic
dna              reactions       theory         work             field            based          mathematical     engineering      project
plants           properties      work           information      ocean            computer       professor        workshop         differential
genes            organic         physics        development      analysis         analysis       year             faculty          investigator
research         program         systems        policy           water            algorithms     science          graduate         space
study            electron        analysis       models           understanding    parallel       equipment        national         mathematical
specific         phase           flow           provide          determine        information    scientists       scientific       principal
system           structure       time           behavior         results          techniques     institute        international    systems
important        temperature     large          analysis         climate          developed      scientific       undergraduate    spaces
function         molecular       processes      political        patterns         time           collaboration    held             analysis
understanding    systems         solar          model            large            network        projects         project          solutions
development      measurements    project        public           processes        structures     national         projects         mathematics
</code></pre></div>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="showtitles(corp, 4996:5000)
"><pre><span class="pl-c1">showtitles</span>(corp, <span class="pl-c1">4996</span><span class="pl-k">:</span><span class="pl-c1">5000</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" • Document 4996 Decision-Making, Modeling and Forecasting Hydrometeorologic Extremes Under Climate Change
 • Document 4997 Mathematical Sciences: Representation Theory Conference, September 13-15, 1991, Eugene, Oregon
 • Document 4998 Irregularity Modeling &amp; Plasma Line Studies at High Latitudes
 • Document 4999 Uses and Simulation of Randomness: Applications to Cryptography,Program Checking and Counting Problems.
 • Document 5000 New Possibilities for Understanding the Role of Neuromelanin
"><pre><code> • Document 4996 Decision-Making, Modeling and Forecasting Hydrometeorologic Extremes Under Climate Change
 • Document 4997 Mathematical Sciences: Representation Theory Conference, September 13-15, 1991, Eugene, Oregon
 • Document 4998 Irregularity Modeling &amp; Plasma Line Studies at High Latitudes
 • Document 4999 Uses and Simulation of Randomness: Applications to Cryptography,Program Checking and Counting Problems.
 • Document 5000 New Possibilities for Understanding the Role of Neuromelanin
</code></pre></div>
<p>Now let's take a look at the predicted topic distributions for these five documents,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="for d in 1:5
    println(&quot;Document &quot;, 4995 + d, &quot;: &quot;, round.(topicdist(test_model, d), digits=3))
end
"><pre><span class="pl-k">for</span> d <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">5</span>
    <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>Document <span class="pl-pds">"</span></span>, <span class="pl-c1">4995</span> <span class="pl-k">+</span> d, <span class="pl-s"><span class="pl-pds">"</span>: <span class="pl-pds">"</span></span>, <span class="pl-c1">round</span>.(<span class="pl-c1">topicdist</span>(test_model, d), digits<span class="pl-k">=</span><span class="pl-c1">3</span>))
<span class="pl-k">end</span></pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Document 4996: [0.0, 0.001, 0.207, 0.188, 0.452, 0.151, 0.0, 0.0, 0.0]
Document 4997: [0.001, 0.026, 0.001, 0.043, 0.001, 0.001, 0.012, 0.386, 0.53]
Document 4998: [0.0, 0.019, 0.583, 0.0, 0.268, 0.122, 0.0, 0.007, 0.0]
Document 4999: [0.002, 0.002, 0.247, 0.037, 0.019, 0.227, 0.002, 0.026, 0.438]
Document 5000: [0.785, 0.178, 0.001, 0.0, 0.034, 0.001, 0.001, 0.001, 0.0]
"><pre><code>Document 4996: [0.0, 0.001, 0.207, 0.188, 0.452, 0.151, 0.0, 0.0, 0.0]
Document 4997: [0.001, 0.026, 0.001, 0.043, 0.001, 0.001, 0.012, 0.386, 0.53]
Document 4998: [0.0, 0.019, 0.583, 0.0, 0.268, 0.122, 0.0, 0.007, 0.0]
Document 4999: [0.002, 0.002, 0.247, 0.037, 0.019, 0.227, 0.002, 0.026, 0.438]
Document 5000: [0.785, 0.178, 0.001, 0.0, 0.034, 0.001, 0.001, 0.001, 0.0]
</code></pre></div>
<h3><a id="user-content-collaborative-topic-poisson-factorization" class="anchor" aria-hidden="true" href="#collaborative-topic-poisson-factorization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Collaborative Topic Poisson Factorization</h3>
<p>For our final model, we take a look at the collaborative topic Poisson factorization (CTPF) model.</p>
<p>CTPF is a collaborative filtering topic model which uses the latent thematic structure of documents to improve the quality of document recommendations beyond what would be possible using just the document-user matrix alone. This blending of thematic structure with known user prefrences not only improves recommendation accuracy, but also mitigates the cold-start problem of recommending to users never-before-seen documents. As an example, let's load the CiteULike dataset into a corpus and then randomly remove a single reader from each of the documents.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="Random.seed!(10);

corp = readcorp(:citeu)

ukeys_test = Int[];
for doc in corp
    index = sample(1:length(doc.readers), 1)[1]
    push!(ukeys_test, doc.readers[index])
    deleteat!(doc.readers, index)
    deleteat!(doc.ratings, index)
end
"><pre>Random<span class="pl-k">.</span><span class="pl-c1">seed!</span>(<span class="pl-c1">10</span>);

corp <span class="pl-k">=</span> <span class="pl-c1">readcorp</span>(<span class="pl-c1">:citeu</span>)

ukeys_test <span class="pl-k">=</span> Int[];
<span class="pl-k">for</span> doc <span class="pl-k">in</span> corp
    index <span class="pl-k">=</span> <span class="pl-c1">sample</span>(<span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">length</span>(doc<span class="pl-k">.</span>readers), <span class="pl-c1">1</span>)[<span class="pl-c1">1</span>]
    <span class="pl-c1">push!</span>(ukeys_test, doc<span class="pl-k">.</span>readers[index])
    <span class="pl-c1">deleteat!</span>(doc<span class="pl-k">.</span>readers, index)
    <span class="pl-c1">deleteat!</span>(doc<span class="pl-k">.</span>ratings, index)
<span class="pl-k">end</span></pre></div>
<p><strong>Important.</strong> We refrain from fixing our corpus in this case, first because the CiteULike dataset is pre-packaged and thus pre-fixed, but more importantly, because removing user keys from documents and then fixing a corpus may result in a re-ordering of its user dictionary, which would in turn invalidate our test set.</p>
<p>After training, we will evaluate model quality by measuring our model's success at imputing the correct user back into each of the document libraries.</p>
<p>It's also worth noting that after removing a single reader from each document, 158 of the documents now have 0 readers,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="sum([isempty(doc.readers) for doc in corp]) # = 158
"><pre><span class="pl-c1">sum</span>([<span class="pl-c1">isempty</span>(doc<span class="pl-k">.</span>readers) <span class="pl-k">for</span> doc <span class="pl-k">in</span> corp]) <span class="pl-c"><span class="pl-c">#</span> = 158</span></pre></div>
<p>Fortunately, since CTPF can if need be depend entirely on thematic structure when making recommendations, this poses no problem for the model.</p>
<p>Now that we've set up our experiment, let's instantiate and train a CTPF model on our corpus. Furthermore, in the interest of time, we'll also go ahead and GPU accelerate it.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model = gpuCTPF(corp, 100)
train!(model, iter=50, checkelbo=Inf)

### training...
"><pre>model <span class="pl-k">=</span> <span class="pl-c1">gpuCTPF</span>(corp, <span class="pl-c1">100</span>)
<span class="pl-c1">train!</span>(model, iter<span class="pl-k">=</span><span class="pl-c1">50</span>, checkelbo<span class="pl-k">=</span><span class="pl-c1">Inf</span>)

<span class="pl-c"><span class="pl-c">#</span>## training...</span></pre></div>
<p>Finally, we evaluate the performance of our model on the test set.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="ranks = Float64[];
for (d, u) in enumerate(ukeys_test)
    urank = findall(model.drecs[d] .== u)[1]
    nrlen = length(model.drecs[d])
    push!(ranks, (nrlen - urank) / (nrlen - 1))
end
"><pre>ranks <span class="pl-k">=</span> Float64[];
<span class="pl-k">for</span> (d, u) <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(ukeys_test)
    urank <span class="pl-k">=</span> <span class="pl-c1">findall</span>(model<span class="pl-k">.</span>drecs[d] <span class="pl-k">.==</span> u)[<span class="pl-c1">1</span>]
    nrlen <span class="pl-k">=</span> <span class="pl-c1">length</span>(model<span class="pl-k">.</span>drecs[d])
    <span class="pl-c1">push!</span>(ranks, (nrlen <span class="pl-k">-</span> urank) <span class="pl-k">/</span> (nrlen <span class="pl-k">-</span> <span class="pl-c1">1</span>))
<span class="pl-k">end</span></pre></div>
<p>The following histogram shows the proportional ranking of each test user within the list of recommendations for their corresponding document.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/ericproffitt/TopicModelsVB.jl/blob/master/images/ctpfbar.png"><img src="https://github.com/ericproffitt/TopicModelsVB.jl/raw/master/images/ctpfbar.png" alt="GPU Benchmark" style="max-width:100%;"></a></p>
<p>Let's also take a look at the top recommendations for a particular document,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="ukeys_test[1] # = 216
ranks[1] # = 0.922

showdrecs(model, 1, 434)
"><pre>ukeys_test[<span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span> = 216</span>
ranks[<span class="pl-c1">1</span>] <span class="pl-c"><span class="pl-c">#</span> = 0.922</span>

<span class="pl-c1">showdrecs</span>(model, <span class="pl-c1">1</span>, <span class="pl-c1">434</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● Document 1
 ●●● The metabolic world of Escherichia coli is not small
 ...
431. #user1647
432. #user1178
433. #user5315
434. #user216
"><pre><code> ●●● Document 1
 ●●● The metabolic world of Escherichia coli is not small
 ...
431. #user1647
432. #user1178
433. #user5315
434. #user216
</code></pre></div>
<p>What the above output tells us is that user 216's test document placed him or her in the top 8% (position 434) of all non-readers.</p>
<p>For evaluating our model's user recommendations, let's take a more holistic approach.</p>
<p>Since large heterogenous libraries make the qualitative assessment of recommendations difficult, let's search for a user with a small focused library,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="showlibs(model, 1741)
"><pre><span class="pl-c1">showlibs</span>(model, <span class="pl-c1">1741</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● User 1741
 • Region-Based Memory Management
 • A Syntactic Approach to Type Soundness
 • Imperative Functional Programming
 • The essence of functional programming
 • Representing monads
 • The marriage of effects and monads
 • A Taste of Linear Logic
 • Monad transformers and modular interpreters
 • Comprehending Monads
 • Monads for functional programming
 • Building interpreters by composing monads
 • Typed memory management via static capabilities
 • Computational Lambda-Calculus and Monads
 • Why functional programming matters
 • Tackling the Awkward Squad: monadic input/output, concurrency, exceptions, and foreign-language calls in Haskell
 • Notions of Computation and Monads
 • Recursion schemes from comonads
 • There and back again: arrows for invertible programming
 • Composing monads using coproducts
 • An Introduction to Category Theory, Category Theory Monads, and Their Relationship to Functional Programming
"><pre><code> ●●● User 1741
 • Region-Based Memory Management
 • A Syntactic Approach to Type Soundness
 • Imperative Functional Programming
 • The essence of functional programming
 • Representing monads
 • The marriage of effects and monads
 • A Taste of Linear Logic
 • Monad transformers and modular interpreters
 • Comprehending Monads
 • Monads for functional programming
 • Building interpreters by composing monads
 • Typed memory management via static capabilities
 • Computational Lambda-Calculus and Monads
 • Why functional programming matters
 • Tackling the Awkward Squad: monadic input/output, concurrency, exceptions, and foreign-language calls in Haskell
 • Notions of Computation and Monads
 • Recursion schemes from comonads
 • There and back again: arrows for invertible programming
 • Composing monads using coproducts
 • An Introduction to Category Theory, Category Theory Monads, and Their Relationship to Functional Programming
</code></pre></div>
<p>The 20 articles in user 1741's library suggest that he or she is interested in programming language theory.</p>
<p>Now compare this with the top 50 recommendations (the top 0.3%) made by our model,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="showurecs(model, 1741, 50)
"><pre><span class="pl-c1">showurecs</span>(model, <span class="pl-c1">1741</span>, <span class="pl-c1">50</span>)</pre></div>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content=" ●●● User 1741
1.  Sets for Mathematics
2.  Can programming be liberated from the von {N}eumann style? {A} functional style and its algebra of programs
3.  On Understanding Types, Data Abstraction, and Polymorphism
4.  Views: a way for pattern matching to cohabit with data abstraction
5.  Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I
6.  Functional pearl: implicit configurations--or, type classes reflect the values of types
7.  Semantic Structures
8.  Haskell's overlooked object system
9.  Discriminative Reranking for Natural Language Parsing
10. Discrimination of non-native consonant contrasts varying in perceptual assimilation to the listener's native phonological system.
11. Dynamic optimization for functional reactive programming using generalized algebraic data types
12. Language identification in the limit
13. Modern {C}ompiler {I}mplementation in {J}ava
14. Contracts for higher-order functions
15. The faculty of language: what's special about it?
16. The motor theory of speech perception revised
17. Visual Programming
18. Functional programming with bananas, lenses, envelopes and barbed wire
19. Why Dependent Types Matter
20. Featherweight Java: A Minimal Core Calculus for Java and GJ
21. The dual of substitution is redecoration
22. Dynamic Logic
23. On the expressive power of programming languages
24. Principles of programming with complex objects and collection types
25. Dependent Types in Practical Programming
26. The Zipper
27. Restrictions on biological adaptation in language evolution.
28. The essence of compiling with continuations
29. Recursive syntactic pattern learning by songbirds
30. The effects of common ground and perspective on domains of referential interpretation
31. A {S}yntactic {T}heory of {D}ynamic {B}inding
32. Parsing expression grammars: a recognition-based syntactic foundation
33. Packrat Parsing: Simple, Powerful, Lazy, Linear Time
34. Foundations for structured programming with GADTs
35. Type Classes with Functional Dependencies
36. Attention, Intentions, and the Structure of Discourse
37. The TRACE model of speech perception.
38. Types and programming languages
39. Adaptive Functional Programming
40. Neuromimetic Semantics
41. Macros as multi-stage computations: type-safe, generative, binding macros in MacroML
42. The Java Memory Model
43. Types, abstraction and parametric polymorphism
44. Learning, Bottlenecks and the Evolution of Recursive Syntax
45. Recognizing spoken words: the neighborhood activation model.
46. The categorical abstract machine
47. Monadic Parsing in Haskell
48. The neurology of syntax: Language use without Broca's area
49. A machine-oriented logic based on the resolution principle
50. The evolution of language
"><pre><code> ●●● User 1741
1.  Sets for Mathematics
2.  Can programming be liberated from the von {N}eumann style? {A} functional style and its algebra of programs
3.  On Understanding Types, Data Abstraction, and Polymorphism
4.  Views: a way for pattern matching to cohabit with data abstraction
5.  Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I
6.  Functional pearl: implicit configurations--or, type classes reflect the values of types
7.  Semantic Structures
8.  Haskell's overlooked object system
9.  Discriminative Reranking for Natural Language Parsing
10. Discrimination of non-native consonant contrasts varying in perceptual assimilation to the listener's native phonological system.
11. Dynamic optimization for functional reactive programming using generalized algebraic data types
12. Language identification in the limit
13. Modern {C}ompiler {I}mplementation in {J}ava
14. Contracts for higher-order functions
15. The faculty of language: what's special about it?
16. The motor theory of speech perception revised
17. Visual Programming
18. Functional programming with bananas, lenses, envelopes and barbed wire
19. Why Dependent Types Matter
20. Featherweight Java: A Minimal Core Calculus for Java and GJ
21. The dual of substitution is redecoration
22. Dynamic Logic
23. On the expressive power of programming languages
24. Principles of programming with complex objects and collection types
25. Dependent Types in Practical Programming
26. The Zipper
27. Restrictions on biological adaptation in language evolution.
28. The essence of compiling with continuations
29. Recursive syntactic pattern learning by songbirds
30. The effects of common ground and perspective on domains of referential interpretation
31. A {S}yntactic {T}heory of {D}ynamic {B}inding
32. Parsing expression grammars: a recognition-based syntactic foundation
33. Packrat Parsing: Simple, Powerful, Lazy, Linear Time
34. Foundations for structured programming with GADTs
35. Type Classes with Functional Dependencies
36. Attention, Intentions, and the Structure of Discourse
37. The TRACE model of speech perception.
38. Types and programming languages
39. Adaptive Functional Programming
40. Neuromimetic Semantics
41. Macros as multi-stage computations: type-safe, generative, binding macros in MacroML
42. The Java Memory Model
43. Types, abstraction and parametric polymorphism
44. Learning, Bottlenecks and the Evolution of Recursive Syntax
45. Recognizing spoken words: the neighborhood activation model.
46. The categorical abstract machine
47. Monadic Parsing in Haskell
48. The neurology of syntax: Language use without Broca's area
49. A machine-oriented logic based on the resolution principle
50. The evolution of language
</code></pre></div>
<p>For the CTPF models, you may access the raw topic distributions by computing,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model.alef ./ model.bet
"><pre>model<span class="pl-k">.</span>alef <span class="pl-k">./</span> model<span class="pl-k">.</span>bet</pre></div>
<p>Raw scores, as well as document and user recommendations, may be accessed via,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model.scores
### M x U matrix
### M = number of documents, ordered identically to the documents in model.corp.docs.
### U = number of users, ordered identically to the keys in model.corp.users.

model.drecs
model.urecs
"><pre>model<span class="pl-k">.</span>scores
<span class="pl-c"><span class="pl-c">#</span>## M x U matrix</span>
<span class="pl-c"><span class="pl-c">#</span>## M = number of documents, ordered identically to the documents in model.corp.docs.</span>
<span class="pl-c"><span class="pl-c">#</span>## U = number of users, ordered identically to the keys in model.corp.users.</span>

model<span class="pl-k">.</span>drecs
model<span class="pl-k">.</span>urecs</pre></div>
<p>Note, as was done by Blei et al. in their original paper, if you would like to warm start your CTPF model using the topic distributions generated by one of the other models, simply do the following prior to training your model,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="ctpf_model.alef = exp.(model.beta)
### For model of type: LDA, fLDA, CTM, fCTM, gpuLDA, gpuCTM.
"><pre>ctpf_model<span class="pl-k">.</span>alef <span class="pl-k">=</span> <span class="pl-c1">exp</span>.(model<span class="pl-k">.</span>beta)
<span class="pl-c"><span class="pl-c">#</span>## For model of type: LDA, fLDA, CTM, fCTM, gpuLDA, gpuCTM.</span></pre></div>
<h3><a id="user-content-gpu-acceleration" class="anchor" aria-hidden="true" href="#gpu-acceleration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPU Acceleration</h3>
<p>GPU accelerating your model runs its performance bottlenecks on the GPU.</p>
<p>There's no reason to instantiate GPU models directly, instead you can simply instantiate the normal version of a supported model, and then use the <code>@gpu</code> macro to train it on the GPU,</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="model = LDA(readcorp(:nsf), 20)
@gpu train!(model, checkelbo=Inf)

### training...
"><pre>model <span class="pl-k">=</span> <span class="pl-c1">LDA</span>(<span class="pl-c1">readcorp</span>(<span class="pl-c1">:nsf</span>), <span class="pl-c1">20</span>)
<span class="pl-c1">@gpu</span> <span class="pl-c1">train!</span>(model, checkelbo<span class="pl-k">=</span><span class="pl-c1">Inf</span>)

<span class="pl-c"><span class="pl-c">#</span>## training...</span></pre></div>
<p><strong>Important.</strong> Notice that we did not check the ELBO at all during training. While you may check the ELBO if you wish, it's recommended that you do so infrequently, as computing the ELBO is done entirely on the CPU.</p>
<p>Here are the log scale benchmarks of the coordinate ascent algorithms for the GPU models, compared against their CPU equivalents,</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/ericproffitt/TopicModelsVB.jl/blob/master/images/gpubar.png"><img src="https://github.com/ericproffitt/TopicModelsVB.jl/raw/master/images/gpubar.png" alt="GPU Benchmark" style="max-width:100%;"></a></p>
<p>As we can see, running your model on the GPU is significantly faster than running it on the CPU.</p>
<p>Note that it's expected that your computer will lag when training on the GPU, since you're effectively siphoning off its rendering resources to fit your model.</p>
<h2><a id="user-content-glossary" class="anchor" aria-hidden="true" href="#glossary"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Glossary</h2>
<h3><a id="user-content-types" class="anchor" aria-hidden="true" href="#types"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Types</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="mutable struct Document
	&quot;Document mutable struct&quot;

	&quot;terms:   A vector{Int} containing keys for the Corpus vocab Dict.&quot;
	&quot;counts:  A Vector{Int} denoting the counts of each term in the Document.&quot;
	&quot;readers: A Vector{Int} denoting the keys for the Corpus users Dict.&quot;
	&quot;ratings: A Vector{Int} denoting the ratings for each reader in the Document.&quot;
	&quot;title:   The title of the document (String).&quot;

	terms::Vector{Int}
	counts::Vector{Int}
	readers::Vector{Int}
	ratings::Vector{Int}
	title::String

mutable struct Corpus
	&quot;Corpus mutable struct.&quot;

	&quot;docs:  A Vector{Document} containing the documents which belong to the Corpus.&quot;
	&quot;vocab: A Dict{Int, String} containing a mapping term Int (key) =&gt; term String (value).&quot;
	&quot;users: A Dict{Int, String} containing a mapping user Int (key) =&gt; user String (value).&quot;

	docs::Vector{Document}
	vocab::Dict{Int, String}
	users::Dict{Int, String}

abstract type TopicModel end

mutable struct LDA &lt;: TopicModel
	&quot;LDA mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct fLDA &lt;: TopicModel
	&quot;fLDA mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct CTM &lt;: TopicModel
	&quot;CTM mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct fCTM &lt;: TopicModel
	&quot;fCTM mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct CTPF &lt;: TopicModel
	&quot;CTPF mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct gpuLDA &lt;: TopicModel
	&quot;gpuLDA mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct gpuCTM &lt;: TopicModel
	&quot;gpuCTM mutable struct.&quot;

	corpus::Corpus
	K::Int
	...

mutable struct gpuCTPF &lt;: TopicModel
	&quot;gpuCTPF mutable struct.&quot;

	corpus::Corpus
	K::Int
	...
"><pre><span class="pl-k">mutable struct</span> Document
	<span class="pl-s"><span class="pl-pds">"</span>Document mutable struct<span class="pl-pds">"</span></span>

	<span class="pl-s"><span class="pl-pds">"</span>terms:   A vector{Int} containing keys for the Corpus vocab Dict.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>counts:  A Vector{Int} denoting the counts of each term in the Document.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>readers: A Vector{Int} denoting the keys for the Corpus users Dict.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>ratings: A Vector{Int} denoting the ratings for each reader in the Document.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>title:   The title of the document (String).<span class="pl-pds">"</span></span>

	terms<span class="pl-k">::</span><span class="pl-c1">Vector{Int}</span>
	counts<span class="pl-k">::</span><span class="pl-c1">Vector{Int}</span>
	readers<span class="pl-k">::</span><span class="pl-c1">Vector{Int}</span>
	ratings<span class="pl-k">::</span><span class="pl-c1">Vector{Int}</span>
	title<span class="pl-k">::</span><span class="pl-c1">String</span>

<span class="pl-k">mutable struct</span> Corpus
	<span class="pl-s"><span class="pl-pds">"</span>Corpus mutable struct.<span class="pl-pds">"</span></span>

	<span class="pl-s"><span class="pl-pds">"</span>docs:  A Vector{Document} containing the documents which belong to the Corpus.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>vocab: A Dict{Int, String} containing a mapping term Int (key) =&gt; term String (value).<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>users: A Dict{Int, String} containing a mapping user Int (key) =&gt; user String (value).<span class="pl-pds">"</span></span>

	docs<span class="pl-k">::</span><span class="pl-c1">Vector{Document}</span>
	vocab<span class="pl-k">::</span><span class="pl-c1">Dict{Int, String}</span>
	users<span class="pl-k">::</span><span class="pl-c1">Dict{Int, String}</span>

<span class="pl-k">abstract type</span> TopicModel <span class="pl-k">end</span>

<span class="pl-k">mutable struct</span> LDA <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>LDA mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> fLDA <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>fLDA mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> CTM <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>CTM mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> fCTM <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>fCTM mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> CTPF <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>CTPF mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> gpuLDA <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>gpuLDA mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> gpuCTM <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>gpuCTM mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span>

<span class="pl-k">mutable struct</span> gpuCTPF <span class="pl-k">&lt;:</span> <span class="pl-c1">TopicModel</span>
	<span class="pl-s"><span class="pl-pds">"</span>gpuCTPF mutable struct.<span class="pl-pds">"</span></span>

	corpus<span class="pl-k">::</span><span class="pl-c1">Corpus</span>
	K<span class="pl-k">::</span><span class="pl-c1">Int</span>
	<span class="pl-k">...</span></pre></div>
<h3><a id="user-content-documentcorpus-functions" class="anchor" aria-hidden="true" href="#documentcorpus-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Document/Corpus Functions</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function check_doc(doc::Document)
	&quot;Check Document parameters.&quot;

function check_corp(corp::Corpus)
	&quot;Check Corpus parameters.&quot;

function readcorp(;docfile::String=&quot;&quot;, vocabfile::String=&quot;&quot;, userfile::String=&quot;&quot;, titlefile::String=&quot;&quot;, delim::Char=',', counts::Bool=false, readers::Bool=false, ratings::Bool=false)	
	&quot;Load a Corpus object from text file(s).&quot;

	### readcorp(:nsf)   	- National Science Foundation Corpus.
	### readcorp(:citeu)	- CiteULike Corpus.

function writecorp(corp::Corpus; docfile::String=&quot;&quot;, vocabfile::String=&quot;&quot;, userfile::String=&quot;&quot;, titlefile::String=&quot;&quot;, delim::Char=',', counts::Bool=false, readers::Bool=false, ratings::Bool=false)	
	&quot;Write a corpus.&quot;

function abridge_corp!(corp::Corpus, n::Integer=0)
	&quot;All terms which appear less than n times in the corpus are removed from all documents.&quot;

function alphabetize_corp!(corp::Corpus; vocab::Bool=true, users::Bool=true)
	&quot;Alphabetize vocab and/or user dictionaries.&quot;

function remove_terms!(corp::Corpus; terms::Vector{String}=[])
	&quot;Vocab keys for specified terms are removed from all documents.&quot;

function compact_corp!(corp::Corpus; vocab::Bool=true, users::Bool=true)
	&quot;Relabel vocab and/or user keys so that they form a unit range.&quot;

function condense_corp!(corp::Corpus)
	&quot;Ignore term order in documents.&quot;
	&quot;Multiple seperate occurrences of terms are stacked and their associated counts increased.&quot;

function pad_corp!(corp::Corpus; vocab::Bool=true, users::Bool=true)
	&quot;Enter generic values for vocab and/or user keys which appear in documents but not in the vocab/user dictionaries.&quot;

function remove_empty_docs!(corp::Corpus)
	&quot;Documents with no terms are removed from the corpus.&quot;

function remove_redundant!(corp::Corpus; vocab::Bool=true, users::Bool=true)
	&quot;Remove vocab and/or user keys which map to redundant values.&quot;
	&quot;Reassign Document term and/or reader keys.&quot;

function stop_corp!(corp::Corpus)
	&quot;Filter stop words in the associated corpus.&quot;

function trim_corp!(corp::Corpus; vocab::Bool=true, users::Bool=true)
	&quot;Those keys which appear in the corpus vocab and/or user dictionaries but not in any of the documents are removed from the corpus.&quot;

function trim_docs!(corp::Corpus; terms::Bool=true, readers::Bool=true)
	&quot;Those vocab and/or user keys which appear in documents but not in the corpus dictionaries are removed from the documents.&quot;

function fixcorp!(corp::Corpus; vocab::Bool=true, users::Bool=true, abridge::Integer=0, alphabetize::Bool=false, condense::Bool=false, pad::Bool=false, remove_empty_docs::Bool=false, remove_redundant::Bool=false, remove_terms::Vector{String}=String[], stop::Bool=false, trim::Bool=false)
	&quot;Generic function to ensure that a Corpus object can be loaded into a TopicModel object.&quot;
	&quot;Either pad_corp! or trim_docs!.&quot;
	&quot;compact_corp!.&quot;
	&quot;Contains other optional keyword arguments.&quot;

function showdocs(corp::Corpus, docs / doc_indices)
	&quot;Display document(s) in readable format.&quot;

function showtitles(corp::Corpus, docs / doc_indices)
	&quot;Display document title(s) in readable format.&quot;

function getvocab(corp::Corpus)

function getusers(corp::Corpus)
"><pre><span class="pl-k">function</span> <span class="pl-en">check_doc</span>(doc<span class="pl-k">::</span><span class="pl-c1">Document</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Check Document parameters.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">check_corp</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Check Corpus parameters.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">readcorp</span>(;docfile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, vocabfile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, userfile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, titlefile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, delim<span class="pl-k">::</span><span class="pl-c1">Char</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>, counts<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, readers<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, ratings<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>)	
	<span class="pl-s"><span class="pl-pds">"</span>Load a Corpus object from text file(s).<span class="pl-pds">"</span></span>

	<span class="pl-c"><span class="pl-c">#</span>## readcorp(:nsf)   	- National Science Foundation Corpus.</span>
	<span class="pl-c"><span class="pl-c">#</span>## readcorp(:citeu)	- CiteULike Corpus.</span>

<span class="pl-k">function</span> <span class="pl-en">writecorp</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; docfile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, vocabfile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, userfile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, titlefile<span class="pl-k">::</span><span class="pl-c1">String</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>, delim<span class="pl-k">::</span><span class="pl-c1">Char</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>,<span class="pl-pds">'</span></span>, counts<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, readers<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, ratings<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>)	
	<span class="pl-s"><span class="pl-pds">"</span>Write a corpus.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">abridge_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>, n<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">0</span>)
	<span class="pl-s"><span class="pl-pds">"</span>All terms which appear less than n times in the corpus are removed from all documents.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">alphabetize_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; vocab<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, users<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Alphabetize vocab and/or user dictionaries.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">remove_terms!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; terms<span class="pl-k">::</span><span class="pl-c1">Vector{String}</span><span class="pl-k">=</span>[])
	<span class="pl-s"><span class="pl-pds">"</span>Vocab keys for specified terms are removed from all documents.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">compact_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; vocab<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, users<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Relabel vocab and/or user keys so that they form a unit range.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">condense_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Ignore term order in documents.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>Multiple seperate occurrences of terms are stacked and their associated counts increased.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">pad_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; vocab<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, users<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Enter generic values for vocab and/or user keys which appear in documents but not in the vocab/user dictionaries.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">remove_empty_docs!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Documents with no terms are removed from the corpus.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">remove_redundant!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; vocab<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, users<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Remove vocab and/or user keys which map to redundant values.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>Reassign Document term and/or reader keys.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">stop_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Filter stop words in the associated corpus.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">trim_corp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; vocab<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, users<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Those keys which appear in the corpus vocab and/or user dictionaries but not in any of the documents are removed from the corpus.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">trim_docs!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; terms<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, readers<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Those vocab and/or user keys which appear in documents but not in the corpus dictionaries are removed from the documents.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">fixcorp!</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>; vocab<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, users<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, abridge<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">0</span>, alphabetize<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, condense<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, pad<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, remove_empty_docs<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, remove_redundant<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, remove_terms<span class="pl-k">::</span><span class="pl-c1">Vector{String}</span><span class="pl-k">=</span>String[], stop<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, trim<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Generic function to ensure that a Corpus object can be loaded into a TopicModel object.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>Either pad_corp! or trim_docs!.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>compact_corp!.<span class="pl-pds">"</span></span>
	<span class="pl-s"><span class="pl-pds">"</span>Contains other optional keyword arguments.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showdocs</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>, docs <span class="pl-k">/</span> doc_indices)
	<span class="pl-s"><span class="pl-pds">"</span>Display document(s) in readable format.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showtitles</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>, docs <span class="pl-k">/</span> doc_indices)
	<span class="pl-s"><span class="pl-pds">"</span>Display document title(s) in readable format.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">getvocab</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>)

<span class="pl-k">function</span> <span class="pl-en">getusers</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>)</pre></div>
<h3><a id="user-content-model-functions" class="anchor" aria-hidden="true" href="#model-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Model Functions</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="function showdocs(model::TopicModel, docs / doc_indices)
	&quot;Display document(s) in readable format.&quot;

function showtitles(model::TopicModel, docs / doc_indices)
	&quot;Display document title(s) in readable format.&quot;

function check_model(model::TopicModel)
	&quot;Check model parameters.&quot;

function train!(model::TopicModel; iter::Integer=150, tol::Real=1.0, niter::Integer=1000, ntol::Real=1/model.K^2, viter::Integer=10, vtol::Real=1/model.K^2, checkelbo::Union{Integer, Inf}=1, printelbo::Bool=true)
	&quot;Train TopicModel.&quot;

	### 'iter'	- maximum number of iterations through the corpus.
	### 'tol'	- absolute tolerance for ∆elbo as a stopping criterion.
	### 'niter'	- maximum number of iterations for Newton's and interior-point Newton's methods. (not included for CTPF and gpuCTPF models.)
	### 'ntol'	- tolerance for change in function value as a stopping criterion for Newton's and interior-point Newton's methods. (not included for CTPF and gpuCTPF models.)
	### 'viter'	- maximum number of iterations for optimizing variational parameters (at the document level).
	### 'vtol'	- tolerance for change in variational parameter values as stopping criterion.
	### 'checkelbo'	- number of iterations between ∆elbo checks (for both evaluation and convergence of the evidence lower-bound).
	### 'printelbo'	- if true, print ∆elbo to REPL.

@gpu train!
	&quot;Train model on GPU.&quot;

function gendoc(model::TopicModel, laplace_smooth::Real=0.0)
	&quot;Generate a generic document from model parameters by running the associated graphical model as a generative process.&quot;

function gencorp(model::TopicModel, M::Integer, laplace_smooth::Real=0.0)
	&quot;Generate a generic corpus of size M from model parameters.&quot;

function showtopics(model::TopicModel, V::Integer=15; topics::Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}=1:model.K, cols::Integer=4)
	&quot;Display the top V words for each topic in topics.&quot;

function showlibs(model::Union{CTPF, gpuCTPF}, users::Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}})
	&quot;Show the document(s) in a user's library.&quot;

function showdrecs(model::Union{CTPF, gpuCTPF}, docs::Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}, U::Integer=16; cols=4)
	&quot;Show the top U user recommendations for a document(s).&quot;

function showurecs(model::Union{CTPF, gpuCTPF}, users::Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}, M::Integer=10; cols=1)
	&quot;Show the top M document recommendations for a user(s).&quot;

function predict(corp::Corpus, train_model::Union{LDA, gpuLDA, fLDA, CTM, gpuCTM, fCTM}; iter::Integer=10, tol::Real=1/train_model.K^2, niter::Integer=1000, ntol::Real=1/train_model.K^2)
	&quot;Predict topic distributions for corpus of documents based on trained LDA or CTM model.&quot;

function topicdist(model::TopicModel, doc_indices::Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}})
	&quot;Get TopicModel topic distributions for document(s) as a probability vector.&quot;
"><pre><span class="pl-k">function</span> <span class="pl-en">showdocs</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>, docs <span class="pl-k">/</span> doc_indices)
	<span class="pl-s"><span class="pl-pds">"</span>Display document(s) in readable format.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showtitles</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>, docs <span class="pl-k">/</span> doc_indices)
	<span class="pl-s"><span class="pl-pds">"</span>Display document title(s) in readable format.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">check_model</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Check model parameters.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">train!</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>; iter<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">150</span>, tol<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">1.0</span>, niter<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">1000</span>, ntol<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">/</span>model<span class="pl-k">.</span>K<span class="pl-k">^</span><span class="pl-c1">2</span>, viter<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">10</span>, vtol<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">/</span>model<span class="pl-k">.</span>K<span class="pl-k">^</span><span class="pl-c1">2</span>, checkelbo<span class="pl-k">::</span><span class="pl-c1">Union{Integer, Inf}</span><span class="pl-k">=</span><span class="pl-c1">1</span>, printelbo<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Train TopicModel.<span class="pl-pds">"</span></span>

	<span class="pl-c"><span class="pl-c">#</span>## 'iter'	- maximum number of iterations through the corpus.</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'tol'	- absolute tolerance for ∆elbo as a stopping criterion.</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'niter'	- maximum number of iterations for Newton's and interior-point Newton's methods. (not included for CTPF and gpuCTPF models.)</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'ntol'	- tolerance for change in function value as a stopping criterion for Newton's and interior-point Newton's methods. (not included for CTPF and gpuCTPF models.)</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'viter'	- maximum number of iterations for optimizing variational parameters (at the document level).</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'vtol'	- tolerance for change in variational parameter values as stopping criterion.</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'checkelbo'	- number of iterations between ∆elbo checks (for both evaluation and convergence of the evidence lower-bound).</span>
	<span class="pl-c"><span class="pl-c">#</span>## 'printelbo'	- if true, print ∆elbo to REPL.</span>

<span class="pl-c1">@gpu</span> train!
	<span class="pl-s"><span class="pl-pds">"</span>Train model on GPU.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">gendoc</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>, laplace_smooth<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">0.0</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Generate a generic document from model parameters by running the associated graphical model as a generative process.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">gencorp</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>, M<span class="pl-k">::</span><span class="pl-c1">Integer</span>, laplace_smooth<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">0.0</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Generate a generic corpus of size M from model parameters.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showtopics</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>, V<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">15</span>; topics<span class="pl-k">::</span><span class="pl-c1">Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}</span><span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">:</span>model<span class="pl-k">.</span>K, cols<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">4</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Display the top V words for each topic in topics.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showlibs</span>(model<span class="pl-k">::</span><span class="pl-c1">Union{CTPF, gpuCTPF}</span>, users<span class="pl-k">::</span><span class="pl-c1">Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Show the document(s) in a user's library.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showdrecs</span>(model<span class="pl-k">::</span><span class="pl-c1">Union{CTPF, gpuCTPF}</span>, docs<span class="pl-k">::</span><span class="pl-c1">Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}</span>, U<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">16</span>; cols<span class="pl-k">=</span><span class="pl-c1">4</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Show the top U user recommendations for a document(s).<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">showurecs</span>(model<span class="pl-k">::</span><span class="pl-c1">Union{CTPF, gpuCTPF}</span>, users<span class="pl-k">::</span><span class="pl-c1">Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}</span>, M<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">10</span>; cols<span class="pl-k">=</span><span class="pl-c1">1</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Show the top M document recommendations for a user(s).<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">predict</span>(corp<span class="pl-k">::</span><span class="pl-c1">Corpus</span>, train_model<span class="pl-k">::</span><span class="pl-c1">Union{LDA, gpuLDA, fLDA, CTM, gpuCTM, fCTM}</span>; iter<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">10</span>, tol<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">/</span>train_model<span class="pl-k">.</span>K<span class="pl-k">^</span><span class="pl-c1">2</span>, niter<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">1000</span>, ntol<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">1</span><span class="pl-k">/</span>train_model<span class="pl-k">.</span>K<span class="pl-k">^</span><span class="pl-c1">2</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Predict topic distributions for corpus of documents based on trained LDA or CTM model.<span class="pl-pds">"</span></span>

<span class="pl-k">function</span> <span class="pl-en">topicdist</span>(model<span class="pl-k">::</span><span class="pl-c1">TopicModel</span>, doc_indices<span class="pl-k">::</span><span class="pl-c1">Union{Integer, Vector{&lt;:Integer}, UnitRange{&lt;:Integer}}</span>)
	<span class="pl-s"><span class="pl-pds">"</span>Get TopicModel topic distributions for document(s) as a probability vector.<span class="pl-pds">"</span></span></pre></div>
<h2><a id="user-content-bibliography" class="anchor" aria-hidden="true" href="#bibliography"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bibliography</h2>
<ol>
<li>Latent Dirichlet Allocation (2003); Blei, Ng, Jordan. <a href="http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf" rel="nofollow">pdf</a></li>
<li>Filtered Latent Dirichlet Allocation: Variational Bayes Algorithm (2016); Proffitt. <a href="https://github.com/esproff/TopicModelsVB.jl/blob/master/fLDAVB.pdf">pdf</a></li>
<li>Correlated Topic Models (2006); Blei, Lafferty. <a href="http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006.pdf" rel="nofollow">pdf</a></li>
<li>Content-based Recommendations with Poisson Factorization (2014); Gopalan, Charlin, Blei. <a href="http://www.cs.columbia.edu/~blei/papers/GopalanCharlinBlei2014.pdf" rel="nofollow">pdf</a></li>
<li>Numerical Optimization (2006); Nocedal, Wright. <a href="https://www.amazon.com/Numerical-Optimization-Operations-Financial-Engineering/dp/0387303030" rel="nofollow">Amazon</a></li>
<li>Machine Learning: A Probabilistic Perspective (2012); Murphy. <a href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=" rel="nofollow">Amazon</a></li>
<li>OpenCL in Action: How to Accelerate Graphics and Computation (2011); Scarpino. <a href="https://www.amazon.com/OpenCL-Action-Accelerate-Graphics-Computations/dp/1617290173" rel="nofollow">Amazon</a></li>
</ol>
</article></div>