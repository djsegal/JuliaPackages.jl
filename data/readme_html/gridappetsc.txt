<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gridappetsc" class="anchor" aria-hidden="true" href="#gridappetsc"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GridapPETSc</h1>
<p dir="auto"><a href="https://gridap.github.io/GridapPETSc.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://gridap.github.io/GridapPETSc.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/gridap/GridapPETSc.jl/actions?query=workflow%3ACI"><img src="https://github.com/gridap/GridapPETSc.jl/workflows/CI/badge.svg?branch=master" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/gridap/GridapPETSc.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/530ac874d65a5b801cb7ceb577ad4d943bae0a55748f999ec215492b568db415/68747470733a2f2f636f6465636f762e696f2f67682f6772696461702f47726964617050455453632e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/gridap/GridapPETSc.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><code>GridapPETSc</code> is a plugin of <a href="https://github.com/gridap/GridapDistributed.jl"><code>GridapDistributed.jl</code></a> that provides the  full set of scalable linear and nonlinear solvers in the <a href="https://petsc.org/release/" rel="nofollow">PETSc</a> library. It also provides serial solvers to <a href="https://github.com/gridap/Gridap.jl"><code>Gridap.jl</code></a>.</p>
<h2 dir="auto"><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Documentation</h2>
<p dir="auto">Take a look at this <a href="https://gridap.github.io/Tutorials/dev/pages/t016_poisson_distributed/#Tutorial-16:-Poisson-equation-on-parallel-distributed-memory-computers-1" rel="nofollow">tutorial</a> for learning how to use <code>GridapPETSc</code> in distributed-memory simulations of PDEs.</p>
<p dir="auto">It can also be used in the serial case, as shown in this <a href="https://github.com/gridap/GridapPETSc.jl/blob/master/test/sequential/PoissonDriver.jl">test</a>.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto"><code>GridapPETSc</code> julia package requires the <code>PETSC</code> library (<a href="https://www.mcs.anl.gov/petsc/" rel="nofollow">Portable, Extensible Toolkit for Scientific Computation</a>) and <code>MPI</code> to work correctly. You have two main options to install these dependencies.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Do nothing [recommended in most cases].</strong> Use the default precompiled <code>MPI</code> installation provided by <a href="https://github.com/JuliaParallel/MPI.jl"><code>MPI.jl</code></a> and the pre-compiled <code>PETSc</code> library provided by <a href="https://github.com/JuliaBinaryWrappers/PETSc_jll.jl"><code>PETSc_jll</code></a>. This will happen under the hood when you install <code>GridapPETSc</code>. You can also force the installation of these default dependencies by setting the environment variables <code>JULIA_MPI_BINARY</code> and <code>JULIA_PETSC_LIBRARY</code> to empty values.</p>
</li>
<li>
<p dir="auto"><strong>Choose a specific installation of <code>MPI</code> and <code>PETSc</code> available in the system [recommended in HPC clusters]</strong>.</p>
<ul dir="auto">
<li>First, choose a <code>MPI</code> installation. See the documentation of  <a href="https://github.com/JuliaParallel/MPI.jl"><code>MPI.jl</code></a> for further details. An easy way to achieve this is to create the environment variable <code>JULIA_MPI_BINARY</code> containing the path to the  <code>MPI</code> binary.</li>
<li>Second, choose a <code>PETSc</code> installation. To this end, create an environment variable <code>JULIA_PETSC_LIBRARY</code> containing the path to the dynamic library object of the <code>PETSC</code> installation (i.e., the <code>.so</code> file in linux systems). <strong>Very important: The chosen <code>PETSc</code> library needs to be configured with the <code>MPI</code> installation considered in the previous step</strong>.</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-notes" class="anchor" aria-hidden="true" href="#notes"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notes</h2>
<ul dir="auto">
<li><code>GridapPETSc</code> default sparse matrix format is 0-based compressed sparse row. This type of sparse matrix storage format can be described by the <code>SparseMatrixCSR{0,PetscReal,PetscInt}</code> and <code>SymSparseMatrixCSR{0,PetscReal,PetscInt}</code> Julia types as implemented in the <a href="https://gridap.github.io/SparseMatricesCSR.jl/stable/" rel="nofollow">SparseMatricesCSR</a> Julia package.</li>
<li><strong>When running in MPI parallel mode</strong> (i.e., with a MPI communicator different from <code>MPI.COMM_SELF</code>), <code>GridapPETSc</code> implements a sort of limited garbage collector in order to automatically deallocate PETSc objects. This garbage collector can be manually triggered by a call to the function <code>GridapPETSc.gridap_petsc_gc()</code>. <code>GridapPETSc</code> automatically calls this function inside at different strategic points, and <strong>this will be sufficient for most applications</strong>. However, for some applications, with a very frequent allocation of PETSc objects, it might be needed to call this function from application code. This need will be signaled by PETSc via the following internal message error <code>PETSC ERROR: No more room in array, limit 256  recompile src/sys/objects/destroy.c with larger value for MAXREGDESOBJS</code></li>
</ul>
</article></div>