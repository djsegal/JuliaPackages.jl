<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-denoisingdiffusionjl" class="anchor" aria-hidden="true" href="#denoisingdiffusionjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DenoisingDiffusion.jl</h1>
<p dir="auto">A pure Julia implementation of denoising diffusion probabilistic models as popularised in <a href="https://arxiv.org/abs/2006.11239" rel="nofollow">Denoising Diffusion Probabilistic Models by Jonathan Ho, Ajay Jain and Pieter Abbeel (2020)</a></p>
<p dir="auto">For detailed examples please the notebooks at the corresponding project at <a href="https://github.com/LiorSinai/DenoisingDiffusion-examples">github.com/LiorSinai/DenoisingDiffusion-examples</a>. The notebooks were originally part of this repository but were removed using <a href="https://github.com/newren/git-filter-repo">git-filter-repo</a> to make this repository more lightweight.</p>
<p dir="auto">For an explanation of the diffusion process and the code please see my blog posts at <a href="https://liorsinai.github.io/coding/2022/12/03/denoising-diffusion-1-spiral.html" rel="nofollow">liorsinai.github.io</a>.</p>
<h2 dir="auto"><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<h3 dir="auto"><a id="user-content-unconditioned-sampling" class="anchor" aria-hidden="true" href="#unconditioned-sampling"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Unconditioned sampling</h3>
<p align="center" dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="images/numbers_reverse.gif"><img src="images/numbers_reverse.gif" width="45%" data-animated-image="" style="max-width: 100%;"></a>
  <a target="_blank" rel="noopener noreferrer" href="images/numbers_estimate.gif"><img src="images/numbers_estimate.gif" width="45%" data-animated-image="" style="max-width: 100%;"></a> 
  </p><p dir="auto">Reverse process (left) and final image estimate (right). These coincide on the final time step.</p>
<p dir="auto"></p>
<p dir="auto">Denoising diffusion starts from an image of pure noise and gradually removes this noise across many time steps, resulting in a natural looking image.
At each time step a model predicts the noise to be removed in order to reach the final image on the final time step from the current time step.
This allows an estimate of the final image to be created, which is updated at every time step.
The above image shows this process with a trained model for number generation.</p>
<h3 dir="auto"><a id="user-content-conditioned-sampling-with-classifier-free-guidance" class="anchor" aria-hidden="true" href="#conditioned-sampling-with-classifier-free-guidance"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Conditioned sampling with classifier free guidance</h3>
<p align="center" dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="images/2d_reverse_guided.gif"><img src="images/2d_reverse_guided.gif" width="400" height="400" data-animated-image="" style="max-width: 100%;"></a>
  </p><p dir="auto">Classifier free guidance</p>
<p dir="auto"></p>
<p dir="auto">It is possible to direct the outcome using classifier free guidance as introduced in
<a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance by Jonathan Ho and Tim Salimans (2022)</a>.
In this mode a label as well as the timestep is passed to the model.
Two candidates of the noise to be removed are generated at each timestep: unconditioned noise made using a generic label (label=1) and conditioned noise made using the target label.
The noise that is removed is then given by a weighted combination of the two:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="noise = ϵ_uncond + guidance_scale * (ϵ_cond - ϵ_uncond)"><pre class="notranslate"><code>noise = ϵ_uncond + guidance_scale * (ϵ_cond - ϵ_uncond)
</code></pre></div>
<p dir="auto">Where <code>guidance_scale &gt;= 1</code>. The difference <code>(ϵ_cond - ϵ_uncond)</code> represents a very rough gradient.</p>
<p dir="auto">The original paper uses <code>ϵ_cond + guidance_scale * (ϵ_cond - ϵ_uncond)</code> but using the baseline as <code>ϵ_uncond</code> instead allows it to be cancelled and skipped for the special case of <code>guidance_scale = 1</code>.</p>
<h2 dir="auto"><a id="user-content-module" class="anchor" aria-hidden="true" href="#module"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Module</h2>
<p dir="auto">The main export is the <code>GaussianDiffusion</code> struct and associated functions.
Various models and building blocks are included.
The models includes a flexible <code>ConditionalChain</code> based on <code>Flux.Chain</code>. It can handle multiple inputs where the first input is given priority.
Two versions of UNets (convolutional autoencoder) are available, <code>UNet</code> and <code>UNetFixed</code>.</p>
<p dir="auto">A <code>UNet</code> model made to the same specifications as <code>UNetFixed</code> is 100% equivalent.</p>
<p dir="auto"><code>UNet</code> is flexible and can have an arbitrary number of downsample/upsample pairs (more than five is not advisable).
It is based on nested skip connections.
<code>UNetFixed</code> is a linear implementation of the same model.
<code>UNetFixed</code> has three downsample/upsample pairs and three middle layers with a total of 16 layers. For the default configuration <code>UNetFixed(1, 8, 100)</code> will have approximately 150,000 parameters.
About 50% of these parameters are in the middle layer - 24% in the attention layer alone.</p>
<p dir="auto">For both models, every doubling of the <code>model_channels</code> will approximately quadruple the number of parameters because the convolution layer size is proportional to the square of the dimension.</p>
<h2 dir="auto"><a id="user-content-fréchet-lenet-distances-fld" class="anchor" aria-hidden="true" href="#fréchet-lenet-distances-fld"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fréchet LeNet Distances (FLD)</h2>
<p dir="auto">For number generation the <a href="https://arxiv.org/abs/1706.08500" rel="nofollow">Frechet Inception Distance (FID)</a> is cumbersome.
The <a href="https://pytorch.org/hub/pytorch_vision_inception_v3/" rel="nofollow">Inception V3</a> model has 27.1 million parameters
which is overkill for number generation. Instead the simpler Fréchet LeNet Distance is proposed.
This uses the same calculation except with a smaller <a href="https://github.com/FluxML/model-zoo/blob/master/vision/conv_mnist/conv_mnist.jl">LeNet model</a> with approximately 44,000 parameters.
The output layer has 84 values as opposed to Inception V3's 2048.</p>
<p dir="auto">No pretrained weights are necessary because the LeNet model can be very easily trained on a CPU.
However results will not be standardised.</p>
<p dir="auto">Example values are:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Parameters</th>
<th>FLD</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>training data</td>
<td>0</td>
<td>0.5</td>
<td></td>
</tr>
<tr>
<td>UNetConditioned</td>
<td>622, 865</td>
<td>7.0</td>
<td>Guidance with <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="d1bda8428c082ca74978f5176260346e">$\gamma=2$</math-renderer> and 1000 samples per label</td>
</tr>
<tr>
<td>UNet</td>
<td>376,913</td>
<td>18.3</td>
<td>No attention layer</td>
</tr>
<tr>
<td>UNet</td>
<td>602,705</td>
<td>23.9</td>
<td></td>
</tr>
<tr>
<td>UNet</td>
<td>602,705</td>
<td>26.3</td>
<td>DDIM <math-renderer class="js-inline-math" style="display: inline" data-static-url="https://github.githubassets.com/static" data-run-id="d1bda8428c082ca74978f5176260346e">$\tau_n = 20; \eta=1$</math-renderer>
</td>
</tr>
<tr>
<td>Random</td>
<td>0</td>
<td>&gt;337</td>
<td></td>
</tr>
</tbody>
</table>
<p dir="auto">The loss is Mean Squared Error. All models were trained for 15 epochs.</p>
<h2 dir="auto">
<a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">Download the GitHub repository (it is not registered). Then in the Julia REPL:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; ] #enter package mode
(@v1.x) pkg&gt; dev path\\to\\DenoisingDiffusion
julia&gt; using Revise # allows dynamic edits to code
julia&gt; using DenoisingDiffusion"><pre class="notranslate"><code>julia&gt; ] #enter package mode
(@v1.x) pkg&gt; dev path\\to\\DenoisingDiffusion
julia&gt; using Revise # allows dynamic edits to code
julia&gt; using DenoisingDiffusion
</code></pre></div>
<p dir="auto">Optionally, tests can be run with:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="(@v1.x) pkg&gt; test DenoisingDiffusion"><pre class="notranslate"><code>(@v1.x) pkg&gt; test DenoisingDiffusion
</code></pre></div>
<p dir="auto">This repository uses FastAi's <a href="https://nbdev.fast.ai/tutorials/git_friendly_jupyter.html" rel="nofollow">nbdev</a> to manage the Jupyter Notebooks for Git. This requires a Python installation of nbdev. To avoid using it, follow the steps in .gitconfig.</p>
<h2 dir="auto">
<a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">To run the examples:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia examples\\train_images.jl --threads auto"><pre class="notranslate"><code>julia examples\\train_images.jl --threads auto
</code></pre></div>
<p dir="auto">Or start the Julia REPL and run it interactively.</p>
<p dir="auto">There are three use cases:</p>
<ul dir="auto">
<li>Spiral (2 values per data point).</li>
<li>Numbers (28×28=784 values per data point.)</li>
<li>Pokemon (48×48×3=6912 values per data point.)</li>
</ul>
<p dir="auto">The spiral use case requires approximately 1,000 parameters. The number generation requires at least 100 times this, and the Pokemon possibly more. So far, satisfying results for the Pokemon have not been achieved.
See however <a href="https://huggingface.co/spaces/ronvolutional/ai-pokemon-card" rel="nofollow">This Pokémon Does Not Exist</a>
for an example trained on 1.3 billion parameter model.</p>
<h2 dir="auto">
<a id="user-content-task-list" class="anchor" aria-hidden="true" href="#task-list"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Task list</h2>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Self-attention blocks.</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> DDIM for more efficient and faster image generation.</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> Guided diffusion.</li>
</ul>
</article></div>