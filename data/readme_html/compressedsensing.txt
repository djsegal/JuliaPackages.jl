<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-compressedsensingjl" class="anchor" aria-hidden="true" href="#compressedsensingjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CompressedSensing.jl</h1>
<p dir="auto"><a href="https://github.com/SebastianAment/CompressedSensing.jl/actions/workflows/CI.yml"><img src="https://github.com/SebastianAment/CompressedSensing.jl/actions/workflows/CI.yml/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/SebastianAment/CompressedSensing.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/8f7246ee0996ae938d9abf2b509e18e27b34b31052a0402a21e6b0d48b7a5bd9/68747470733a2f2f636f6465636f762e696f2f67682f53656261737469616e416d656e742f436f6d7072657373656453656e73696e672e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d4e50594332314d495154" alt="codecov" data-canonical-src="https://codecov.io/gh/SebastianAment/CompressedSensing.jl/branch/main/graph/badge.svg?token=NPYC21MIQT" style="max-width: 100%;"></a></p>
<p dir="auto">Contains a wide-ranging collection of compressed sensing and feature selection algorithms.
Examples include matching pursuit algorithms, forward and backward stepwise regression, sparse Bayesian learning, and basis pursuit.</p>
<h2 dir="auto"><a id="user-content-matching-pursuits" class="anchor" aria-hidden="true" href="#matching-pursuits"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Matching Pursuits</h2>
<p dir="auto">The package contains implementations of <a href="https://en.wikipedia.org/wiki/Matching_pursuit" rel="nofollow">Matching Pursuit (MP)</a>,
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=342465" rel="nofollow">Orthogonal Matching Pursuit (OMP)</a>,
and <a href="https://arxiv.org/pdf/1111.6664.pdf" rel="nofollow">Generalized OMP (GOMP)</a>,
all three of which take advantage of the efficient updating algorithms contained in <a href="https://github.com/SebastianAment/UpdatableQRFactorizations.jl">UpdatableQRFactorizations.jl</a> to compute the QR factorization of the atoms in the active set.</p>
<h2 dir="auto"><a id="user-content-stepwise-regression" class="anchor" aria-hidden="true" href="#stepwise-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Stepwise Regression</h2>
<ul dir="auto">
<li>Forward Regression</li>
<li>Backward Regression</li>
<li></li>
</ul>
<h2 dir="auto"><a id="user-content-two-stage-algorithms" class="anchor" aria-hidden="true" href="#two-stage-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Two-Stage Algorithms</h2>
<ul dir="auto">
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4839056&amp;casa_token=fyP4fT6vvjAAAAAA:cT_80KeMMH3WycQA0f-HqXUj0hViY-fSajRgENqYmyOhOHWXTq5EIRE5rcpZl675JyHO917Trw" rel="nofollow">Subspace Pursuit (SP)</a>.</li>
<li>Relevance Matching Pursuit (RMP) introduced in <a href="https://proceedings.mlr.press/v139/ament21a.html" rel="nofollow">Sparse Bayesian Learning via Stepwise Regression</a>.</li>
<li>Stepwise Regression with Replacement (SRR) introduced in <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415082&amp;casa_token=gmwN6_yXZSAAAAAA:uzKJOGwZFFwZzum2SoZWNtsvcpSQ34Rdib_0PlyU3oNDY-ZkB9PULGNGGnuHjSC2U51YiywiSQ" rel="nofollow">On the Optimality of Backward Regression: Sparse Recovery and Subset Selection</a>.</li>
</ul>
<h2 dir="auto"><a id="user-content-sparse-bayesian-learning" class="anchor" aria-hidden="true" href="#sparse-bayesian-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Sparse Bayesian Learning</h2>
<ul dir="auto">
<li>Original SBL algorithm introduced in <a href="https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf" rel="nofollow">Sparse Bayesian Learning and the Relevance Vector Machine</a>.</li>
<li><a href="http://www.miketipping.com/papers/met-fastsbl.pdf" rel="nofollow">Fast Marginal Likelihood Maximisation for
Sparse Bayesian Models</a></li>
</ul>
<h2 dir="auto"><a id="user-content-basis-pursuit" class="anchor" aria-hidden="true" href="#basis-pursuit"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Basis Pursuit</h2>
<p dir="auto">Basis Pursuit (BP) with reweighting schemes, like the ones related to entropy regularization and the Automatic Relevance Determination (ARD) or SBL prior.</p>
<h2 dir="auto"><a id="user-content-citing-this-package" class="anchor" aria-hidden="true" href="#citing-this-package"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citing this Package</h2>
<p dir="auto">This package was written in the course of a research project on sparsity-promiting algorithms and was published with the paper <a href="https://proceedings.mlr.press/v139/ament21a.html" rel="nofollow">Sparse Bayesian Learning via Stepwise Regression</a>.
Consider using the following citation, when referring to this package in a publication.</p>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@InProceedings{pmlr-v139-ament21a,
  title = 	 {Sparse Bayesian Learning via Stepwise Regression},
  author =       {Ament, Sebastian E. and Gomes, Carla P.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {264--274},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ament21a/ament21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ament21a.html},
  abstract = 	 {Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity in probabilistic models. Herein, we propose a coordinate ascent algorithm for SBL termed Relevance Matching Pursuit (RMP) and show that, as its noise variance parameter goes to zero, RMP exhibits a surprising connection to Stepwise Regression. Further, we derive novel guarantees for Stepwise Regression algorithms, which also shed light on RMP. Our guarantees for Forward Regression improve on deterministic and probabilistic results for Orthogonal Matching Pursuit with noise. Our analysis of Backward Regression culminates in a bound on the residual of the optimal solution to the subset selection problem that, if satisfied, guarantees the optimality of the result. To our knowledge, this bound is the first that can be computed in polynomial time and depends chiefly on the smallest singular value of the matrix. We report numerical experiments using a variety of feature selection algorithms. Notably, RMP and its limiting variant are both efficient and maintain strong performance with correlated features.}
}"><pre><span class="pl-k">@InProceedings</span>{<span class="pl-en">pmlr-v139-ament21a</span>,
  <span class="pl-s">title</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>Sparse Bayesian Learning via Stepwise Regression<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span> =       <span class="pl-s"><span class="pl-pds">{</span>Ament, Sebastian E. and Gomes, Carla P.<span class="pl-pds">}</span></span>,
  <span class="pl-s">booktitle</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>Proceedings of the 38th International Conference on Machine Learning<span class="pl-pds">}</span></span>,
  <span class="pl-s">pages</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>264--274<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>2021<span class="pl-pds">}</span></span>,
  <span class="pl-s">editor</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>Meila, Marina and Zhang, Tong<span class="pl-pds">}</span></span>,
  <span class="pl-s">volume</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>139<span class="pl-pds">}</span></span>,
  <span class="pl-s">series</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>Proceedings of Machine Learning Research<span class="pl-pds">}</span></span>,
  <span class="pl-s">month</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>18--24 Jul<span class="pl-pds">}</span></span>,
  <span class="pl-s">publisher</span> =    <span class="pl-s"><span class="pl-pds">{</span>PMLR<span class="pl-pds">}</span></span>,
  <span class="pl-s">pdf</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>http://proceedings.mlr.press/v139/ament21a/ament21a.pdf<span class="pl-pds">}</span></span>,
  <span class="pl-s">url</span> = 	 <span class="pl-s"><span class="pl-pds">{</span>https://proceedings.mlr.press/v139/ament21a.html<span class="pl-pds">}</span></span>,
  abstract = 	 {Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity in probabilistic models. Herein, we propose a coordinate ascent algorithm for SBL termed Relevance Matching Pursuit (RMP) and show that, as its noise variance parameter goes to zero, RMP exhibits a surprising connection to Stepwise Regression. Further, we derive novel guarantees for Stepwise Regression algorithms, which also shed light on RMP. Our guarantees for Forward Regression improve on deterministic and probabilistic results for Orthogonal Matching Pursuit with noise. Our analysis of Backward Regression culminates in a bound on the residual of the optimal solution to the subset selection problem that, if satisfied, guarantees the optimality of the result. To our knowledge, this bound is the first that can be computed in polynomial time and depends chiefly on the smallest singular value of the matrix. We report numerical experiments using a variety of feature selection algorithms. Notably, RMP and its limiting variant are both efficient and maintain strong performance with correlated features.}
}</pre></div>
</article></div>