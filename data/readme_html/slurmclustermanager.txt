<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-slurmclustermanagerjl" class="anchor" aria-hidden="true" href="#slurmclustermanagerjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>SlurmClusterManager.jl</h1>
<p dir="auto"><a href="https://github.com/kleinhenz/SlurmClusterManager.jl/actions"><img src="https://github.com/kleinhenz/SlurmClusterManager.jl/workflows/CI/badge.svg?branch=master" alt="Build Status" style="max-width: 100%;"></a></p>
<p dir="auto">This package provides support for using julia within the Slurm cluster environment.
The code is adapted from <a href="https://github.com/JuliaParallel/ClusterManagers.jl">ClusterManagers.jl</a> with some modifications.</p>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">This script uses all resources from a Slurm allocation as julia workers and prints the id and hostname on each one.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="#!/usr/bin/env julia

using Distributed, SlurmClusterManager
addprocs(SlurmManager())
@everywhere println(&quot;hello from $(myid()):$(gethostname())&quot;)"><pre><span class="pl-c"><span class="pl-c">#</span>!/usr/bin/env julia</span>

<span class="pl-k">using</span> Distributed, SlurmClusterManager
<span class="pl-c1">addprocs</span>(<span class="pl-c1">SlurmManager</span>())
<span class="pl-c1">@everywhere</span> <span class="pl-c1">println</span>(<span class="pl-s"><span class="pl-pds">"</span>hello from <span class="pl-v">$(<span class="pl-c1">myid</span>())</span>:<span class="pl-v">$(<span class="pl-c1">gethostname</span>())</span><span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">If the code is saved in <code>script.jl</code> it can be queued and executed on two nodes using 64 workers per node by running</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="sbatch -N 2 --ntasks-per-node=64 script.jl"><pre class="notranslate"><code>sbatch -N 2 --ntasks-per-node=64 script.jl
</code></pre></div>
<h2 dir="auto"><a id="user-content-differences-from-clustermanagersjl" class="anchor" aria-hidden="true" href="#differences-from-clustermanagersjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Differences from <code>ClusterManagers.jl</code></h2>
<ul dir="auto">
<li>Only supports Slurm (see this <a href="https://github.com/JuliaParallel/ClusterManagers.jl/issues/58" data-hovercard-type="issue" data-hovercard-url="/JuliaParallel/ClusterManagers.jl/issues/58/hovercard">issue</a> for some background).</li>
<li>Requires that <code>SlurmManager</code> be created inside a Slurm allocation created by sbatch/salloc.
Specifically <code>SLURM_JOBID</code> and <code>SLURM_NTASKS</code> must be defined in order to construct <code>SlurmManager</code>.
This matches typical HPC workflows where resources are requested using sbatch and then used by the application code.
In contrast <code>ClusterManagers.jl</code> will  <em>dynamically</em> request resources when run outside of an existing Slurm allocation.
I found that this was basically never what I wanted since this leaves the manager process running on a login node,
and makes the script wait until resources are granted which is better handled by the actual Slurm queueing system.</li>
<li>Does not take any Slurm arguments. All Slurm arguments are inherited from the external Slurm allocation created by sbatch/salloc.</li>
<li>Output from workers is redirected to the manager process instead of requiring a separate output file for every task.</li>
</ul>
</article></div>