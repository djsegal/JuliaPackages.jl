<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/crstnbr/BinningAnalysis.jl/blob/master/docs/src/assets/logo_with_text.png"><img src="https://github.com/crstnbr/BinningAnalysis.jl/raw/master/docs/src/assets/logo_with_text.png" alt="logo" style="max-width:100%;"></a></p>
<table>
<thead>
<tr>
<th align="center"><strong>Build Status</strong></th>
<th align="center"><strong>Community</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/f8b75bf59f0258f267d191f906b5d1309741f821beaf3e8d7473eacc02e5b2e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6966656379636c652d737461626c652d626c75652e737667"><img src="https://camo.githubusercontent.com/f8b75bf59f0258f267d191f906b5d1309741f821beaf3e8d7473eacc02e5b2e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6966656379636c652d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/lifecycle-stable-blue.svg" style="max-width:100%;"></a> <a href="https://github.com/crstnbr/BinningAnalysis.jl/actions?query=workflow%3A%22Run+tests%22"><img src="https://github.com/crstnbr/BinningAnalysis.jl/workflows/Run%20tests/badge.svg" alt="" style="max-width:100%;"></a> <a href="http://codecov.io/github/crstnbr/BinningAnalysis.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/7263dec15768b420665c3c294036b51fa4b1e357496ba42802164165caa81398/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f637273746e62722f42696e6e696e67416e616c797369732e6a6c2f6d61737465722e7376673f6c6162656c3d636f6465636f76" alt="" data-canonical-src="https://img.shields.io/codecov/c/github/crstnbr/BinningAnalysis.jl/master.svg?label=codecov" style="max-width:100%;"></a> <a href="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/report.html" rel="nofollow"><img src="https://camo.githubusercontent.com/7609feedd7d088cbe6dd1d3a76f27630cfc8a8e68bb1474dfb1614a5831defcf/68747470733a2f2f6a756c696163692e6769746875622e696f2f4e616e6f736f6c646965725265706f7274732f706b676576616c5f6261646765732f422f42696e6e696e67416e616c797369732e737667" alt="" data-canonical-src="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/B/BinningAnalysis.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://slackinvite.julialang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/c192b6b30d22427a9ad86f7832a70c27f8dcbb028dae7dc2ca07181ef7dd9e13/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230736c61636b2d79656c6c6f772e737667" alt="" data-canonical-src="https://img.shields.io/badge/chat-on%20slack-yellow.svg" style="max-width:100%;"></a> <a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/3a50d5c687932b4693d23d07cfa8fc5a7b6738c97689f8631e26f23d8740ee14/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d7265642e737667" alt="" data-canonical-src="https://img.shields.io/badge/License-MIT-red.svg" style="max-width:100%;"></a> <a href="https://doi.org/10.5281/zenodo.3603347" rel="nofollow"><img src="https://camo.githubusercontent.com/dbc1512a83fc27014564a13ebded340a49fc7f7f2df226d507cefcf4ce3cac3a/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333630333334372e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.3603347.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>This package provides tools to estimate <a href="https://en.wikipedia.org/wiki/Standard_error" rel="nofollow">standard errors</a> and <a href="https://en.wikipedia.org/wiki/Autocorrelation" rel="nofollow">autocorrelation times</a> of correlated time series. A typical example is a <a href="https://en.wikipedia.org/wiki/Markov_chain" rel="nofollow">Markov chain</a> obtained in a <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" rel="nofollow">Metropolis Monte Carlo simulation</a>.</p>
<p><strong>Binning tools:</strong></p>
<ul>
<li>Logarithmic Binning
<ul>
<li>Size complexity: <code>O(log(N))</code></li>
<li>Time complexity: <code>O(N)</code></li>
</ul>
</li>
<li>Full Binning (all bin sizes that work out evenly)</li>
</ul>
<p><strong>Statistical resampling methods:</strong></p>
<ul>
<li>Jackknife resampling.</li>
</ul>
<br>
<p>As per usual, you can install the registered package with</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="] add BinningAnalysis
"><pre>] add BinningAnalysis</pre></div>
<p>Note that there is <a href="https://github.com/crstnbr/BinningAnalysisPlots.jl">BinningAnalysisPlots.jl</a> which defines some <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> recipes for <code>LogBinner</code> and <code>FullBinner</code> to facilitate visualizing the error convergence.</p>
<h2><a id="user-content-binning-tools" class="anchor" aria-hidden="true" href="#binning-tools"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Binning tools</h2>
<h3><a id="user-content-logarithmic-binning" class="anchor" aria-hidden="true" href="#logarithmic-binning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Logarithmic Binning</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="B = LogBinner()
# As per default, 2^32-1 ≈ 4 billion values can be added to the binner. This value can be
# tuned with the `capacity` keyword argument.

push!(B, 4.2)
append!(B, [1,2,3]) # multiple values at once

x  = mean(B)
Δx = std_error(B) # standard error of the mean
tau_x = tau(B) # autocorrelation time

# Alternatively you can provide a time series already in the constructor
x = rand(100)
B = LogBinner(x)

Δx = std_error(B)
"><pre>B <span class="pl-k">=</span> <span class="pl-c1">LogBinner</span>()
<span class="pl-c"><span class="pl-c">#</span> As per default, 2^32-1 ≈ 4 billion values can be added to the binner. This value can be</span>
<span class="pl-c"><span class="pl-c">#</span> tuned with the `capacity` keyword argument.</span>

<span class="pl-c1">push!</span>(B, <span class="pl-c1">4.2</span>)
<span class="pl-c1">append!</span>(B, [<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>]) <span class="pl-c"><span class="pl-c">#</span> multiple values at once</span>

x  <span class="pl-k">=</span> <span class="pl-c1">mean</span>(B)
Δx <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(B) <span class="pl-c"><span class="pl-c">#</span> standard error of the mean</span>
tau_x <span class="pl-k">=</span> <span class="pl-c1">tau</span>(B) <span class="pl-c"><span class="pl-c">#</span> autocorrelation time</span>

<span class="pl-c"><span class="pl-c">#</span> Alternatively you can provide a time series already in the constructor</span>
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)
B <span class="pl-k">=</span> <span class="pl-c1">LogBinner</span>(x)

Δx <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(B)</pre></div>

<h3><a id="user-content-full-binning" class="anchor" aria-hidden="true" href="#full-binning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Full Binning</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="B = FullBinner() # &lt;: AbstractVector (lightweight wrapper)

push!(B, 2.0)
append!(B, [1,2,3])

x  = mean(B)
Δx = std_error(B) # standard error of the mean

# Alternatively you can provide a time series already in the constructor
x = rand(100)
F = FullBinner(x)

push!(F, 2.0) # will modify x as F is just a thin wrapper

Δx = std_error(F)
"><pre>B <span class="pl-k">=</span> <span class="pl-c1">FullBinner</span>() <span class="pl-c"><span class="pl-c">#</span> &lt;: AbstractVector (lightweight wrapper)</span>

<span class="pl-c1">push!</span>(B, <span class="pl-c1">2.0</span>)
<span class="pl-c1">append!</span>(B, [<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>])

x  <span class="pl-k">=</span> <span class="pl-c1">mean</span>(B)
Δx <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(B) <span class="pl-c"><span class="pl-c">#</span> standard error of the mean</span>

<span class="pl-c"><span class="pl-c">#</span> Alternatively you can provide a time series already in the constructor</span>
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)
F <span class="pl-k">=</span> <span class="pl-c1">FullBinner</span>(x)

<span class="pl-c1">push!</span>(F, <span class="pl-c1">2.0</span>) <span class="pl-c"><span class="pl-c">#</span> will modify x as F is just a thin wrapper</span>

Δx <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(F)</pre></div>
<h3><a id="user-content-incremental-binning" class="anchor" aria-hidden="true" href="#incremental-binning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Incremental Binning</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Averages pushed values more and more, starting with no averaging
# Averaging includes 2x more values for every blocksize averages saved
B = IncrementBinner(0.0, blocksize=50)

for x in rand(10_000)
    push!(B, x)
end

# Returns the effective indices for the values saved
# I.e. [1, 2, ...49, 50, 51.5, 53.5, ..., 146.5, 148.5, 151.5, ...]
xs = indices(B)
# Returns the averaged values saved
ys = values(B)
"><pre><span class="pl-c"><span class="pl-c">#</span> Averages pushed values more and more, starting with no averaging</span>
<span class="pl-c"><span class="pl-c">#</span> Averaging includes 2x more values for every blocksize averages saved</span>
B <span class="pl-k">=</span> <span class="pl-c1">IncrementBinner</span>(<span class="pl-c1">0.0</span>, blocksize<span class="pl-k">=</span><span class="pl-c1">50</span>)

<span class="pl-k">for</span> x <span class="pl-k">in</span> <span class="pl-c1">rand</span>(<span class="pl-c1">10_000</span>)
    <span class="pl-c1">push!</span>(B, x)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Returns the effective indices for the values saved</span>
<span class="pl-c"><span class="pl-c">#</span> I.e. [1, 2, ...49, 50, 51.5, 53.5, ..., 146.5, 148.5, 151.5, ...]</span>
xs <span class="pl-k">=</span> <span class="pl-c1">indices</span>(B)
<span class="pl-c"><span class="pl-c">#</span> Returns the averaged values saved</span>
ys <span class="pl-k">=</span> <span class="pl-c1">values</span>(B)</pre></div>
<h2><a id="user-content-resampling-methods" class="anchor" aria-hidden="true" href="#resampling-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resampling methods</h2>
<h3><a id="user-content-jackknife" class="anchor" aria-hidden="true" href="#jackknife"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Jackknife</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="x = rand(100)

xmean, Δx = jackknife(identity, x) # jackknife estimates for mean and standard error of &lt;x&gt;

# in this example
# isapprox(Δx, std(x)/sqrt(length(x))) == true

x_inv_mean, Δx_inv = jackknife(identity, 1 ./ x) # # jackknife estimates for mean and standard error of &lt;1/x&gt;

# Multiple time series
x = rand(100)
y = rand(100)

# The inputs of the function `g` must be provided as arguments in `jackknife`.
g(x, y, xy) = x * y / xy  # &lt;x&gt;&lt;y&gt; / &lt;xy&gt;
g_mean, Δg = jackknife(g, x, y, x .* y)
"><pre>x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)

xmean, Δx <span class="pl-k">=</span> <span class="pl-c1">jackknife</span>(identity, x) <span class="pl-c"><span class="pl-c">#</span> jackknife estimates for mean and standard error of &lt;x&gt;</span>

<span class="pl-c"><span class="pl-c">#</span> in this example</span>
<span class="pl-c"><span class="pl-c">#</span> isapprox(Δx, std(x)/sqrt(length(x))) == true</span>

x_inv_mean, Δx_inv <span class="pl-k">=</span> <span class="pl-c1">jackknife</span>(identity, <span class="pl-c1">1</span> <span class="pl-k">./</span> x) <span class="pl-c"><span class="pl-c">#</span> # jackknife estimates for mean and standard error of &lt;1/x&gt;</span>

<span class="pl-c"><span class="pl-c">#</span> Multiple time series</span>
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)

<span class="pl-c"><span class="pl-c">#</span> The inputs of the function `g` must be provided as arguments in `jackknife`.</span>
<span class="pl-en">g</span>(x, y, xy) <span class="pl-k">=</span> x <span class="pl-k">*</span> y <span class="pl-k">/</span> xy  <span class="pl-c"><span class="pl-c">#</span> &lt;x&gt;&lt;y&gt; / &lt;xy&gt;</span>
g_mean, Δg <span class="pl-k">=</span> <span class="pl-c1">jackknife</span>(g, x, y, x <span class="pl-k">.*</span> y)</pre></div>
<h3><a id="user-content-error-propagator" class="anchor" aria-hidden="true" href="#error-propagator"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Error Propagator</h3>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="ep = ErrorPropagator(N_args=1)
# Essentially a LogBinner that can hold multiple variables. Errors can be derived
# for functions which depend on these variables. The memory overhead of this
# type is O(N_args^2 log(N_samples)), making it much cheaper than jackknife for
# few variables

push!(ep, rand())
append!(ep, rand(99))

# Mean and error of the (first) input
xmean = mean(ep, 1)
Δx = std_error(ep, 1)

# To compute the mean and error of a function we need its gradient
f(x) = x.^2
dfdx(x) = 2x
y = mean(ep, f)[1]
Δy = std_error(ep, dfdx)[1]

# Error propagator with multiple variables:
ep = ErrorPropagator(N_args=3)

# Multiple time series
x = rand(100)
y = rand(100)
append!(ep, x, y, x.*y)

# means and standard error of inputs:
xs = means(ep)
Δxs = std_errors(ep)

# mean and error of a function dependant on x, y and xy
# Note that this function takes a vector input
g(v) = v[1] * v[2] / v[3]  # &lt;x&gt;&lt;y&gt; / &lt;xy&gt;
dgdx(v) = [v[2]/v[3], v[1]/v[3], -v[1]*v[2]/v[3]^2]
g_mean = mean(ep, g)
Δg = std_error(ep, dgdx)
"><pre>ep <span class="pl-k">=</span> <span class="pl-c1">ErrorPropagator</span>(N_args<span class="pl-k">=</span><span class="pl-c1">1</span>)
<span class="pl-c"><span class="pl-c">#</span> Essentially a LogBinner that can hold multiple variables. Errors can be derived</span>
<span class="pl-c"><span class="pl-c">#</span> for functions which depend on these variables. The memory overhead of this</span>
<span class="pl-c"><span class="pl-c">#</span> type is O(N_args^2 log(N_samples)), making it much cheaper than jackknife for</span>
<span class="pl-c"><span class="pl-c">#</span> few variables</span>

<span class="pl-c1">push!</span>(ep, <span class="pl-c1">rand</span>())
<span class="pl-c1">append!</span>(ep, <span class="pl-c1">rand</span>(<span class="pl-c1">99</span>))

<span class="pl-c"><span class="pl-c">#</span> Mean and error of the (first) input</span>
xmean <span class="pl-k">=</span> <span class="pl-c1">mean</span>(ep, <span class="pl-c1">1</span>)
Δx <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(ep, <span class="pl-c1">1</span>)

<span class="pl-c"><span class="pl-c">#</span> To compute the mean and error of a function we need its gradient</span>
<span class="pl-en">f</span>(x) <span class="pl-k">=</span> x<span class="pl-k">.^</span><span class="pl-c1">2</span>
<span class="pl-en">dfdx</span>(x) <span class="pl-k">=</span> <span class="pl-c1">2</span>x
y <span class="pl-k">=</span> <span class="pl-c1">mean</span>(ep, f)[<span class="pl-c1">1</span>]
Δy <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(ep, dfdx)[<span class="pl-c1">1</span>]

<span class="pl-c"><span class="pl-c">#</span> Error propagator with multiple variables:</span>
ep <span class="pl-k">=</span> <span class="pl-c1">ErrorPropagator</span>(N_args<span class="pl-k">=</span><span class="pl-c1">3</span>)

<span class="pl-c"><span class="pl-c">#</span> Multiple time series</span>
x <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">100</span>)
<span class="pl-c1">append!</span>(ep, x, y, x<span class="pl-k">.*</span>y)

<span class="pl-c"><span class="pl-c">#</span> means and standard error of inputs:</span>
xs <span class="pl-k">=</span> <span class="pl-c1">means</span>(ep)
Δxs <span class="pl-k">=</span> <span class="pl-c1">std_errors</span>(ep)

<span class="pl-c"><span class="pl-c">#</span> mean and error of a function dependant on x, y and xy</span>
<span class="pl-c"><span class="pl-c">#</span> Note that this function takes a vector input</span>
<span class="pl-en">g</span>(v) <span class="pl-k">=</span> v[<span class="pl-c1">1</span>] <span class="pl-k">*</span> v[<span class="pl-c1">2</span>] <span class="pl-k">/</span> v[<span class="pl-c1">3</span>]  <span class="pl-c"><span class="pl-c">#</span> &lt;x&gt;&lt;y&gt; / &lt;xy&gt;</span>
<span class="pl-en">dgdx</span>(v) <span class="pl-k">=</span> [v[<span class="pl-c1">2</span>]<span class="pl-k">/</span>v[<span class="pl-c1">3</span>], v[<span class="pl-c1">1</span>]<span class="pl-k">/</span>v[<span class="pl-c1">3</span>], <span class="pl-k">-</span>v[<span class="pl-c1">1</span>]<span class="pl-k">*</span>v[<span class="pl-c1">2</span>]<span class="pl-k">/</span>v[<span class="pl-c1">3</span>]<span class="pl-k">^</span><span class="pl-c1">2</span>]
g_mean <span class="pl-k">=</span> <span class="pl-c1">mean</span>(ep, g)
Δg <span class="pl-k">=</span> <span class="pl-c1">std_error</span>(ep, dgdx)</pre></div>
<h2><a id="user-content-convenience-wrapper" class="anchor" aria-hidden="true" href="#convenience-wrapper"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Convenience wrapper</h2>
<p>If you want to calculate the standard error of an existing time series there you can use the convenience wrapper <code>std_error(x[; method=:log])</code>. It takes a keyword argument <code>method</code>, which can be <code>:log</code>, <code>:full</code>, or <code>:jackknife</code>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="ts = rand(1000);
std_error(ts) # default is logarithmic binning
std_error(ts, method=:full)
"><pre>ts <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">1000</span>);
<span class="pl-c1">std_error</span>(ts) <span class="pl-c"><span class="pl-c">#</span> default is logarithmic binning</span>
<span class="pl-c1">std_error</span>(ts, method<span class="pl-k">=</span><span class="pl-c1">:full</span>)</pre></div>
<h2><a id="user-content-supported-types" class="anchor" aria-hidden="true" href="#supported-types"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supported types</h2>
<p>All statistical tools should work with number-like (<code>&lt;: Number</code>) and array-like (<code>&lt;: AbstractArray</code>) elements. Regarding complex numbers, we follow base Julia and define
<code>var(x) = var(real(x)) + var(imag(x))</code>.</p>
<p>If you observe unexpected behavior please file an issue!</p>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li>J. Gubernatis, N. Kawashima, and P. Werner, <a href="https://www.cambridge.org/core/books/quantum-monte-carlo-methods/AEA92390DA497360EEDA153CF1CEC7AC" rel="nofollow">Quantum Monte Carlo Methods: Algorithms for Lattice Models</a>, Book (2016)</li>
<li>V. Ambegaokar, and M. Troyer, <a href="http://aapt.scitation.org/doi/10.1119/1.3247985" rel="nofollow">Estimating errors reliably in Monte Carlo simulations of the Ehrenfest model</a>, American Journal of Physics 78, 150 (2010)</li>
</ul>
</article></div>