<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-lossfunctions" class="anchor" aria-hidden="true" href="#lossfunctions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LossFunctions</h1>
<p><em>LossFunctions.jl is a Julia package that provides efficient and
well-tested implementations for a diverse set of loss functions
that are commonly used in Machine Learning.</em></p>
<table>
<thead>
<tr>
<th align="center"><strong>Package Status</strong></th>
<th align="center"><strong>Package Evaluator</strong></th>
<th align="center"><strong>Build Status</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="LICENSE.md"><img src="https://camo.githubusercontent.com/4440d5deb3a53c4f8661ee765378e6071e7878e8/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d627269676874677265656e2e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" style="max-width:100%;"></a> <a href="https://JuliaML.github.io/LossFunctions.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Docs" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="http://pkg.julialang.org/?pkg=LossFunctions" rel="nofollow"><img src="https://camo.githubusercontent.com/6e72d8b5399fe7f1b3f9192aeb488101204e0a93/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f4c6f737346756e6374696f6e735f302e362e737667" alt="Pkg Eval 0.6" data-canonical-src="http://pkg.julialang.org/badges/LossFunctions_0.6.svg" style="max-width:100%;"></a> <a href="http://pkg.julialang.org/?pkg=LossFunctions" rel="nofollow"><img src="https://camo.githubusercontent.com/6cf9a08d63d8a241ca1229129da8c9ad298086d7/687474703a2f2f706b672e6a756c69616c616e672e6f72672f6261646765732f4c6f737346756e6374696f6e735f302e372e737667" alt="Pkg Eval 0.7" data-canonical-src="http://pkg.julialang.org/badges/LossFunctions_0.7.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://travis-ci.org/JuliaML/LossFunctions.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/205b0b91f1e795827a13fecc44a2654c5518afa0/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614d4c2f4c6f737346756e6374696f6e732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaML/LossFunctions.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://ci.appveyor.com/project/Evizero/losses-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e3a70281a43d71f2fb92daff9ca372f63c30cee5/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f78627763326669656c343062616a73703f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/xbwc2fiel40bajsp?svg=true" style="max-width:100%;"></a> <a href="https://coveralls.io/github/JuliaML/LossFunctions.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/dcf8bc1a67203522db727f8ae9d763532dcf5f97/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4a756c69614d4c2f4c6f737346756e6374696f6e732e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/JuliaML/LossFunctions.jl/badge.svg?branch=master" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h2><a id="user-content-available-losses" class="anchor" aria-hidden="true" href="#available-losses"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Available Losses</h2>
<table>
<thead>
<tr>
<th align="center"><strong>Distance-based (Regression)</strong></th>
<th align="center"><strong>Margin-based (Classification)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/80ae72e5878d98aacb0eed6863958c02fd73b1b3/68747470733a2f2f7261776769746875622e636f6d2f4a756c69614d4c2f46696c6553746f726167652f6d61737465722f4c6f737346756e6374696f6e732f64697374616e63652e737667"><img src="https://camo.githubusercontent.com/80ae72e5878d98aacb0eed6863958c02fd73b1b3/68747470733a2f2f7261776769746875622e636f6d2f4a756c69614d4c2f46696c6553746f726167652f6d61737465722f4c6f737346756e6374696f6e732f64697374616e63652e737667" alt="distance_losses" data-canonical-src="https://rawgithub.com/JuliaML/FileStorage/master/LossFunctions/distance.svg" style="max-width:100%;"></a></td>
<td align="center"><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/80dc81d308845dd34ff70a953aafffb30b2c5c3a/68747470733a2f2f7261776769746875622e636f6d2f4a756c69614d4c2f46696c6553746f726167652f6d61737465722f4c6f737346756e6374696f6e732f6d617267696e2e737667"><img src="https://camo.githubusercontent.com/80dc81d308845dd34ff70a953aafffb30b2c5c3a/68747470733a2f2f7261776769746875622e636f6d2f4a756c69614d4c2f46696c6553746f726167652f6d61737465722f4c6f737346756e6374696f6e732f6d617267696e2e737667" alt="margin_losses" data-canonical-src="https://rawgithub.com/JuliaML/FileStorage/master/LossFunctions/margin.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>Others: <code>PeriodicLoss</code>, <code>PoissonLoss</code>, <code>ScaledLoss</code>,
<code>WeightedBinaryLoss</code></p>
<h2><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>Typically, the loss functions we work with in Machine Learning
fall into the category of supervised losses. These are
multivariate functions of two variables, the <strong>true target</strong> <code>y</code>,
which represents the "ground truth" (i.e. correct answer), and
the <strong>predicted output</strong> <code>ŷ</code>, which is what our model thinks the
truth is. A supervised loss function takes these two variables as
input and returns a value that quantifies how "bad" our
prediction is in comparison to the truth. In other words: <em>the
lower the loss, the better the prediction.</em></p>
<p>This package provides a considerable amount of carefully
implemented loss functions, as well as an API to query their
properties (e.g. convexity). Furthermore, we expose methods to
compute their values, derivatives, and second derivatives for
single observations as well as arbitrarily sized arrays of
observations. In the case of arrays a user additionally has the
ability to define if and how element-wise results are averaged or
summed over.</p>
<h2><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example</h2>
<p>The following code snippets show a simple "hello world" scenario
of how this package can be used to work with loss functions in
various ways.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> LossFunctions</pre></div>
<p>All the concrete loss "functions" that this package provides are
actually defined as immutable types, instead of native Julia
functions. We can compute the value of some type of loss using
the function <code>value()</code>. Let us start with an example of how to
compute the loss for a group of three of observations. By default
the loss will be computed element-wise.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> true_targets <span class="pl-k">=</span> [  <span class="pl-c1">1</span>,  <span class="pl-c1">0</span>, <span class="pl-k">-</span><span class="pl-c1">2</span>];

julia<span class="pl-k">&gt;</span> pred_outputs <span class="pl-k">=</span> [<span class="pl-c1">0.5</span>,  <span class="pl-c1">2</span>, <span class="pl-k">-</span><span class="pl-c1">1</span>];

julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.25</span>
<span class="pl-c"><span class="pl-c">#</span>  4.0</span>
<span class="pl-c"><span class="pl-c">#</span>  1.0</span></pre></div>
<p>Alternatively, one can also use an instance of a loss just like
one would use any other Julia function. This can make the code
significantly more readable while not impacting performance, as
it is a zero-cost abstraction (i.e. it compiles down to the same
code).</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> loss <span class="pl-k">=</span> <span class="pl-c1">L2DistLoss</span>()
<span class="pl-c"><span class="pl-c">#</span> LossFunctions.LPDistLoss{2}()</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.25</span>
<span class="pl-c"><span class="pl-c">#</span>  4.0</span>
<span class="pl-c"><span class="pl-c">#</span>  1.0</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">loss</span>(<span class="pl-c1">1</span>, <span class="pl-c1">0.5</span>f0) <span class="pl-c"><span class="pl-c">#</span> single observation</span>
<span class="pl-c"><span class="pl-c">#</span> 0.25f0</span></pre></div>
<p>If you are not actually interested in the element-wise results
individually, but some accumulation of those (such as mean or
sum), you can additionally specify an average mode. This will
avoid allocating a temporary array and directly compute the
result.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs, AvgMode<span class="pl-k">.</span><span class="pl-c1">Sum</span>())
<span class="pl-c"><span class="pl-c">#</span> 5.25</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs, AvgMode<span class="pl-k">.</span><span class="pl-c1">Mean</span>())
<span class="pl-c"><span class="pl-c">#</span> 1.75</span></pre></div>
<p>Aside from these standard unweighted average modes, we also
provide weighted alternatives. These expect a weight-factor for
each observation in the predicted outputs and so allow to give
certain observations a stronger influence over the result.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs, AvgMode<span class="pl-k">.</span><span class="pl-c1">WeightedSum</span>([<span class="pl-c1">2</span>,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>]))
<span class="pl-c"><span class="pl-c">#</span> 5.5</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs, AvgMode<span class="pl-k">.</span><span class="pl-c1">WeightedMean</span>([<span class="pl-c1">2</span>,<span class="pl-c1">1</span>,<span class="pl-c1">1</span>]))
<span class="pl-c"><span class="pl-c">#</span> 1.375</span></pre></div>
<p>We do not restrict the targets and outputs to be vectors, but
instead allow them to be arrays of any arbitrary shape. The shape
of an array may or may not have an interpretation that is
relevant for computing the loss. It is possible to explicitly
specify which dimension denotes the observations. This is
particularly useful for multivariate regression where one could
want to accumulate the loss per individual observation.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> A <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)
<span class="pl-c"><span class="pl-c">#</span> 2×3 Array{Float64,2}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.0939946  0.97639   0.568107</span>
<span class="pl-c"><span class="pl-c">#</span>  0.183244   0.854832  0.962534</span>

julia<span class="pl-k">&gt;</span> B <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">2</span>,<span class="pl-c1">3</span>)
<span class="pl-c"><span class="pl-c">#</span> 2×3 Array{Float64,2}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.0538206  0.77055  0.996922</span>
<span class="pl-c"><span class="pl-c">#</span>  0.598317   0.72043  0.912274</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), A, B, AvgMode<span class="pl-k">.</span><span class="pl-c1">Sum</span>())
<span class="pl-c"><span class="pl-c">#</span> 0.420741920634</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), A, B, AvgMode<span class="pl-k">.</span><span class="pl-c1">Sum</span>(), ObsDim<span class="pl-k">.</span><span class="pl-c1">First</span>())
<span class="pl-c"><span class="pl-c">#</span> 2-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.227866</span>
<span class="pl-c"><span class="pl-c">#</span>  0.192876</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">value</span>(<span class="pl-c1">L2DistLoss</span>(), A, B, AvgMode<span class="pl-k">.</span><span class="pl-c1">Sum</span>(), ObsDim<span class="pl-k">.</span><span class="pl-c1">Last</span>())
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.1739</span>
<span class="pl-c"><span class="pl-c">#</span>  0.060434</span>
<span class="pl-c"><span class="pl-c">#</span>  0.186408</span></pre></div>
<p>All these function signatures of <code>value</code> also apply for computing
the derivatives using <code>deriv</code> and the second derivatives using
<code>deriv2</code>.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">deriv</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  -1.0</span>
<span class="pl-c"><span class="pl-c">#</span>   4.0</span>
<span class="pl-c"><span class="pl-c">#</span>   2.0</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">deriv2</span>(<span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  2.0</span>
<span class="pl-c"><span class="pl-c">#</span>  2.0</span>
<span class="pl-c"><span class="pl-c">#</span>  2.0</span></pre></div>
<p>For computing the first and second derivatives we additionally
expose a convenience syntax which allows for a more math-like
look of the code.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> loss <span class="pl-k">=</span> <span class="pl-c1">L2DistLoss</span>()
<span class="pl-c"><span class="pl-c">#</span> LossFunctions.LPDistLoss{2}()</span>

julia<span class="pl-k">&gt;</span> loss<span class="pl-k">'</span>(true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  -1.0</span>
<span class="pl-c"><span class="pl-c">#</span>   4.0</span>
<span class="pl-c"><span class="pl-c">#</span>   2.0</span>

julia<span class="pl-k">&gt;</span> loss<span class="pl-k">''</span>(true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  2.0</span>
<span class="pl-c"><span class="pl-c">#</span>  2.0</span>
<span class="pl-c"><span class="pl-c">#</span>  2.0</span></pre></div>
<p>Additionally, we provide mutating versions for the subset of
methods that return an array. These have the same function
signatures with the only difference of requiring an additional
parameter as the first argument. This variable should always be
the preallocated array that is to be used as storage.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> buffer <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(<span class="pl-c1">3</span>)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  0.0</span>
<span class="pl-c"><span class="pl-c">#</span>  0.0</span>
<span class="pl-c"><span class="pl-c">#</span>  0.0</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">deriv!</span>(buffer, <span class="pl-c1">L2DistLoss</span>(), true_targets, pred_outputs)
<span class="pl-c"><span class="pl-c">#</span> 3-element Array{Float64,1}:</span>
<span class="pl-c"><span class="pl-c">#</span>  -1.0</span>
<span class="pl-c"><span class="pl-c">#</span>   4.0</span>
<span class="pl-c"><span class="pl-c">#</span>   2.0</span></pre></div>
<p>Note that this only shows a small part of the functionality this
package provides. For more information please have a look at
the documentation.</p>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Documentation</h2>
<p>Check out the <strong><a href="https://JuliaML.github.io/LossFunctions.jl/stable" rel="nofollow">latest documentation</a></strong></p>
<p>Additionally, you can make use of Julia's native docsystem.
The following example shows how to get additional information
on <code>HingeLoss</code> within Julia's REPL:</p>
<div class="highlight highlight-source-julia"><pre>?HingeLoss</pre></div>
<pre><code>search: HingeLoss L2HingeLoss L1HingeLoss SmoothedL1HingeLoss

  L1HingeLoss &lt;: MarginLoss

  The hinge loss linearly penalizes every predicition where the
  resulting agreement a = y⋅ŷ &lt; 1 . It is Lipschitz continuous
  and convex, but not strictly convex.

  L(a) = \max \{ 0, 1 - a \}

  --------------------------------------------------------------------

                Lossfunction                     Derivative
        ┌────────────┬────────────┐      ┌────────────┬────────────┐
      3 │'\.                      │    0 │                  ┌------│
        │  ''_                    │      │                  |      │
        │     \.                  │      │                  |      │
        │       '.                │      │                  |      │
      L │         ''_             │   L' │                  |      │
        │            \.           │      │                  |      │
        │              '.         │      │                  |      │
      0 │                ''_______│   -1 │------------------┘      │
        └────────────┴────────────┘      └────────────┴────────────┘
        -2                        2      -2                        2
                   y ⋅ ŷ                            y ⋅ ŷ
</code></pre>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>This package is registered in <code>METADATA.jl</code> and can be installed
as usual</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">import</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>LossFunctions<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>License</h2>
<p>This code is free to use under the terms of the MIT license.</p>
</article></div>