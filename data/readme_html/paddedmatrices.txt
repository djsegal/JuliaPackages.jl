<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-paddedmatrices" class="anchor" aria-hidden="true" href="#paddedmatrices"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PaddedMatrices</h1>
<p><a href="https://chriselrod.github.io/PaddedMatrices.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://chriselrod.github.io/PaddedMatrices.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/57bae07ecd50a99519ad0516d91f4ec8f0f48e12/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="Latest" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.com/chriselrod/PaddedMatrices.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/8a19822c6d57b156cb66414e469093fa2b8ef2a1/68747470733a2f2f7472617669732d63692e636f6d2f6368726973656c726f642f5061646465644d617472696365732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/chriselrod/PaddedMatrices.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/chriselrod/PaddedMatrices-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/63780ba7497719a70d560d5d8a995e0b0a379b29/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f6368726973656c726f642f5061646465644d617472696365732e6a6c3f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/chriselrod/PaddedMatrices.jl?svg=true" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/chriselrod/PaddedMatrices.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/1aef99954945d230b74fbfb84920682bc77824a5/68747470733a2f2f636f6465636f762e696f2f67682f6368726973656c726f642f5061646465644d617472696365732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/chriselrod/PaddedMatrices.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h1><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h1>
<p>This library provides a few array types, as well as pure-Julia matrix multiplication.</p>
<p>The native types are optionally statically sized, and optionally given padding to ensure that all columns are aligned. The following chart shows benchmarks on a 10980XE CPU, comparing:</p>
<ul>
<li><code>SMatrix</code> and <code>MMatrix</code> multiplication from <a href="https://github.com/JuliaArrays/StaticArrays.jl">StaticArrays.jl</a>.</li>
<li><code>FixedSizeArray</code> from this library without any padding.</li>
<li><code>FixedSizeArray</code> from this library with padding, named <code>PaddedArray</code> in the legend.</li>
<li>The base <code>Matrix{Float64}</code> type, using the <code>PaddedMatrices.jmul!</code> method.</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="docs/src/assets/sizedarraybenchmarks.svg"><img src="docs/src/assets/sizedarraybenchmarks.svg" alt="SizedArrayBenchmarks" style="max-width:100%;"></a></p>
<p>All matrices were square and filled with <code>Float64</code> elements. Size refers to the number of rows and columns.
Inplace multiplication was used for all but the <code>SArray</code>. For the <code>FixedSizeArray</code>s, <code>LinearAlgebra.mul!</code> simple redirects to <code>jmul!</code>, which is capable of taking advantage of static size information.</p>
<p><code>StaticArray</code>s currently relies on unrolling the operations, and taking advantage of LLVM's powerful <a href="https://llvm.org/docs/Vectorizers.html#the-slp-vectorizer" rel="nofollow">SLP vectorizer</a>. It performs best for <code>2x2</code>, <code>3x3</code>, <code>4x4</code>, and <code>8x8</code> matrices on this architecture. Between stack allocation and marking these operations for inline, <code>SMatrix</code> achieves better performance than the alternatives at these sizes.</p>
<p>PaddedMatrices relies on <a href="https://github.com/chriselrod/LoopVectorization.jl">LoopVectorization.jl</a> to generate microkernels. Perhaps I should make it unroll more agressively at small static sizes, and also mark it for inlining. For now, it doesn't achieve quite the same performance as an <code>SMatrix</code> at <code>8x8</code>, <code>6x6</code>, and <code>4x4</code> and below. However, at <code>9x9</code> and beyond, even the dynamically sized <code>PaddedMatrices.jmul!</code> method achieves better performance than <code>SMatrix</code> or <code>MMatrix</code>. Of course, <code>StaticArrays</code> is primarily concerned with <code>10x10</code> matrices and smaller, and the <code>SMatrix</code> type allows you to use the more convenient non-mutating API without worrying about allocations or memory management.</p>
<p>How does <code>jmul!</code> compare with OpenBLAS and MKL at larger sizes? Single threaded <code>Float64</code> benchmarks:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmf64.svg"><img src="docs/src/assets/gemmf64.svg" alt="dgemmbenchmarks" style="max-width:100%;"></a>
It's slower than both OpenBLAS and MKL, but (on this architecture) it's closer to MKL than MKL is to OpenBLAS at large sizes. I also added <a href="https://github.com/MasonProtter/Gaius.jl">Gaius.jl</a> for comparison. It also uses LoopVectorization to generate the microkernels, but uses divide and conquer to improve cache locality, rather than tiling and packing like the others. The divide and conquer approach yields much better performance than not handling cache locality; I may add a naive implementation for comparison for purposes of comparison eventually, but Julia's generic matmul -- which still makes some effort for cache optimality -- could give some perspective in the integer benchmarks below.</p>
<p>But before moving onto integers, <code>Float32</code> benchmarks:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmf32.svg"><img src="docs/src/assets/gemmf32.svg" alt="sgemmbenchmarks" style="max-width:100%;"></a>
Both BLAS libraries again beat <code>jmul!</code>. OpenBLAS and MKL are now neck and neck.</p>
<p>The BLAS libraries do not support integer multiplication, so the comparison is now with Julia's <a href="https://github.com/JuliaLang/julia/blob/b1f51df1088b2ab4e1c954537fd8c22b9b5f19ac/stdlib/LinearAlgebra/src/matmul.jl#L730">generic matmul</a>; <code>Int64</code>:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmi64.svg"><img src="docs/src/assets/gemmi64.svg" alt="i64gemmbenchmarks" style="max-width:100%;"></a>
64-bit integer multiplication is very slow on most platforms. With AVX2, it is implemented with repeated 32-bit integer multiplications, shifts, and additions (<code>(a + b)*(c + d) = ad + bc + bd</code>; you can drop the <code>ac</code> because it overflows). With AVX512 (like the benchmark rig), it uses the <code>vpmullq</code> instruction, which is slow.</p>
<p><a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmi32.svg"><img src="docs/src/assets/gemmi32.svg" alt="i32gemmbenchmarks" style="max-width:100%;"></a>
<code>Int32</code> multiplication is much faster, but still lags behind <code>Float64</code> performance.
For some reason, the generic matmul is slower for <code>Int32</code>; I have not investigated why.</p>
<p>There is also a threaded <code>PaddedMatrices.jmult!</code>, however it is not well optimized. It currently naively spawns a new task for each packed block of <code>A</code> and <code>B</code>. When <code>A</code> and <code>B</code> aren't large, this leads to too few tasks for much parallelism. When they are large, the number of tasks is excessive.</p>
<p>Additionally, the library uses <a href="https://github.com/chriselrod/VectorizedRNG.jl">VectorizedRNG.jl</a> for random number generation.</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> PaddedMatrices, StaticArrays, BenchmarkTools

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@SMatrix</span> <span class="pl-c1">rand</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">95.751</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">96.082</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">96.325</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">147.361</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">977</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@FixedSize</span> <span class="pl-c1">rand</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">624</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">1</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">33.949</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">42.572</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">50.327</span> ns (<span class="pl-c1">13.49</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">805.577</span> ns (<span class="pl-c1">79.77</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">994</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@SMatrix</span> <span class="pl-c1">randn</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">261.055</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">268.551</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">268.758</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">384.105</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">343</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@FixedSize</span> <span class="pl-c1">randn</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">624</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">1</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">97.017</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">102.932</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">111.741</span> ns (<span class="pl-c1">6.15</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">935.145</span> ns (<span class="pl-c1">79.35</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">950</span></pre></div>
<p>and it uses <a href="https://github.com/chriselrod/LoopVectorization.jl">LoopVectorization.jl</a> for broadcasts:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> PaddedMatrices, StaticArrays, BenchmarkTools

julia<span class="pl-k">&gt;</span> Afs <span class="pl-k">=</span> <span class="pl-c1">@FixedSize</span> <span class="pl-c1">randn</span>(<span class="pl-c1">13</span>,<span class="pl-c1">29</span>); Asm <span class="pl-k">=</span> <span class="pl-c1">SMatrix</span><span class="pl-c1">{13,29}</span>(<span class="pl-c1">Array</span>(Afs));

julia<span class="pl-k">&gt;</span> bfs <span class="pl-k">=</span> <span class="pl-c1">@FixedSize</span> <span class="pl-c1">rand</span>(<span class="pl-c1">13</span>); bsv <span class="pl-k">=</span> <span class="pl-c1">SVector</span><span class="pl-c1">{13}</span>(bfs);

julia<span class="pl-k">&gt;</span> cfs <span class="pl-k">=</span> <span class="pl-c1">@FixedSize</span> <span class="pl-c1">rand</span>(<span class="pl-c1">29</span>); csv <span class="pl-k">=</span> <span class="pl-c1">SVector</span><span class="pl-c1">{29}</span>(cfs);

julia<span class="pl-k">&gt;</span> Dfs <span class="pl-k">=</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(Afs) <span class="pl-k">+</span> bfs <span class="pl-k">*</span> <span class="pl-c1">log</span>(cfs<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> Dfs <span class="pl-k">≈</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(Asm) <span class="pl-k">+</span> bsv <span class="pl-k">*</span> <span class="pl-c1">log</span>(csv<span class="pl-k">'</span>)
<span class="pl-c1">true</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(<span class="pl-k">$</span>Afs) <span class="pl-k">+</span> <span class="pl-k">$</span>bfs <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-k">$</span>cfs<span class="pl-k">'</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">4.06</span> KiB
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">1</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">645.871</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">703.147</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">790.847</span> ns (<span class="pl-c1">10.55</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">11.067</span> μs (<span class="pl-c1">84.82</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">170</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(<span class="pl-k">$</span>Asm) <span class="pl-k">+</span> <span class="pl-k">$</span>bsv <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-k">$</span>csv<span class="pl-k">'</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">3.620</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">3.658</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">3.669</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">6.189</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">8</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@.</span> <span class="pl-k">$</span>Dfs <span class="pl-k">=</span> <span class="pl-c1">exp</span>(<span class="pl-k">$</span>Afs) <span class="pl-k">+</span> <span class="pl-k">$</span>bfs <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-k">$</span>cfs<span class="pl-k">'</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">461.500</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">462.199</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">462.732</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">599.357</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">196</span></pre></div>
</article></div>