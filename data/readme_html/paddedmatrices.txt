<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-paddedmatricesjl" class="anchor" aria-hidden="true" href="#paddedmatricesjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PaddedMatrices.jl</h1>
<h1><a id="user-content-️-deprecated-paddedmatricesjl-is-now-deprecated-in-favor-of-stridearraysjl" class="anchor" aria-hidden="true" href="#️-deprecated-paddedmatricesjl-is-now-deprecated-in-favor-of-stridearraysjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> DEPRECATED: PaddedMatrices.jl is now deprecated in favor of <a href="https://github.com/chriselrod/StrideArrays.jl">StrideArrays.jl</a></h1>
<p><a href="https://chriselrod.github.io/PaddedMatrices.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://chriselrod.github.io/PaddedMatrices.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="Latest" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width:100%;"></a>
<a href="https://github.com/chriselrod/PaddedMatrices.jl/actions?query=workflow%3ACI"><img src="https://github.com/chriselrod/PaddedMatrices.jl/workflows/CI/badge.svg" alt="CI" style="max-width:100%;"></a>
<a href="https://github.com/chriselrod/PaddedMatrices.jl/actions?query=workflow%3A%22CI+%28Julia+nightly%29%22"><img src="https://github.com/chriselrod/PaddedMatrices.jl/workflows/CI%20(Julia%20nightly)/badge.svg" alt="CI (Julia nightly)" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/chriselrod/PaddedMatrices.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/1f552359621efab187732e063b0a70ae3f2cf77fc2731b3e101e765a28e345d4/68747470733a2f2f636f6465636f762e696f2f67682f6368726973656c726f642f5061646465644d617472696365732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/chriselrod/PaddedMatrices.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick Start</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using PaddedMatrices

julia&gt; matmul!(C, A, B) # (multi-threaded) multiply A×B and store the result in C (overwriting the contents of C)

julia&gt; matmul_serial!(C, A, B) # (single-threaded) multiply A×B and store the result in C (overwriting the contents of C)

julia&gt; matmul(A, B) # (multi-threaded) multiply A×B and return the result

julia&gt; matmul_serial(A, B) # (single-threaded) multiply A×B and return the result
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> PaddedMatrices

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matmul!</span>(C, A, B) <span class="pl-c"><span class="pl-c">#</span> (multi-threaded) multiply A×B and store the result in C (overwriting the contents of C)</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matmul_serial!</span>(C, A, B) <span class="pl-c"><span class="pl-c">#</span> (single-threaded) multiply A×B and store the result in C (overwriting the contents of C)</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matmul</span>(A, B) <span class="pl-c"><span class="pl-c">#</span> (multi-threaded) multiply A×B and return the result</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">matmul_serial</span>(A, B) <span class="pl-c"><span class="pl-c">#</span> (single-threaded) multiply A×B and return the result</span></pre></div>
<p>If you want to use the multi-threaded functions, you need to start Julia with multiple threads.
PaddedMatrices will use a maximum of <code>min(VectorizationBase.NUM_CORES, Threads.nthreads() - 1)</code> threads.
Therefore, if you have a system with <code>N</code> cores, you should start Julia with <code>N + 1</code> threads.
For example, if you have a 8 core system, you should start Julia with 9 threads.
To start Julia with 9 threads, either:</p>
<ul>
<li>Start Julia with <code>julia -t 9</code></li>
<li>Set the <code>JULIA_NUM_THREADS</code> environment variable to <code>9</code> <strong>before</strong> starting Julia</li>
</ul>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p>This library provides a few array types, as well as pure-Julia matrix multiplication.</p>
<p>The native types are optionally statically sized, and optionally given padding (the default) to ensure that all columns are aligned. The following chart shows single-threaded benchmarks on a few different CPUS, comparing:</p>
<ul>
<li><code>SMatrix</code> and <code>MMatrix</code> multiplication from <a href="https://github.com/JuliaArrays/StaticArrays.jl">StaticArrays.jl</a>. Beyond <code>14</code>x<code>14</code>x<code>14</code>, MMatrix will switch to using <code>LinearAlgebra.BLAS.gemm!</code>.</li>
<li><code>StrideArray</code>, with compile-time known sizes, and an unsafe <code>PtrArray</code> that once upon a time had a lower constant overhead, but that problem seems to have been solved on Julia 1.6 (the Cascadelake-AVX512 benchmarks, below).</li>
<li>The base <code>Matrix{Float64}</code> type, using the pure-Julia <code>matmul!</code> method.</li>
</ul>
<p>All matrices were square; the <code>x</code>-axis reports size of each dimension. Benchmarks ranged from <code>2</code>x<code>2</code> matrices through <code>48</code>x<code>48</code>. The <code>y</code>-axis reports double-precision GFLOPS. That is billions of double precision floating point operations per second. Higher is better.</p>
<p>10980XE, a Cascadelake-X CPU with AVX512:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/sizedarraybenchmarks_cascadelake_AVX512.svg"><img src="docs/src/assets/sizedarraybenchmarks_cascadelake_AVX512.svg" alt="Cascadelake-X SizedArrayBenchmarks" style="max-width:100%;"></a>
i7 1165G7, a Tigerlake laptop CPU with AVX512:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/sizedarraybenchmarks_tigerlake_AVX512.svg"><img src="docs/src/assets/sizedarraybenchmarks_tigerlake_AVX512.svg" alt="Skylake SizedArrayBenchmarks" style="max-width:100%;"></a>
i3-4010U, a Haswell CPU with AVX2:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/sizedarraybenchmarksAVX2_haswell.svg"><img src="docs/src/assets/sizedarraybenchmarksAVX2_haswell.svg" alt="Haswell SizedArrayBenchmarks" style="max-width:100%;"></a></p>
<p><code>MMatrix</code> performed much better beyond <code>14</code>x<code>14</code> relative to the others on Haswell because <code>LinearAlgebra.BLAS.gemm!</code> on that computer was using <code>MKL</code> instead of <code>OpenBLAS</code> (the easiest way to change this is using <a href="https://github.com/JuliaComputing/MKL.jl">MKL.jl</a>).</p>
<p><code>StaticArray</code>s currently relies on unrolling the operations, and taking advantage of LLVM's <a href="https://llvm.org/docs/Vectorizers.html#the-slp-vectorizer" rel="nofollow">SLP vectorizer</a>. This approach can work well for very small arrays, but scales poorly. With AVX2, dynamically-sized matrix multiplication of regular <code>Array{Float64,2}</code> arrays was faster starting from <code>7</code>x<code>7</code>, despite not being able to specialize on the size of the arrays, unlike the <code>SMatrix</code> and <code>MMatrix</code> versions. This also means that the method didn't have to recompile (in order to specialize) on the <code>7</code>x<code>7</code> <code>Matrix{Float64}</code>s.
With AVX512, the <code>SMatrix</code> method was faster than the dynamically sized method until the matrices were <code>9</code>x<code>9</code>, but quickly fell behind after this.
I did not benchmark <code>StaticArray</code>s larger than <code>20</code>x<code>20</code> on some platforms to speed up the benchmarks and skip the long compile times.</p>
<p>The size-specializing methods for <code>FixedSizeArray</code>s and <code>PtrArray</code>s matched <code>SMatrix</code>'s performance from the beginning, leaving the <code>SMatrix</code> method behind starting with <code>5</code>x<code>5</code> on the AVX2 systems, and <code>3</code>x<code>3</code> with AVX512.</p>
<p>PaddedMatrices relies on <a href="https://github.com/chriselrod/LoopVectorization.jl">LoopVectorization.jl</a> for code-generation.</p>
<p>One of the goals of PaddedMatrices.jl is to provide good performance across a range of practical sizes.</p>
<p>How does the dynamic <code>matmul!</code> compare with OpenBLAS and MKL at larger sizes? Below are more single-threaded <code>Float64</code> benchmarks on the 10980XE. Size range from <code>2</code>x<code>2</code> through <code>256</code>x<code>256</code>:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmFloat64_2_256_cascadelake_AVX512.svg"><img src="docs/src/assets/gemmFloat64_2_256_cascadelake_AVX512.svg" alt="dgemmbenchmarkssmall" style="max-width:100%;"></a>
Tigerlake laptop (same as earlier):
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmFloat64_2_256_tigerlake_AVX512.svg"><img src="docs/src/assets/gemmFloat64_2_256_tigerlake_AVX512.svg" alt="dgemmbenchmarkssmall" style="max-width:100%;"></a></p>
<p>Performance is quite strong over this size range, especially compared the the default OpenBLAS, which does not adaptively change packing strategy as a function of size.</p>
<p>Extending the benchmarks from <code>256</code>x<code>256</code> through <code>2000</code>x<code>2000</code>, we see that performance does start to fall behind after a few hundred:
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmFloat64_256_2000_cascadelake_AVX512.svg"><img src="docs/src/assets/gemmFloat64_256_2000_cascadelake_AVX512.svg" alt="dgemmbenchmarksmedium" style="max-width:100%;"></a>
<a target="_blank" rel="noopener noreferrer" href="docs/src/assets/gemmFloat64_256_2000_tigerlake_AVX512.svg"><img src="docs/src/assets/gemmFloat64_256_2000_tigerlake_AVX512.svg" alt="dgemmbenchmarksmedium" style="max-width:100%;"></a></p>
<p>Performance still needs work. In particular</p>
<ol>
<li>Tuning of blocking parameters at larger sizes</li>
<li>Possibly switching packed arrays from column to panel-major storage.</li>
<li>Better prefetching.</li>
<li>Diagnosing and fixing the cause of erratic performance.</li>
</ol>
<p>As an illustration of the last point, consider multiplication of <code>71</code>x<code>71</code> matrices. Setup:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using PaddedMatrices, LinuxPerf

julia&gt; M = K = N = 71;

julia&gt; A = rand(M,K); B = rand(K,N); C2 = @time(A * B); C1 = similar(C2);
  0.000093 seconds (2 allocations: 39.516 KiB)

julia&gt; @time(matmul!(C1,A,B)) ≈ C1 # time to first matmul
  9.937127 seconds (21.34 M allocations: 1.234 GiB, 2.46% gc time)
true

julia&gt; foreachmklmul!(C, A, B, N) = foreach(_ -&gt; matmul!(C, A, B), Base.OneTo(N))
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> PaddedMatrices, LinuxPerf

julia<span class="pl-k">&gt;</span> M <span class="pl-k">=</span> K <span class="pl-k">=</span> N <span class="pl-k">=</span> <span class="pl-c1">71</span>;

julia<span class="pl-k">&gt;</span> A <span class="pl-k">=</span> <span class="pl-c1">rand</span>(M,K); B <span class="pl-k">=</span> <span class="pl-c1">rand</span>(K,N); C2 <span class="pl-k">=</span> <span class="pl-c1">@time</span>(A <span class="pl-k">*</span> B); C1 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(C2);
  <span class="pl-c1">0.000093</span> seconds (<span class="pl-c1">2</span> allocations<span class="pl-k">:</span> <span class="pl-c1">39.516</span> KiB)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@time</span>(<span class="pl-c1">matmul!</span>(C1,A,B)) <span class="pl-k">≈</span> C1 <span class="pl-c"><span class="pl-c">#</span> time to first matmul</span>
  <span class="pl-c1">9.937127</span> seconds (<span class="pl-c1">21.34</span> M allocations<span class="pl-k">:</span> <span class="pl-c1">1.234</span> GiB, <span class="pl-c1">2.46</span><span class="pl-k">%</span> gc time)
<span class="pl-c1">true</span>

julia<span class="pl-k">&gt;</span> <span class="pl-en">foreachmklmul!</span>(C, A, B, N) <span class="pl-k">=</span> <span class="pl-c1">foreach</span>(_ <span class="pl-k">-&gt;</span> <span class="pl-c1">matmul!</span>(C, A, B), Base<span class="pl-k">.</span><span class="pl-c1">OneTo</span>(N))</pre></div>
<p>And then sample <code>@pstats</code> results:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="julia&gt; @pstats &quot;cpu-cycles,(instructions,branch-instructions,branch-misses),(task-clock,context-switches,cpu-migrations,page-faults),(L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses),(dTLB-load-misses,dTLB-loads),(iTLB-load-misses,iTLB-loads)&quot; (@time foreachmul!(C2, A, B, 1_000_000))
  6.701603 seconds
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╶ cpu-cycles               2.74e+10   42.9%  #  4.1 cycles per ns
┌ instructions             7.96e+10   28.6%  #  2.9 insns per cycle
│ branch-instructions      1.80e+09   28.6%  #  2.3% of instructions
└ branch-misses            3.39e+06   28.6%  #  0.2% of branch instructions
┌ task-clock               6.70e+09  100.0%
│ context-switches         9.00e+00  100.0%
│ cpu-migrations           0.00e+00  100.0%
└ page-faults              0.00e+00  100.0%
┌ L1-dcache-load-misses    6.78e+09   14.3%  # 26.4% of loads
│ L1-dcache-loads          2.57e+10   14.3%
└ L1-icache-load-misses    3.08e+06   14.3%
┌ dTLB-load-misses         3.43e+02   14.3%
└ dTLB-loads               2.56e+10   14.3%
┌ iTLB-load-misses         1.03e+05   28.6%
└ iTLB-loads               3.16e+04   28.6%
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

julia&gt; @pstats &quot;cpu-cycles,(instructions,branch-instructions,branch-misses),(task-clock,context-switches,cpu-migrations,page-faults),(L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses),(dTLB-load-misses,dTLB-loads),(iTLB-load-misses,iTLB-loads)&quot; (@time foreachmul!(C2, A, B, 1_000_000))
  7.222783 seconds
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╶ cpu-cycles               2.95e+10   42.9%  #  4.1 cycles per ns
┌ instructions             7.96e+10   28.6%  #  2.7 insns per cycle
│ branch-instructions      1.80e+09   28.6%  #  2.3% of instructions
└ branch-misses            3.50e+06   28.6%  #  0.2% of branch instructions
┌ task-clock               7.22e+09  100.0%
│ context-switches         1.00e+01  100.0%
│ cpu-migrations           0.00e+00  100.0%
└ page-faults              0.00e+00  100.0%
┌ L1-dcache-load-misses    6.81e+09   14.3%  # 26.6% of loads
│ L1-dcache-loads          2.57e+10   14.3%
└ L1-icache-load-misses    3.38e+06   14.3%
┌ dTLB-load-misses         2.10e+02   14.3%
└ dTLB-loads               2.57e+10   14.3%
┌ iTLB-load-misses         1.12e+05   28.6%
└ iTLB-loads               3.60e+04   28.6%
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
"><pre><code>julia&gt; @pstats "cpu-cycles,(instructions,branch-instructions,branch-misses),(task-clock,context-switches,cpu-migrations,page-faults),(L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses),(dTLB-load-misses,dTLB-loads),(iTLB-load-misses,iTLB-loads)" (@time foreachmul!(C2, A, B, 1_000_000))
  6.701603 seconds
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╶ cpu-cycles               2.74e+10   42.9%  #  4.1 cycles per ns
┌ instructions             7.96e+10   28.6%  #  2.9 insns per cycle
│ branch-instructions      1.80e+09   28.6%  #  2.3% of instructions
└ branch-misses            3.39e+06   28.6%  #  0.2% of branch instructions
┌ task-clock               6.70e+09  100.0%
│ context-switches         9.00e+00  100.0%
│ cpu-migrations           0.00e+00  100.0%
└ page-faults              0.00e+00  100.0%
┌ L1-dcache-load-misses    6.78e+09   14.3%  # 26.4% of loads
│ L1-dcache-loads          2.57e+10   14.3%
└ L1-icache-load-misses    3.08e+06   14.3%
┌ dTLB-load-misses         3.43e+02   14.3%
└ dTLB-loads               2.56e+10   14.3%
┌ iTLB-load-misses         1.03e+05   28.6%
└ iTLB-loads               3.16e+04   28.6%
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

julia&gt; @pstats "cpu-cycles,(instructions,branch-instructions,branch-misses),(task-clock,context-switches,cpu-migrations,page-faults),(L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses),(dTLB-load-misses,dTLB-loads),(iTLB-load-misses,iTLB-loads)" (@time foreachmul!(C2, A, B, 1_000_000))
  7.222783 seconds
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
╶ cpu-cycles               2.95e+10   42.9%  #  4.1 cycles per ns
┌ instructions             7.96e+10   28.6%  #  2.7 insns per cycle
│ branch-instructions      1.80e+09   28.6%  #  2.3% of instructions
└ branch-misses            3.50e+06   28.6%  #  0.2% of branch instructions
┌ task-clock               7.22e+09  100.0%
│ context-switches         1.00e+01  100.0%
│ cpu-migrations           0.00e+00  100.0%
└ page-faults              0.00e+00  100.0%
┌ L1-dcache-load-misses    6.81e+09   14.3%  # 26.6% of loads
│ L1-dcache-loads          2.57e+10   14.3%
└ L1-icache-load-misses    3.38e+06   14.3%
┌ dTLB-load-misses         2.10e+02   14.3%
└ dTLB-loads               2.57e+10   14.3%
┌ iTLB-load-misses         1.12e+05   28.6%
└ iTLB-loads               3.60e+04   28.6%
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
</code></pre></div>
<p>The difference in <code>L1-dcache-load-misses</code> looks relatively minor between these runs, yet we see nearly an 8% difference in performance and instructions per clock cycle. The first time corresponds to a mean of nearly 107 GFLOPS across the 1 million matrix multiplications, while the second corresponds to less than 100. I'm still investigating the cause. I do not see such erratic performance with MKL, for example.</p>
<p>Additionally, the library uses <a href="https://github.com/chriselrod/VectorizedRNG.jl">VectorizedRNG.jl</a> for random number generation. While we may pay the price of the <code>GC</code> vs <code>StaticArrays.SMatrix</code>, we still actually end up faster:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using PaddedMatrices, StaticArrays, BenchmarkTools

julia&gt; @benchmark @SMatrix rand(8,8)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     72.999 ns (0.00% GC)
  median time:      73.078 ns (0.00% GC)
  mean time:        76.228 ns (0.00% GC)
  maximum time:     139.795 ns (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     973

julia&gt; @benchmark @StrideArray rand(8,8)
BenchmarkTools.Trial:
  memory estimate:  544 bytes
  allocs estimate:  1
  --------------
  minimum time:     23.592 ns (0.00% GC)
  median time:      46.570 ns (0.00% GC)
  mean time:        54.569 ns (9.42% GC)
  maximum time:     711.433 ns (91.35% GC)
  --------------
  samples:          10000
  evals/sample:     993

julia&gt; @benchmark @SMatrix randn(8,8)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     176.371 ns (0.00% GC)
  median time:      187.033 ns (0.00% GC)
  mean time:        191.685 ns (0.00% GC)
  maximum time:     370.407 ns (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     728

julia&gt; @benchmark @StrideArray randn(8,8)
BenchmarkTools.Trial:
  memory estimate:  544 bytes
  allocs estimate:  1
  --------------
  minimum time:     58.704 ns (0.00% GC)
  median time:      68.225 ns (0.00% GC)
  mean time:        77.303 ns (8.01% GC)
  maximum time:     816.444 ns (89.50% GC)
  --------------
  samples:          10000
  evals/sample:     982
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> PaddedMatrices, StaticArrays, BenchmarkTools

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@SMatrix</span> <span class="pl-c1">rand</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">72.999</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">73.078</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">76.228</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">139.795</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">973</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@StrideArray</span> <span class="pl-c1">rand</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">544</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">1</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">23.592</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">46.570</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">54.569</span> ns (<span class="pl-c1">9.42</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">711.433</span> ns (<span class="pl-c1">91.35</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">993</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@SMatrix</span> <span class="pl-c1">randn</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">176.371</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">187.033</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">191.685</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">370.407</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">728</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@StrideArray</span> <span class="pl-c1">randn</span>(<span class="pl-c1">8</span>,<span class="pl-c1">8</span>)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">544</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">1</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">58.704</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">68.225</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">77.303</span> ns (<span class="pl-c1">8.01</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">816.444</span> ns (<span class="pl-c1">89.50</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">982</span></pre></div>
<p>Thus, if you're generating a lot of random matrices, you're actually better off (performance-wise) biting the GC-bullet!</p>
<p>Of course, you can still preallocate for even better performance:.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; A = StrideArray{Float64}(undef, (StaticInt(8),StaticInt(8)));

julia&gt; @benchmark rand!($A)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     11.436 ns (0.00% GC)
  median time:      11.645 ns (0.00% GC)
  mean time:        12.382 ns (0.00% GC)
  maximum time:     72.118 ns (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     999

julia&gt; @benchmark randn!($A)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     55.393 ns (0.00% GC)
  median time:      63.583 ns (0.00% GC)
  mean time:        64.500 ns (0.00% GC)
  maximum time:     464.260 ns (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     984
"><pre>julia<span class="pl-k">&gt;</span> A <span class="pl-k">=</span> <span class="pl-c1">StrideArray</span><span class="pl-c1">{Float64}</span>(undef, (<span class="pl-c1">StaticInt</span>(<span class="pl-c1">8</span>),<span class="pl-c1">StaticInt</span>(<span class="pl-c1">8</span>)));

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">rand!</span>(<span class="pl-k">$</span>A)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">11.436</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">11.645</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">12.382</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">72.118</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">999</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">randn!</span>(<span class="pl-k">$</span>A)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">55.393</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">63.583</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">64.500</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">464.260</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">984</span></pre></div>
<p>By preallocating arrays, you can gain additional performance.</p>
<p>In the future, I'll try to ensure that a large number of basic functions and operations (e.g. matrix multiplication, broadcasting, creation) do not force heap-allocation even when they do not inline, to make it easier to write non-allocating code with these mutable arrays.</p>
<p>These arrays also use <a href="https://github.com/chriselrod/LoopVectorization.jl">LoopVectorization.jl</a> for broadcasts:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using PaddedMatrices, StaticArrays, BenchmarkTools

julia&gt; Afs = @StrideArray randn(13,29); Asm = SMatrix{13,29}(Array(Afs));

julia&gt; bfs = @StrideArray rand(13); bsv = SVector{13}(bfs);

julia&gt; cfs = @StrideArray rand(29); csv = SVector{29}(cfs);

julia&gt; Dfs = @. exp(Afs) + bfs * log(cfs');

julia&gt; Dfs ≈ @. exp(Asm) + bsv * log(csv')
true

julia&gt; @benchmark @. exp($Afs) + $bfs * log($cfs') # StrideArrays, allocating
BenchmarkTools.Trial:
  memory estimate:  3.12 KiB
  allocs estimate:  1
  --------------
  minimum time:     513.243 ns (0.00% GC)
  median time:      591.307 ns (0.00% GC)
  mean time:        774.968 ns (16.80% GC)
  maximum time:     23.365 μs (74.48% GC)
  --------------
  samples:          10000
  evals/sample:     189

julia&gt; @benchmark @. exp($Asm) + $bsv * log($csv') # StaticArrays, non-allocating but much slower
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     2.650 μs (0.00% GC)
  median time:      2.736 μs (0.00% GC)
  mean time:        2.881 μs (0.00% GC)
  maximum time:     13.431 μs (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     9

julia&gt; @benchmark @. $Dfs = exp($Afs) + $bfs * log($cfs') # StrideArrays, using pre-allocated output
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     344.828 ns (0.00% GC)
  median time:      386.982 ns (0.00% GC)
  mean time:        388.061 ns (0.00% GC)
  maximum time:     785.701 ns (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     221
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> PaddedMatrices, StaticArrays, BenchmarkTools

julia<span class="pl-k">&gt;</span> Afs <span class="pl-k">=</span> <span class="pl-c1">@StrideArray</span> <span class="pl-c1">randn</span>(<span class="pl-c1">13</span>,<span class="pl-c1">29</span>); Asm <span class="pl-k">=</span> <span class="pl-c1">SMatrix</span><span class="pl-c1">{13,29}</span>(<span class="pl-c1">Array</span>(Afs));

julia<span class="pl-k">&gt;</span> bfs <span class="pl-k">=</span> <span class="pl-c1">@StrideArray</span> <span class="pl-c1">rand</span>(<span class="pl-c1">13</span>); bsv <span class="pl-k">=</span> <span class="pl-c1">SVector</span><span class="pl-c1">{13}</span>(bfs);

julia<span class="pl-k">&gt;</span> cfs <span class="pl-k">=</span> <span class="pl-c1">@StrideArray</span> <span class="pl-c1">rand</span>(<span class="pl-c1">29</span>); csv <span class="pl-k">=</span> <span class="pl-c1">SVector</span><span class="pl-c1">{29}</span>(cfs);

julia<span class="pl-k">&gt;</span> Dfs <span class="pl-k">=</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(Afs) <span class="pl-k">+</span> bfs <span class="pl-k">*</span> <span class="pl-c1">log</span>(cfs<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> Dfs <span class="pl-k">≈</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(Asm) <span class="pl-k">+</span> bsv <span class="pl-k">*</span> <span class="pl-c1">log</span>(csv<span class="pl-k">'</span>)
<span class="pl-c1">true</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(<span class="pl-k">$</span>Afs) <span class="pl-k">+</span> <span class="pl-k">$</span>bfs <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-k">$</span>cfs<span class="pl-k">'</span>) <span class="pl-c"><span class="pl-c">#</span> StrideArrays, allocating</span>
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">3.12</span> KiB
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">1</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">513.243</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">591.307</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">774.968</span> ns (<span class="pl-c1">16.80</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">23.365</span> μs (<span class="pl-c1">74.48</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">189</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@.</span> <span class="pl-c1">exp</span>(<span class="pl-k">$</span>Asm) <span class="pl-k">+</span> <span class="pl-k">$</span>bsv <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-k">$</span>csv<span class="pl-k">'</span>) <span class="pl-c"><span class="pl-c">#</span> StaticArrays, non-allocating but much slower</span>
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">2.650</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">2.736</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">2.881</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">13.431</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">9</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">@.</span> <span class="pl-k">$</span>Dfs <span class="pl-k">=</span> <span class="pl-c1">exp</span>(<span class="pl-k">$</span>Afs) <span class="pl-k">+</span> <span class="pl-k">$</span>bfs <span class="pl-k">*</span> <span class="pl-c1">log</span>(<span class="pl-k">$</span>cfs<span class="pl-k">'</span>) <span class="pl-c"><span class="pl-c">#</span> StrideArrays, using pre-allocated output</span>
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">344.828</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">386.982</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">388.061</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">785.701</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">221</span></pre></div>
</article></div>