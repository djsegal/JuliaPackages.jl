<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-expectationmaximizationpcajl" class="anchor" aria-hidden="true" href="#expectationmaximizationpcajl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ExpectationMaximizationPCA.jl</h1>
<p dir="auto">ExpectationMaximizationPCA.jl (EMPCA) is a Julia rewrite of <a href="https://github.com/sbailey/empca">empca</a> which provides weighted Expectation Maximization PCA, an iterative method for solving PCA while properly weighting data.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">The most current, tagged version of <a href="https://github.com/christiangil/ExpectationMaximizationPCA.jl">ExpectationMaximizationPCA.jl</a> can be easily installed using Julia's Pkg</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="Pkg.add(&quot;ExpectationMaximizationPCA&quot;)"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>ExpectationMaximizationPCA<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">If you would like to contribute to the package, or just want to run the latest (untagged) version, you can use the following</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="Pkg.develop(&quot;ExpectationMaximizationPCA&quot;)"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">develop</span>(<span class="pl-s"><span class="pl-pds">"</span>ExpectationMaximizationPCA<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="import ExpectationMaximizationPCA as EMPCA

## making data
nx = 200  # dimensionality of observations
nt = 50  # number of observations
σ = ((((((1:nx) .- nx/2).^2) ./ (nx/2)^2) .+ 1)* ones(nt)') ./ 3 # noise, edges are twice as noisy as the center
data = rand(nt)' .* sin.(((1:nx) ./ nx) * 2π) + (0.2 .* rand(nt)') .* cos.(((1:nx) ./ nx) * 2π)  # a mixture of sin and cos signals
data .+= σ .* randn(size(data))  # add Gaussian noise

## performing EMPCA
nb = 2  # number of basis vectors
μ = vec(mean(data; dims=2))  # mean observation
weights = 1 ./ σ.^2  # use inverse variance as the weights
# weights = ones(size(data))  # uniform weights replicates PCA
basis_vecs, scores = EMPCA.EMPCA(μ, nb, data, weights)  # perform EMPCA  on `data` .- `μ` with `nb` basis vectors using `weights` for weighting"><pre><span class="pl-k">import</span> ExpectationMaximizationPCA <span class="pl-k">as</span> EMPCA

<span class="pl-c"><span class="pl-c">#</span># making data</span>
nx <span class="pl-k">=</span> <span class="pl-c1">200</span>  <span class="pl-c"><span class="pl-c">#</span> dimensionality of observations</span>
nt <span class="pl-k">=</span> <span class="pl-c1">50</span>  <span class="pl-c"><span class="pl-c">#</span> number of observations</span>
σ <span class="pl-k">=</span> ((((((<span class="pl-c1">1</span><span class="pl-k">:</span>nx) <span class="pl-k">.-</span> nx<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">.</span><span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">./</span> (nx<span class="pl-k">/</span><span class="pl-c1">2</span>)<span class="pl-k">^</span><span class="pl-c1">2</span>) <span class="pl-k">.+</span> <span class="pl-c1">1</span>)<span class="pl-k">*</span> <span class="pl-c1">ones</span>(nt)<span class="pl-k">'</span>) <span class="pl-k">./</span> <span class="pl-c1">3</span> <span class="pl-c"><span class="pl-c">#</span> noise, edges are twice as noisy as the center</span>
data <span class="pl-k">=</span> <span class="pl-c1">rand</span>(nt)<span class="pl-k">'</span> <span class="pl-k">.*</span> <span class="pl-c1">sin</span>.(((<span class="pl-c1">1</span><span class="pl-k">:</span>nx) <span class="pl-k">./</span> nx) <span class="pl-k">*</span> <span class="pl-c1">2</span>π) <span class="pl-k">+</span> (<span class="pl-c1">0.2</span> <span class="pl-k">.*</span> <span class="pl-c1">rand</span>(nt)<span class="pl-k">'</span>) <span class="pl-k">.*</span> <span class="pl-c1">cos</span>.(((<span class="pl-c1">1</span><span class="pl-k">:</span>nx) <span class="pl-k">./</span> nx) <span class="pl-k">*</span> <span class="pl-c1">2</span>π)  <span class="pl-c"><span class="pl-c">#</span> a mixture of sin and cos signals</span>
data <span class="pl-k">.+</span><span class="pl-k">=</span> σ <span class="pl-k">.*</span> <span class="pl-c1">randn</span>(<span class="pl-c1">size</span>(data))  <span class="pl-c"><span class="pl-c">#</span> add Gaussian noise</span>

<span class="pl-c"><span class="pl-c">#</span># performing EMPCA</span>
nb <span class="pl-k">=</span> <span class="pl-c1">2</span>  <span class="pl-c"><span class="pl-c">#</span> number of basis vectors</span>
μ <span class="pl-k">=</span> <span class="pl-c1">vec</span>(<span class="pl-c1">mean</span>(data; dims<span class="pl-k">=</span><span class="pl-c1">2</span>))  <span class="pl-c"><span class="pl-c">#</span> mean observation</span>
weights <span class="pl-k">=</span> <span class="pl-c1">1</span> <span class="pl-k">./</span> σ<span class="pl-k">.^</span><span class="pl-c1">2</span>  <span class="pl-c"><span class="pl-c">#</span> use inverse variance as the weights</span>
<span class="pl-c"><span class="pl-c">#</span> weights = ones(size(data))  # uniform weights replicates PCA</span>
basis_vecs, scores <span class="pl-k">=</span> EMPCA<span class="pl-k">.</span><span class="pl-c1">EMPCA</span>(μ, nb, data, weights)  <span class="pl-c"><span class="pl-c">#</span> perform EMPCA  on `data` .- `μ` with `nb` basis vectors using `weights` for weighting</span></pre></div>
<h2 dir="auto"><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Documentation</h2>
<p dir="auto">The documentation for this package is available <a href="https://christiangil.github.io/ExpectationMaximizationPCA.jl/" rel="nofollow">here</a>.</p>
<p dir="auto">The original python version can be found <a href="https://github.com/sbailey/empca">here</a>.</p>
<h2 dir="auto"><a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<p dir="auto">The paper S. Bailey 2012, PASP, 124, 1015 describes the underlying math
and is available as a pre-print at:
<a href="http://arxiv.org/abs/1208.4122" rel="nofollow">http://arxiv.org/abs/1208.4122</a></p>
<p dir="auto">If you use this code in an academic paper, please include a citation
as described in CITATION.txt, and optionally an acknowledgement such as:</p>
<blockquote>
<p dir="auto">This work uses the Weighted EMPCA code by Stephen Bailey,
available at <a href="https://github.com/sbailey/empca/">https://github.com/sbailey/empca/</a></p>
</blockquote>
</article></div>