<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h1>
<p><a href="https://travis-ci.org/ITensor/ITensors.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/9920ca36d545128d4b870602c1d6c37d39f0a797/68747470733a2f2f7472617669732d63692e6f72672f4954656e736f722f4954656e736f72732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/ITensor/ITensors.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://codecov.io/gh/ITensor/ITensors.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/df0593c8584cb01b0d28dce34c4f239284ebc143/68747470733a2f2f636f6465636f762e696f2f67682f4954656e736f722f4954656e736f72732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/ITensor/ITensors.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>PLEASE NOTE THIS IS PRE-RELEASE SOFTWARE FOR PREVIEW PURPOSES ONLY</p>
<p>THIS SOFTWARE IS SUBJECT TO BREAKING CHANGES AND NOT YET OFFICIALLY SUPPORTED</p>
<p>ITensors is a library for rapidly creating correct and efficient
tensor network algorithms.</p>
<p>An ITensor is a tensor whose interface
is independent of its memory layout. ITensor indices are
objects which carry extra information and which
'recognize' each other (compare equal to each other).</p>
<p>The ITensor library also includes composable and extensible
algorithms for optimizing and transforming tensor networks, such as
matrix product state and matrix product operators, such as
the DMRG algorithm.</p>
<p>Development of ITensor is supported by the Flatiron Institute, a division of the Simons Foundation.</p>
<h2><a id="user-content-steps-to-install-pre-release-version" class="anchor" aria-hidden="true" href="#steps-to-install-pre-release-version"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Steps to Install Pre-Release Version</h2>
<ol>
<li>
<p>Install the latest version of Julia: <a href="https://julialang.org/downloads/" rel="nofollow">https://julialang.org/downloads/</a></p>
</li>
<li>
<p>Run the <code>julia</code> command to begin an interactive Julia session (entering the so-called REPL).</p>
</li>
<li>
<p>Type <code>]</code> on your keyboard to enter Julia's interactive package manager.</p>
</li>
<li>
<p>Run the command</p>
<pre><code>add https://github.com/ITensor/ITensors.jl
</code></pre>
<p>The package system will update itself, then install some dependencies before finally installing ITensors.jl.</p>
</li>
<li>
<p>Hit the backspace key to go back to the normal interactive Julia prompt, or type Ctrl+D to exit the Julia REPL.</p>
</li>
<li>
<p>You can now do <code>using ITensors</code> to use the ITensor library in an interactive session, or run Julia code files (.jl files) which use ITensor, with some examples given below and in our examples folder. The test folder also has many examples of ITensor code you can run.</p>
</li>
</ol>
<h2><a id="user-content-code-examples" class="anchor" aria-hidden="true" href="#code-examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Code Examples</h2>
<h3><a id="user-content-basic-overview" class="anchor" aria-hidden="true" href="#basic-overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Basic Overview</h3>
<p>ITensor construction, setting of elements, contraction, and addition.
Before constructing an ITensor, one constructs Index objects
representing tensor indices.</p>
<pre lang="jldoctest;"><code>using ITensors
let
  i = Index(3)
  j = Index(5)
  k = Index(2)
  l = Index(7)

  A = ITensor(i,j,k)
  B = ITensor(j,l)

  A[i=&gt;1,j=&gt;1,k=&gt;1] = 11.1
  A[i=&gt;2,j=&gt;1,k=&gt;2] = -21.2
  A[k=&gt;1,i=&gt;3,j=&gt;1] = 31.1  # can provide Index values in any order
  # ...

  # A[k(1),i(3),j(1)] = 31.1  # alternative notation

  # Contract over shared index j
  C = A * B

  @show hasinds(C,i,k,l) # = true

  D = randomITensor(k,j,i) # ITensor with random elements

  # Add two ITensors
  # must have same set of indices
  # but can be in any order
  R = A + D

  nothing
end

# output

hasinds(C, i, k, l) = true
</code></pre>
<h3><a id="user-content-singular-value-decomposition-svd-of-a-matrix" class="anchor" aria-hidden="true" href="#singular-value-decomposition-svd-of-a-matrix"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Singular Value Decomposition (SVD) of a Matrix</h3>
<p>In this example, we create a random 10x20 matrix
and compute its SVD. The resulting factors can
be simply multiplied back together using the
ITensor <code>*</code> operation, which automatically recognizes
the matching indices between U and S, and between S and V
and contracts (sums over) them.</p>
<pre lang="jldoctest;"><code>using ITensors
let
  i = Index(10)           # index of dimension 10
  j = Index(20)           # index of dimension 20
  M = randomITensor(i,j)  # random matrix, indices i,j
  U,S,V = svd(M,i)        # compute SVD with i as row index
  @show M ≈ U*S*V         # = true

  nothing
end

# output

M ≈ U * S * V = true
</code></pre>
<h3><a id="user-content-singular-value-decomposition-svd-of-a-tensor" class="anchor" aria-hidden="true" href="#singular-value-decomposition-svd-of-a-tensor"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Singular Value Decomposition (SVD) of a Tensor</h3>
<p>In this example, we create a random 4x4x4x4 tensor
and compute its SVD, temporarily treating the first
and third indices (i and k) as the "row" index and the second
and fourth indices (j and l) as the "column" index for the purposes
of the SVD. The resulting factors can
be simply multiplied back together using the
ITensor <code>*</code> operation, which automatically recognizes
the matching indices between U and S, and between S and V
and contracts (sums over) them.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ITensors
<span class="pl-k">let</span>
  i <span class="pl-k">=</span> <span class="pl-c1">Index</span>(<span class="pl-c1">4</span>,<span class="pl-s"><span class="pl-pds">"</span>i<span class="pl-pds">"</span></span>)
  j <span class="pl-k">=</span> <span class="pl-c1">Index</span>(<span class="pl-c1">4</span>,<span class="pl-s"><span class="pl-pds">"</span>j<span class="pl-pds">"</span></span>)
  k <span class="pl-k">=</span> <span class="pl-c1">Index</span>(<span class="pl-c1">4</span>,<span class="pl-s"><span class="pl-pds">"</span>k<span class="pl-pds">"</span></span>)
  l <span class="pl-k">=</span> <span class="pl-c1">Index</span>(<span class="pl-c1">4</span>,<span class="pl-s"><span class="pl-pds">"</span>l<span class="pl-pds">"</span></span>)
  T <span class="pl-k">=</span> <span class="pl-c1">randomITensor</span>(i,j,k,l)
  U,S,V <span class="pl-k">=</span> <span class="pl-c1">svd</span>(T,i,k)   <span class="pl-c"><span class="pl-c">#</span> compute SVD with (i,k) as row indices (indices of U)</span>
  <span class="pl-c1">@show</span> <span class="pl-c1">hasinds</span>(U,i,k) <span class="pl-c"><span class="pl-c">#</span> = true</span>
  <span class="pl-c1">@show</span> <span class="pl-c1">hasinds</span>(V,j,l) <span class="pl-c"><span class="pl-c">#</span> = true</span>
  <span class="pl-c1">@show</span> T <span class="pl-k">≈</span> U<span class="pl-k">*</span>S<span class="pl-k">*</span>V      <span class="pl-c"><span class="pl-c">#</span> = true</span>

  <span class="pl-c1">nothing</span>
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> output</span>

<span class="pl-en">hasinds</span>(U,i,k) <span class="pl-k">=</span> <span class="pl-c1">true</span>
<span class="pl-en">hasinds</span>(V,j,l) <span class="pl-k">=</span> <span class="pl-c1">true</span>
M <span class="pl-k">≈</span> U <span class="pl-k">*</span> S <span class="pl-k">*</span> V <span class="pl-k">=</span> <span class="pl-c1">true</span></pre></div>
<h3><a id="user-content-tensor-indices-tags-and-prime-levels" class="anchor" aria-hidden="true" href="#tensor-indices-tags-and-prime-levels"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tensor Indices: Tags and Prime Levels</h3>
<p>Before making an ITensor, you have to define its indices.
Tensor Index objects carry extra information beyond just their dimension.</p>
<p>All Index objects carry a permanent, immutable id number which is
determined when it is constructed, and allow it to be matched
(compare equal) with copies of itself.</p>
<p>Additionally, an Index can have up to four tag strings, and an
integer primelevel. If two Index objects have different tags or
different prime levels, they do not compare equal even if they
have the same id.</p>
<p>Tags are also useful for identifying Index objects when printing
tensors, and for performing certain Index manipulations (e.g.
priming indices having certain sets of tags).</p>
<pre lang="jldoctest;"><code>using ITensors
let
  i = Index(3)     # Index of dimension 3
  @show dim(i)     # = 3
  @show id(i)      # = 0x5d28aa559dd13001 or similar

  ci = copy(i)
  @show ci == i    # = true

  j = Index(5,"j") # Index with a tag "j"

  @show j == i     # = false

  s = Index(2,"n=1,Site") # Index with two tags,
                          # "Site" and "n=1"
  @show hastags(s,"Site") # = true
  @show hastags(s,"n=1")  # = true

  i1 = prime(i) # i1 has a "prime level" of 1
                # but otherwise same properties as i
  @show i1 == i # = false, prime levels do not match

  nothing
end

# output

dim(i) = 3
id(i) = 0x5d28aa559dd13001
ci == i = true
j == i = false
hastags(s, "Site") = true
hastags(s, "n=1") = true
i1 == i = false
</code></pre>
<h3><a id="user-content-dmrg-calculation" class="anchor" aria-hidden="true" href="#dmrg-calculation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DMRG Calculation</h3>
<p>DMRG is an iterative algorithm for finding the dominant
eigenvector of an exponentially large, Hermitian matrix.
It originates in physics with the purpose of finding
eigenvectors of Hamiltonian (energy) matrices which model
the behavior of quantum systems.</p>
<pre lang="jldoctest;"><code>using ITensors
let
  # Create 100 spin-one indices
  N = 100
  sites = siteinds("S=1",N)

  # Input operator terms which define 
  # a Hamiltonian matrix, and convert
  # these terms to an MPO tensor network
  # (here we make the 1D Heisenberg model)
  ampo = AutoMPO()
  for j=1:N-1
    ampo +=     ("Sz",j,"Sz",j+1)
    ampo += (0.5,"S+",j,"S-",j+1)
    ampo += (0.5,"S-",j,"S+",j+1)
  end
  H = MPO(ampo,sites)

  # Create an initial random matrix product state
  psi0 = randomMPS(sites)

  # Plan to do 5 passes or 'sweeps' of DMRG,
  # setting maximum MPS internal dimensions 
  # for each sweep and maximum truncation cutoff
  # used when adapting internal dimensions:
  sweeps = Sweeps(5)
  maxdim!(sweeps, 10,20,100,100,200)
  cutoff!(sweeps, 1E-10)
  @show sweeps

  # Run the DMRG algorithm, returning energy 
  # (dominant eigenvalue) and optimized MPS
  energy, psi = dmrg(H,psi0, sweeps)
  println("Final energy = $energy")

  nothing
end

# output

sweeps = Sweeps
1 cutoff=1.0E-10, maxdim=10, mindim=1
2 cutoff=1.0E-10, maxdim=20, mindim=1
3 cutoff=1.0E-10, maxdim=100, mindim=1
4 cutoff=1.0E-10, maxdim=100, mindim=1
5 cutoff=1.0E-10, maxdim=200, mindim=1

After sweep 1 energy=-137.845841178879 maxlinkdim=9 time=8.538
After sweep 2 energy=-138.935378608196 maxlinkdim=20 time=0.316
After sweep 3 energy=-138.940079710492 maxlinkdim=88 time=1.904
After sweep 4 energy=-138.940086018149 maxlinkdim=100 time=4.179
After sweep 5 energy=-138.940086075413 maxlinkdim=96 time=4.184
Final energy = -138.94008607296038
</code></pre>
</article></div>