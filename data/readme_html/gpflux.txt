<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-gpfluxjl" class="anchor" aria-hidden="true" href="#gpfluxjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPFlux.jl</h1>
<p>A new Gaussian process package, which facilitates user to integrate deep neural network into Gaussian process model (e.g. use neural network as mean function or kernel function to improve the power of their GP model). It use <a href="https://github.com/FluxML/Zygote.jl.git">Zygote</a> to compute derivatives w.r.t model parameters, and is naturally compatible with <a href="https://github.com/FluxML/Flux.jl.git">Flux</a>.</p>
<p><strong>Key features</strong></p>
<ul>
<li>Building the GP mean function with a Flux's neural network</li>
<li>Implement neural kernel network (arxiv 1806.04326), which makes it easy to build various composite kernels</li>
</ul>
<p>This package is still under development, <strong>suggestions</strong>, <strong>bug report</strong> and <strong>pull request</strong> are welcome :), detailed documentation will come later...</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>Installing GPFlux requires run the following code in a Julia REPL:</p>
<div class="highlight highlight-source-julia"><pre>] add GPFlux</pre></div>
<h2><a id="user-content-brief-introduction-to-gp" class="anchor" aria-hidden="true" href="#brief-introduction-to-gp"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Brief introduction to GP</h2>
<p><a href="http://www.gaussianprocess.org/gpml/chapters/RW1.pdf" rel="nofollow">Gaussian processe</a> is a powerful algorithm in statistical machine learning and probabilistic modelling, it models the underlying distribution of a dataset by a prior belief ( which is a parametrized multivariate normal distribution ) and a Gaussian likelihood, learning is done by maximizing the log likelihood (MLE), which is tractable for Gaussian process. Gaussian process is widely used in surrogate function modelling, geostatitics, pattern recognition, etc.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examples</h2>
<h3><a id="user-content-simple-regression" class="anchor" aria-hidden="true" href="#simple-regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://github.com/HamletWantToCode/GPFlux.jl/blob/master/notebook/simple_gpr.ipynb">Simple regression</a></h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/HamletWantToCode/GPFlux.jl/blob/master/assets/simple_gpr.png"><img src="https://github.com/HamletWantToCode/GPFlux.jl/raw/master/assets/simple_gpr.png" alt="sin_regr" style="max-width:100%;"></a></p>
<h3><a id="user-content-time-series-prediction" class="anchor" aria-hidden="true" href="#time-series-prediction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://github.com/HamletWantToCode/GPFlux.jl/blob/master/notebook/time_series_NKN.ipynb">Time series prediction</a></h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/HamletWantToCode/GPFlux.jl/blob/master/assets/time_series.png"><img src="https://github.com/HamletWantToCode/GPFlux.jl/raw/master/assets/time_series.png" alt="airline_regr" style="max-width:100%;"></a></p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p>Gaussian process is determined by a mean function and a kernel function, they can be specified in GPFlux as follows</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> mean function</span>
c <span class="pl-k">=</span> [<span class="pl-c1">0.0</span>] <span class="pl-c"><span class="pl-c">#</span> mean constant</span>
zero_mean <span class="pl-k">=</span> <span class="pl-c1">ConstantMean</span>(c)
<span class="pl-c"><span class="pl-c">#</span> square exponential kernel</span>
ll <span class="pl-k">=</span> [<span class="pl-c1">0.0</span>] <span class="pl-c"><span class="pl-c">#</span> length scale in log scale</span>
lσ <span class="pl-k">=</span> [<span class="pl-c1">0.0</span>] <span class="pl-c"><span class="pl-c">#</span> scaling factor in log scale</span>
se_kernel <span class="pl-k">=</span> <span class="pl-c1">IsoGaussKernel</span>(ll, lσ)
<span class="pl-c"><span class="pl-c">#</span> build Gauss process</span>
lnoise <span class="pl-k">=</span> [<span class="pl-k">-</span><span class="pl-c1">2.0</span>] <span class="pl-c"><span class="pl-c">#</span> noise in log scale</span>
gp <span class="pl-k">=</span> <span class="pl-c1">GaussProcess</span>(zero_mean, se_kernel, lnoise)</pre></div>
<p>The parameters in the above <code>gp</code> model are <code>c</code>, <code>ll</code>, <code>lσ</code> and <code>lnoise</code>, one can extract all parameters by:</p>
<div class="highlight highlight-source-julia"><pre>ps <span class="pl-k">=</span> <span class="pl-c1">params</span>(gp)</pre></div>
<p>Given data <code>X</code>,<code>y</code>, one can compute the negative log likelihood and it's gradient w.r.t all the parameters by:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">negloglik</span>(gp, X, y) <span class="pl-c"><span class="pl-c">#</span> (X, y) is the dataset</span>
<span class="pl-c1">gradient</span>(()<span class="pl-k">-&gt;</span><span class="pl-c1">negloglik</span>(gp, X, y), ps)</pre></div>
<p>which are straight forward if you are familiar with Flux and Zygote.</p>
<p>One can also build composite kernel by using <code>ProductCompositeKernel</code> and <code>AddCompositeKernel</code>( <strong>Note: AD works for arbitrary composite kernels</strong> ).</p>
<div class="highlight highlight-source-julia"><pre>se_kernel <span class="pl-k">=</span> <span class="pl-c1">IsoGaussKernel</span>(ll, lσ)
per_kernel <span class="pl-k">=</span> <span class="pl-c1">IsoPeriodKernel</span>(lp, ll, lσ)
se_mul_periodic_kernel <span class="pl-k">=</span> <span class="pl-c1">ProductCompositeKernel</span>(se_kernel, per_kernel)
se_add_periodic_kernel <span class="pl-k">=</span> <span class="pl-c1">AddCompositeKernel</span>(se_kernel, per_kernel)

<span class="pl-c1">params</span>(se_mul_periodic_kernel) <span class="pl-c"><span class="pl-c">#</span> provide parameters of se_mul_periodic_kernel</span>
<span class="pl-c1">params</span>(se_add_periodic_kernel) <span class="pl-c"><span class="pl-c">#</span> provide parameters of se_add_periodic_kernel</span></pre></div>
<p>The most significant feature of GPFlux is that it allows to use Flux's neural network to build mean function and Neural Kernel Network (NKN) to build kernel function, the computation of negative log likelihood and it's gradient is same as above cases.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> build the mean function with neural network using Flux</span>
nn_mean <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">5</span>, <span class="pl-c1">10</span>, relu), <span class="pl-c1">Dense</span>(<span class="pl-c1">10</span>, <span class="pl-c1">1</span>))
<span class="pl-c"><span class="pl-c">#</span> build the kernel function with neural kernel network</span>
nkn <span class="pl-k">=</span> <span class="pl-c1">NeuralKernelNetwork</span>(<span class="pl-c1">Primitive</span>(se_kernel, per_kernel), <span class="pl-c1">Linear</span>(<span class="pl-c1">2</span>, <span class="pl-c1">4</span>), z<span class="pl-k">-&gt;</span><span class="pl-c1">Product</span>(z, step<span class="pl-k">=</span><span class="pl-c1">4</span>))
<span class="pl-c"><span class="pl-c">#</span> build GP</span>
nn_gp <span class="pl-k">=</span> <span class="pl-c1">GaussProcess</span>(nn_mean, nkn, lnoise)

<span class="pl-c"><span class="pl-c">#</span> compute negative log likelihood and gradient</span>
<span class="pl-c1">negloglik</span>(nn_gp, X, y)
<span class="pl-c1">gradient</span>(()<span class="pl-k">-&gt;</span><span class="pl-c1">negloglik</span>(nn_gp, X, y), <span class="pl-c1">params</span>(nn_gp))</pre></div>
<p>Once we have negative log likelihood and gradients, we can either use <a href="https://github.com/JuliaNLSolvers/Optim.jl.git">Optim.jl</a> or Flux's optimizers to do optimization.</p>
</article></div>