<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-decisiontreejl" class="anchor" aria-hidden="true" href="#decisiontreejl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DecisionTree.jl</h1>
<p dir="auto"><a href="https://github.com/JuliaAI/DecisionTree.jl/actions?query=workflow%3ACI"><img src="https://github.com/JuliaAI/DecisionTree.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaAI/DecisionTree.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f28ede01b1abf8af3b70f03cd3a2d1dbc875fd79baf3eb298101e696bc837b9f/68747470733a2f2f636f6465636f762e696f2f67682f4a756c696141492f4465636973696f6e547265652e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/JuliaAI/DecisionTree.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Docs Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://doi.org/10.5281/zenodo.7359268" rel="nofollow"><img src="https://camo.githubusercontent.com/c368e5a0af03cd55f192d4070e1f88eceed15b5fa7a5bf1020e667c03c4d83be/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e373335393236382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.7359268.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Julia implementation of Decision Tree (CART) and Random Forest algorithms</p>
<p dir="auto">Created and developed by Ben Sadeghi (@bensadeghi). Now maintained by
the <a href="https://github.com/JuliaAI">JuliaAI</a> organization.</p>
<p dir="auto">Available via:</p>
<ul dir="auto">
<li><a href="https://github.com/IBM/AutoMLPipeline.jl">AutoMLPipeline.jl</a> - create complex ML pipeline structures using simple expressions</li>
<li><a href="https://github.com/ppalmes/CombineML.jl">CombineML.jl</a> - a heterogeneous ensemble learning package</li>
<li><a href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="nofollow">MLJ.jl</a> - a machine learning framework for Julia</li>
<li><a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a> - Julia implementation of the scikit-learn API</li>
</ul>
<h2 dir="auto"><a id="user-content-classification" class="anchor" aria-hidden="true" href="#classification"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Classification</h2>
<ul dir="auto">
<li>pre-pruning (max depth, min leaf size)</li>
<li>post-pruning (pessimistic pruning)</li>
<li>multi-threaded bagging (random forests)</li>
<li>adaptive boosting (decision stumps), using <a href="https://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0002/0003/a008/" rel="nofollow">SAMME</a></li>
<li>cross validation (n-fold)</li>
<li>support for ordered features (encoded as <code>Real</code>s or <code>String</code>s)</li>
</ul>
<h2 dir="auto"><a id="user-content-regression" class="anchor" aria-hidden="true" href="#regression"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Regression</h2>
<ul dir="auto">
<li>pre-pruning (max depth, min leaf size)</li>
<li>multi-threaded bagging (random forests)</li>
<li>cross validation (n-fold)</li>
<li>support for numerical features</li>
</ul>
<p dir="auto"><strong>Note that regression is implied if labels/targets are of type Array{Float}</strong></p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">You can install DecisionTree.jl using Julia's package manager</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="Pkg.add(&quot;DecisionTree&quot;)"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>DecisionTree<span class="pl-pds">"</span></span>)</pre></div>
<h2 dir="auto"><a id="user-content-scikitlearnjl-api" class="anchor" aria-hidden="true" href="#scikitlearnjl-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ScikitLearn.jl API</h2>
<p dir="auto">DecisionTree.jl supports the <a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a> interface and algorithms (cross-validation, hyperparameter tuning, pipelines, etc.)</p>
<p dir="auto">Available models: <code>DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier</code>.
See each model's help (eg. <code>?DecisionTreeRegressor</code> at the REPL) for more information</p>
<h3 dir="auto"><a id="user-content-classification-example" class="anchor" aria-hidden="true" href="#classification-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Classification Example</h3>
<p dir="auto">Load DecisionTree package</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using DecisionTree"><pre><span class="pl-k">using</span> DecisionTree</pre></div>
<p dir="auto">Separate Fisher's Iris dataset features and labels</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="features, labels = load_data(&quot;iris&quot;)    # also see &quot;adult&quot; and &quot;digits&quot; datasets

# the data loaded are of type Array{Any}
# cast them to concrete types for better performance
features = float.(features)
labels   = string.(labels)"><pre>features, labels <span class="pl-k">=</span> <span class="pl-c1">load_data</span>(<span class="pl-s"><span class="pl-pds">"</span>iris<span class="pl-pds">"</span></span>)    <span class="pl-c"><span class="pl-c">#</span> also see "adult" and "digits" datasets</span>

<span class="pl-c"><span class="pl-c">#</span> the data loaded are of type Array{Any}</span>
<span class="pl-c"><span class="pl-c">#</span> cast them to concrete types for better performance</span>
features <span class="pl-k">=</span> <span class="pl-c1">float</span>.(features)
labels   <span class="pl-k">=</span> <span class="pl-c1">string</span>.(labels)</pre></div>
<p dir="auto">Pruned Tree Classifier</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# train depth-truncated classifier
model = DecisionTreeClassifier(max_depth=2)
fit!(model, features, labels)
# pretty print of the tree, to a depth of 5 nodes (optional)
print_tree(model, 5)
# apply learned model
predict(model, [5.9,3.0,5.1,1.9])
# get the probability of each label
predict_proba(model, [5.9,3.0,5.1,1.9])
println(get_classes(model)) # returns the ordering of the columns in predict_proba's output
# run n-fold cross validation over 3 CV folds
# See ScikitLearn.jl for installation instructions
using ScikitLearn.CrossValidation: cross_val_score
accuracy = cross_val_score(model, features, labels, cv=3)"><pre><span class="pl-c"><span class="pl-c">#</span> train depth-truncated classifier</span>
model <span class="pl-k">=</span> <span class="pl-c1">DecisionTreeClassifier</span>(max_depth<span class="pl-k">=</span><span class="pl-c1">2</span>)
<span class="pl-c1">fit!</span>(model, features, labels)
<span class="pl-c"><span class="pl-c">#</span> pretty print of the tree, to a depth of 5 nodes (optional)</span>
<span class="pl-c1">print_tree</span>(model, <span class="pl-c1">5</span>)
<span class="pl-c"><span class="pl-c">#</span> apply learned model</span>
<span class="pl-c1">predict</span>(model, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>])
<span class="pl-c"><span class="pl-c">#</span> get the probability of each label</span>
<span class="pl-c1">predict_proba</span>(model, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>])
<span class="pl-c1">println</span>(<span class="pl-c1">get_classes</span>(model)) <span class="pl-c"><span class="pl-c">#</span> returns the ordering of the columns in predict_proba's output</span>
<span class="pl-c"><span class="pl-c">#</span> run n-fold cross validation over 3 CV folds</span>
<span class="pl-c"><span class="pl-c">#</span> See ScikitLearn.jl for installation instructions</span>
<span class="pl-k">using</span> ScikitLearn<span class="pl-k">.</span>CrossValidation<span class="pl-k">:</span> cross_val_score
accuracy <span class="pl-k">=</span> <span class="pl-c1">cross_val_score</span>(model, features, labels, cv<span class="pl-k">=</span><span class="pl-c1">3</span>)</pre></div>
<p dir="auto">Also, have a look at these <a href="https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb">classification</a> and <a href="https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb">regression</a> notebooks.</p>
<h2 dir="auto"><a id="user-content-native-api" class="anchor" aria-hidden="true" href="#native-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Native API</h2>
<h3 dir="auto"><a id="user-content-classification-example-1" class="anchor" aria-hidden="true" href="#classification-example-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Classification Example</h3>
<p dir="auto">Decision Tree Classifier</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# train full-tree classifier
model = build_tree(labels, features)
# prune tree: merge leaves having &gt;= 90% combined purity (default: 100%)
model = prune_tree(model, 0.9)
# pretty print of the tree, to a depth of 5 nodes (optional)
print_tree(model, 5)
# apply learned model
apply_tree(model, [5.9,3.0,5.1,1.9])
# apply model to all the sames
preds = apply_tree(model, features)
# generate confusion matrix, along with accuracy and kappa scores
DecisionTree.confusion_matrix(labels, preds)
# get the probability of each label
apply_tree_proba(model, [5.9,3.0,5.1,1.9], [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;])
# run 3-fold cross validation of pruned tree,
n_folds=3
accuracy = nfoldCV_tree(labels, features, n_folds)

# set of classification parameters and respective default values
# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)
# max_depth: maximum depth of the decision tree (default: -1, no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
# n_subfeatures: number of features to select at random (default: 0, keep all)
# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)
n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2
min_purity_increase=0.0; pruning_purity = 1.0; seed=3

model    =   build_tree(labels, features,
                        n_subfeatures,
                        max_depth,
                        min_samples_leaf,
                        min_samples_split,
                        min_purity_increase;
                        rng = seed)

accuracy = nfoldCV_tree(labels, features,
                        n_folds,
                        pruning_purity,
                        max_depth,
                        min_samples_leaf,
                        min_samples_split,
                        min_purity_increase;
                        verbose = true,
                        rng = seed)"><pre><span class="pl-c"><span class="pl-c">#</span> train full-tree classifier</span>
model <span class="pl-k">=</span> <span class="pl-c1">build_tree</span>(labels, features)
<span class="pl-c"><span class="pl-c">#</span> prune tree: merge leaves having &gt;= 90% combined purity (default: 100%)</span>
model <span class="pl-k">=</span> <span class="pl-c1">prune_tree</span>(model, <span class="pl-c1">0.9</span>)
<span class="pl-c"><span class="pl-c">#</span> pretty print of the tree, to a depth of 5 nodes (optional)</span>
<span class="pl-c1">print_tree</span>(model, <span class="pl-c1">5</span>)
<span class="pl-c"><span class="pl-c">#</span> apply learned model</span>
<span class="pl-c1">apply_tree</span>(model, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>])
<span class="pl-c"><span class="pl-c">#</span> apply model to all the sames</span>
preds <span class="pl-k">=</span> <span class="pl-c1">apply_tree</span>(model, features)
<span class="pl-c"><span class="pl-c">#</span> generate confusion matrix, along with accuracy and kappa scores</span>
DecisionTree<span class="pl-k">.</span><span class="pl-c1">confusion_matrix</span>(labels, preds)
<span class="pl-c"><span class="pl-c">#</span> get the probability of each label</span>
<span class="pl-c1">apply_tree_proba</span>(model, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>], [<span class="pl-s"><span class="pl-pds">"</span>Iris-setosa<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Iris-versicolor<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Iris-virginica<span class="pl-pds">"</span></span>])
<span class="pl-c"><span class="pl-c">#</span> run 3-fold cross validation of pruned tree,</span>
n_folds<span class="pl-k">=</span><span class="pl-c1">3</span>
accuracy <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_tree</span>(labels, features, n_folds)

<span class="pl-c"><span class="pl-c">#</span> set of classification parameters and respective default values</span>
<span class="pl-c"><span class="pl-c">#</span> pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)</span>
<span class="pl-c"><span class="pl-c">#</span> max_depth: maximum depth of the decision tree (default: -1, no maximum)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_split: the minimum number of samples in needed for a split (default: 2)</span>
<span class="pl-c"><span class="pl-c">#</span> min_purity_increase: minimum purity needed for a split (default: 0.0)</span>
<span class="pl-c"><span class="pl-c">#</span> n_subfeatures: number of features to select at random (default: 0, keep all)</span>
<span class="pl-c"><span class="pl-c">#</span> keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)</span>
n_subfeatures<span class="pl-k">=</span><span class="pl-c1">0</span>; max_depth<span class="pl-k">=</span><span class="pl-k">-</span><span class="pl-c1">1</span>; min_samples_leaf<span class="pl-k">=</span><span class="pl-c1">1</span>; min_samples_split<span class="pl-k">=</span><span class="pl-c1">2</span>
min_purity_increase<span class="pl-k">=</span><span class="pl-c1">0.0</span>; pruning_purity <span class="pl-k">=</span> <span class="pl-c1">1.0</span>; seed<span class="pl-k">=</span><span class="pl-c1">3</span>

model    <span class="pl-k">=</span>   <span class="pl-c1">build_tree</span>(labels, features,
                        n_subfeatures,
                        max_depth,
                        min_samples_leaf,
                        min_samples_split,
                        min_purity_increase;
                        rng <span class="pl-k">=</span> seed)

accuracy <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_tree</span>(labels, features,
                        n_folds,
                        pruning_purity,
                        max_depth,
                        min_samples_leaf,
                        min_samples_split,
                        min_purity_increase;
                        verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>,
                        rng <span class="pl-k">=</span> seed)</pre></div>
<p dir="auto">Random Forest Classifier</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# train random forest classifier
# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6
model = build_forest(labels, features, 2, 10, 0.5, 6)
# apply learned model
apply_forest(model, [5.9,3.0,5.1,1.9])
# get the probability of each label
apply_forest_proba(model, [5.9,3.0,5.1,1.9], [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;])
# add 7 more trees
model = build_forest(model, labels, features, 2, 7, 0.5, 6)
# run 3-fold cross validation for forests, using 2 random features per split
n_folds=3; n_subfeatures=2
accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)

# set of classification parameters and respective default values
# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))
# n_trees: number of trees to train (default: 10)
# partial_sampling: fraction of samples to train each tree on (default: 0.7)
# max_depth: maximum depth of the decision trees (default: no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)
#              multi-threaded forests must be seeded with an `Int`
n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1
min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3

model    =   build_forest(labels, features,
                          n_subfeatures,
                          n_trees,
                          partial_sampling,
                          max_depth,
                          min_samples_leaf,
                          min_samples_split,
                          min_purity_increase;
                          rng = seed)

accuracy = nfoldCV_forest(labels, features,
                          n_folds,
                          n_subfeatures,
                          n_trees,
                          partial_sampling,
                          max_depth,
                          min_samples_leaf,
                          min_samples_split,
                          min_purity_increase;
                          verbose = true,
                          rng = seed)"><pre><span class="pl-c"><span class="pl-c">#</span> train random forest classifier</span>
<span class="pl-c"><span class="pl-c">#</span> using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6</span>
model <span class="pl-k">=</span> <span class="pl-c1">build_forest</span>(labels, features, <span class="pl-c1">2</span>, <span class="pl-c1">10</span>, <span class="pl-c1">0.5</span>, <span class="pl-c1">6</span>)
<span class="pl-c"><span class="pl-c">#</span> apply learned model</span>
<span class="pl-c1">apply_forest</span>(model, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>])
<span class="pl-c"><span class="pl-c">#</span> get the probability of each label</span>
<span class="pl-c1">apply_forest_proba</span>(model, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>], [<span class="pl-s"><span class="pl-pds">"</span>Iris-setosa<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Iris-versicolor<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Iris-virginica<span class="pl-pds">"</span></span>])
<span class="pl-c"><span class="pl-c">#</span> add 7 more trees</span>
model <span class="pl-k">=</span> <span class="pl-c1">build_forest</span>(model, labels, features, <span class="pl-c1">2</span>, <span class="pl-c1">7</span>, <span class="pl-c1">0.5</span>, <span class="pl-c1">6</span>)
<span class="pl-c"><span class="pl-c">#</span> run 3-fold cross validation for forests, using 2 random features per split</span>
n_folds<span class="pl-k">=</span><span class="pl-c1">3</span>; n_subfeatures<span class="pl-k">=</span><span class="pl-c1">2</span>
accuracy <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_forest</span>(labels, features, n_folds, n_subfeatures)

<span class="pl-c"><span class="pl-c">#</span> set of classification parameters and respective default values</span>
<span class="pl-c"><span class="pl-c">#</span> n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))</span>
<span class="pl-c"><span class="pl-c">#</span> n_trees: number of trees to train (default: 10)</span>
<span class="pl-c"><span class="pl-c">#</span> partial_sampling: fraction of samples to train each tree on (default: 0.7)</span>
<span class="pl-c"><span class="pl-c">#</span> max_depth: maximum depth of the decision trees (default: no maximum)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_split: the minimum number of samples in needed for a split (default: 2)</span>
<span class="pl-c"><span class="pl-c">#</span> min_purity_increase: minimum purity needed for a split (default: 0.0)</span>
<span class="pl-c"><span class="pl-c">#</span> keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)</span>
<span class="pl-c"><span class="pl-c">#</span>              multi-threaded forests must be seeded with an `Int`</span>
n_subfeatures<span class="pl-k">=</span><span class="pl-k">-</span><span class="pl-c1">1</span>; n_trees<span class="pl-k">=</span><span class="pl-c1">10</span>; partial_sampling<span class="pl-k">=</span><span class="pl-c1">0.7</span>; max_depth<span class="pl-k">=</span><span class="pl-k">-</span><span class="pl-c1">1</span>
min_samples_leaf<span class="pl-k">=</span><span class="pl-c1">5</span>; min_samples_split<span class="pl-k">=</span><span class="pl-c1">2</span>; min_purity_increase<span class="pl-k">=</span><span class="pl-c1">0.0</span>; seed<span class="pl-k">=</span><span class="pl-c1">3</span>

model    <span class="pl-k">=</span>   <span class="pl-c1">build_forest</span>(labels, features,
                          n_subfeatures,
                          n_trees,
                          partial_sampling,
                          max_depth,
                          min_samples_leaf,
                          min_samples_split,
                          min_purity_increase;
                          rng <span class="pl-k">=</span> seed)

accuracy <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_forest</span>(labels, features,
                          n_folds,
                          n_subfeatures,
                          n_trees,
                          partial_sampling,
                          max_depth,
                          min_samples_leaf,
                          min_samples_split,
                          min_purity_increase;
                          verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>,
                          rng <span class="pl-k">=</span> seed)</pre></div>
<p dir="auto">Adaptive-Boosted Decision Stumps Classifier</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# train adaptive-boosted stumps, using 7 iterations
model, coeffs = build_adaboost_stumps(labels, features, 7);
# apply learned model
apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])
# get the probability of each label
apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;])
# run 3-fold cross validation for boosted stumps, using 7 iterations
n_iterations=7; n_folds=3
accuracy = nfoldCV_stumps(labels, features,
                          n_folds,
                          n_iterations;
                          verbose = true)"><pre><span class="pl-c"><span class="pl-c">#</span> train adaptive-boosted stumps, using 7 iterations</span>
model, coeffs <span class="pl-k">=</span> <span class="pl-c1">build_adaboost_stumps</span>(labels, features, <span class="pl-c1">7</span>);
<span class="pl-c"><span class="pl-c">#</span> apply learned model</span>
<span class="pl-c1">apply_adaboost_stumps</span>(model, coeffs, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>])
<span class="pl-c"><span class="pl-c">#</span> get the probability of each label</span>
<span class="pl-c1">apply_adaboost_stumps_proba</span>(model, coeffs, [<span class="pl-c1">5.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>], [<span class="pl-s"><span class="pl-pds">"</span>Iris-setosa<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Iris-versicolor<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>Iris-virginica<span class="pl-pds">"</span></span>])
<span class="pl-c"><span class="pl-c">#</span> run 3-fold cross validation for boosted stumps, using 7 iterations</span>
n_iterations<span class="pl-k">=</span><span class="pl-c1">7</span>; n_folds<span class="pl-k">=</span><span class="pl-c1">3</span>
accuracy <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_stumps</span>(labels, features,
                          n_folds,
                          n_iterations;
                          verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>)</pre></div>
<h3 dir="auto"><a id="user-content-regression-example" class="anchor" aria-hidden="true" href="#regression-example"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Regression Example</h3>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="n, m = 10^3, 5
features = randn(n, m)
weights = rand(-2:2, m)
labels = features * weights"><pre>n, m <span class="pl-k">=</span> <span class="pl-c1">10</span><span class="pl-k">^</span><span class="pl-c1">3</span>, <span class="pl-c1">5</span>
features <span class="pl-k">=</span> <span class="pl-c1">randn</span>(n, m)
weights <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-k">-</span><span class="pl-c1">2</span><span class="pl-k">:</span><span class="pl-c1">2</span>, m)
labels <span class="pl-k">=</span> features <span class="pl-k">*</span> weights</pre></div>
<p dir="auto">Regression Tree</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# train regression tree
model = build_tree(labels, features)
# apply learned model
apply_tree(model, [-0.9,3.0,5.1,1.9,0.0])
# run 3-fold cross validation, returns array of coefficients of determination (R^2)
n_folds = 3
r2 = nfoldCV_tree(labels, features, n_folds)

# set of regression parameters and respective default values
# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)
# max_depth: maximum depth of the decision tree (default: -1, no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
# n_subfeatures: number of features to select at random (default: 0, keep all)
# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)
n_subfeatures = 0; max_depth = -1; min_samples_leaf = 5
min_samples_split = 2; min_purity_increase = 0.0; pruning_purity = 1.0 ; seed=3

model = build_tree(labels, features,
                   n_subfeatures,
                   max_depth,
                   min_samples_leaf,
                   min_samples_split,
                   min_purity_increase;
                   rng = seed)

r2 =  nfoldCV_tree(labels, features,
                   n_folds,
                   pruning_purity,
                   max_depth,
                   min_samples_leaf,
                   min_samples_split,
                   min_purity_increase;
                   verbose = true,
                   rng = seed)"><pre><span class="pl-c"><span class="pl-c">#</span> train regression tree</span>
model <span class="pl-k">=</span> <span class="pl-c1">build_tree</span>(labels, features)
<span class="pl-c"><span class="pl-c">#</span> apply learned model</span>
<span class="pl-c1">apply_tree</span>(model, [<span class="pl-k">-</span><span class="pl-c1">0.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>,<span class="pl-c1">0.0</span>])
<span class="pl-c"><span class="pl-c">#</span> run 3-fold cross validation, returns array of coefficients of determination (R^2)</span>
n_folds <span class="pl-k">=</span> <span class="pl-c1">3</span>
r2 <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_tree</span>(labels, features, n_folds)

<span class="pl-c"><span class="pl-c">#</span> set of regression parameters and respective default values</span>
<span class="pl-c"><span class="pl-c">#</span> pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)</span>
<span class="pl-c"><span class="pl-c">#</span> max_depth: maximum depth of the decision tree (default: -1, no maximum)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_split: the minimum number of samples in needed for a split (default: 2)</span>
<span class="pl-c"><span class="pl-c">#</span> min_purity_increase: minimum purity needed for a split (default: 0.0)</span>
<span class="pl-c"><span class="pl-c">#</span> n_subfeatures: number of features to select at random (default: 0, keep all)</span>
<span class="pl-c"><span class="pl-c">#</span> keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)</span>
n_subfeatures <span class="pl-k">=</span> <span class="pl-c1">0</span>; max_depth <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">1</span>; min_samples_leaf <span class="pl-k">=</span> <span class="pl-c1">5</span>
min_samples_split <span class="pl-k">=</span> <span class="pl-c1">2</span>; min_purity_increase <span class="pl-k">=</span> <span class="pl-c1">0.0</span>; pruning_purity <span class="pl-k">=</span> <span class="pl-c1">1.0</span> ; seed<span class="pl-k">=</span><span class="pl-c1">3</span>

model <span class="pl-k">=</span> <span class="pl-c1">build_tree</span>(labels, features,
                   n_subfeatures,
                   max_depth,
                   min_samples_leaf,
                   min_samples_split,
                   min_purity_increase;
                   rng <span class="pl-k">=</span> seed)

r2 <span class="pl-k">=</span>  <span class="pl-c1">nfoldCV_tree</span>(labels, features,
                   n_folds,
                   pruning_purity,
                   max_depth,
                   min_samples_leaf,
                   min_samples_split,
                   min_purity_increase;
                   verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>,
                   rng <span class="pl-k">=</span> seed)</pre></div>
<p dir="auto">Regression Random Forest</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="# train regression forest, using 2 random features, 10 trees,
# averaging of 5 samples per leaf, and 0.7 portion of samples per tree
model = build_forest(labels, features, 2, 10, 0.7, 5)
# apply learned model
apply_forest(model, [-0.9,3.0,5.1,1.9,0.0])
# run 3-fold cross validation on regression forest, using 2 random features per split
n_subfeatures=2; n_folds=3
r2 = nfoldCV_forest(labels, features, n_folds, n_subfeatures)

# set of regression build_forest() parameters and respective default values
# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))
# n_trees: number of trees to train (default: 10)
# partial_sampling: fraction of samples to train each tree on (default: 0.7)
# max_depth: maximum depth of the decision trees (default: no maximum)
# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)
# min_samples_split: the minimum number of samples in needed for a split (default: 2)
# min_purity_increase: minimum purity needed for a split (default: 0.0)
# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)
#              multi-threaded forests must be seeded with an `Int`
n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1
min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3

model = build_forest(labels, features,
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     rng = seed)

r2 =  nfoldCV_forest(labels, features,
                     n_folds,
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     verbose = true,
                     rng = seed)"><pre><span class="pl-c"><span class="pl-c">#</span> train regression forest, using 2 random features, 10 trees,</span>
<span class="pl-c"><span class="pl-c">#</span> averaging of 5 samples per leaf, and 0.7 portion of samples per tree</span>
model <span class="pl-k">=</span> <span class="pl-c1">build_forest</span>(labels, features, <span class="pl-c1">2</span>, <span class="pl-c1">10</span>, <span class="pl-c1">0.7</span>, <span class="pl-c1">5</span>)
<span class="pl-c"><span class="pl-c">#</span> apply learned model</span>
<span class="pl-c1">apply_forest</span>(model, [<span class="pl-k">-</span><span class="pl-c1">0.9</span>,<span class="pl-c1">3.0</span>,<span class="pl-c1">5.1</span>,<span class="pl-c1">1.9</span>,<span class="pl-c1">0.0</span>])
<span class="pl-c"><span class="pl-c">#</span> run 3-fold cross validation on regression forest, using 2 random features per split</span>
n_subfeatures<span class="pl-k">=</span><span class="pl-c1">2</span>; n_folds<span class="pl-k">=</span><span class="pl-c1">3</span>
r2 <span class="pl-k">=</span> <span class="pl-c1">nfoldCV_forest</span>(labels, features, n_folds, n_subfeatures)

<span class="pl-c"><span class="pl-c">#</span> set of regression build_forest() parameters and respective default values</span>
<span class="pl-c"><span class="pl-c">#</span> n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))</span>
<span class="pl-c"><span class="pl-c">#</span> n_trees: number of trees to train (default: 10)</span>
<span class="pl-c"><span class="pl-c">#</span> partial_sampling: fraction of samples to train each tree on (default: 0.7)</span>
<span class="pl-c"><span class="pl-c">#</span> max_depth: maximum depth of the decision trees (default: no maximum)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)</span>
<span class="pl-c"><span class="pl-c">#</span> min_samples_split: the minimum number of samples in needed for a split (default: 2)</span>
<span class="pl-c"><span class="pl-c">#</span> min_purity_increase: minimum purity needed for a split (default: 0.0)</span>
<span class="pl-c"><span class="pl-c">#</span> keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)</span>
<span class="pl-c"><span class="pl-c">#</span>              multi-threaded forests must be seeded with an `Int`</span>
n_subfeatures<span class="pl-k">=</span><span class="pl-k">-</span><span class="pl-c1">1</span>; n_trees<span class="pl-k">=</span><span class="pl-c1">10</span>; partial_sampling<span class="pl-k">=</span><span class="pl-c1">0.7</span>; max_depth<span class="pl-k">=</span><span class="pl-k">-</span><span class="pl-c1">1</span>
min_samples_leaf<span class="pl-k">=</span><span class="pl-c1">5</span>; min_samples_split<span class="pl-k">=</span><span class="pl-c1">2</span>; min_purity_increase<span class="pl-k">=</span><span class="pl-c1">0.0</span>; seed<span class="pl-k">=</span><span class="pl-c1">3</span>

model <span class="pl-k">=</span> <span class="pl-c1">build_forest</span>(labels, features,
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     rng <span class="pl-k">=</span> seed)

r2 <span class="pl-k">=</span>  <span class="pl-c1">nfoldCV_forest</span>(labels, features,
                     n_folds,
                     n_subfeatures,
                     n_trees,
                     partial_sampling,
                     max_depth,
                     min_samples_leaf,
                     min_samples_split,
                     min_purity_increase;
                     verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>,
                     rng <span class="pl-k">=</span> seed)</pre></div>
<h2 dir="auto"><a id="user-content-saving-models" class="anchor" aria-hidden="true" href="#saving-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Saving Models</h2>
<p dir="auto">Models can be saved to disk and loaded back with the use of the <a href="https://github.com/JuliaIO/JLD2.jl">JLD2.jl</a> package.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using JLD2
@save &quot;model_file.jld2&quot; model"><pre><span class="pl-k">using</span> JLD2
<span class="pl-c1">@save</span> <span class="pl-s"><span class="pl-pds">"</span>model_file.jld2<span class="pl-pds">"</span></span> model</pre></div>
<p dir="auto">Note that even though features and labels of type <code>Array{Any}</code> are supported, it is highly recommended that data be cast to explicit types (ie with <code>float.(), string.()</code>, etc). This significantly improves model training and prediction execution times, and also drastically reduces the size of saved models.</p>
<h2 dir="auto"><a id="user-content-mljjl-api" class="anchor" aria-hidden="true" href="#mljjl-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>MLJ.jl API</h2>
<p dir="auto">To use DecsionTree.jl models in
<a href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="nofollow">MLJ</a>, first
ensure MLJ.jl and MLJDecisionTreeInterface.jl are both in your Julia
environment. For example, to install in a fresh environment:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.activate(&quot;my_fresh_mlj_environment&quot;, shared=true)
Pkg.add(&quot;MLJ&quot;)
Pkg.add(&quot;MLJDecisionTreeInterface&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">activate</span>(<span class="pl-s"><span class="pl-pds">"</span>my_fresh_mlj_environment<span class="pl-pds">"</span></span>, shared<span class="pl-k">=</span><span class="pl-c1">true</span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MLJ<span class="pl-pds">"</span></span>)
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>MLJDecisionTreeInterface<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">Detailed usage instructions are available for each model using the
<code>doc</code> method. For example:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using MLJ
doc(&quot;DecisionTreeClassifier&quot;, pkg=&quot;DecisionTree&quot;)"><pre><span class="pl-k">using</span> MLJ
<span class="pl-c1">doc</span>(<span class="pl-s"><span class="pl-pds">"</span>DecisionTreeClassifier<span class="pl-pds">"</span></span>, pkg<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>DecisionTree<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">Available models are: <code>AdaBoostStumpClassifier</code>,
<code>DecisionTreeClassifier</code>, <code>DecisionTreeRegressor</code>,
<code>RandomForestClassifier</code>, <code>RandomForestRegressor</code>.</p>
<h2 dir="auto"><a id="user-content-feature-importances" class="anchor" aria-hidden="true" href="#feature-importances"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Feature Importances</h2>
<p dir="auto">The following methods provide measures of feature importance for all models:
<code>impurity_importance</code>, <code>split_importance</code>, <code>permutation_importance</code>. Query the document
strings for details.</p>
<h2 dir="auto"><a id="user-content-visualization" class="anchor" aria-hidden="true" href="#visualization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Visualization</h2>
<p dir="auto">A <code>DecisionTree</code> model can be visualized using the <code>print_tree</code>-function of its native interface
(for an example see above in section 'Classification Example').</p>
<p dir="auto">In addition, an abstraction layer using <code>AbstractTrees.jl</code> has been implemented with the intention to facilitate visualizations, which don't rely on any implementation details of <code>DecisionTree</code>. For more information have a look at the docs in <code>src/abstract_trees.jl</code> and the <a href="@ref"><code>wrap</code></a>-function, which creates this layer for a <code>DecisionTree</code> model.</p>
<p dir="auto">Apart from this, <code>AbstractTrees.jl</code> brings its own implementation of <code>print_tree</code>.</p>
<h2 dir="auto"><a id="user-content-citing-the-package-in-publications" class="anchor" aria-hidden="true" href="#citing-the-package-in-publications"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citing the package in publications</h2>
<p dir="auto">DOI: <a href="https://doi.org/10.5281/zenodo.7359268" rel="nofollow"><img src="https://camo.githubusercontent.com/c368e5a0af03cd55f192d4070e1f88eceed15b5fa7a5bf1020e667c03c4d83be/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e373335393236382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.7359268.svg" style="max-width: 100%;"></a>.</p>
<p dir="auto">BibTeX entry:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="@software{ben_sadeghi_2022_7359268,
  author       = {Ben Sadeghi and
                  Poom Chiarawongse and
                  Kevin Squire and
                  Daniel C. Jones and
                  Andreas Noack and
                  Cdric St-Jean and
                  Rik Huijzer and
                  Roland Schtzle and
                  Ian Butterworth and
                  Yu-Fong Peng and
                  Anthony Blaom},
  title        = {{DecisionTree.jl - A Julia implementation of the 
                   CART Decision Tree and Random Forest algorithms}},
  month        = nov,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {0.11.3},
  doi          = {10.5281/zenodo.7359268},
  url          = {https://doi.org/10.5281/zenodo.7359268}
}"><pre class="notranslate"><code>@software{ben_sadeghi_2022_7359268,
  author       = {Ben Sadeghi and
                  Poom Chiarawongse and
                  Kevin Squire and
                  Daniel C. Jones and
                  Andreas Noack and
                  Cdric St-Jean and
                  Rik Huijzer and
                  Roland Schtzle and
                  Ian Butterworth and
                  Yu-Fong Peng and
                  Anthony Blaom},
  title        = {{DecisionTree.jl - A Julia implementation of the 
                   CART Decision Tree and Random Forest algorithms}},
  month        = nov,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {0.11.3},
  doi          = {10.5281/zenodo.7359268},
  url          = {https://doi.org/10.5281/zenodo.7359268}
}
</code></pre></div>
<blockquote>
</blockquote>
</article></div>