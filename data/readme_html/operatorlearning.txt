<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-operatorlearningjl" class="anchor" aria-hidden="true" href="#operatorlearningjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>OperatorLearning.jl</h1>
<p align="center" dir="auto">
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ff4bc3bf64812e4244bf311def944a0e4a8f44df576181c3cbe9e75a9de56dcf/68747470733a2f2f6f70657261746f726c6561726e696e672e7363696d6c2e61692f6465762f6173736574732f6c6f676f2e706e67"><img width="400px" src="https://camo.githubusercontent.com/ff4bc3bf64812e4244bf311def944a0e4a8f44df576181c3cbe9e75a9de56dcf/68747470733a2f2f6f70657261746f726c6561726e696e672e7363696d6c2e61692f6465762f6173736574732f6c6f676f2e706e67" data-canonical-src="https://operatorlearning.sciml.ai/dev/assets/logo.png" style="max-width: 100%;"></a>
</p>
<p dir="auto"><a href="https://operatorlearning.sciml.ai/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://operatorlearning.sciml.ai/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/pzimbrod/OperatorLearning.jl/actions/workflows/CI.yml?query=branch%3Amaster++"><img src="https://github.com/pzimbrod/OperatorLearning.jl/actions/workflows/CI.yml/badge.svg?branch=master" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://buildkite.com/julialang/operatorlearning-dot-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4d3edd26a00b6be26bab3e82cd5c4aebf43bc9d20b28d4427a2938f1421636b8/68747470733a2f2f62616467652e6275696c646b6974652e636f6d2f66306233373433343230636533326337623666386665393734343430623666656430386436386335613234343334383932342e737667" alt="Build status" data-canonical-src="https://badge.buildkite.com/f0b3743420ce32c7b6f8fe974440b6fed08d68c5a244348924.svg" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/pzimbrod/OperatorLearning.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/78e1f9e1c8999c9ea4c83b1f0eb1165a8e0d22d072358f8aa0c8743ddc4c7158/68747470733a2f2f636f6465636f762e696f2f67682f707a696d62726f642f4f70657261746f724c6561726e696e672e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d4e4d31364c3553344658" alt="codecov" data-canonical-src="https://codecov.io/gh/pzimbrod/OperatorLearning.jl/branch/master/graph/badge.svg?token=NM16L5S4FX" style="max-width: 100%;"></a></p>
<p dir="auto">A Package that provides Layers for the learning of (nonlinear) operators in order to solve parametric PDEs.</p>
<p dir="auto">For now, this package contains the Fourier Neural Operator originally proposed by Li et al [1] as well as the DeepONet conceived by Lu et al [2].</p>
<p dir="auto">I decided to implement this method in Julia because coding up a layer using PyTorch in Python is rather cumbersome in comparison and Julia as a whole simply runs at comparable or faster speed than Python.</p>
<p dir="auto">The implementation of the layers is influenced heavily by the basic layers provided in the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> package.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">Simply install by running in a REPL:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="pkg&gt; add OperatorLearning"><pre>pkg<span class="pl-k">&gt;</span> add OperatorLearning</pre></div>
<h2 dir="auto"><a id="user-content-usageexamples" class="anchor" aria-hidden="true" href="#usageexamples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage/Examples</h2>
<h3 dir="auto"><a id="user-content-fourier-layer" class="anchor" aria-hidden="true" href="#fourier-layer"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fourier Layer</h3>
<p dir="auto">The basic workflow is more or less in line with the layer architectures that <code>Flux</code> provides, i.e. you construct individual layers, chain them if desired and pass the inputs as arguments to the layers.</p>
<p dir="auto">The Fourier Layer performs a linear transform as well as convolution (linear transform in fourier space), adds them and passes it through the activation.
Additionally, higher Fourier modes are filtered out in the convolution path where you can specify the amount of modes to be kept.</p>
<p dir="auto">The syntax for a single Fourier Layer is:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using OperatorLearning
using Flux

# Input = 101, Output = 101, Grid points = 100, Fourier modes = 16
# Activation: sigmoid (you need to import Flux in your Script to access the activations)
model = FourierLayer(101, 101, 100, 16, σ)

# Same as above, but perform strict convolution in Fourier Space
model = FourierLayer(101, 101, 100, 16, σ; bias_fourier=false)"><pre><span class="pl-k">using</span> OperatorLearning
<span class="pl-k">using</span> Flux

<span class="pl-c"><span class="pl-c">#</span> Input = 101, Output = 101, Grid points = 100, Fourier modes = 16</span>
<span class="pl-c"><span class="pl-c">#</span> Activation: sigmoid (you need to import Flux in your Script to access the activations)</span>
model <span class="pl-k">=</span> <span class="pl-c1">FourierLayer</span>(<span class="pl-c1">101</span>, <span class="pl-c1">101</span>, <span class="pl-c1">100</span>, <span class="pl-c1">16</span>, σ)

<span class="pl-c"><span class="pl-c">#</span> Same as above, but perform strict convolution in Fourier Space</span>
model <span class="pl-k">=</span> <span class="pl-c1">FourierLayer</span>(<span class="pl-c1">101</span>, <span class="pl-c1">101</span>, <span class="pl-c1">100</span>, <span class="pl-c1">16</span>, σ; bias_fourier<span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p dir="auto">To see a full implementation, check the Burgers equation example at <code>examples/burgers_FNO.jl</code>.
Compared to the original implementation by <a href="https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_1d.py">Li et al.</a> using PyTorch, this version written in Julia clocks in about 20 - 25% faster when running on a NVIDIA RTX A5000 GPU.</p>
<p dir="auto">If you'd like to replicate the example, you need to get the dataset for learning the Burgers equation. You can get it <a href="https://drive.google.com/drive/folders/1UnbQh2WWc6knEHbLn-ZaXrKUZhp7pjt-" rel="nofollow">here</a> or alternatively use the provided <a href="https://github.com/zongyi-li/fourier_neural_operator/tree/master/data_generation/burgers">scripts</a>.</p>
<h3 dir="auto"><a id="user-content-deeponet" class="anchor" aria-hidden="true" href="#deeponet"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DeepONet</h3>
<p dir="auto">The <code>DeepONet</code> function basically sets up two separate Flux <code>Chain</code> structs and transforms the two input arrays into one via einsum/dot product.</p>
<p dir="auto">You can either set up a "vanilla" DeepONet via the constructor function which sets up <code>Dense</code> layers for you or, if you feel fancy, pass two Chains directly to the function so you can use other architectures such as CNN or RNN as well.
The former takes two tuples that describe each architecture. E.g. <code>(32,64,72)</code> sets up a DNN with 32 neurons in the first, 64 in the second and 72 in the last layer.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using OperatorLearning
using Flux

# Create a DeepONet with branch 32 -&gt; 64 -&gt; 72 and sigmoid activation
# and trunk 24 -&gt; 64 -&gt; 72 and tanh activation without biases
model = DeepONet((32,64,72), (24,64,72), σ, tanh; init_branch=Flux.glorot_normal, bias_trunk=false)

# Alternatively, set up your own nets altogether and pass them to DeepONet
branch = Chain(Dense(2,128),Dense(128,64),Dense(64,72))
trunk = Chain(Dense(1,24),Dense(24,72))
model = DeepONet(branch,trunk)"><pre><span class="pl-k">using</span> OperatorLearning
<span class="pl-k">using</span> Flux

<span class="pl-c"><span class="pl-c">#</span> Create a DeepONet with branch 32 -&gt; 64 -&gt; 72 and sigmoid activation</span>
<span class="pl-c"><span class="pl-c">#</span> and trunk 24 -&gt; 64 -&gt; 72 and tanh activation without biases</span>
model <span class="pl-k">=</span> <span class="pl-c1">DeepONet</span>((<span class="pl-c1">32</span>,<span class="pl-c1">64</span>,<span class="pl-c1">72</span>), (<span class="pl-c1">24</span>,<span class="pl-c1">64</span>,<span class="pl-c1">72</span>), σ, tanh; init_branch<span class="pl-k">=</span>Flux<span class="pl-k">.</span>glorot_normal, bias_trunk<span class="pl-k">=</span><span class="pl-c1">false</span>)

<span class="pl-c"><span class="pl-c">#</span> Alternatively, set up your own nets altogether and pass them to DeepONet</span>
branch <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">2</span>,<span class="pl-c1">128</span>),<span class="pl-c1">Dense</span>(<span class="pl-c1">128</span>,<span class="pl-c1">64</span>),<span class="pl-c1">Dense</span>(<span class="pl-c1">64</span>,<span class="pl-c1">72</span>))
trunk <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">1</span>,<span class="pl-c1">24</span>),<span class="pl-c1">Dense</span>(<span class="pl-c1">24</span>,<span class="pl-c1">72</span>))
model <span class="pl-k">=</span> <span class="pl-c1">DeepONet</span>(branch,trunk)</pre></div>
<p dir="auto">For usage, check the Burgers equation example at <code>examples/burgers_DeepONet.jl</code>.</p>
<h2 dir="auto"><a id="user-content-license" class="anchor" aria-hidden="true" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto"><a href="https://choosealicense.com/licenses/mit/" rel="nofollow">MIT</a></p>
<h2 dir="auto"><a id="user-content-todos" class="anchor" aria-hidden="true" href="#todos"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ToDos</h2>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> 1D Fourier Layer</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> 2D / 3D Fourier Layer</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""> DeepONet</li>
<li class="task-list-item"><input type="checkbox" id="" disabled="" class="task-list-item-checkbox"> Physics informed Loss</li>
</ul>
<h2 dir="auto"><a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">Contributions are always welcome! Please submit a PR if you'd like to participate in the project.</p>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<p dir="auto">[1] Z. Li et al., „Fourier Neural Operator for Parametric Partial Differential Equations“, <a href="https://arxiv.org/abs/2010.08895" rel="nofollow">arXiv:2010.08895</a> [cs, math], May 2021</p>
<p dir="auto">[2] L. Lu, P. Jin, and G. E. Karniadakis, „DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators“, <a href="http://arxiv.org/abs/1910.03193" rel="nofollow">arXiv:1910.03193</a> [cs, stat], Apr. 2020</p>
</article></div>