<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-parallelsparsematmul" class="anchor" aria-hidden="true" href="#parallelsparsematmul"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ParallelSparseMatMul</h1>

<p>A Julia library for parallel sparse matrix multiplication using shared memory.
This library implements SharedSparseMatrixCSC and SharedBilinearOperator types
to make it easy to multiply by sparse matrices in parallel on shared memory systems.</p>
<h1><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h1>
<p>To install, just open a Julia prompt and call</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="Pkg.clone(&quot;git@github.com:madeleineudell/ParallelSparseMatMul.jl.git&quot;)
"><pre><code>Pkg.clone("git@github.com:madeleineudell/ParallelSparseMatMul.jl.git")
</code></pre></div>
<h1><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h1>
<p>Before you begin, initialize all the processes you want to participate in multiplying by your matrix.
You'll suffer decreased performance if you add more processes
than you have hyperthreads on your shared-memory computer.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="addprocs(3)
using ParallelSparseMatMul
"><pre><code>addprocs(3)
using ParallelSparseMatMul
</code></pre></div>
<p>Create a shared sparse matrix by sharing a sparse matrix.
For example, if <code>typeof(A) == SparseMatrixCSC</code>, then you can share it by calling</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="S = share(A)
"><pre><code>S = share(A)
</code></pre></div>
<p>If you're just experimenting, you might try calling one of the matrix creation functions,
eg random uniform entries <code>shsprand</code>, random normal entries <code>shsprandn</code>,
or an identity matrix <code>shspeye</code>.
These are often faster than their serial counterparts, since
they parallelize the generation of random numbers.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="m,n,p = 100,30,.1 # generate an 100 x 30 matrix with 10% fill
S = shsprandn(m,n,p) # entries are normal random variables
"><pre><code>m,n,p = 100,30,.1 # generate an 100 x 30 matrix with 10% fill
S = shsprandn(m,n,p) # entries are normal random variables
</code></pre></div>
<p>You can multiply by your matrix and its transpose.
Multiplication by shared arrays will be faster than multiplication by other kinds of vectors,
which have to be shared first.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="x = Base.shmem_randn(n) # create a shared memory array of length n
y = S*x
x = S'*y
"><pre><code>x = Base.shmem_randn(n) # create a shared memory array of length n
y = S*x
x = S'*y
</code></pre></div>
<p>The matrices are stored in CSC format, which means that transpose multiplication <code>x = S'*y</code>
will be faster than multiplication <code>y = S*x</code>.
You can examine the entries of a shared sparse matrix by indexing into it,
eg <code>S[3,5]</code>.
Setting entries is not yet supported.
Instead, you can always convert your matrix to a local sparse matrix,
set entries, and re-share it.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="A = localize(S)
A[3,5] = 42
S = share(A)
"><pre><code>A = localize(S)
A[3,5] = 42
S = share(A)
</code></pre></div>
<p>Shared bilinear operators implement fast multiplication by A and A'.
This feature is useful in iterative algorithms that require many multiplications
by a fixed matrix and its transpose.</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="L = operator(A) # make A into a shared bilinear operator L
# multiplication by L' should be faster than multiplication by A'
y = L*x 
x = L'*y
"><pre><code>L = operator(A) # make A into a shared bilinear operator L
# multiplication by L' should be faster than multiplication by A'
y = L*x 
x = L'*y
</code></pre></div>
<p>The command <code>L=operator(A)</code> forms and stores <code>A'</code>.
This allows multiplication by <code>A</code> to be as fast as multiplication by <code>A'</code>,
at the cost of doubling the storage requirements.</p>
</article></div>