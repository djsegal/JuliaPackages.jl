<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p><a href="https://travis-ci.org/baggepinnen/FluxOptTools.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/09f4acd84427ac51f1335a8c77118cb8cae00ad5/68747470733a2f2f7472617669732d63692e6f72672f626167676570696e6e656e2f466c75784f7074546f6f6c732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/baggepinnen/FluxOptTools.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/baggepinnen/FluxOptTools.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/e85e7d7a8cc43dda535b2554f4e477b2e7b940c0/68747470733a2f2f636f6465636f762e696f2f67682f626167676570696e6e656e2f466c75784f7074546f6f6c732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/baggepinnen/FluxOptTools.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h1><a id="user-content-fluxopttools" class="anchor" aria-hidden="true" href="#fluxopttools"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>FluxOptTools</h1>
<p>This package contains some utilities to enhance training of <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> models.</p>
<h2><a id="user-content-train-using-optim" class="anchor" aria-hidden="true" href="#train-using-optim"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Train using Optim</h2>
<p><a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> can be used to train Flux models (if Flux is on version 0.10 or above), here's an example how</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Flux, Zygote, Optim, FluxOptTools, Statistics
m      <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">1</span>,<span class="pl-c1">3</span>,tanh) , <span class="pl-c1">Dense</span>(<span class="pl-c1">3</span>,<span class="pl-c1">1</span>))
x      <span class="pl-k">=</span> <span class="pl-c1">LinRange</span>(<span class="pl-k">-</span>pi,pi,<span class="pl-c1">100</span>)<span class="pl-k">'</span>
y      <span class="pl-k">=</span> <span class="pl-c1">sin</span>.(x)
<span class="pl-en">loss</span>() <span class="pl-k">=</span> <span class="pl-c1">mean</span>(abs2, <span class="pl-c1">m</span>(x) <span class="pl-k">.-</span> y)
Zygote<span class="pl-k">.</span><span class="pl-c1">refresh</span>()
pars   <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">params</span>(m)
lossfun, gradfun, fg!, p0 <span class="pl-k">=</span> <span class="pl-c1">optfuns</span>(loss, pars)
res <span class="pl-k">=</span> Optim<span class="pl-k">.</span><span class="pl-c1">optimize</span>(Optim<span class="pl-k">.</span><span class="pl-c1">only_fg!</span>(fg!), p0, Optim<span class="pl-k">.</span><span class="pl-c1">Options</span>(iterations<span class="pl-k">=</span><span class="pl-c1">1000</span>, store_trace<span class="pl-k">=</span><span class="pl-c1">true</span>))</pre></div>
<p>The utility provided by this package is the function <code>optfuns</code> which returns three functions and <code>p0</code>, a vectorized version of <code>pars</code>. BFGS typically has better convergence properties than, e.g., the ADAM optimizer. Here's a benchmark where BFGS in red beats ADAGrad with tuned step size in blue, and a <a href="https://arxiv.org/abs/1802.04310" rel="nofollow">stochastic L-BFGS [1]</a> (<a href="https://github.com/baggepinnen/FluxOptTools.jl/blob/master/src/SLBFGS.jl">implemented</a> in this repository) in green performs somewhere in between.
<a target="_blank" rel="noopener noreferrer" href="figs/losses.svg"><img src="figs/losses.svg" alt="losses" style="max-width:100%;"></a></p>
<p>The code for this benchmark is in the <code>runtests.jl</code>.</p>
<h2><a id="user-content-visualize-loss-landscape" class="anchor" aria-hidden="true" href="#visualize-loss-landscape"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Visualize loss landscape</h2>
<p>We define a plot recipe such that a loss landscape can be plotted with</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Plots
<span class="pl-c1">plot</span>(loss, pars, l<span class="pl-k">=</span><span class="pl-c1">0.1</span>, npoints<span class="pl-k">=</span><span class="pl-c1">50</span>, seriestype<span class="pl-k">=</span><span class="pl-c1">:contour</span>)</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="figs/landscape.svg"><img src="figs/landscape.svg" alt="landscape" style="max-width:100%;"></a></p>
<p>The landscape is plotted by selecting two random directions and extending the current point (<code>pars</code>) a distance <code>l*norm(pars)</code> (both negative and positive) along the two random directions. The number of loss evaluations will be <code>npoints^2</code>.</p>
<h2><a id="user-content-flatten-and-unflatten" class="anchor" aria-hidden="true" href="#flatten-and-unflatten"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Flatten and Unflatten</h2>
<p>What this package really does is flattening and reassembling the types <code>Flux.Params</code> and <code>Zygote.Grads</code> to and from vectors. These functions are used like so</p>
<div class="highlight highlight-source-julia"><pre>p <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(pars)  <span class="pl-c"><span class="pl-c">#</span> Creates a vector of length sum(length, pars)</span>
<span class="pl-c1">copyto!</span>(p,pars)  <span class="pl-c"><span class="pl-c">#</span> Store pars in vector p</span>
<span class="pl-c1">copyto!</span>(pars,p)  <span class="pl-c"><span class="pl-c">#</span> Reverse</span>

g <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(grads) <span class="pl-c"><span class="pl-c">#</span> Creates a vector of length sum(length, grads)</span>
<span class="pl-c1">copyto!</span>(g,grads) <span class="pl-c"><span class="pl-c">#</span> Store grads in vector g</span>
<span class="pl-c1">copyto!</span>(grads,g) <span class="pl-c"><span class="pl-c">#</span> Reverse</span></pre></div>
<p>This is what is used under the hood in the functions returned from <code>optfuns</code> in order to have everything on a form that Optim understands.</p>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h1>
<p><a href="https://arxiv.org/abs/1802.04310" rel="nofollow">[1] "Stochastic quasi-Newton with adaptive step lengths for large-scale problems", Adrian Wills, Thomas Sch√∂n, 2018</a></p>
</article></div>