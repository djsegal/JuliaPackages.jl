<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a href="https://github.com/baggepinnen/FluxOptTools.jl/actions"><img src="https://github.com/baggepinnen/FluxOptTools.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/baggepinnen/FluxOptTools.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/5a3f37c9f3f9b2dfc0b560b06cf09a302f58ff17e867cc4049b79fe73ac62a04/68747470733a2f2f636f6465636f762e696f2f67682f626167676570696e6e656e2f466c75784f7074546f6f6c732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/baggepinnen/FluxOptTools.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-fluxopttools" class="anchor" aria-hidden="true" href="#fluxopttools"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FluxOptTools</h1>
<p dir="auto">This package contains some utilities to enhance training of <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> models.</p>
<h2 dir="auto"><a id="user-content-train-using-optim" class="anchor" aria-hidden="true" href="#train-using-optim"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Train using Optim</h2>
<p dir="auto"><a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> can be used to train Flux models (if Flux is on version 0.10 or above), here's an example how</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Flux, Zygote, Optim, FluxOptTools, Statistics
m      = Chain(Dense(1,3,tanh) , Dense(3,1))
x      = LinRange(-pi,pi,100)'
y      = sin.(x)
loss() = mean(abs2, m(x) .- y)
Zygote.refresh()
pars   = Flux.params(m)
lossfun, gradfun, fg!, p0 = optfuns(loss, pars)
res = Optim.optimize(Optim.only_fg!(fg!), p0, Optim.Options(iterations=1000, store_trace=true))"><pre><span class="pl-k">using</span> Flux, Zygote, Optim, FluxOptTools, Statistics
m      <span class="pl-k">=</span> <span class="pl-c1">Chain</span>(<span class="pl-c1">Dense</span>(<span class="pl-c1">1</span>,<span class="pl-c1">3</span>,tanh) , <span class="pl-c1">Dense</span>(<span class="pl-c1">3</span>,<span class="pl-c1">1</span>))
x      <span class="pl-k">=</span> <span class="pl-c1">LinRange</span>(<span class="pl-k">-</span><span class="pl-c1">pi</span>,<span class="pl-c1">pi</span>,<span class="pl-c1">100</span>)<span class="pl-k">'</span>
y      <span class="pl-k">=</span> <span class="pl-c1">sin</span>.(x)
<span class="pl-en">loss</span>() <span class="pl-k">=</span> <span class="pl-c1">mean</span>(abs2, <span class="pl-c1">m</span>(x) <span class="pl-k">.-</span> y)
Zygote<span class="pl-k">.</span><span class="pl-c1">refresh</span>()
pars   <span class="pl-k">=</span> Flux<span class="pl-k">.</span><span class="pl-c1">params</span>(m)
lossfun, gradfun, fg!, p0 <span class="pl-k">=</span> <span class="pl-c1">optfuns</span>(loss, pars)
res <span class="pl-k">=</span> Optim<span class="pl-k">.</span><span class="pl-c1">optimize</span>(Optim<span class="pl-k">.</span><span class="pl-c1">only_fg!</span>(fg!), p0, Optim<span class="pl-k">.</span><span class="pl-c1">Options</span>(iterations<span class="pl-k">=</span><span class="pl-c1">1000</span>, store_trace<span class="pl-k">=</span><span class="pl-c1">true</span>))</pre></div>
<p dir="auto">The utility provided by this package is the function <code>optfuns</code> which returns three functions and <code>p0</code>, a vectorized version of <code>pars</code>. BFGS typically has better convergence properties than, e.g., the ADAM optimizer. Here's a benchmark where BFGS in red beats ADAGrad with tuned step size in blue, and a <a href="https://arxiv.org/abs/1802.04310" rel="nofollow">stochastic L-BFGS [1]</a> (<a href="https://github.com/baggepinnen/FluxOptTools.jl/blob/master/src/SLBFGS.jl">implemented</a> in this repository) in green performs somewhere in between.
<a target="_blank" rel="noopener noreferrer" href="figs/losses.svg"><img src="figs/losses.svg" alt="losses" style="max-width: 100%;"></a></p>
<p dir="auto">From a computational time perspective, S-LBFGS is about 2 times slower than ADAM (with additionnal memory complexity) while the traditional L-BFGS algorithm is around 3 times slower than ADAM (but similar memory burden as SL-BFGS).
<a target="_blank" rel="noopener noreferrer" href="figs/times.svg"><img src="figs/times.svg" alt="times" style="max-width: 100%;"></a></p>
<p dir="auto">The code for this benchmark is in the <code>runtests.jl</code>.</p>
<h2 dir="auto"><a id="user-content-visualize-loss-landscape" class="anchor" aria-hidden="true" href="#visualize-loss-landscape"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Visualize loss landscape</h2>
<p dir="auto">Based on the work on <a href="https://arxiv.org/abs/1712.09913" rel="nofollow">loss landscape visualization [2]</a>, we define a plot recipe such that a loss landscape can be plotted with</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Plots
contourf(() -&gt; log10(1 + loss()), pars, color=:turbo, npoints=50, lnorm=1)"><pre><span class="pl-k">using</span> Plots
<span class="pl-c1">contourf</span>(() <span class="pl-k">-&gt;</span> <span class="pl-c1">log10</span>(<span class="pl-c1">1</span> <span class="pl-k">+</span> <span class="pl-c1">loss</span>()), pars, color<span class="pl-k">=</span><span class="pl-c1">:turbo</span>, npoints<span class="pl-k">=</span><span class="pl-c1">50</span>, lnorm<span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="figs/landscape.svg"><img src="figs/landscape.svg" alt="landscape" style="max-width: 100%;"></a></p>
<p dir="auto">The landscape is plotted by selecting two random directions and extending the current point (<code>pars</code>) a distance <code>lnorm * norm(pars)</code> (both negative and positive) along the two random directions. The number of loss evaluations will be <code>npoints^2</code>.</p>
<h2 dir="auto"><a id="user-content-flatten-and-unflatten" class="anchor" aria-hidden="true" href="#flatten-and-unflatten"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Flatten and Unflatten</h2>
<p dir="auto">What this package really does is flattening and reassembling the types <code>Flux.Params</code> and <code>Zygote.Grads</code> to and from vectors. These functions are used like so</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="p = zeros(pars)  # Creates a vector of length sum(length, pars)
copy!(p,pars)  # Store pars in vector p
copy!(pars,p)  # Reverse

g = zeros(grads) # Creates a vector of length sum(length, grads)
copy!(g,grads) # Store grads in vector g
copy!(grads,g) # Reverse"><pre>p <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(pars)  <span class="pl-c"><span class="pl-c">#</span> Creates a vector of length sum(length, pars)</span>
<span class="pl-c1">copy!</span>(p,pars)  <span class="pl-c"><span class="pl-c">#</span> Store pars in vector p</span>
<span class="pl-c1">copy!</span>(pars,p)  <span class="pl-c"><span class="pl-c">#</span> Reverse</span>

g <span class="pl-k">=</span> <span class="pl-c1">zeros</span>(grads) <span class="pl-c"><span class="pl-c">#</span> Creates a vector of length sum(length, grads)</span>
<span class="pl-c1">copy!</span>(g,grads) <span class="pl-c"><span class="pl-c">#</span> Store grads in vector g</span>
<span class="pl-c1">copy!</span>(grads,g) <span class="pl-c"><span class="pl-c">#</span> Reverse</span></pre></div>
<p dir="auto">This is what is used under the hood in the functions returned from <code>optfuns</code> in order to have everything on a form that Optim understands.</p>
<h1 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h1>
<p dir="auto"><a href="https://arxiv.org/abs/1802.04310" rel="nofollow">[1] "Stochastic quasi-Newton with adaptive step lengths for large-scale problems", Adrian Wills, Thomas Sch√∂n, 2018</a></p>
<p dir="auto"><a href="https://arxiv.org/abs/1712.09913" rel="nofollow">[2] "Visualizing the Loss Landscape of Neural Nets", Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein, 2018</a></p>
</article></div>