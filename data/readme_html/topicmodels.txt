<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-topic-models-for-julia" class="anchor" aria-hidden="true" href="#topic-models-for-julia"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Topic Models for Julia</h1>
<p dir="auto">Topic models are Bayesian, hierarchical mixture models of discrete data.<br>
This package implements utilities for reading and manipulating data commonly
associated with topic models as well as inference and prediction procedures
for such models.</p>
<h2 dir="auto"><a id="user-content-model-description" class="anchor" aria-hidden="true" href="#model-description"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model description</h2>
<p dir="auto">The bulk of the package is designed for a particular topic model, Latent
Dirichlet Allocation (LDA, Blei et al., 2003).  This model assumes a corpus
composed of a collection of bags of words; each bag of words is termed a
document.  The space whence the words are drawn is termed the lexicon.</p>
<p dir="auto">Formally, the model is defined as</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="  For each topic k,
    phi_k ~ Dirichlet(beta)
  For each document d,
    theta ~ Dirichlet(alpha)
    For each word w,
      z ~ Multinomial(theta)
      w ~ Multinomial(phi_z)"><pre class="notranslate"><code>  For each topic k,
    phi_k ~ Dirichlet(beta)
  For each document d,
    theta ~ Dirichlet(alpha)
    For each word w,
      z ~ Multinomial(theta)
      w ~ Multinomial(phi_z)
</code></pre></div>
<p dir="auto">alpha and beta are hyperparameters of the model.  The number of topics, K,
is a fixed parameter of the model, and w is observed.  This package fits
the topics using collapsed Gibbs sampling (Griffiths and Steyvers, 2004).</p>
<h2 dir="auto"><a id="user-content-package-usage" class="anchor" aria-hidden="true" href="#package-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Package usage</h2>
<p dir="auto">We describe the functions of the package using an example. First we load
corpora from data files as follows:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="  testDocuments = readDocuments(open(&quot;cora.documents&quot;))
  testLexicon = readLexicon(open(&quot;cora.lexicon&quot;))"><pre class="notranslate"><code>  testDocuments = readDocuments(open("cora.documents"))
  testLexicon = readLexicon(open("cora.lexicon"))
</code></pre></div>
<p dir="auto">These read files in LDA-C format.  The lexicon file is assumed to have one
word per line.  The document file consists of one document per line.  Each
document consists of a collection of tuples; the first element of each tuple
expresses the word while the second element expresses the number of times
that word appears in the document.  The words are indicated by an index
into the lexicon into the lexicon file, starting at zero.  The tuples are
separated by spaces and the entire line is prefixed by a number indicating
the number of tuples for that document.</p>
<p dir="auto">With the documents loaded, we instantiate a model that we want to train:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="  model = Model(fill(0.1, 10), 0.01, length(testLexicon), testDocuments)"><pre class="notranslate"><code>  model = Model(fill(0.1, 10), 0.01, length(testLexicon), testDocuments)
</code></pre></div>
<p dir="auto">This is a model with 10 topics.  alpha is set to a uniform Dirichlet prior
with 0.1 weight on each topic (the dimension of this variable is used
to determine the number of topics).  The second parameter indicates that
the prior weight on phi (i.e. beta) should be set to 0.01.  The third
parameter is the lexicon size; here we just use the lexicon we have
just read.  The fourth parameter is the collection of documents.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="  trainModel(testDocuments, model, 30)"><pre class="notranslate"><code>  trainModel(testDocuments, model, 30)
</code></pre></div>
<p dir="auto">With the model defined, we can train the model on a corpus of documents.
The trainModel command takes the corpus as the first argument, the model
as the second argument, and the number of iterations of collapsed Gibbs
samplign to perform as the third argument.  The contents of the model
will be mutated in place.</p>
<p dir="auto">Finally we can examine the output of the trained model using topTopicWords.</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="  topWords = topTopicWords(model, testLexicon, 10)"><pre class="notranslate"><code>  topWords = topTopicWords(model, testLexicon, 10)
</code></pre></div>
<p dir="auto">This function retrieves the top words associated with each topic; this
serves as a useful summary of the model.  The first parameter is the model,
the second is the lexicon backing the corpus, and the third parameter
is the number of words to retrieve for each topic.  The output is an array
of arrays of the words in sorted order of prevalence in the topic.</p>
<h2 dir="auto"><a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>See also</h2>
<p dir="auto">The R package whence much of this code was derived at
<a href="https://github.com/slycoder/R-lda-deprecated">https://github.com/slycoder/R-lda-deprecated</a>.</p>
</article></div>