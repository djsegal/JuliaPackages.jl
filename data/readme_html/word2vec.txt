<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-word2vec" class="anchor" aria-hidden="true" href="#word2vec"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Word2Vec</h1>
<p dir="auto"><a href="LICENSE.md"><img src="https://camo.githubusercontent.com/bbf49a2eb96e6f718803f2493bd7aa3baae61abb09b7f8fc185a94e08c504dc6/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d627269676874677265656e2e7376673f7374796c653d666c6174" alt="License" data-canonical-src="http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat" style="max-width: 100%;"></a>
<a href="https://github.com/JuliaText/Word2Vec.jl/actions?query=workflow%3ACI"><img src="https://github.com/juliatext/Word2Vec.jl/workflows/CI/badge.svg?event=push&amp;branch=master" alt="CI" style="max-width: 100%;"></a>
<a href="https://juliahub.com/ui/Packages/Word2Vec/x04dc" rel="nofollow"><img src="https://camo.githubusercontent.com/ffc739df2b2884a5e05786f1df5b8b5ac8cad3f676ddbaa17a4a3d17c8ecd3f9/68747470733a2f2f6a756c69616875622e636f6d2f646f63732f576f7264325665632f76657273696f6e2e737667" alt="version" data-canonical-src="https://juliahub.com/docs/Word2Vec/version.svg" style="max-width: 100%;"></a>
<a href="https://juliahub.com/ui/Packages/Word2Vec/x04dc" rel="nofollow"><img src="https://camo.githubusercontent.com/e36b738490806ab8bca05e8393aa3109a4a8d1f5c45797f6a28bc39e5164d562/68747470733a2f2f6a756c69616875622e636f6d2f646f63732f576f7264325665632f706b676576616c2e737667" alt="pkgeval" data-canonical-src="https://juliahub.com/docs/Word2Vec/pkgeval.svg" style="max-width: 100%;"></a>
<a href="https://juliahub.com/ui/Packages/Word2Vec/x04dc?t=2" rel="nofollow"><img src="https://camo.githubusercontent.com/d7ddab13a778d44d5c5ccede817aef2a8e49a7005416051d6db59ee638747db9/68747470733a2f2f6a756c69616875622e636f6d2f646f63732f576f7264325665632f646570732e737667" alt="deps" data-canonical-src="https://juliahub.com/docs/Word2Vec/deps.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Julia interface to <a href="https://code.google.com/p/word2vec/" rel="nofollow">word2vec</a></p>
<p dir="auto">Word2Vec takes a text corpus as input and produces the word vectors as
output. Training is done using the original C code, other
functionalities are pure Julia. See <a href="http://nbviewer.ipython.org/github/JuliaText/Word2Vec.jl/blob/master/examples/demo.ipynb" rel="nofollow">demo</a> for more details.</p>
<ul dir="auto">
<li><a href="https://github.com/JuliaText/Word2Vec.jl/blob/master/NEWS.md">Release Notes</a></li>
</ul>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="Pkg.add(&quot;Word2Vec&quot;)"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>Word2Vec<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto"><strong>Note</strong>: Only linux and OS X are supported.</p>
<h2 dir="auto"><a id="user-content-functions" class="anchor" aria-hidden="true" href="#functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Functions</h2>
<p dir="auto">All exported functions are documented, i.e., we can type <code>? functionname</code>
to get help. For a list of functions, see <a href="https://github.com/JuliaText/Word2Vec.jl/blob/master/doc/README.md">here</a>.</p>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">We first download some text corpus, for example <a href="http://mattmahoney.net/dc/text8.zip" rel="nofollow">http://mattmahoney.net/dc/text8.zip</a>.</p>
<p dir="auto">Suppose the file <code>text8</code> is stored in the current working directory.
We can train the model with the function <code>word2vec</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; word2vec(&quot;text8&quot;, &quot;text8-vec.txt&quot;, verbose = true)
Starting training using file text8
Vocab size: 71291
Words in train file: 16718843
Alpha: 0.000002  Progress: 100.04%  Words/thread/sec: 350.44k  "><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">word2vec</span>(<span class="pl-s"><span class="pl-pds">"</span>text8<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>text8-vec.txt<span class="pl-pds">"</span></span>, verbose <span class="pl-k">=</span> <span class="pl-c1">true</span>)
Starting training <span class="pl-k">using</span> file text8
Vocab size<span class="pl-k">:</span> <span class="pl-c1">71291</span>
Words <span class="pl-k">in</span> train file<span class="pl-k">:</span> <span class="pl-c1">16718843</span>
Alpha<span class="pl-k">:</span> <span class="pl-c1">0.000002</span>  Progress<span class="pl-k">:</span> <span class="pl-c1">100.04</span><span class="pl-k">%</span>  Words<span class="pl-k">/</span>thread<span class="pl-k">/</span>sec<span class="pl-k">:</span> <span class="pl-c1">350.44</span>k  </pre></div>
<p dir="auto">Now we can import the word vectors <code>text8-vec.txt</code> to Julia.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; model = wordvectors(&quot;./text8-vec&quot;)
WordVectors 71291 words, 100-element Float64 vectors"><pre>julia<span class="pl-k">&gt;</span> model <span class="pl-k">=</span> <span class="pl-c1">wordvectors</span>(<span class="pl-s"><span class="pl-pds">"</span>./text8-vec<span class="pl-pds">"</span></span>)
WordVectors <span class="pl-c1">71291</span> words, <span class="pl-c1">100</span><span class="pl-k">-</span>element Float64 vectors</pre></div>
<p dir="auto">The vector representation of a word can be obtained using
<code>get_vector</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; get_vector(model, &quot;book&quot;)'
100-element Array{Float64,1}:
 -0.05446138539336186
  0.001090934639284009
  0.06498087707990222
  ⋮
 -0.0024113040415322516
  0.04755140828570571
  0.039764719065723826"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">get_vector</span>(model, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)<span class="pl-k">'</span>
<span class="pl-c1">100</span><span class="pl-k">-</span>element Array{Float64,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-k">-</span><span class="pl-c1">0.05446138539336186</span>
  <span class="pl-c1">0.001090934639284009</span>
  <span class="pl-c1">0.06498087707990222</span>
  ⋮
 <span class="pl-k">-</span><span class="pl-c1">0.0024113040415322516</span>
  <span class="pl-c1">0.04755140828570571</span>
  <span class="pl-c1">0.039764719065723826</span></pre></div>
<p dir="auto">The cosine similarity of <code>book</code>, for example, can be computed using
<code>cosine_similar_words</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; cosine_similar_words(model, &quot;book&quot;)
10-element Array{String,1}:
 &quot;book&quot;
 &quot;books&quot;
 &quot;diary&quot;
 &quot;story&quot;
 &quot;chapter&quot;
 &quot;novel&quot;
 &quot;preface&quot;
 &quot;poem&quot;
 &quot;tale&quot;
 &quot;bible&quot;"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">cosine_similar_words</span>(model, <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>)
<span class="pl-c1">10</span><span class="pl-k">-</span>element Array{String,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-s"><span class="pl-pds">"</span>book<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>books<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>diary<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>story<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>chapter<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>novel<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>preface<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>poem<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>tale<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>bible<span class="pl-pds">"</span></span></pre></div>
<p dir="auto">Word vectors have many interesting properties. For example,
<code>vector("king") - vector("man") + vector("woman")</code> is close to
<code>vector("queen")</code>.</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="5-element Array{String,1}:
 &quot;queen&quot;
 &quot;empress&quot;
 &quot;prince&quot;
 &quot;princess&quot;
 &quot;throne&quot;"><pre><span class="pl-c1">5</span><span class="pl-k">-</span>element Array{String,<span class="pl-c1">1</span>}<span class="pl-k">:</span>
 <span class="pl-s"><span class="pl-pds">"</span>queen<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>empress<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>prince<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>princess<span class="pl-pds">"</span></span>
 <span class="pl-s"><span class="pl-pds">"</span>throne<span class="pl-pds">"</span></span></pre></div>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<ul dir="auto">
<li>
<p dir="auto">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean,
"Efficient Estimation of Word Representations in Vector Space",
<em>In Proceedings of Workshop at ICLR</em>, 2013.
<a href="http://arxiv.org/pdf/1301.3781.pdf" rel="nofollow">[pdf]</a></p>
</li>
<li>
<p dir="auto">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
"Distributed Representations of Words and Phrases and their
Compositionality", <em>In Proceedings of NIPS</em>, 2013.
<a href="http://arxiv.org/pdf/1310.4546.pdf" rel="nofollow">[pdf]</a></p>
</li>
<li>
<p dir="auto">Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig,
"Linguistic Regularities in Continuous Space Word Representations",
<em>In Proceedings of NAACL HLT</em>, 2013.
<a href="http://research.microsoft.com/pubs/189726/rvecs.pdf" rel="nofollow">[pdf]</a></p>
</li>
</ul>
<h2 dir="auto"><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">The design of the package is inspired by Daniel Rodriguez
(@danielfrg)'s
<a href="https://github.com/danielfrg/word2vec">Python word2vec interface</a>.</p>
<h2 dir="auto"><a id="user-content-reporting-bugs" class="anchor" aria-hidden="true" href="#reporting-bugs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reporting Bugs</h2>
<p dir="auto">Please <a href="https://github.com/JuliaText/Word2Vec.jl/issues/new">file an issue</a> to report a bug or request a feature.</p>
</article></div>