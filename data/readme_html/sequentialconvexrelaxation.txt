<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-sequentialconvexrelaxationjl" class="anchor" aria-hidden="true" href="#sequentialconvexrelaxationjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SequentialConvexRelaxation.jl</h1>
<p>An introduction to the package can also be found <a href="https://rdoelman.bitbucket.io/2019/09/09/SequentialConvexRelaxationjl.html" rel="nofollow">here</a>.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="(v1.1) pkg&gt; add https://github.com/rdoelman/SequentialConvexRelaxation.jl
"><pre lang="julia-repl"><code>(v1.1) pkg&gt; add https://github.com/rdoelman/SequentialConvexRelaxation.jl
</code></pre></div>
<h2><a id="user-content-what-does-this-package-do" class="anchor" aria-hidden="true" href="#what-does-this-package-do"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What does this package do?</h2>
<p>This package gives a convenient approach to attempt to solve nonconvex optimization problems that can be expressed as convex problems with additional bilinear equality constraints.</p>
<p>Bilinear equality constraints are constraints of the form A(x) * P * B(x) == C(x), where A(x) and B(x) are (matrix valued) decision variables (any <code>AbstractExpr</code> in the Convex.jl modelling framework), P is a constant matrix and C(x) is either a decision variable or a constant matrix (again, any <code>AbstractExpr</code>).
The additional bilinear equality constraints make the overall optimization problem (in general) non-convex and non-linear.
This package uses a convex <strong>heuristic</strong> approach to find good and feasible solutions.</p>
<p>The advantages:</p>
<ul>
<li>The ease of use of Convex.jl to model your optimization problem.</li>
<li>Only an SDP solver needed (LP solver only in some cases).</li>
<li>No initial feasible guess is necessary for the solver to start.</li>
<li>Only 1 regularization parameter to tune (if necessary) for every bilinear equality constraint.</li>
</ul>
<p>The disadvantages:</p>
<ul>
<li>No convergence guarantees (the problem is non-convex and non-linear after all). No guarantees of finding global optima, or feasible solutions.</li>
<li>High computational complexity of iterations.</li>
<li>Depending on the solver and the size of the problem, numerical issues in the solvers can spoil the fun.</li>
</ul>
<h2><a id="user-content-what-kind-of-problems-can-i-try-to-solve-with-this" class="anchor" aria-hidden="true" href="#what-kind-of-problems-can-i-try-to-solve-with-this"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What kind of problems can I try to solve with this?</h2>
<p>The class of problems that can be formulated as "convex + bilinear (or quadratic) equality constraints" is very large. Although some of these have elegant solutions, often one has to look for specific solvers.</p>
<p>Some examples of "convex + bilinear equality constraints" include:</p>
<ul>
<li>matrix factorization</li>
<li>binary variables + LP/QP/SDP</li>
<li>sudokus (see the examples folder)</li>
</ul>
<p>More examples are listed <a href="https://rdoelman.bitbucket.io/2019/09/09/SequentialConvexRelaxationjl.html" rel="nofollow">here</a>.
If you have an interesting example that you want to share, please do not hesitate to get in touch!</p>
<h2><a id="user-content-how-do-i-use-it" class="anchor" aria-hidden="true" href="#how-do-i-use-it"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How do I use it?</h2>
<p>In this example we try to find the minimizers for the nonconvex <a href="https://en.wikipedia.org/wiki/Rosenbrock_function" rel="nofollow">Rosenbrock function</a>.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using Convex
using SequentialConvexRelaxation
using SCS

# We're minimizing the Rosenbrock function with standard values for a and b.
# f(x,y) = (a-x)^2 + b*(y-x^2)^2
a = 1.
b = 100.

# Another way to write the function is (a-x)^2 + b*z^2 where y-z = x^2.
x = Variable()
y = Variable()
z = Variable()

# This is the convex part
p = minimize(norm([a-x; sqrt(b)*z]))

# Constructing the bilinear equality constraint
# In general this is of the form A*P*B=C.
# Here A=B=x, P=1., C=y-z . X and Y are guesses for -A and -B.
x0 = -1 # some guess for the optimal x
λ = 0.1 # this is a regularization parameter
bc = BilinearConstraint(x,1.,x,y-z,X=-x0,Y=-x0,λ=λ)

# The bilinear problem is the convex problem with the bilinear constraint
bp = BilinearProblem(p,bc)

# Call the solve!() function on the bilinear problem
r = solve!(bp,() -&gt; SCS.Optimizer(verbose=0),iterations=2)

# Use Convex.jl to inspect the result. It turns out to be the global minimizer
xopt = evaluate(x)
yopt = evaluate(y)
"><pre><span class="pl-k">using</span> Convex
<span class="pl-k">using</span> SequentialConvexRelaxation
<span class="pl-k">using</span> SCS

<span class="pl-c"><span class="pl-c">#</span> We're minimizing the Rosenbrock function with standard values for a and b.</span>
<span class="pl-c"><span class="pl-c">#</span> f(x,y) = (a-x)^2 + b*(y-x^2)^2</span>
a <span class="pl-k">=</span> <span class="pl-c1">1.</span>
b <span class="pl-k">=</span> <span class="pl-c1">100.</span>

<span class="pl-c"><span class="pl-c">#</span> Another way to write the function is (a-x)^2 + b*z^2 where y-z = x^2.</span>
x <span class="pl-k">=</span> <span class="pl-c1">Variable</span>()
y <span class="pl-k">=</span> <span class="pl-c1">Variable</span>()
z <span class="pl-k">=</span> <span class="pl-c1">Variable</span>()

<span class="pl-c"><span class="pl-c">#</span> This is the convex part</span>
p <span class="pl-k">=</span> <span class="pl-c1">minimize</span>(<span class="pl-c1">norm</span>([a<span class="pl-k">-</span>x; <span class="pl-c1">sqrt</span>(b)<span class="pl-k">*</span>z]))

<span class="pl-c"><span class="pl-c">#</span> Constructing the bilinear equality constraint</span>
<span class="pl-c"><span class="pl-c">#</span> In general this is of the form A*P*B=C.</span>
<span class="pl-c"><span class="pl-c">#</span> Here A=B=x, P=1., C=y-z . X and Y are guesses for -A and -B.</span>
x0 <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">1</span> <span class="pl-c"><span class="pl-c">#</span> some guess for the optimal x</span>
λ <span class="pl-k">=</span> <span class="pl-c1">0.1</span> <span class="pl-c"><span class="pl-c">#</span> this is a regularization parameter</span>
bc <span class="pl-k">=</span> <span class="pl-c1">BilinearConstraint</span>(x,<span class="pl-c1">1.</span>,x,y<span class="pl-k">-</span>z,X<span class="pl-k">=</span><span class="pl-k">-</span>x0,Y<span class="pl-k">=</span><span class="pl-k">-</span>x0,λ<span class="pl-k">=</span>λ)

<span class="pl-c"><span class="pl-c">#</span> The bilinear problem is the convex problem with the bilinear constraint</span>
bp <span class="pl-k">=</span> <span class="pl-c1">BilinearProblem</span>(p,bc)

<span class="pl-c"><span class="pl-c">#</span> Call the solve!() function on the bilinear problem</span>
r <span class="pl-k">=</span> <span class="pl-c1">solve!</span>(bp,() <span class="pl-k">-&gt;</span> SCS<span class="pl-k">.</span><span class="pl-c1">Optimizer</span>(verbose<span class="pl-k">=</span><span class="pl-c1">0</span>),iterations<span class="pl-k">=</span><span class="pl-c1">2</span>)

<span class="pl-c"><span class="pl-c">#</span> Use Convex.jl to inspect the result. It turns out to be the global minimizer</span>
xopt <span class="pl-k">=</span> <span class="pl-c1">evaluate</span>(x)
yopt <span class="pl-k">=</span> <span class="pl-c1">evaluate</span>(y)</pre></div>
<p>To solve the problem with <code>Mosek</code> instead, replace the <code>solve!</code> call above with</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using MosekTools
r = solve!(bp,() -&gt; Mosek.Optimizer(QUIET=true),iterations=2)
"><pre><span class="pl-k">using</span> MosekTools
r <span class="pl-k">=</span> <span class="pl-c1">solve!</span>(bp,() <span class="pl-k">-&gt;</span> Mosek<span class="pl-k">.</span><span class="pl-c1">Optimizer</span>(QUIET<span class="pl-k">=</span><span class="pl-c1">true</span>),iterations<span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<p>For more examples, check the files in the 'examples' folder. It contains an interactive demo of exactly what the method does, an interactive demo with the Rosenbrock function and a demo on solving a sudoku puzzle.</p>
<p>The package exposes 4 functions: <code>BilinearConstraint</code>, <code>BinaryConstraint</code>,<code>BilinearProblem</code> and <code>solve!</code>.</p>
<p>A <code>BilinearConstraint(A,P,B,C)</code> is a constraint of the form A(x) * P * B(x) = C(x), where A(x) is a variable (constructed using Convex.jl), P is a constant (scalar or matrix), B(x) is a variable, C(x) is variable or a constant (scalar or matrix).</p>
<p>A <code>BinaryConstraint(A)</code> is a constraint of the form  A ∈ {0,1}.</p>
<p>A <code>BilinearProblem(p,bc)</code> consists of two elements: a convex problem <code>p</code> (as in <code>p = Convex.minimize(x)</code>), and (a vector of) additional bilinear equality constraints <code>bc</code> (making the complete problem non-convex in general).</p>
<p><code>solve!(bp, solver)</code> then attempts to solve the non-linear non-convex problem with a heuristic method (no guarantees here). It returns a structure with the result, that is also stored in a field of the <code>BilinearProblem</code>.</p>
<p>The values of the variables can be inspected using Convex.jl's <code>evaluate(x)</code>.
It is recommended to verify whether the bilinear equality constraints hold!</p>
<p>More description on how the functions work can be found using <code>?functionname</code>.</p>
<h2><a id="user-content-how-does-it-mathematically-work" class="anchor" aria-hidden="true" href="#how-does-it-mathematically-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How does it mathematically work?</h2>
<p>This package attempts to solve problems in the "convex + bilinear equality constraint" class and it constructs a regularized convex problem that is solved with conventional convex optimization solvers.
It does so by reformulating the problem to a rank constraint.
The package constructs a convex heuristic problem to handle this rank constraint, using the current guesses for the variables in the constraints (if no guess is supplied, a randomly generated matrix is used).
The resulting heuristic problem uses semidefinite constraints, so the convex optimization solver needs to be able to handle those. Use for example SCS or Mosek.
The solution provides new guesses for the optimal values, and the process is iterated for a specified number of iterations.</p>
<p>More details can also be found <a href="https://rdoelman.bitbucket.io/2019/09/09/SequentialConvexRelaxationjl.html" rel="nofollow">here</a>.</p>
<p>For mathematical background, see:
Doelman, Reinier, and Michel Verhaegen. "Sequential convex relaxation for convex optimization with bilinear matrix equalities." 2016 European Control Conference (ECC). IEEE, 2016.</p>
<h2><a id="user-content-citing-the-package-andor-method" class="anchor" aria-hidden="true" href="#citing-the-package-andor-method"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Citing the package and/or method</h2>
<p>The method:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="@inproceedings{doelman2016sequential,
  title={Sequential convex relaxation for convex optimization with bilinear matrix equalities},
  author={Doelman, Reinier and Verhaegen, Michel},
  booktitle={2016 European Control Conference (ECC)},
  pages={1946--1951},
  year={2016},
  organization={IEEE}
}
"><pre><code>@inproceedings{doelman2016sequential,
  title={Sequential convex relaxation for convex optimization with bilinear matrix equalities},
  author={Doelman, Reinier and Verhaegen, Michel},
  booktitle={2016 European Control Conference (ECC)},
  pages={1946--1951},
  year={2016},
  organization={IEEE}
}
</code></pre></div>
<p>The software:</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="@misc{SequentialConvexRelaxation,
  author={Reinier Doelman},
  title={SequentialConvexRelaxation.jl},
  howpublished={\url{https://github.com/rdoelman/SequentialConvexRelaxation.jl/}},
  year={2019}
}
"><pre><code>@misc{SequentialConvexRelaxation,
  author={Reinier Doelman},
  title={SequentialConvexRelaxation.jl},
  howpublished={\url{https://github.com/rdoelman/SequentialConvexRelaxation.jl/}},
  year={2019}
}
</code></pre></div>
<h1><a id="user-content-compatibility-issues" class="anchor" aria-hidden="true" href="#compatibility-issues"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Compatibility issues</h1>
<p>Currently Convex.jl and many solvers are in a process to switch to different optimization interface packages, which might break this package's functionality.
Currently the tested versions are:</p>
<ul>
<li>Convex.jl v0.13.1</li>
<li>MosekTools.jl v0.9.3</li>
<li>SCS.jl v0.6.6</li>
</ul>
</article></div>