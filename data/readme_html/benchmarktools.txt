<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-benchmarktoolsjl" class="anchor" aria-hidden="true" href="#benchmarktoolsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>BenchmarkTools.jl</h1>
<p><a href="https://JuliaCI.github.io/BenchmarkTools.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://JuliaCI.github.io/BenchmarkTools.jl/dev/" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://github.com/JuliaCI/BenchmarkTools.jl/actions/workflows/CI.yml?query=branch%3Amaster"><img src="https://github.com/JuliaCI/BenchmarkTools.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/JuliaCI/BenchmarkTools.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/7bfce6b181f3f113f7af223e2bfb87f0c63aa46eb985dda0a1e846810fbbe772/68747470733a2f2f636f6465636f762e696f2f67682f4a756c696143492f42656e63686d61726b546f6f6c732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f6c6162656c3d636f6465636f7626746f6b656e3d63634e374e5a706b4278" alt="Code Coverage" data-canonical-src="https://codecov.io/gh/JuliaCI/BenchmarkTools.jl/branch/master/graph/badge.svg?label=codecov&amp;token=ccN7NZpkBx" style="max-width:100%;"></a></p>
<p>BenchmarkTools makes <strong>performance tracking of Julia code easy</strong> by supplying a framework for <strong>writing and running groups of benchmarks</strong> as well as <strong>comparing benchmark results</strong>.</p>
<p>This package is used to write and run the benchmarks found in <a href="https://github.com/JuliaCI/BaseBenchmarks.jl">BaseBenchmarks.jl</a>.</p>
<p>The CI infrastructure for automated performance testing of the Julia language is not in this package, but can be found in <a href="https://github.com/JuliaCI/Nanosoldier.jl">Nanosoldier.jl</a>.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p>
BenchmarkTools is a  
    <a href="https://julialang.org" rel="nofollow">
        <img src="https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia.ico" width="16em" style="max-width:100%;">
        Julia Language
    </a>
      package. To install BenchmarkTools,
    please <a href="https://docs.julialang.org/en/v1/manual/getting-started/" rel="nofollow">open
    Julia's interactive session (known as REPL)</a> and press <kbd>]</kbd> key in the REPL to use the package mode, then type the following command
</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="pkg&gt; add BenchmarkTools
"><pre>pkg<span class="pl-k">&gt;</span> add BenchmarkTools</pre></div>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h2>
<p>If you're just getting started, check out the <a href="doc/manual.md">manual</a> for a thorough explanation of BenchmarkTools.</p>
<p>If you want to explore the BenchmarkTools API, see the <a href="doc/reference.md">reference document</a>.</p>
<p>If you want a short example of a toy benchmark suite, see the sample file in this repo (<a href="benchmark/benchmarks.jl">benchmark/benchmarks.jl</a>).</p>
<p>If you want an extensive example of a benchmark suite being used in the real world, you can look at the source code of <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/tree/nanosoldier">BaseBenchmarks.jl</a>.</p>
<p>If you're benchmarking on Linux, I wrote up a series of <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/linuxtips.md">tips and tricks</a> to help eliminate noise during performance tests.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick Start</h2>
<p>The primary macro provided by BenchmarkTools is <code>@benchmark</code>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; using BenchmarkTools

# The `setup` expression is run once per sample, and is not included in the
# timing results. Note that each sample can require multiple evaluations
# benchmark kernel evaluations. See the BenchmarkTools manual for details.
julia&gt; @benchmark sin(x) setup=(x=rand())
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     4.248 ns (0.00% GC)
  median time:      4.631 ns (0.00% GC)
  mean time:        5.502 ns (0.00% GC)
  maximum time:     60.995 ns (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     1000
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BenchmarkTools

<span class="pl-c"><span class="pl-c">#</span> The `setup` expression is run once per sample, and is not included in the</span>
<span class="pl-c"><span class="pl-c">#</span> timing results. Note that each sample can require multiple evaluations</span>
<span class="pl-c"><span class="pl-c">#</span> benchmark kernel evaluations. See the BenchmarkTools manual for details.</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">sin</span>(x) setup<span class="pl-k">=</span>(x<span class="pl-k">=</span><span class="pl-c1">rand</span>())
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">4.248</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">4.631</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">5.502</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">60.995</span> ns (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">1000</span></pre></div>
<p>For quick sanity checks, one can use the <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md#benchmarking-basics"><code>@btime</code> macro</a>, which is a convenience wrapper around <code>@benchmark</code> whose output is analogous to Julia's built-in <a href="https://docs.julialang.org/en/v1/base/base/#Base.@time" rel="nofollow"><code>@time</code> macro</a>:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; @btime sin(x) setup=(x=rand())
  4.361 ns (0 allocations: 0 bytes)
0.49587200950472454
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">sin</span>(x) setup<span class="pl-k">=</span>(x<span class="pl-k">=</span><span class="pl-c1">rand</span>())
  <span class="pl-c1">4.361</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">0.49587200950472454</span></pre></div>
<p>If the expression you want to benchmark depends on external variables, you should use <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md#interpolating-values-into-benchmark-expressions"><code>$</code> to "interpolate"</a> them into the benchmark expression to
<a href="https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-global-variables" rel="nofollow">avoid the problems of benchmarking with globals</a>.
Essentially, any interpolated variable <code>$x</code> or expression <code>$(...)</code> is "pre-computed" before benchmarking begins:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; A = rand(3,3);

julia&gt; @btime inv($A);            # we interpolate the global variable A with $A
  1.191 μs (10 allocations: 2.31 KiB)

julia&gt; @btime inv($(rand(3,3)));  # interpolation: the rand(3,3) call occurs before benchmarking
  1.192 μs (10 allocations: 2.31 KiB)

julia&gt; @btime inv(rand(3,3));     # the rand(3,3) call is included in the benchmark time
  1.295 μs (11 allocations: 2.47 KiB)
"><pre>julia<span class="pl-k">&gt;</span> A <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">inv</span>(<span class="pl-k">$</span>A);            <span class="pl-c"><span class="pl-c">#</span> we interpolate the global variable A with $A</span>
  <span class="pl-c1">1.191</span> μs (<span class="pl-c1">10</span> allocations<span class="pl-k">:</span> <span class="pl-c1">2.31</span> KiB)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">inv</span>(<span class="pl-k">$</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>)));  <span class="pl-c"><span class="pl-c">#</span> interpolation: the rand(3,3) call occurs before benchmarking</span>
  <span class="pl-c1">1.192</span> μs (<span class="pl-c1">10</span> allocations<span class="pl-k">:</span> <span class="pl-c1">2.31</span> KiB)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">inv</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">3</span>,<span class="pl-c1">3</span>));     <span class="pl-c"><span class="pl-c">#</span> the rand(3,3) call is included in the benchmark time</span>
  <span class="pl-c1">1.295</span> μs (<span class="pl-c1">11</span> allocations<span class="pl-k">:</span> <span class="pl-c1">2.47</span> KiB)</pre></div>
<p>Sometimes, interpolating variables into very simple expressions can give the compiler more information than you intended, causing it to "cheat" the benchmark by hoisting the calculation out of the benchmark code</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; a = 1; b = 2
2

julia&gt; @btime $a + $b
  0.024 ns (0 allocations: 0 bytes)
3
"><pre>julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">1</span>; b <span class="pl-k">=</span> <span class="pl-c1">2</span>
<span class="pl-c1">2</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-k">$</span>a <span class="pl-k">+</span> <span class="pl-k">$</span>b
  <span class="pl-c1">0.024</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">3</span></pre></div>
<p>As a rule of thumb, if a benchmark reports that it took less than a nanosecond to perform, this hoisting probably occured. You can avoid this by referencing and dereferencing the interpolated variables</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="julia&gt; @btime $(Ref(a))[] + $(Ref(b))[]
  1.277 ns (0 allocations: 0 bytes)
3
"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-k">$</span>(<span class="pl-c1">Ref</span>(a))[] <span class="pl-k">+</span> <span class="pl-k">$</span>(<span class="pl-c1">Ref</span>(b))[]
  <span class="pl-c1">1.277</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">3</span></pre></div>
<p>As described the <a href="doc/manual.md">manual</a>, the BenchmarkTools package supports many other features, both for additional output and for more fine-grained control over the benchmarking process.</p>
<h2><a id="user-content-why-does-this-package-exist" class="anchor" aria-hidden="true" href="#why-does-this-package-exist"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why does this package exist?</h2>
<p>Our story begins with two packages, "Benchmarks" and "BenchmarkTrackers". The Benchmarks package implemented an execution strategy for collecting and summarizing individual benchmark results, while BenchmarkTrackers implemented a framework for organizing, running, and determining regressions of groups of benchmarks. Under the hood, BenchmarkTrackers relied on Benchmarks for actual benchmark execution.</p>
<p>For a while, the Benchmarks + BenchmarkTrackers system was used for automated performance testing of Julia's Base library. It soon became apparent that the system suffered from a variety of issues:</p>
<ol>
<li>Individual sample noise could significantly change the execution strategy used to collect further samples.</li>
<li>The estimates used to characterize benchmark results and to detect regressions were statistically vulnerable to noise (i.e. not robust).</li>
<li>Different benchmarks have different noise tolerances, but there was no way to tune this parameter on a per-benchmark basis.</li>
<li>Running benchmarks took a long time - an order of magnitude longer than theoretically necessary for many functions.</li>
<li>Using the system in the REPL (for example, to reproduce regressions locally) was often cumbersome.</li>
</ol>
<p>The BenchmarkTools package is a response to these issues, designed by examining user reports and the benchmark data generated by the old system. BenchmarkTools offers the following solutions to the corresponding issues above:</p>
<ol>
<li>Benchmark execution parameters are configured separately from the execution of the benchmark itself. This means that subsequent experiments are performed more consistently, avoiding branching "substrategies" based on small numbers of samples.</li>
<li>A variety of simple estimators are supported, and the user can pick which one to use for regression detection.</li>
<li>Noise tolerance has been made a per-benchmark configuration parameter.</li>
<li>Benchmark configuration parameters can be easily cached and reloaded, significantly reducing benchmark execution time.</li>
<li>The API is simpler, more transparent, and overall easier to use.</li>
</ol>
<h2><a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgements</h2>
<p>This package was authored primarily by Jarrett Revels (@jrevels). Additionally, I'd like to thank the following people:</p>
<ul>
<li>John Myles White, for authoring the original Benchmarks package, which greatly inspired BenchmarkTools</li>
<li>Andreas Noack, for statistics help and investigating weird benchmark time distributions</li>
<li>Oscar Blumberg, for discussions on noise robustness</li>
<li>Jiahao Chen, for discussions on error analysis</li>
</ul>
</article></div>