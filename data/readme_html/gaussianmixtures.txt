<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-gaussianmixtures" class="anchor" aria-hidden="true" href="#gaussianmixtures"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GaussianMixtures</h1>
<h2><a id="user-content-a-julia-package-for-gaussian-mixture-models-gmms" class="anchor" aria-hidden="true" href="#a-julia-package-for-gaussian-mixture-models-gmms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>A Julia package for Gaussian Mixture Models (GMMs).</h2>
<p><a href="https://travis-ci.org/davidavdav/GaussianMixtures.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/47787ea8dd9c17d117396bae5d24c6e5ab2c995f/68747470733a2f2f7472617669732d63692e6f72672f646176696461766461762f476175737369616e4d697874757265732e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/davidavdav/GaussianMixtures.jl.svg?branch=master" style="max-width:100%;"></a></p>
<p>This package contains support for Gaussian Mixture Models.  Basic training, likelihood calculation, model adaptation, and i/o are implemented.</p>
<p>This Julia type is more specific than Dahua Lin's <a href="http://distributionsjl.readthedocs.org/en/latest/mixture.html" rel="nofollow">MixtureModels</a>, in that it deals only with normal (multivariate) distributions (a.k.a Gaussians), but it does so more efficiently, hopefully.  We have support for switching between GMM and MixtureModels types.</p>
<p>At this moment, we have implemented both diagonal covariance and full covariance GMMs, and full covariance variational Bayes GMMs.</p>
<p>In training the parameters of a GMM using the Expectation Maximization (EM) algorithm, the inner loop (computing the Baum-Welch statistics) can be executed efficiently using Julia's standard parallelization infrastructure, e.g., by using SGE.  We further support very large data (larger than will fit in the combined memory of the computing cluster) though <a href="https://github.com/davidavdav/BigData.jl">BigData</a>, which has now been incorporated in this package.</p>
<h2><a id="user-content-install" class="anchor" aria-hidden="true" href="#install"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install</h2>
<div class="highlight highlight-source-julia"><pre>Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>GaussianMixtures<span class="pl-pds">"</span></span>)</pre></div>
<h2><a id="user-content-vector-dimensions" class="anchor" aria-hidden="true" href="#vector-dimensions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vector dimensions</h2>
<p>Some remarks on the dimension.  There are three main indexing variables:</p>
<ul>
<li>The Gaussian index</li>
<li>The data point</li>
<li>The feature dimension (for full covariance this adds to two dimensions)</li>
</ul>
<p>Often data is stored in 2D slices, and computations can be done efficiently as
matrix multiplications.  For this it is nice to have the data in standard row,column order.
However, we can't have these consistently over all three indexes.</p>
<p>My approach is to have:</p>
<ul>
<li>The data index (<code>i</code>) always be a the first (row) index</li>
<li>The feature dimension index (<code>k</code>) always to be a the second (column) index</li>
<li>The Gaussian index (<code>j</code>) to be mixed, depending on how it is combined with either dimension above.</li>
</ul>
<p>The consequence is that "data points run down" in a matrix, just like records do in a DataFrame.  Hence, statistics per feature dimension occur consecutive in memory which may be advantageous for caching efficiency.  On the other hand, features belonging to the same data point are separated in memory, which probably is not according to the way they are generated, and does not extend to streamlined implementation.  The choice in which direction the data must run is an almost philosophical problem that I haven't come to a final conclusion about.</p>
<p>Please note that the choice for "data points run down" is the opposite of the convention used in <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>, so if you convert <code>GMM</code>s to <code>MixtureModel</code>s in order to benefit from the methods provided for these distributions, you need to transpose the data.</p>
<h2><a id="user-content-type" class="anchor" aria-hidden="true" href="#type"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Type</h2>
<p>A simplified version of the type definition for the Gaussian Mixture Model is</p>
<div class="highlight highlight-source-julia"><pre>type GMM
    n<span class="pl-k">::</span><span class="pl-c1">Int</span>                         <span class="pl-c"><span class="pl-c">#</span> number of Gaussians</span>
    d<span class="pl-k">::</span><span class="pl-c1">Int</span>                         <span class="pl-c"><span class="pl-c">#</span> dimension of Gaussian</span>
    w<span class="pl-k">::</span><span class="pl-c1">Vector</span>                      <span class="pl-c"><span class="pl-c">#</span> weights: n</span>
    μ<span class="pl-k">::</span><span class="pl-c1">Array</span>                       <span class="pl-c"><span class="pl-c">#</span> means: n x d</span>
    Σ<span class="pl-k">::</span><span class="pl-c1">Union(Array, Vector{Array})</span> <span class="pl-c"><span class="pl-c">#</span> diagonal covariances n x d, or Vector n of d x d full covariances</span>
    hist<span class="pl-k">::</span><span class="pl-c1">Array{History}</span>           <span class="pl-c"><span class="pl-c">#</span> history of this GMM</span>
<span class="pl-k">end</span></pre></div>
<p>Currently, the variable <code>Σ</code> is heterogeneous both in form and interpretation, depending on whether it represents full covariance or diagonal covariance matrices.</p>
<ul>
<li>full covariance: <code>Σ</code> is represented by a <code>Vector</code> of <code>chol(inv(Σ), :U)</code></li>
<li>diagonal covariance: <code>Σ</code> is formed by vertically stacking row-vectors of <code>diag(Σ)</code></li>
</ul>
<h2><a id="user-content-constructors" class="anchor" aria-hidden="true" href="#constructors"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Constructors</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">GMM</span>(weights<span class="pl-k">::</span><span class="pl-c1">Vector</span>, μ<span class="pl-k">::</span><span class="pl-c1">Array</span>,  Σ<span class="pl-k">::</span><span class="pl-c1">Array</span>, hist<span class="pl-k">::</span><span class="pl-c1">Vector</span>)</pre></div>
<p>This is the basic outer constructor for GMM.  Here we have</p>
<ul>
<li><code>weights</code> a Vector of length <code>n</code> with the weights of the mixtures</li>
<li><code>μ</code> a matrix of <code>n</code> by <code>d</code> means of the Gaussians</li>
<li><code>Σ</code> either a matrix of <code>n</code> by <code>d</code> variances of diagonal Gaussians,
or a vector of <code>n</code> <code>Triangular</code> matrices of <code>d</code> by <code>d</code>,
representing the Cholesky decomposition of the full covariance</li>
<li><code>hist</code> a vector of <code>History</code> objects describing how the GMM was
obtained. (The type <code>History</code> simply contains a time <code>t</code> and a comment
string <code>s</code>)</li>
</ul>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">GMM</span>(x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>; kind<span class="pl-k">=</span><span class="pl-c1">:diag</span>)
<span class="pl-c1">GMM</span>(x<span class="pl-k">::</span><span class="pl-c1">Vector</span>)</pre></div>
<p>Create a GMM with 1 mixture, i.e., a multivariate Gaussian, and initialize with mean an variance of the data in <code>x</code>.  The data in <code>x</code> must be a <code>nx</code> x <code>d</code> Matrix, where <code>nx</code> is the number of data points, or a Vector of length <code>nx</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">GMM</span>(n<span class="pl-k">:</span>Int, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>; method<span class="pl-k">=</span><span class="pl-c1">:kmeans</span>, kind<span class="pl-k">=</span><span class="pl-c1">:diag</span>, nInit<span class="pl-k">=</span><span class="pl-c1">50</span>, nIter<span class="pl-k">=</span><span class="pl-c1">10</span>, nFinal<span class="pl-k">=</span>nIter)</pre></div>
<p>Create a GMM with <code>n</code> mixtures, given the training data <code>x</code> and using the Expectation Maximization algorithm.  There are two ways of arriving at <code>n</code> Gaussians: <code>method=:kmeans</code> uses K-means clustering from the Clustering package to initialize with <code>n</code> centers.  <code>nInit</code> is the number of iterations for the K-means algorithm, <code>nIter</code> the number of iterations in EM.  The method <code>:split</code> works by initializing a single Gaussian with the data <code>x</code> and subsequently splitting the Gaussians followed by retraining using the EM algorithm until <code>n</code> Gaussians are obtained.  <code>n</code> must be a power of 2 for <code>method=:split</code>.  <code>nIter</code> is the number of iterations in the EM algorithm, and <code>nFinal</code> the number of iterations in the final step.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">GMM</span>(n<span class="pl-k">::</span><span class="pl-c1">Int</span>, d<span class="pl-k">::</span><span class="pl-c1">Int</span>; kind<span class="pl-k">=</span><span class="pl-c1">:diag</span>)</pre></div>
<p>Initialize a GMM with <code>n</code> multivariate Gaussians of dimension <code>d</code>.
The means are all set to <strong>0</strong> (the origin) and the variances to
<strong>I</strong>, which is silly by itself.  If <code>kind=:full</code> is specified, the
covariances are full rather than diagonal.  One should replace the
values of the weights, means and covariances afterwards.</p>
<h2><a id="user-content-training-functions" class="anchor" aria-hidden="true" href="#training-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training functions</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">em!</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>; nIter<span class="pl-k">::</span><span class="pl-c1">Int</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>, varfloor<span class="pl-k">=</span><span class="pl-c1">1e-3</span>)</pre></div>
<p>Update the parameters of the GMM using the Expectation Maximization (EM) algorithm <code>nIter</code> times, optimizing the log-likelihood given the data <code>x</code>.   The function <code>em!()</code> returns a vector of average log likelihoods for each of the intermediate iterations of the GMM given the training data.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">llpg</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>)</pre></div>
<p>Returns <code>ll_ij = log p(x_i | gauss_j)</code>, the Log Likelihood Per Gaussian <code>j</code> given data point <code>x_i</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">avll</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>)</pre></div>
<p>Computes the average log likelihood of the GMM given all data points,
further normalized by the feature dimension <code>d = size(x,2)</code>. A
1-mixture GMM then has an <code>avll</code> of <code>-(log(2π) +0.5 +log(σ))</code> if the
data <code>x</code> is distributed as a multivariate diagonal covariance Gaussian
with <code>Σ = σI</code>.  With <code>σ=1</code> we then have <code>avll≈-1.42</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">gmmposterior</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>)</pre></div>
<p>Returns a tuple containing  <code>p_ij = p(j | gmm, x_i)</code>, the posterior probability that data point <code>x_i</code> 'belongs' to Gaussian <code>j</code>, and the Log Likelihood Per Gaussian <code>j</code> given data point <code>x_i</code> as in <code>llpg</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">history</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>)</pre></div>
<p>Shows the history of the GMM, i.e., how it was initialized, split, how the parameters were trained, etc.  A history item contains a time of completion and an event string.</p>
<p>You can examine a minimal example <a href="https://gist.github.com/mbeltagy/22e7fdf7e3be3fbfd97f1bde7a789b91">using GMM for clustering</a>.</p>
<h2><a id="user-content-other-functions" class="anchor" aria-hidden="true" href="#other-functions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Other functions</h2>
<ul>
<li><code>split(gmm; minweight=1e-5, covfactor=0.2)</code>: Doubles the number of Gaussians by splitting each Gaussian into two Gaussians.  <code>minweight</code> is used for pruning Gaussians with too little weight, these are replaced by an extra split of the Gaussian with the highest weight.  <code>covfactor</code> controls how far apart the means of the split Gaussian are positioned.</li>
<li><code>kind(gmm)</code>: returns <code>:diag</code> or <code>:full</code>, depending on the type of covariance matrix</li>
<li><code>eltype(gmm)</code>: returns the datatype of <code>w</code>, <code>μ</code> and <code>Σ</code> in the GMM</li>
<li><code>weights{gmm)</code>: returns the weights vector <code>w</code></li>
<li><code>means(gmm)</code>: returns the means <code>μ</code> as an <code>n</code> by <code>d</code> matrix</li>
<li><code>covars(gmm)</code>: returns the covariances <code>Σ</code></li>
<li><code>copy(gmm)</code>: returns a deep copy of the GMM</li>
<li><code>full(gmm)</code>: returns a full covariance GMM based on <code>gmm</code></li>
<li><code>diag(gmm)</code>: returns a diagonal GMM, by ignoring off-diagonal elementsin <code>gmm</code></li>
</ul>
<h2><a id="user-content-converting-the-gmms" class="anchor" aria-hidden="true" href="#converting-the-gmms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Converting the GMMs</h2>
<p>The element type in the GMM can be changed like you would expect.  We also have import and export functions for <code>MixtureModel</code>, which currently only has <code>Float64</code> element type.</p>
<ul>
<li><code>convert(::Type{GMM{datatype}}, gmm)</code>: convert the data type of the GMM</li>
<li><code>float16(gmm)</code>, <code>float32(gmm)</code>, <code>float64(gmm)</code>: convenience functions for <code>convert()</code></li>
<li><code>MixtureModel(gmm)</code>: construct an instance of type <code>MixtureModel</code> from the GMM.  Please note that for functions like <code>pdf(m::MixtureModel, x::Matrix)</code> the data <code>x</code> run "sideways" rather than "down" as in this package.</li>
<li><code>GMM(m::MixtureModel{Multivariate,Continuous,MvNormal})</code>: construct a GMM from the right kind of MixtureModel.</li>
</ul>
<h2><a id="user-content-paralellization" class="anchor" aria-hidden="true" href="#paralellization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Paralellization</h2>
<p>Training a large GMM with huge quantities of data can take a significant amount of time.  We have built-in support for the parallelization infrastructure in Julia.</p>
<p>The method <code>stats()</code>, which is at the heart of EM algorithm, can detect multiple processors available (through <code>nprocs()</code>).  If there is more than 1 processor available, the data is split into chunks, each chunk is mapped to a separate processor, and afterwards all the statistics from the sub-processes are aggregated.  In an SGE environment you can obtain more cores (in the example below 20) by issuing</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> ClusterManagers
ClusterManagers<span class="pl-k">.</span><span class="pl-c1">addprocs_sge</span>(<span class="pl-c1">20</span>)                                        
<span class="pl-c1">@everywhere</span> <span class="pl-k">using</span> GaussianMixtures</pre></div>
<p>The size of the GMM and the data determine whether or not it is advantageous to do this.</p>
<h2><a id="user-content-memory" class="anchor" aria-hidden="true" href="#memory"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Memory</h2>
<p>The <code>stats()</code> method (see below) needs to be very efficient because for many algorithms it is at the inner loop of the calculation.  We have a highly optimized BLAS friendly and parallizable implementation, but this requires a fair bit of memory.  Therefore the input data is processed in blocks in such a way that only a limited amount of memory is used.  By default this is set at 2GB, but it can be specified though a gobal setting:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">setmem</span>(gig) </pre></div>
<p>Set the memory approximately used in <code>stats()</code>, in Gigabytes.</p>
<h2><a id="user-content-baum-welch-statistics" class="anchor" aria-hidden="true" href="#baum-welch-statistics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Baum-Welch statistics</h2>
<p>At the heart of EM training, and to many other operations with GMMs, lies the computation of the Baum-Welch statistics of the data when aligning them to the GMM.  We have optimized implementations of the basic calculation of the statistics:</p>
<ul>
<li><code>stats(gmm::GMM, x::Matrix; order=2, parallel=true)</code> Computes the Baum-Welch statistics up to order <code>order</code> for the alignment of the data <code>x</code> to the Universal Background GMM <code>gmm</code>.  The 1st and 2nd order statistics are retuned as an <code>n</code> x <code>d</code> matrix, so for obtaining statistics in supervector format, flattening needs to be carried out in the right direction.  Theses statistics are <em>uncentered</em>.</li>
</ul>
<h2><a id="user-content-random-gmms" class="anchor" aria-hidden="true" href="#random-gmms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Random GMMs</h2>
<p>Sometimes is it insteresting to generate random GMMs, and use these to genrate random points.</p>
<div class="highlight highlight-source-julia"><pre>g <span class="pl-k">=</span> <span class="pl-c1">rand</span>(GMM, n, d; kind<span class="pl-k">=</span><span class="pl-c1">:full</span>, sep<span class="pl-k">=</span><span class="pl-c1">2.0</span>)</pre></div>
<p>This generates a GMM with normally distributed means according to N(x|μ=0,Σ=sep*I).  The covariance matrices are also chosen random.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">rand</span>(g<span class="pl-k">::</span><span class="pl-c1">GMM</span>, n)</pre></div>
<p>Generate <code>n</code> datapoints sampled from the GMM, resulting in a <code>n</code> times <code>g.d</code> array.</p>
<h2><a id="user-content-speaker-recognition-methods" class="anchor" aria-hidden="true" href="#speaker-recognition-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Speaker recognition methods</h2>
<p>The following methods are used in speaker- and language recognition, they may eventually move to another module.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">csstats</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>, order<span class="pl-k">=</span><span class="pl-c1">2</span>)</pre></div>
<p>Computes <em>centered</em> and <em>scaled</em> statistics.  These are similar as above, but centered w.r.t the UBM mean and scaled by the covariance.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">CSstats</span>(x<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>)</pre></div>
<p>This constructor return a <code>CSstats</code> object for centered stats of order 1.  The type is currently defined as:</p>
<div class="highlight highlight-source-julia"><pre>type CSstats
    n<span class="pl-k">::</span><span class="pl-c1">Vector</span>           <span class="pl-c"><span class="pl-c">#</span> zero-order stats, ng</span>
    f<span class="pl-k">::</span><span class="pl-c1">Matrix</span>          <span class="pl-c"><span class="pl-c">#</span> first-order stats, ng * d</span>
<span class="pl-k">end</span></pre></div>
<p>The CSstats type can be used for MAP adaptation and a simple but elegant dotscoring speaker recognition system.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">dotscore</span>(x<span class="pl-k">::</span><span class="pl-c1">CSstats</span>, y<span class="pl-k">::</span><span class="pl-c1">CSstats</span>, r<span class="pl-k">::</span><span class="pl-c1">Float64</span><span class="pl-k">=</span><span class="pl-c1">1.</span>) </pre></div>
<p>Computes the dot-scoring approximation to the GMM/UBM log likelihood ratio for a GMM MAP adapted from the UBM (means only) using the data from <code>x</code> and a relevance factor of <code>r</code>, and test data from <code>y</code>.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">maxapost</span>(gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>, x<span class="pl-k">::</span><span class="pl-c1">Matrix</span>, r<span class="pl-k">=</span><span class="pl-c1">16.</span>; means<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">true</span>, weights<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>, covars<span class="pl-k">::</span><span class="pl-c1">Bool</span><span class="pl-k">=</span><span class="pl-c1">false</span>)</pre></div>
<p>Perform Maximum A Posterior (MAP) adaptation of the UBM <code>gmm</code> to the data from <code>x</code> using relevance <code>r</code>.  <code>means</code>, <code>weights</code> and <code>covars</code> indicate which parts of the UBM need to be updated.</p>
<h2><a id="user-content-saving--loading-a-gmm" class="anchor" aria-hidden="true" href="#saving--loading-a-gmm"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Saving / loading a GMM</h2>
<p>Using package JLD, two methods allow saving a GMM or an array of GMMs to disk:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> JLD
<span class="pl-c1">save</span>(filename<span class="pl-k">::</span><span class="pl-c1">String</span>, name<span class="pl-k">::</span><span class="pl-c1">String</span>, gmm<span class="pl-k">::</span><span class="pl-c1">GMM</span>)
<span class="pl-c1">save</span>(filename<span class="pl-k">::</span><span class="pl-c1">String</span>, name<span class="pl-k">::</span><span class="pl-c1">String</span>, gmms<span class="pl-k">::</span><span class="pl-c1">Array{GMM}</span>)</pre></div>
<p>This saves a GMM of an array of GMMs under the name <code>name</code> in a file <code>filename</code>. The data can be loaded back into a julia session using plain JLD's</p>
<div class="highlight highlight-source-julia"><pre>gmm <span class="pl-k">=</span> <span class="pl-c1">load</span>(filename, name)</pre></div>
<p>In case of using <code>kind=:full</code> covariance matrices make sure you have also loaded <code>LinearAlgebra</code> module into the current session. Without it JLD is unable to reconstruct <code>LinearAlgebra.UpperTriangular</code> type and gmm object won't be created</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> JLD
<span class="pl-k">using</span> GaussianMixtures
<span class="pl-k">using</span> LinearAlgebra
gmm <span class="pl-k">=</span> <span class="pl-c1">load</span>(filename, name)</pre></div>
<h2><a id="user-content-support-for-large-amounts-of-training-data" class="anchor" aria-hidden="true" href="#support-for-large-amounts-of-training-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Support for large amounts of training data</h2>
<p>In many of the functions defined above, a <code>Data</code> type is accepted in the place where the data matrix <code>x</code> is indicated.  An object of type <code>Data</code> is basically a list of either matrices of filenames, see  <a href="https://github.com/davidavdav/BigData.jl">BigData</a>.</p>
<p>If <code>kind(x::Data)==:file</code>, then the matrix <code>x</code> is represented by vertically stacking the matrices that can be obtained by loading the files listed in <code>d</code> from disc.  The functions in GaussianMixtures try to run computations in parallel by processing the files in <code>d</code> simultaneously on multiple cores/machines, and they further try to limit the number of times the data needs to be loaded form disc.  In parallel execution on a computer cluster, an attempt is made to ensure the same data is always processed by the same worker, so that local file caching could work to your advantage.</p>
<h2><a id="user-content-variational-bayes-training" class="anchor" aria-hidden="true" href="#variational-bayes-training"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Variational Bayes training</h2>
<p>We have started support for Variational Bayes GMMs.  Here, the parameters of the GMM are not point estimates but are rather represented by a distribution.  Training of the parameters that govern these distributions can be carried out by an EM-like algorithm.</p>
<p>In our implementation, we follow the approach from section 10.2 of <a href="http://research.microsoft.com/en-us/um/people/cmbishop/PRML/" rel="nofollow">Christopher Bishop's book</a>.  In the current version of GaussianMixtures we have not attempted to optimize the code for efficiency.</p>
<p>The variational Bayes training uses two new types, a prior and a variational GMM:</p>
<div class="highlight highlight-source-julia"><pre>type GMMprior
    α<span class="pl-c1">0</span>                       <span class="pl-c"><span class="pl-c">#</span> effective prior number of observations</span>
    β<span class="pl-c1">0</span>
    m0<span class="pl-k">::</span><span class="pl-c1">Vector</span>               <span class="pl-c"><span class="pl-c">#</span> prior on μ</span>
    ν<span class="pl-c1">0</span>                       <span class="pl-c"><span class="pl-c">#</span> scale precision</span>
    W0<span class="pl-k">::</span><span class="pl-c1">Matrix</span>               <span class="pl-c"><span class="pl-c">#</span> prior precision</span>
<span class="pl-k">end</span>

type VGMM
    n<span class="pl-k">::</span><span class="pl-c1">Int</span>                   <span class="pl-c"><span class="pl-c">#</span> number of Gaussians</span>
    d<span class="pl-k">::</span><span class="pl-c1">Int</span>                   <span class="pl-c"><span class="pl-c">#</span> dimension of Gaussian</span>
    π<span class="pl-k">::</span><span class="pl-c1">GMMprior</span>              <span class="pl-c"><span class="pl-c">#</span> The prior used in this VGMM</span>
    α<span class="pl-k">::</span><span class="pl-c1">Vector</span>                <span class="pl-c"><span class="pl-c">#</span> Dirichlet, n</span>
    β<span class="pl-k">::</span><span class="pl-c1">Vector</span>                <span class="pl-c"><span class="pl-c">#</span> scale of precision, n</span>
    m<span class="pl-k">::</span><span class="pl-c1">Matrix</span>                <span class="pl-c"><span class="pl-c">#</span> means of means, n * d</span>
    ν<span class="pl-k">::</span><span class="pl-c1">Vector</span>                <span class="pl-c"><span class="pl-c">#</span> no. degrees of freedom, n</span>
    W<span class="pl-k">::</span><span class="pl-c1">Vector{Matrix}</span>        <span class="pl-c"><span class="pl-c">#</span> scale matrix for precision? n * d * d</span>
    hist<span class="pl-k">::</span><span class="pl-c1">Vector{History}</span>    <span class="pl-c"><span class="pl-c">#</span> history</span>
<span class="pl-k">end</span></pre></div>
<p>Please note that we currently only have full covariance VGMMs.<br>
Training using observed data <code>x</code> needs some initial GMM and a prior.</p>
<div class="highlight highlight-source-julia"><pre>g <span class="pl-k">=</span> <span class="pl-c1">GMM</span>(<span class="pl-c1">8</span>, x, kind<span class="pl-k">=</span><span class="pl-c1">:full</span>, nIter<span class="pl-k">=</span><span class="pl-c1">0</span>) <span class="pl-c"><span class="pl-c">#</span># only do k-means initialization of GMM g</span>
p <span class="pl-k">=</span> <span class="pl-c1">GMMprior</span>(g<span class="pl-k">.</span>d, <span class="pl-c1">0.1</span>, <span class="pl-c1">1.0</span>)  <span class="pl-c"><span class="pl-c">#</span># set α0=0.1 and β0=1, and other values to a default</span>
v <span class="pl-k">=</span> <span class="pl-c1">VGMM</span>(g, p) <span class="pl-c"><span class="pl-c">#</span># initialize variational GMM v with g</span></pre></div>
<p>Training can then proceed with</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">em!</span>(v, x)</pre></div>
<p>This EM training checks if the occupancy of the Gaussians still is nonzero after each M-step.  In case it isn't, the Gaussian is removed.  The effect is that the total number of Gaussians can reduce in this procedure.</p>
<h2><a id="user-content-working-with-distributionsjl" class="anchor" aria-hidden="true" href="#working-with-distributionsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Working with Distributions.jl</h2>
<p>A GMM model can used to build a <code>MixtureModel</code> in the <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> package. For example:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> GaussianMixtures
<span class="pl-k">using</span> Distributions
g <span class="pl-k">=</span> <span class="pl-c1">rand</span>(GMM, <span class="pl-c1">3</span>, <span class="pl-c1">4</span>)
m <span class="pl-k">=</span> <span class="pl-c1">MixtureModel</span>(g)</pre></div>
<p>This can be conveniently use for sampling from the GMM, e.g.</p>
<div class="highlight highlight-source-julia"><pre>sample<span class="pl-k">=</span> <span class="pl-c1">rand</span>(m)</pre></div>
<p>Furthermore, a Gaussian mixture model constructed using <code>MixtureModel</code> can be
converted to GMM via a constructor call</p>
<div class="highlight highlight-source-julia"><pre>gg <span class="pl-k">=</span> <span class="pl-c1">GMM</span>(m)
<span class="pl-s"><span class="pl-pds">`</span></span></pre></div>
</article></div>