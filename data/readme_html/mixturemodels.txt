<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h2><a id="user-content-mixturemodelsjl" class="anchor" aria-hidden="true" href="#mixturemodelsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MixtureModels.jl</h2>
<p>A Julia package for probabilistic mixture models</p>
<p>A <em>mixture model</em> is a probabilistic model that combines multiple components to capture data distribution. Generally, a mixture model is characterized by a collection of components and a discrete distribution over them. From a generative perspective, the procedure to generate a sample from a mixture model consists of two steps: (1) choose a particular component from the discrete distribution, and (2) use the chosen component to generate the sample.</p>
<h2><a id="user-content-finite-mixture-model" class="anchor" aria-hidden="true" href="#finite-mixture-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Finite Mixture Model</h2>
<p><em>Finite mixture model</em> is a classical formulation of mixture model, which requires the number of components, often denoted by <em>K</em>, to be fixed before training. The <em>Expectation-Maximization (EM)</em> algorithm is usually used to estimate a mixture model from data.</p>
<h4><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Basic Usage</h4>
<p>The function <code>fit_fmm</code> fits a mixture model to a given data set:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fit_fmm</span>(C, data, K, alg)</pre></div>
<p>Parameters:</p>
<ul>
<li><code>C</code>: the component type, e.g. one can use <code>MultivariateNormal{PDMat}</code> to indicate Gaussian components with full covariance matrix.</li>
<li><code>data</code>: the input samples</li>
<li><code>K</code>: the number of components to be estimated</li>
<li><code>alg</code>: the algorithm option struct.</li>
</ul>
<p>Here, <code>alg</code> can be constructed using the <code>fmm_em</code> function, as below</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fmm_em</span>(;
	maxiter<span class="pl-k">::</span><span class="pl-c1">Integer</span><span class="pl-k">=</span><span class="pl-c1">100</span>,    <span class="pl-c"><span class="pl-c">#</span> maximum number of iterations</span>
	tol<span class="pl-k">::</span><span class="pl-c1">Real</span><span class="pl-k">=</span><span class="pl-c1">1.0e-6</span>,        <span class="pl-c"><span class="pl-c">#</span> tolerable change of objective value upon convergence</span>
	display<span class="pl-k">::</span><span class="pl-c1">Symbol</span><span class="pl-k">=</span><span class="pl-c1">:none</span>,   <span class="pl-c"><span class="pl-c">#</span> can take value :none, :proc, or :iter</span>
	alpha<span class="pl-k">::</span><span class="pl-c1">Float64</span><span class="pl-k">=</span><span class="pl-c1">1.0</span>)      <span class="pl-c"><span class="pl-c">#</span> Dirichlet prior coefficient for pi</span></pre></div>
<p>This function returns a struct of type <code>FiniteMixtureEMResults</code>, which contains the following fields:</p>
<ul>
<li><code>mixture</code>: an instance of type <code>Mixture</code>, which has two fields: <code>components</code> (a list of components) and <code>pi</code> (component proportions);</li>
<li><code>Q</code>: soft assignment matrix, of size <code>(n, K)</code>. In particular, <code>Q[i, k]</code> indicates the probability that the i-th sample belongs to the k-th component;</li>
<li><code>L</code>: likelihood matrix, of size <code>(n, K)</code>. In particular, <code>L[i, k]</code> is the logpdf of the i-th sample w.r.t. the k-th component;</li>
<li><code>niters</code>: the number of elapsed iterations;</li>
<li><code>converged</code>: whether the procedure converged;</li>
<li><code>objective</code>: the objective function value of the last iteration.</li>
</ul>
<h4><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Examples</h4>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> train a Gaussian mixture model with 5 components, showing progress information at each iteration</span>
r <span class="pl-k">=</span> <span class="pl-c1">fit_fmm</span>(MultivariateNormal{PDMat}, x, <span class="pl-c1">5</span>, <span class="pl-c1">fmm_em</span>(maxiter<span class="pl-k">=</span><span class="pl-c1">50</span>, display<span class="pl-k">=</span><span class="pl-c1">:iter</span>))

<span class="pl-c"><span class="pl-c">#</span> print each component</span>
<span class="pl-k">for</span> comp <span class="pl-k">in</span> r<span class="pl-k">.</span>components
    <span class="pl-c1">println</span>(comp)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> If you are satisfied with default option values</span>
r <span class="pl-k">=</span> <span class="pl-c1">fit_fmm</span>(MultivariateNormal{PDMat}, x, <span class="pl-c1">5</span>, <span class="pl-c1">fmm_em</span>())</pre></div>
<p><strong>Note:</strong></p>
<p>In <a href="https://github.com/JuliaStats/Distributions.jl"><em>Distributions.jl</em></a>, the type <code>MultivariateNormal</code> takes a type argument that specifies the form of covariance matrix. Specifically, <code>MultivariateNormal{PDMat}</code> uses full covariance, <code>MultivariateNormal{PDiagMat}</code> uses diagonal covariance, while <code>MultivariateNormal{ScalMat}</code> uses a covariance of the form <code>s * I</code>.</p>
<p>The package provides a demo in <code>demo/gmm.jl</code> to demonstrate the use of this package in fitting Gaussian mixture model. Below is a screenshot of the result:</p>
<p><a target="_blank" rel="noopener noreferrer" href="demo/gmm_demo.png"><img src="demo/gmm_demo.png" alt="gmm_demo" style="max-width:100%;"></a></p>
<h4><a id="user-content-use-user-supplied-q-matrix" class="anchor" aria-hidden="true" href="#use-user-supplied-q-matrix"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use User-supplied Q matrix</h4>
<p>By default, this function initializes the soft assignment matrix <code>Q</code> randomly. The users can also provide their own version of initial Q-matrix, using the function <code>fit_fmm!</code>:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c1">fit_fmm!</span>(C, data, Q, alg)</pre></div>
<p>This function updates <code>Q</code> inplace.</p>
<p>This package provides a convenient function <code>qmatrix</code> to construct <code>Q</code>. For example, if you have a label vector <code>labels</code>, which may be obtained by running a clustering algorithm, you can use this vector to construct <code>Q</code>:</p>
<div class="highlight highlight-source-julia"><pre>r <span class="pl-k">=</span> <span class="pl-c1">fit_fmm!</span>(C, data, <span class="pl-c1">qmatrix</span>(labels, K), alg)</pre></div>
<p>Here, <code>qmatrix</code> constructs a matrix <code>Q</code> of size <code>(n, K)</code> such that <code>Q[i, labels[i]] = 1.0</code> and each row sums to one.</p>
<h2><a id="user-content-future-plan" class="anchor" aria-hidden="true" href="#future-plan"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Plan</h2>
<ul>
<li>Implement variants of Finite Mixture Models</li>
<li>Implement Bayesian Nonparametric Mixture Models (e.g. DPMM)</li>
</ul>
</article></div>