<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-loopvectorization" class="anchor" aria-hidden="true" href="#loopvectorization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LoopVectorization</h1>
<p><a href="https://chriselrod.github.io/LoopVectorization.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/f7b92a177c912c1cc007fc9b40f17ff3ee3bb414/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://chriselrod.github.io/LoopVectorization.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/57bae07ecd50a99519ad0516d91f4ec8f0f48e12/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="Latest" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.com/chriselrod/LoopVectorization.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c061c54c070289b96fb08259db1cd197f259c328/68747470733a2f2f7472617669732d63692e636f6d2f6368726973656c726f642f4c6f6f70566563746f72697a6174696f6e2e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/chriselrod/LoopVectorization.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://ci.appveyor.com/project/chriselrod/LoopVectorization-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/88a77833b5faa5e5b36d61908023b5db14d04eac/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f6368726973656c726f642f4c6f6f70566563746f72697a6174696f6e2e6a6c3f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/chriselrod/LoopVectorization.jl?svg=true" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/chriselrod/LoopVectorization.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/4c366a0a82e7e8560785271279109cc2d529139e/68747470733a2f2f636f6465636f762e696f2f67682f6368726973656c726f642f4c6f6f70566563746f72697a6174696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/chriselrod/LoopVectorization.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>LoopVectorization<span class="pl-pds">"</span></span>)</pre></div>
<p>LoopVectorization is supported on Julia 1.1 and later. It is tested on Julia 1.1, 1.3, and nightly.</p>
<h2><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<p>This library provides the <code>@avx</code> macro, which may be used to prefix a <code>for</code> loop or broadcast statement.
It then tries to vectorize the loop to improve runtime performance.</p>
<p>The macro assumes that loop iterations can be reordered. It also currently supports simple nested loops, where loop bounds of inner loops are constant across iterations of the outer loop, and only a single loop at each level of noop lest. These limitations should be removed in a future version.</p>
<h2><a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmarks</h2>
<p>Please see the documentation for benchmarks versus base Julia, Clang-Polly, icc, ifort, gfortran, and Eigen. If you would believe any code or compiler flags can be improved, would like to submit your own benchmarks, or have Julia code using LoopVectorization that you would like to be tested for performance regressions on a semi-regular basis, please feel file an issue or PR with the code sample.</p>
<h2><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Examples</h2>
<h3><a id="user-content-dot-product" class="anchor" aria-hidden="true" href="#dot-product"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dot Product</h3>
<details>
 
<p>
</p><p>A simple example with a single loop is the dot product:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> LoopVectorization, BenchmarkTools

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mydot</span>(a, b)
           s <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
           <span class="pl-c1">@inbounds</span> <span class="pl-c1">@simd</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">eachindex</span>(a,b)
               s <span class="pl-k">+=</span> a[i]<span class="pl-k">*</span>b[i]
           <span class="pl-k">end</span>
           s
       <span class="pl-k">end</span>
mydot (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mydotavx</span>(a, b)
           s <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
           <span class="pl-c1">@avx</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">eachindex</span>(a,b)
               s <span class="pl-k">+=</span> a[i]<span class="pl-k">*</span>b[i]
           <span class="pl-k">end</span>
           s
       <span class="pl-k">end</span>
mydotavx (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">256</span>); b <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">256</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydot</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b)
  <span class="pl-c1">12.273</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">62.61049816874535</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydotavx</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b)
  <span class="pl-c1">11.618</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">62.61049816874536</span>

julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">255</span>); b <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">255</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydot</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b)
  <span class="pl-c1">36.539</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">62.29537331565549</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydotavx</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b)
  <span class="pl-c1">11.739</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">62.29537331565549</span></pre></div>
<p>On most recent CPUs, the performance of the dot product is bounded by
the speed at which it can load data; most recent x86_64 CPUs can perform
two aligned loads and two fused multiply adds (<code>fma</code>) per clock cycle.
However, the dot product requires two loads per <code>fma</code>.</p>
<p>A self-dot function, on the otherhand, requires one load per fma:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">myselfdot</span>(a)
           s <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
           <span class="pl-c1">@inbounds</span> <span class="pl-c1">@simd</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">eachindex</span>(a)
               s <span class="pl-k">+=</span> a[i]<span class="pl-k">*</span>a[i]
           <span class="pl-k">end</span>
           s
       <span class="pl-k">end</span>
myselfdot (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">myselfdotavx</span>(a)
           s <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
           <span class="pl-c1">@avx</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">eachindex</span>(a)
               s <span class="pl-k">+=</span> a[i]<span class="pl-k">*</span>a[i]
           <span class="pl-k">end</span>
           s
       <span class="pl-k">end</span>
myselfdotavx (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">256</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">myselfdot</span>(<span class="pl-k">$</span>a)
  <span class="pl-c1">8.578</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">90.16636687132868</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">myselfdotavx</span>(<span class="pl-k">$</span>a)
  <span class="pl-c1">9.560</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">90.16636687132868</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">myselfdot</span>(<span class="pl-k">$</span>b)
  <span class="pl-c1">28.923</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">83.20114563267853</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">myselfdotavx</span>(<span class="pl-k">$</span>b)
  <span class="pl-c1">9.174</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">83.20114563267856</span></pre></div>
<p>For this reason, the <code>@avx</code> version is roughly twice as fast. The <code>@inbounds @simd</code> version, however, is not, because it runs into the problem of loop carried dependencies: to add <code>a[i]*b[i]</code> to <code>s_new = s_old + a[i-j]*b[i-j]</code>, we must have first finished calculating <code>s_new</code>, but -- while two <code>fma</code> instructions can be initiated per cycle -- they each take several clock cycles to complete.
For this reason, we need to unroll the operation to run several independent instances concurrently. The <code>@avx</code> macro models this cost to try and pick an optimal unroll factor.</p>
<p></p>
</details>
<h3><a id="user-content-matrix-multiply" class="anchor" aria-hidden="true" href="#matrix-multiply"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Matrix Multiply</h3>
<details>
 
<p>
</p><p>We can also vectorize fancier loops. A likely familiar example to dive into:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mygemm!</span>(C, A, B)
           <span class="pl-c1">@inbounds</span> <span class="pl-c1">@fastmath</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
               Cmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
               <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
                   Cmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
               <span class="pl-k">end</span>
               C[m,n] <span class="pl-k">=</span> Cmn
           <span class="pl-k">end</span>
       <span class="pl-k">end</span>
mygemm! (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mygemmavx!</span>(C, A, B)
           <span class="pl-c1">@avx</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
               Cmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
               <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
                   Cmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
               <span class="pl-k">end</span>
               C[m,n] <span class="pl-k">=</span> Cmn
           <span class="pl-k">end</span>
       <span class="pl-k">end</span>
mygemmavx! (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> M, K, N <span class="pl-k">=</span> <span class="pl-c1">72</span>, <span class="pl-c1">75</span>, <span class="pl-c1">71</span>;

julia<span class="pl-k">&gt;</span> C1 <span class="pl-k">=</span> <span class="pl-c1">Matrix</span><span class="pl-c1">{Float64}</span>(undef, M, N); A <span class="pl-k">=</span> <span class="pl-c1">randn</span>(M, K); B <span class="pl-k">=</span> <span class="pl-c1">randn</span>(K, N);

julia<span class="pl-k">&gt;</span> C2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(C1); C3 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(C1);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">mygemmavx!</span>(<span class="pl-k">$</span>C1, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span> 
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">7.381</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">7.415</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">7.432</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">15.444</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">4</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">mygemm!</span>(<span class="pl-k">$</span>C2, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span> 
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">230.790</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">231.288</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">231.882</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">275.460</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">1</span>

julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> LinearAlgebra, Test

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> <span class="pl-c1">all</span>(C1 <span class="pl-k">.≈</span> C2)
Test Passed

julia<span class="pl-k">&gt;</span> BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>); BLAS<span class="pl-k">.</span><span class="pl-c1">vendor</span>()
<span class="pl-c1">:mkl</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">mul!</span>(<span class="pl-k">$</span>C3, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span> 
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">6.830</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">6.861</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">6.869</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">15.125</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">5</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> <span class="pl-c1">all</span>(C1 <span class="pl-k">.≈</span> C3)
Test Passed</pre></div>
<p>It can produce a decent macro kernel.
In the future, I would like it to also model the cost of memory movement in the L1 and L2 cache, and use these to generate loops around the macro kernel following the work of <a href="http://www.cs.utexas.edu/users/flame/pubs/TOMS-BLIS-Analytical.pdf" rel="nofollow">Low, et al. (2016)</a>.</p>
<p>Until then, performance will degrade rapidly compared to BLAS as the size of the matrices increase. The advantage of the <code>@avx</code> macro, however, is that it is general. Not every operation is supported by BLAS.</p>
<p>For example, what if <code>A</code> were the outer product of two vectors?</p>


<p></p>
</details>
<h3><a id="user-content-broadcasting" class="anchor" aria-hidden="true" href="#broadcasting"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Broadcasting</h3>
<details>
 
<p>
</p><p>Another example, a straightforward operation expressed well via broadcasting and <code>*ˡ</code> (which is typed <code>*\^l</code>), the lazy matrix multiplication operator:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> LoopVectorization, LinearAlgebra, BenchmarkTools, Test; BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>)

julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">48</span>); B <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">48</span>, <span class="pl-c1">51</span>); c <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">51</span>); d <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">49</span>);

julia<span class="pl-k">&gt;</span> X1 <span class="pl-k">=</span>        a <span class="pl-k">.+</span> B <span class="pl-k">*</span> (c <span class="pl-k">.+</span> d<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> X2 <span class="pl-k">=</span> <span class="pl-c1">@avx</span> <span class="pl-c1">@.</span> a <span class="pl-k">+</span> B <span class="pl-k">*</span>ˡ (c <span class="pl-k">+</span> d<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> X1 <span class="pl-k">≈</span> X2
Test Passed

julia<span class="pl-k">&gt;</span> buf1 <span class="pl-k">=</span> <span class="pl-c1">Matrix</span><span class="pl-c1">{Float64}</span>(undef, <span class="pl-c1">length</span>(c), <span class="pl-c1">length</span>(d));

julia<span class="pl-k">&gt;</span> buf2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(X1);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-k">$</span>X1 <span class="pl-k">.=</span> <span class="pl-k">$</span>a <span class="pl-k">.+</span> <span class="pl-c1">mul!</span>(<span class="pl-k">$</span>buf2, <span class="pl-k">$</span>B, (<span class="pl-k">$</span>buf1 <span class="pl-k">.=</span> <span class="pl-k">$</span>c <span class="pl-k">.+</span> <span class="pl-k">$</span>d<span class="pl-k">'</span>));
  <span class="pl-c1">3.499</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">@avx</span> <span class="pl-c1">@.</span> <span class="pl-k">$</span>X2 <span class="pl-k">=</span> <span class="pl-k">$</span>a <span class="pl-k">+</span> <span class="pl-k">$</span>B <span class="pl-k">*</span>ˡ (<span class="pl-k">$</span>c <span class="pl-k">+</span> <span class="pl-k">$</span>d<span class="pl-k">'</span>);
  <span class="pl-c1">3.289</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> X1 <span class="pl-k">≈</span> X2
Test Passed</pre></div>
<p>The lazy matrix multiplication operator <code>*ˡ</code> escapes broadcasts and fuses, making it easy to write code that avoids intermediates. However, I would recomend always checking if splitting the operation into pieces, or at least isolating the matrix multiplication, increases performance. That will often be the case, especially if the matrices are large, where a separate multiplication can leverage BLAS (and perhaps take advantage of threads).
This may improve as the optimizations within LoopVectorization improve.</p>
<p></p>
</details>
<h3><a id="user-content-dealing-with-structs" class="anchor" aria-hidden="true" href="#dealing-with-structs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dealing with structs</h3>
<details>
 
<p>
</p><p>The key to the <code>@avx</code> macro's performance gains is leveraging knowledge of exactly how data like <code>Float64</code>s and <code>Int</code>s are handled by a CPU. As such, it is not strightforward to generalize the <code>@avx</code> macro to work on arrays containing structs such as <code>Matrix{Complex{Float64}}</code>. Instead, it is currently recommended that users wishing to apply <code>@avx</code> to arrays of structs use packages such as <a href="https://github.com/JuliaArrays/StructArrays.jl">StructArrays.jl</a> which transform an array where each element is a struct into a struct where each element is an array. Using StructArrays.jl, we can write a matrix multiply (gemm) kernel that works on matrices of <code>Complex{Float64}</code>s and <code>Complex{Int}</code>s:</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-k">using</span> LoopVectorization, LinearAlgebra, StructArrays, BenchmarkTools, Test

BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>); <span class="pl-c1">@show</span> BLAS<span class="pl-k">.</span><span class="pl-c1">vendor</span>()

<span class="pl-k">const</span> MatrixFInt64 <span class="pl-k">=</span> Union{Matrix{Float64}, Matrix{Int}}

<span class="pl-k">function</span> <span class="pl-en">mul_avx!</span>(C<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, A<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, B<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>)
    <span class="pl-c1">@avx</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
        Cmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
        <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
            Cmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
        <span class="pl-k">end</span>
        C[m,n] <span class="pl-k">=</span> Cmn
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">mul_add_avx!</span>(C<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, A<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, B<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, factor<span class="pl-k">=</span><span class="pl-c1">1</span>)
    <span class="pl-c1">@avx</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
        ΔCmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
        <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
            ΔCmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
        <span class="pl-k">end</span>
        C[m,n] <span class="pl-k">+=</span> factor <span class="pl-k">*</span> ΔCmn
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">const</span> StructMatrixComplexFInt64 <span class="pl-k">=</span> Union{StructArray{ComplexF64,<span class="pl-c1">2</span>}, StructArray{Complex{Int},<span class="pl-c1">2</span>}}

<span class="pl-k">function</span> <span class="pl-en">mul_avx!</span>(C<span class="pl-k">::</span> <span class="pl-c1">StructMatrixComplexFInt64</span>, A<span class="pl-k">::</span><span class="pl-c1">StructMatrixComplexFInt64</span>, B<span class="pl-k">::</span><span class="pl-c1">StructMatrixComplexFInt64</span>)
    <span class="pl-c1">mul_avx!</span>(    C<span class="pl-k">.</span>re, A<span class="pl-k">.</span>re, B<span class="pl-k">.</span>re)     <span class="pl-c"><span class="pl-c">#</span> C.re = A.re * B.re</span>
    <span class="pl-c1">mul_add_avx!</span>(C<span class="pl-k">.</span>re, A<span class="pl-k">.</span>im, B<span class="pl-k">.</span>im, <span class="pl-k">-</span><span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> C.re = C.re - A.im * B.im</span>
    <span class="pl-c1">mul_avx!</span>(    C<span class="pl-k">.</span>im, A<span class="pl-k">.</span>re, B<span class="pl-k">.</span>im)     <span class="pl-c"><span class="pl-c">#</span> C.im = A.re * B.im</span>
    <span class="pl-c1">mul_add_avx!</span>(C<span class="pl-k">.</span>im, A<span class="pl-k">.</span>im, B<span class="pl-k">.</span>re)     <span class="pl-c"><span class="pl-c">#</span> C.im = C.im + A.im * B.re</span>
<span class="pl-k">end</span></pre></div>
<p>this <code>mul_avx!</code> kernel can now accept <code>StructArray</code> matrices of complex numbers and multiply them efficiently:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> M, K, N <span class="pl-k">=</span> <span class="pl-c1">56</span>, <span class="pl-c1">57</span>, <span class="pl-c1">58</span>
(<span class="pl-c1">56</span>, <span class="pl-c1">57</span>, <span class="pl-c1">58</span>)

julia<span class="pl-k">&gt;</span> A  <span class="pl-k">=</span> <span class="pl-c1">StructArray</span>(<span class="pl-c1">randn</span>(ComplexF64, M, K));

julia<span class="pl-k">&gt;</span> B  <span class="pl-k">=</span> <span class="pl-c1">StructArray</span>(<span class="pl-c1">randn</span>(ComplexF64, K, N));

julia<span class="pl-k">&gt;</span> C1 <span class="pl-k">=</span> <span class="pl-c1">StructArray</span>(<span class="pl-c1">Matrix</span><span class="pl-c1">{ComplexF64}</span>(undef, M, N));

julia<span class="pl-k">&gt;</span> C2 <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">similar</span>(C1));

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mul_avx!</span>(<span class="pl-k">$</span>C1, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
  <span class="pl-c1">13.634</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mul!</span>(    <span class="pl-k">$</span>C2, <span class="pl-k">$</span>(<span class="pl-c1">collect</span>(A)), <span class="pl-k">$</span>(<span class="pl-c1">collect</span>(B))); <span class="pl-c"><span class="pl-c">#</span> collect turns the StructArray into a regular Array</span>
  <span class="pl-c1">14.007</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> C1 <span class="pl-k">≈</span> C2
Test Passed</pre></div>
<p>Similar approaches can be taken to make kernels working with a variety of numeric struct types such as <a href="https://github.com/JuliaDiff/DualNumbers.jl">dual numbers</a>, <a href="https://github.com/JuliaMath/DoubleFloats.jl">DoubleFloats</a>, etc.</p>
<p></p>
</details>
<h2><a id="user-content-packages-using-loopvectorization" class="anchor" aria-hidden="true" href="#packages-using-loopvectorization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Packages using LoopVectorization</h2>
<ul>
<li><a href="https://github.com/MasonProtter/Gaius.jl">Gaius</a></li>
</ul>
<p>If you're using LoopVectorization, please feel free to file a PR adding yours to the list!</p>
</article></div>