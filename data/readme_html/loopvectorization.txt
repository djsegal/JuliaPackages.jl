<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaSIMD/LoopVectorization.jl/blob/main/docs/src/assets/logo.svg"><img src="https://github.com/JuliaSIMD/LoopVectorization.jl/raw/main/docs/src/assets/logo.svg" width="300" style="max-width: 100%;"></a></p>
<hr>
<h1 dir="auto"><a id="user-content-loopvectorization" class="anchor" aria-hidden="true" href="#loopvectorization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LoopVectorization</h1>
<p dir="auto"><a href="https://JuliaSIMD.github.io/LoopVectorization.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://JuliaSIMD.github.io/LoopVectorization.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="Latest" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width: 100%;"></a>
<a href="https://github.com/JuliaSIMD/LoopVectorization.jl/actions?query=workflow%3ACI"><img src="https://github.com/JuliaSIMD/LoopVectorization.jl/workflows/CI/badge.svg" alt="CI" style="max-width: 100%;"></a>
<a href="https://github.com/JuliaSIMD/LoopVectorization.jl/actions?query=workflow%3A%22CI+%28Julia+nightly%29%22"><img src="https://github.com/JuliaSIMD/LoopVectorization.jl/workflows/CI%20(Julia%20nightly)/badge.svg" alt="CI (Julia nightly)" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/JuliaSIMD/LoopVectorization.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/66d30e8006217a21bdbefc7ca0b05602792a64afeb2493bdbec5ff3592d02bb4/68747470733a2f2f636f6465636f762e696f2f67682f4a756c696153494d442f4c6f6f70566563746f72697a6174696f6e2e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/JuliaSIMD/LoopVectorization.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<p dir="auto"><a href="https://pkgs.genieframework.com?packages=LoopVectorization" rel="nofollow"><img src="https://camo.githubusercontent.com/6422d79fe3d44ce013734d4975d6d2b96dc1cbbb473585971b914dc61d4f1aac/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f706b67732e67656e69656672616d65776f726b2e636f6d2f6170692f76312f62616467652f4c6f6f70566563746f72697a6174696f6e" alt="LoopVectorization Downloads" data-canonical-src="https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/LoopVectorization" style="max-width: 100%;"></a></p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(&quot;LoopVectorization&quot;)"><pre><span class="pl-k">using</span> Pkg
Pkg<span class="pl-k">.</span><span class="pl-c1">add</span>(<span class="pl-s"><span class="pl-pds">"</span>LoopVectorization<span class="pl-pds">"</span></span>)</pre></div>
<p dir="auto">LoopVectorization is supported on Julia 1.1 and later. It is tested on Julia 1.5 and nightly.</p>
<h2 dir="auto"><a id="user-content-warning" class="anchor" aria-hidden="true" href="#warning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Warning</h2>
<p dir="auto">Misusing LoopVectorization can have <a href="http://catb.org/jargon/html/N/nasal-demons.html" rel="nofollow">serious consequences</a>. Like <code>@inbounds</code>, misusing it can lead to segfaults and memory corruption.
We expect that any time you use the <code>@turbo</code> macro with a given block of code that you:</p>
<ol dir="auto">
<li>Are not indexing an array out of bounds. <code>@turbo</code> does not perform any bounds checking.</li>
<li>Are not iterating over an empty collection. Iterating over an empty loop such as <code>for i ∈ eachindex(Float64[])</code> is undefined behavior, and will likely result in the out of bounds memory accesses. Ensure that loops behave correctly.</li>
<li>Are not relying on a specific execution order. <code>@turbo</code> can and will re-order operations and loops inside its scope, so the correctness cannot depend on a particular order. You cannot implement <code>cumsum</code> with <code>@turbo</code>.</li>
<li>Are not using multiple loops at the same level in nested loops.</li>
</ol>
<h2 dir="auto"><a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">This library provides the <code>@turbo</code> macro, which may be used to prefix a <code>for</code> loop or broadcast statement.
It then tries to vectorize the loop to improve runtime performance.</p>
<p dir="auto">The macro assumes that loop iterations can be reordered. It also currently supports simple nested loops, where loop bounds of inner loops are constant across iterations of the outer loop, and only a single loop at each level of loop nest. These limitations should be removed in a future version.</p>
<h2 dir="auto"><a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Benchmarks</h2>
<p dir="auto">Please see the documentation for benchmarks versus base Julia, Clang, icc, ifort, gfortran, and Eigen. If you believe any code or compiler flags can be improved, would like to submit your own benchmarks, or have Julia code using LoopVectorization that you would like to be tested for performance regressions on a semi-regular basis, please feel free to file an issue or PR with the code sample.</p>
<h2 dir="auto"><a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<h3 dir="auto"><a id="user-content-dot-product" class="anchor" aria-hidden="true" href="#dot-product"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dot Product</h3>
<p dir="auto">LLVM/Julia by default generate essentially optimal code for a primary vectorized part of this loop. In many cases -- such as the dot product -- this vectorized part of the loop computes 4*SIMD-vector-width iterations at a time.
On the CPU I'm running these benchmarks on with <code>Float64</code> data, the SIMD-vector-width is 8, meaning it will compute 32 iterations at a time.
However, LLVM is very slow at handling the tails, <code>length(iterations) % 32</code>. For this reason, <a href="https://JuliaSIMD.github.io/LoopVectorization.jl/latest/examples/dot_product/" rel="nofollow">in benchmark plots</a> you can see performance drop as the size of the remainder increases.</p>
<p dir="auto">For simple loops like a dot product, LoopVectorization.jl's most important optimization is to handle these tails more efficiently:</p>
<details>
 
<p dir="auto">
</p><div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using LoopVectorization, BenchmarkTools

julia&gt; function mydot(a, b)
          s = 0.0
          @inbounds @simd for i ∈ eachindex(a,b)
              s += a[i]*b[i]
          end
          s
      end
mydot (generic function with 1 method)

julia&gt; function mydotavx(a, b)
          s = 0.0
          @turbo for i ∈ eachindex(a,b)
              s += a[i]*b[i]
          end
          s
      end
mydotavx (generic function with 1 method)

julia&gt; a = rand(256); b = rand(256);

julia&gt; @btime mydot($a, $b)
 12.220 ns (0 allocations: 0 bytes)
62.67140864639772

julia&gt; @btime mydotavx($a, $b) # performance is similar
 12.104 ns (0 allocations: 0 bytes)
62.67140864639772

julia&gt; a = rand(255); b = rand(255);

julia&gt; @btime mydot($a, $b) # with loops shorter by 1, the remainder is now 32, and it is slow
 36.530 ns (0 allocations: 0 bytes)
61.25056244423578

julia&gt; @btime mydotavx($a, $b) # performance remains mostly unchanged.
 12.226 ns (0 allocations: 0 bytes)
61.250562444235776"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> LoopVectorization, BenchmarkTools

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mydot</span>(a, b)
          s <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
          <span class="pl-c1">@inbounds</span> <span class="pl-c1">@simd</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">eachindex</span>(a,b)
              s <span class="pl-k">+=</span> a[i]<span class="pl-k">*</span>b[i]
          <span class="pl-k">end</span>
          s
      <span class="pl-k">end</span>
mydot (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mydotavx</span>(a, b)
          s <span class="pl-k">=</span> <span class="pl-c1">0.0</span>
          <span class="pl-c1">@turbo</span> <span class="pl-k">for</span> i <span class="pl-k">∈</span> <span class="pl-c1">eachindex</span>(a,b)
              s <span class="pl-k">+=</span> a[i]<span class="pl-k">*</span>b[i]
          <span class="pl-k">end</span>
          s
      <span class="pl-k">end</span>
mydotavx (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">256</span>); b <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">256</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydot</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b)
 <span class="pl-c1">12.220</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">62.67140864639772</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydotavx</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b) <span class="pl-c"><span class="pl-c">#</span> performance is similar</span>
 <span class="pl-c1">12.104</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">62.67140864639772</span>

julia<span class="pl-k">&gt;</span> a <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">255</span>); b <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">255</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydot</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b) <span class="pl-c"><span class="pl-c">#</span> with loops shorter by 1, the remainder is now 32, and it is slow</span>
 <span class="pl-c1">36.530</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">61.25056244423578</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mydotavx</span>(<span class="pl-k">$</span>a, <span class="pl-k">$</span>b) <span class="pl-c"><span class="pl-c">#</span> performance remains mostly unchanged.</span>
 <span class="pl-c1">12.226</span> ns (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">61.250562444235776</span></pre></div>
<p dir="auto"></p>
</details>
<h3 dir="auto"><a id="user-content-matrix-multiply" class="anchor" aria-hidden="true" href="#matrix-multiply"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Matrix Multiply</h3>
<details>
 
<p dir="auto">
</p><p dir="auto">We can also vectorize fancier loops. A likely familiar example to dive into:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; function mygemm!(C, A, B)
           @inbounds @fastmath for m ∈ axes(A,1), n ∈ axes(B,2)
               Cmn = zero(eltype(C))
               for k ∈ axes(A,2)
                   Cmn += A[m,k] * B[k,n]
               end
               C[m,n] = Cmn
           end
       end
mygemm! (generic function with 1 method)

julia&gt; function mygemmavx!(C, A, B)
           @turbo for m ∈ axes(A,1), n ∈ axes(B,2)
               Cmn = zero(eltype(C))
               for k ∈ axes(A,2)
                   Cmn += A[m,k] * B[k,n]
               end
               C[m,n] = Cmn
           end
       end
mygemmavx! (generic function with 1 method)

julia&gt; M, K, N = 191, 189, 171;

julia&gt; C1 = Matrix{Float64}(undef, M, N); A = randn(M, K); B = randn(K, N);

julia&gt; C2 = similar(C1); C3 = similar(C1);

julia&gt; @benchmark mygemmavx!($C1, $A, $B)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     111.722 μs (0.00% GC)
  median time:      112.528 μs (0.00% GC)
  mean time:        112.673 μs (0.00% GC)
  maximum time:     189.400 μs (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     1

julia&gt; @benchmark mygemm!($C2, $A, $B)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     4.891 ms (0.00% GC)
  median time:      4.899 ms (0.00% GC)
  mean time:        4.899 ms (0.00% GC)
  maximum time:     5.049 ms (0.00% GC)
  --------------
  samples:          1021
  evals/sample:     1

julia&gt; using LinearAlgebra, Test

julia&gt; @test all(C1 .≈ C2)
Test Passed

julia&gt; BLAS.set_num_threads(1); BLAS.vendor()
:mkl

julia&gt; @benchmark mul!($C3, $A, $B)
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     117.221 μs (0.00% GC)
  median time:      118.745 μs (0.00% GC)
  mean time:        118.892 μs (0.00% GC)
  maximum time:     193.826 μs (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     1

julia&gt; @test all(C1 .≈ C3)
Test Passed

julia&gt; 2e-9M*K*N ./ (111.722e-6, 4.891e-3, 117.221e-6)
(110.50516460500171, 2.524199141279902, 105.32121377568868)"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mygemm!</span>(C, A, B)
           <span class="pl-c1">@inbounds</span> <span class="pl-c1">@fastmath</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(B,<span class="pl-c1">2</span>)
               Cmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
               <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">2</span>)
                   Cmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
               <span class="pl-k">end</span>
               C[m,n] <span class="pl-k">=</span> Cmn
           <span class="pl-k">end</span>
       <span class="pl-k">end</span>
mygemm! (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">mygemmavx!</span>(C, A, B)
           <span class="pl-c1">@turbo</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(B,<span class="pl-c1">2</span>)
               Cmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
               <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">2</span>)
                   Cmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
               <span class="pl-k">end</span>
               C[m,n] <span class="pl-k">=</span> Cmn
           <span class="pl-k">end</span>
       <span class="pl-k">end</span>
mygemmavx! (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> M, K, N <span class="pl-k">=</span> <span class="pl-c1">191</span>, <span class="pl-c1">189</span>, <span class="pl-c1">171</span>;

julia<span class="pl-k">&gt;</span> C1 <span class="pl-k">=</span> <span class="pl-c1">Matrix</span><span class="pl-c1">{Float64}</span>(undef, M, N); A <span class="pl-k">=</span> <span class="pl-c1">randn</span>(M, K); B <span class="pl-k">=</span> <span class="pl-c1">randn</span>(K, N);

julia<span class="pl-k">&gt;</span> C2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(C1); C3 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(C1);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">mygemmavx!</span>(<span class="pl-k">$</span>C1, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">111.722</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">112.528</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">112.673</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">189.400</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">1</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">mygemm!</span>(<span class="pl-k">$</span>C2, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">4.891</span> ms (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">4.899</span> ms (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">4.899</span> ms (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">5.049</span> ms (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">1021</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">1</span>

julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> LinearAlgebra, Test

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> <span class="pl-c1">all</span>(C1 <span class="pl-k">.≈</span> C2)
Test Passed

julia<span class="pl-k">&gt;</span> BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>); BLAS<span class="pl-k">.</span><span class="pl-c1">vendor</span>()
<span class="pl-c1">:mkl</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">mul!</span>(<span class="pl-k">$</span>C3, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">117.221</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">118.745</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">118.892</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">193.826</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">1</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> <span class="pl-c1">all</span>(C1 <span class="pl-k">.≈</span> C3)
Test Passed

julia<span class="pl-k">&gt;</span> <span class="pl-c1">2e-9</span>M<span class="pl-k">*</span>K<span class="pl-k">*</span>N <span class="pl-k">./</span> (<span class="pl-c1">111.722e-6</span>, <span class="pl-c1">4.891e-3</span>, <span class="pl-c1">117.221e-6</span>)
(<span class="pl-c1">110.50516460500171</span>, <span class="pl-c1">2.524199141279902</span>, <span class="pl-c1">105.32121377568868</span>)</pre></div>
<p dir="auto">It can produce a good macro kernel. An implementation of matrix multiplication able to handle large matrices would need to perform blocking and packing of arrays to prevent the operations from being memory bottle-necked.
Some day, LoopVectorization may itself try to model the costs of memory movement in the L1 and L2 cache, and use these to generate loops around the macro kernel following the work of <a href="http://www.cs.utexas.edu/users/flame/pubs/TOMS-BLIS-Analytical.pdf" rel="nofollow">Low, et al. (2016)</a>.</p>
<p dir="auto">But for now, you should view it as a tool for generating efficient computational kernels, leaving tasks of parallelization and cache efficiency to you.</p>


<p dir="auto"></p>
</details>
<h3 dir="auto"><a id="user-content-broadcasting" class="anchor" aria-hidden="true" href="#broadcasting"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Broadcasting</h3>
<details>
 
<p dir="auto">
</p><p dir="auto">Another example, a straightforward operation expressed well via broadcasting and <code>*ˡ</code> (which is typed <code>*\^l</code>), the lazy matrix multiplication operator:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; using LoopVectorization, LinearAlgebra, BenchmarkTools, Test; BLAS.set_num_threads(1)

julia&gt; A = rand(5,77); B = rand(77, 51); C = rand(51,49); D = rand(49,51);

julia&gt; X1 =      view(A,1,:) .+ B  *  (C .+ D');

julia&gt; X2 = @turbo view(A,1,:) .+ B .*ˡ (C .+ D');

julia&gt; @test X1 ≈ X2
Test Passed

julia&gt; buf1 = Matrix{Float64}(undef, size(C,1), size(C,2));

julia&gt; buf2 = similar(X1);

julia&gt; @btime $X1 .= view($A,1,:) .+ mul!($buf2, $B, ($buf1 .= $C .+ $D'));
  9.188 μs (0 allocations: 0 bytes)

julia&gt; @btime @turbo $X2 .= view($A,1,:) .+ $B .*ˡ ($C .+ $D');
  6.751 μs (0 allocations: 0 bytes)

julia&gt; @test X1 ≈ X2
Test Passed

julia&gt; AmulBtest!(X1, B, C, D, view(A,1,:))

julia&gt; AmulBtest2!(X2, B, C, D, view(A,1,:))

julia&gt; @test X1 ≈ X2
Test Passed"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> LoopVectorization, LinearAlgebra, BenchmarkTools, Test; BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>)

julia<span class="pl-k">&gt;</span> A <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">5</span>,<span class="pl-c1">77</span>); B <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">77</span>, <span class="pl-c1">51</span>); C <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">51</span>,<span class="pl-c1">49</span>); D <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">49</span>,<span class="pl-c1">51</span>);

julia<span class="pl-k">&gt;</span> X1 <span class="pl-k">=</span>      <span class="pl-c1">view</span>(A,<span class="pl-c1">1</span>,:) <span class="pl-k">.+</span> B  <span class="pl-k">*</span>  (C <span class="pl-k">.+</span> D<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> X2 <span class="pl-k">=</span> <span class="pl-c1">@turbo</span> <span class="pl-c1">view</span>(A,<span class="pl-c1">1</span>,:) <span class="pl-k">.+</span> B <span class="pl-k">.*</span>ˡ (C <span class="pl-k">.+</span> D<span class="pl-k">'</span>);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> X1 <span class="pl-k">≈</span> X2
Test Passed

julia<span class="pl-k">&gt;</span> buf1 <span class="pl-k">=</span> <span class="pl-c1">Matrix</span><span class="pl-c1">{Float64}</span>(undef, <span class="pl-c1">size</span>(C,<span class="pl-c1">1</span>), <span class="pl-c1">size</span>(C,<span class="pl-c1">2</span>));

julia<span class="pl-k">&gt;</span> buf2 <span class="pl-k">=</span> <span class="pl-c1">similar</span>(X1);

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-k">$</span>X1 <span class="pl-k">.=</span> <span class="pl-c1">view</span>(<span class="pl-k">$</span>A,<span class="pl-c1">1</span>,:) <span class="pl-k">.+</span> <span class="pl-c1">mul!</span>(<span class="pl-k">$</span>buf2, <span class="pl-k">$</span>B, (<span class="pl-k">$</span>buf1 <span class="pl-k">.=</span> <span class="pl-k">$</span>C <span class="pl-k">.+</span> <span class="pl-k">$</span>D<span class="pl-k">'</span>));
  <span class="pl-c1">9.188</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">@turbo</span> <span class="pl-k">$</span>X2 <span class="pl-k">.=</span> <span class="pl-c1">view</span>(<span class="pl-k">$</span>A,<span class="pl-c1">1</span>,:) <span class="pl-k">.+</span> <span class="pl-k">$</span>B <span class="pl-k">.*</span>ˡ (<span class="pl-k">$</span>C <span class="pl-k">.+</span> <span class="pl-k">$</span>D<span class="pl-k">'</span>);
  <span class="pl-c1">6.751</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> X1 <span class="pl-k">≈</span> X2
Test Passed

julia<span class="pl-k">&gt;</span> <span class="pl-c1">AmulBtest!</span>(X1, B, C, D, <span class="pl-c1">view</span>(A,<span class="pl-c1">1</span>,:))

julia<span class="pl-k">&gt;</span> <span class="pl-c1">AmulBtest2!</span>(X2, B, C, D, <span class="pl-c1">view</span>(A,<span class="pl-c1">1</span>,:))

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> X1 <span class="pl-k">≈</span> X2
Test Passed</pre></div>
<p dir="auto">The lazy matrix multiplication operator <code>*ˡ</code> escapes broadcasts and fuses, making it easy to write code that avoids intermediates. However, I would recommend always checking if splitting the operation into pieces, or at least isolating the matrix multiplication, increases performance. That will often be the case, especially if the matrices are large, where a separate multiplication can leverage BLAS (and perhaps take advantage of threads).
This may improve as the optimizations within LoopVectorization improve.</p>
<p dir="auto">Note that loops will be faster than broadcasting in general. This is because the behavior of broadcasts is determined by runtime information (i.e., dimensions other than the leading dimension of size <code>1</code> will be broadcasted; it is not known which these will be at compile time).</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; function AmulBtest!(C,A,Bk,Bn,d)
          @turbo for m ∈ axes(A,1), n ∈ axes(Bk,2)
             ΔCmn = zero(eltype(C))
             for k ∈ axes(A,2)
                ΔCmn += A[m,k] * (Bk[k,n] + Bn[n,k])
             end
             C[m,n] = ΔCmn + d[m]
          end
       end
AmulBtest! (generic function with 1 method)

julia&gt; AmulBtest!(X2, B, C, D, view(A,1,:))

julia&gt; @test X1 ≈ X2
Test Passed

julia&gt; @benchmark AmulBtest!($X2, $B, $C, $D, view($A,1,:))
BenchmarkTools.Trial:
  memory estimate:  0 bytes
  allocs estimate:  0
  --------------
  minimum time:     5.793 μs (0.00% GC)
  median time:      5.816 μs (0.00% GC)
  mean time:        5.824 μs (0.00% GC)
  maximum time:     14.234 μs (0.00% GC)
  --------------
  samples:          10000
  evals/sample:     6"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">function</span> <span class="pl-en">AmulBtest!</span>(C,A,Bk,Bn,d)
          <span class="pl-c1">@turbo</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(Bk,<span class="pl-c1">2</span>)
             ΔCmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
             <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">axes</span>(A,<span class="pl-c1">2</span>)
                ΔCmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> (Bk[k,n] <span class="pl-k">+</span> Bn[n,k])
             <span class="pl-k">end</span>
             C[m,n] <span class="pl-k">=</span> ΔCmn <span class="pl-k">+</span> d[m]
          <span class="pl-k">end</span>
       <span class="pl-k">end</span>
AmulBtest! (generic <span class="pl-k">function</span> with <span class="pl-c1">1</span> method)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">AmulBtest!</span>(X2, B, C, D, <span class="pl-c1">view</span>(A,<span class="pl-c1">1</span>,:))

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> X1 <span class="pl-k">≈</span> X2
Test Passed

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@benchmark</span> <span class="pl-c1">AmulBtest!</span>(<span class="pl-k">$</span>X2, <span class="pl-k">$</span>B, <span class="pl-k">$</span>C, <span class="pl-k">$</span>D, <span class="pl-c1">view</span>(<span class="pl-k">$</span>A,<span class="pl-c1">1</span>,:))
BenchmarkTools<span class="pl-k">.</span>Trial<span class="pl-k">:</span>
  memory estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span> bytes
  allocs estimate<span class="pl-k">:</span>  <span class="pl-c1">0</span>
  <span class="pl-k">--------------</span>
  minimum time<span class="pl-k">:</span>     <span class="pl-c1">5.793</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  median time<span class="pl-k">:</span>      <span class="pl-c1">5.816</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  mean time<span class="pl-k">:</span>        <span class="pl-c1">5.824</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  maximum time<span class="pl-k">:</span>     <span class="pl-c1">14.234</span> μs (<span class="pl-c1">0.00</span><span class="pl-k">%</span> GC)
  <span class="pl-k">--------------</span>
  samples<span class="pl-k">:</span>          <span class="pl-c1">10000</span>
  evals<span class="pl-k">/</span>sample<span class="pl-k">:</span>     <span class="pl-c1">6</span></pre></div>
<p dir="auto"></p>
</details>
<h3 dir="auto"><a id="user-content-dealing-with-structs" class="anchor" aria-hidden="true" href="#dealing-with-structs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dealing with structs</h3>
<details>
 
<p dir="auto">
</p><p dir="auto">The key to the <code>@turbo</code> macro's performance gains is leveraging knowledge of exactly how data like <code>Float64</code>s and <code>Int</code>s are handled by a CPU. As such, it is not strightforward to generalize the <code>@turbo</code> macro to work on arrays containing structs such as <code>Matrix{Complex{Float64}}</code>. Instead, it is currently recommended that users wishing to apply <code>@turbo</code> to arrays of structs use packages such as <a href="https://github.com/JuliaArrays/StructArrays.jl">StructArrays.jl</a> which transform an array where each element is a struct into a struct where each element is an array. Using StructArrays.jl, we can write a matrix multiply (gemm) kernel that works on matrices of <code>Complex{Float64}</code>s and <code>Complex{Int}</code>s:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using LoopVectorization, LinearAlgebra, StructArrays, BenchmarkTools, Test

BLAS.set_num_threads(1); @show BLAS.vendor()

const MatrixFInt64 = Union{Matrix{Float64}, Matrix{Int}}

function mul_avx!(C::MatrixFInt64, A::MatrixFInt64, B::MatrixFInt64)
    @turbo for m ∈ 1:size(A,1), n ∈ 1:size(B,2)
        Cmn = zero(eltype(C))
        for k ∈ 1:size(A,2)
            Cmn += A[m,k] * B[k,n]
        end
        C[m,n] = Cmn
    end
end

function mul_add_avx!(C::MatrixFInt64, A::MatrixFInt64, B::MatrixFInt64, factor=1)
    @turbo for m ∈ 1:size(A,1), n ∈ 1:size(B,2)
        ΔCmn = zero(eltype(C))
        for k ∈ 1:size(A,2)
            ΔCmn += A[m,k] * B[k,n]
        end
        C[m,n] += factor * ΔCmn
    end
end

const StructMatrixComplexFInt64 = Union{StructArray{ComplexF64,2}, StructArray{Complex{Int},2}}

function mul_avx!(C:: StructMatrixComplexFInt64, A::StructMatrixComplexFInt64, B::StructMatrixComplexFInt64)
    mul_avx!(    C.re, A.re, B.re)     # C.re = A.re * B.re
    mul_add_avx!(C.re, A.im, B.im, -1) # C.re = C.re - A.im * B.im
    mul_avx!(    C.im, A.re, B.im)     # C.im = A.re * B.im
    mul_add_avx!(C.im, A.im, B.re)     # C.im = C.im + A.im * B.re
end"><pre><span class="pl-k">using</span> LoopVectorization, LinearAlgebra, StructArrays, BenchmarkTools, Test

BLAS<span class="pl-k">.</span><span class="pl-c1">set_num_threads</span>(<span class="pl-c1">1</span>); <span class="pl-c1">@show</span> BLAS<span class="pl-k">.</span><span class="pl-c1">vendor</span>()

<span class="pl-k">const</span> MatrixFInt64 <span class="pl-k">=</span> Union{Matrix{Float64}, Matrix{Int}}

<span class="pl-k">function</span> <span class="pl-en">mul_avx!</span>(C<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, A<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, B<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>)
    <span class="pl-c1">@turbo</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
        Cmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
        <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
            Cmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
        <span class="pl-k">end</span>
        C[m,n] <span class="pl-k">=</span> Cmn
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">function</span> <span class="pl-en">mul_add_avx!</span>(C<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, A<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, B<span class="pl-k">::</span><span class="pl-c1">MatrixFInt64</span>, factor<span class="pl-k">=</span><span class="pl-c1">1</span>)
    <span class="pl-c1">@turbo</span> <span class="pl-k">for</span> m <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">1</span>), n <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(B,<span class="pl-c1">2</span>)
        ΔCmn <span class="pl-k">=</span> <span class="pl-c1">zero</span>(<span class="pl-c1">eltype</span>(C))
        <span class="pl-k">for</span> k <span class="pl-k">∈</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">size</span>(A,<span class="pl-c1">2</span>)
            ΔCmn <span class="pl-k">+=</span> A[m,k] <span class="pl-k">*</span> B[k,n]
        <span class="pl-k">end</span>
        C[m,n] <span class="pl-k">+=</span> factor <span class="pl-k">*</span> ΔCmn
    <span class="pl-k">end</span>
<span class="pl-k">end</span>

<span class="pl-k">const</span> StructMatrixComplexFInt64 <span class="pl-k">=</span> Union{StructArray{ComplexF64,<span class="pl-c1">2</span>}, StructArray{Complex{Int},<span class="pl-c1">2</span>}}

<span class="pl-k">function</span> <span class="pl-en">mul_avx!</span>(C<span class="pl-k">::</span> <span class="pl-c1">StructMatrixComplexFInt64</span>, A<span class="pl-k">::</span><span class="pl-c1">StructMatrixComplexFInt64</span>, B<span class="pl-k">::</span><span class="pl-c1">StructMatrixComplexFInt64</span>)
    <span class="pl-c1">mul_avx!</span>(    C<span class="pl-k">.</span>re, A<span class="pl-k">.</span>re, B<span class="pl-k">.</span>re)     <span class="pl-c"><span class="pl-c">#</span> C.re = A.re * B.re</span>
    <span class="pl-c1">mul_add_avx!</span>(C<span class="pl-k">.</span>re, A<span class="pl-k">.</span>im, B<span class="pl-k">.</span>im, <span class="pl-k">-</span><span class="pl-c1">1</span>) <span class="pl-c"><span class="pl-c">#</span> C.re = C.re - A.im * B.im</span>
    <span class="pl-c1">mul_avx!</span>(    C<span class="pl-k">.</span>im, A<span class="pl-k">.</span>re, B<span class="pl-k">.</span>im)     <span class="pl-c"><span class="pl-c">#</span> C.im = A.re * B.im</span>
    <span class="pl-c1">mul_add_avx!</span>(C<span class="pl-k">.</span>im, A<span class="pl-k">.</span>im, B<span class="pl-k">.</span>re)     <span class="pl-c"><span class="pl-c">#</span> C.im = C.im + A.im * B.re</span>
<span class="pl-k">end</span></pre></div>
<p dir="auto">this <code>mul_avx!</code> kernel can now accept <code>StructArray</code> matrices of complex numbers and multiply them efficiently:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="julia&gt; M, K, N = 56, 57, 58
(56, 57, 58)

julia&gt; A  = StructArray(randn(ComplexF64, M, K));

julia&gt; B  = StructArray(randn(ComplexF64, K, N));

julia&gt; C1 = StructArray(Matrix{ComplexF64}(undef, M, N));

julia&gt; C2 = collect(similar(C1));

julia&gt; @btime mul_avx!($C1, $A, $B)
  13.525 μs (0 allocations: 0 bytes)

julia&gt; @btime mul!(    $C2, $(collect(A)), $(collect(B))); # collect turns the StructArray into a regular Array
  14.003 μs (0 allocations: 0 bytes)

julia&gt; @test C1 ≈ C2
Test Passed"><pre>julia<span class="pl-k">&gt;</span> M, K, N <span class="pl-k">=</span> <span class="pl-c1">56</span>, <span class="pl-c1">57</span>, <span class="pl-c1">58</span>
(<span class="pl-c1">56</span>, <span class="pl-c1">57</span>, <span class="pl-c1">58</span>)

julia<span class="pl-k">&gt;</span> A  <span class="pl-k">=</span> <span class="pl-c1">StructArray</span>(<span class="pl-c1">randn</span>(ComplexF64, M, K));

julia<span class="pl-k">&gt;</span> B  <span class="pl-k">=</span> <span class="pl-c1">StructArray</span>(<span class="pl-c1">randn</span>(ComplexF64, K, N));

julia<span class="pl-k">&gt;</span> C1 <span class="pl-k">=</span> <span class="pl-c1">StructArray</span>(<span class="pl-c1">Matrix</span><span class="pl-c1">{ComplexF64}</span>(undef, M, N));

julia<span class="pl-k">&gt;</span> C2 <span class="pl-k">=</span> <span class="pl-c1">collect</span>(<span class="pl-c1">similar</span>(C1));

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mul_avx!</span>(<span class="pl-k">$</span>C1, <span class="pl-k">$</span>A, <span class="pl-k">$</span>B)
  <span class="pl-c1">13.525</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">mul!</span>(    <span class="pl-k">$</span>C2, <span class="pl-k">$</span>(<span class="pl-c1">collect</span>(A)), <span class="pl-k">$</span>(<span class="pl-c1">collect</span>(B))); <span class="pl-c"><span class="pl-c">#</span> collect turns the StructArray into a regular Array</span>
  <span class="pl-c1">14.003</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@test</span> C1 <span class="pl-k">≈</span> C2
Test Passed</pre></div>
<p dir="auto">Similar approaches can be taken to make kernels working with a variety of numeric struct types such as <a href="https://github.com/JuliaDiff/DualNumbers.jl">dual numbers</a>, <a href="https://github.com/JuliaMath/DoubleFloats.jl">DoubleFloats</a>, etc.</p>
<p dir="auto"></p>
</details>
<h2 dir="auto"><a id="user-content-packages-using-loopvectorization" class="anchor" aria-hidden="true" href="#packages-using-loopvectorization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Packages using LoopVectorization</h2>
<ul dir="auto">
<li><a href="https://github.com/MasonProtter/Gaius.jl">Gaius.jl</a></li>
<li><a href="https://github.com/YingboMa/MaBLAS.jl">MaBLAS.jl</a></li>
<li><a href="https://github.com/JuliaLinearAlgebra/Octavian.jl">Octavian.jl</a></li>
<li><a href="https://github.com/JuliaSIMD/PaddedMatrices.jl">PaddedMatrices.jl</a></li>
<li><a href="https://github.com/YingboMa/RecursiveFactorization.jl">RecursiveFactorization.jl</a></li>
<li><a href="https://github.com/OpenMendel/SnpArrays.jl">SnpArrays.jl</a></li>
<li><a href="https://github.com/mcabbott/Tullio.jl">Tullio.jl</a></li>
<li><a href="https://github.com/SkyWorld117/DianoiaML.jl">DianoiaML.jl</a></li>
<li><a href="https://github.com/TensorBFS/TropicalGEMM.jl">TropicalGEMM.jl</a></li>
<li><a href="https://github.com/trixi-framework/Trixi.jl">Trixi.jl</a></li>
<li><a href="https://github.com/JuliaSIMD/VectorizedStatistics.jl">VectorizedStatistics.jl</a></li>
<li><a href="https://github.com/brenhinkeller/NaNStatistics.jl">NaNStatistics.jl</a></li>
<li><a href="https://github.com/andrewjradcliffe/VectorizedReduction.jl">VectorizedReduction.jl</a></li>
<li><a href="https://github.com/SymbolicML/SymbolicRegression.jl">DynamicExpressions.jl</a></li>
<li><a href="https://github.com/MilesCranmer/PySR">PySR</a> and <a href="https://github.com/MilesCranmer/SymbolicRegression.jl">SymbolicRegression.jl</a></li>
</ul>
<p dir="auto">If you're using LoopVectorization, please feel free to file a PR adding yours to the list!</p>
</article></div>