<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><table>
<thead>
<tr>
<th align="center">Status</th>
<th align="center">Coverage</th>
<th align="center">Docs</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/mateuszbaran/CovarianceEstimation.jl/actions?query=workflow%3ACI+branch%3Amaster"><img src="https://github.com/mateuszbaran/CovarianceEstimation.jl/workflows/CI/badge.svg" alt="CI" style="max-width:100%;"></a></td>
<td align="center"><a href="http://codecov.io/github/mateuszbaran/CovarianceEstimation.jl?branch=master" rel="nofollow"> <img src="https://camo.githubusercontent.com/08be02f0adce87b67803e72a255fe3f4735c1aafa832a6a76c68258eee301251/687474703a2f2f636f6465636f762e696f2f6769746875622f6d61746575737a626172616e2f436f76617269616e6365457374696d6174696f6e2e6a6c2f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="codecov.io" data-canonical-src="http://codecov.io/github/mateuszbaran/CovarianceEstimation.jl/coverage.svg?branch=master" style="max-width:100%;"></a></td>
<td align="center"><a href="https://mateuszbaran.github.io/CovarianceEstimation.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a> <a href="https://mateuszbaran.github.io/CovarianceEstimation.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h1><a id="user-content-covarianceestimationjl" class="anchor" aria-hidden="true" href="#covarianceestimationjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CovarianceEstimation.jl</h1>
<p>Lightweight robust covariance estimation in Julia i.e. if you have a data matrix <code>X</code> of size <code>n×p</code> corresponding to <code>n</code> observations with <code>p</code> features, this package will help you to obtain an estimator of the covariance matrix associated with this data.</p>
<p><strong>Note</strong>: if you are interested in covariance estimation in the context of a linear regression, consider for now the package <a href="https://github.com/gragusa/CovarianceMatrices.jl">CovarianceMatrices.jl</a> which focuses around that case.</p>
<h2><a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick start</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using CovarianceEstimation

X = randn(5, 7)

S_uncorrected  = cov(SimpleCovariance(), X)
S_corrected    = cov(SimpleCovariance(corrected=true), X)

# using linear shrinkage with different targets
LSE = LinearShrinkage
# - Ledoit-Wolf target + shrinkage
method = LSE(ConstantCorrelation())
S_ledoitwolf = cov(method, X)
# - Chen target + shrinkage (using the more verbose call)
method = LSE(target=DiagonalCommonVariance(), shrinkage=:rblw)
S_chen_rblw = cov(method, X)
method = LSE(target=DiagonalCommonVariance(), shrinkage=:oas)
S_chen_oas = cov(method, X)

# a pre-defined shrinkage can be used as well
method = LinearShrinkage(DiagonalUnitVariance(), 0.5)
# using a given shrinkage
S_05 = cov(method, X)
"><pre><span class="pl-k">using</span> CovarianceEstimation

X <span class="pl-k">=</span> <span class="pl-c1">randn</span>(<span class="pl-c1">5</span>, <span class="pl-c1">7</span>)

S_uncorrected  <span class="pl-k">=</span> <span class="pl-c1">cov</span>(<span class="pl-c1">SimpleCovariance</span>(), X)
S_corrected    <span class="pl-k">=</span> <span class="pl-c1">cov</span>(<span class="pl-c1">SimpleCovariance</span>(corrected<span class="pl-k">=</span><span class="pl-c1">true</span>), X)

<span class="pl-c"><span class="pl-c">#</span> using linear shrinkage with different targets</span>
LSE <span class="pl-k">=</span> LinearShrinkage
<span class="pl-c"><span class="pl-c">#</span> - Ledoit-Wolf target + shrinkage</span>
method <span class="pl-k">=</span> <span class="pl-c1">LSE</span>(<span class="pl-c1">ConstantCorrelation</span>())
S_ledoitwolf <span class="pl-k">=</span> <span class="pl-c1">cov</span>(method, X)
<span class="pl-c"><span class="pl-c">#</span> - Chen target + shrinkage (using the more verbose call)</span>
method <span class="pl-k">=</span> <span class="pl-c1">LSE</span>(target<span class="pl-k">=</span><span class="pl-c1">DiagonalCommonVariance</span>(), shrinkage<span class="pl-k">=</span><span class="pl-c1">:rblw</span>)
S_chen_rblw <span class="pl-k">=</span> <span class="pl-c1">cov</span>(method, X)
method <span class="pl-k">=</span> <span class="pl-c1">LSE</span>(target<span class="pl-k">=</span><span class="pl-c1">DiagonalCommonVariance</span>(), shrinkage<span class="pl-k">=</span><span class="pl-c1">:oas</span>)
S_chen_oas <span class="pl-k">=</span> <span class="pl-c1">cov</span>(method, X)

<span class="pl-c"><span class="pl-c">#</span> a pre-defined shrinkage can be used as well</span>
method <span class="pl-k">=</span> <span class="pl-c1">LinearShrinkage</span>(<span class="pl-c1">DiagonalUnitVariance</span>(), <span class="pl-c1">0.5</span>)
<span class="pl-c"><span class="pl-c">#</span> using a given shrinkage</span>
S_05 <span class="pl-k">=</span> <span class="pl-c1">cov</span>(method, X)</pre></div>
<h2><a id="user-content-currently-supported-algorithms" class="anchor" aria-hidden="true" href="#currently-supported-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Currently supported algorithms</h2>
<p>In this section, <code>X</code> is the data matrix of size <code>n × p</code>, <code>S</code> is the sample covariance matrix with <code>S = κ (Xc' * Xc)</code> where <code>κ</code> is either <code>n</code> (uncorrected) or <code>n-1</code> (corrected) and <code>Xc</code> is the centred data matrix (see <a href="https://mateuszbaran.github.io/CovarianceEstimation.jl/dev" rel="nofollow">docs</a>).</p>
<ul>
<li><code>SimpleCovariance</code>: basic corrected or uncorrected sample covariance (implemented in <code>StatsBase.jl</code>).</li>
</ul>
<p><strong>Time complexity</strong>: <code>O(p²n)</code> with a low constant</p>
<h3><a id="user-content-sample-covariance-based-methods" class="anchor" aria-hidden="true" href="#sample-covariance-based-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Sample covariance based methods</h3>
<p>These methods build an estimator of the covariance derived from <code>S</code>. They are implemented using abstract covariance estimation interface from <code>StatsBase.jl</code>.</p>
<ul>
<li><code>LinearShrinkage</code>: James-Stein type estimator of the form <code>(1-λ)S+λF</code> where <code>F</code> is a target and <code>λ∈[0,1]</code> a shrinkage intensity.
<ul>
<li>common targets are implemented following the taxonomy given in [<strong>1</strong>] along with Ledoit-Wolf optimal shrinkage intensities [<strong>2</strong>].</li>
<li>in the case of the <code>DiagonalCommonVariance</code> target, a Rao-Blackwellised Ledoit-Wolf shrinkage (<code>:rblw</code>) and Oracle-Approximating shrinkage (<code>:oas</code>) are also supported (see [<strong>3</strong>]).</li>
<li><strong>Note</strong>: <code>S</code> is symmetric semi-positive definite so that if the <code>F</code> is symmetric positive definite and provided <code>λ</code> is non-zero, the estimator obtained after shrinkage is also symmetric positive definite. For the diagonal targets <code>DiagonalUnitVariance</code>, <code>DiagonalCommonVariance</code> and <code>DiagonalUnequalVariance</code> the target is necessarily SPD.</li>
</ul>
</li>
<li><code>AnalyticalNonlinearShrinkage</code>: estimator of the form <code>MΛM'</code> where <code>M</code> and <code>Λ</code> are matrices derived from the eigen decomposition of <code>S</code>.[<strong>4</strong>]</li>
</ul>
<p><strong>Time complexity</strong>:</p>
<ul>
<li>Linear shrinkage: <code>O(p²n)</code> with a low constant (main cost is forming <code>S</code>)</li>
<li>Nonlinear shrinkage:
<ul>
<li>if <code>p&lt;n</code>: <code>O(p²n + n²)</code> with a moderate constant (main cost is forming <code>S</code> and manipulating a matrix of <code>n²</code> elements)</li>
<li>if <code>p&gt;n</code>: <code>O(p³)</code> with a low constant (main cost is computing the eigen decomposition of <code>S</code>).</li>
</ul>
</li>
</ul>
<h3><a id="user-content-other-estimators-coming" class="anchor" aria-hidden="true" href="#other-estimators-coming"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Other estimators (coming)</h3>
<p>These are estimators that may be implemented in the future, see also <a href="https://arxiv.org/pdf/1504.02995.pdf" rel="nofollow">this review  paper</a>.</p>
<ul>
<li>Sparsity based estimators for either the covariance or the precision</li>
<li>Rank based approaches</li>
<li><a href="https://arxiv.org/pdf/1201.0175.pdf" rel="nofollow">POET</a></li>
<li>HAC</li>
<li>...</li>
</ul>
<p>For HAC (and other estimators of covariance of coefficient of regression models) you can currently use the <a href="https://github.com/gragusa/CovarianceMatrices.jl">CovarianceMatrices.jl</a> package.</p>
<h2><a id="user-content-comparison-to-existing-libraries" class="anchor" aria-hidden="true" href="#comparison-to-existing-libraries"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Comparison to existing libraries</h2>
<p>Rough benchmarks are run over random matrices of various sizes (<code>40x20, 20x40, 400x200, 200x400</code>).
These benchmarks should (as usual) be taken with a pinch of salt but essentially a significant speedup should be expected for a standard problem.</p>
<ul>
<li><strong>Sklearn</strong> (implements <code>DiagonalCommonVariance</code> target with <code>oas</code> and <code>lw</code> shrinkage)
<ul>
<li>average speedup: <code>5x</code></li>
</ul>
</li>
<li><strong>Corpcor</strong> (implements <code>DiagonalUnequalVariance</code> target with <code>ss</code> shrinkage)
<ul>
<li>average speedup: <code>22x</code></li>
</ul>
</li>
<li><strong>Ledoit-Wolf 1</strong> (implements <code>ConstantCorrelation</code> target with <code>lw</code> shrinkage, we used Octave for the comparison)
<ul>
<li>average speedup: <code>12x</code></li>
</ul>
</li>
<li><strong>Ledoit-Wolf 2</strong> (implements <code>AnalyticalNonlinearShrinkage</code>)
<ul>
<li>average speedup: <code>25x</code></li>
</ul>
</li>
</ul>
<h2><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul>
<li>[<strong>1</strong>] J. Schäfer and K. Strimmer, <em><a href="http://strimmerlab.org/publications/journals/shrinkcov2005.pdf" rel="nofollow">A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics</a></em>, Statistical Applications in Genetics and Molecular Biology, 2005.</li>
<li>[<strong>2</strong>] O. Ledoit and M. Wolf, <em><a href="http://www.ledoit.net/honey.pdf" rel="nofollow">Honey, I Shrunk the Sample Covariance Matrix</a></em>, The Journal of Portfolio Management, 2004.</li>
<li>[<strong>3</strong>] Y. Chen, A. Wiesel, Y. C. Eldar, and A. O. Hero, <em><a href="https://arxiv.org/pdf/0907.4698.pdf" rel="nofollow">Shrinkage Algorithms for MMSE Covariance Estimation</a></em>, IEEE Transactions on Signal Processing, 2010.</li>
<li>[<strong>4</strong>] O. Ledoit and M. Wolf, <em><a href="http://www.econ.uzh.ch/static/wp/econwp264.pdf" rel="nofollow">Analytical Nonlinear Shrinkage of Large-Dimensional Covariance Matrices</a></em>, Working Paper, 2018.</li>
</ul>
</article></div>