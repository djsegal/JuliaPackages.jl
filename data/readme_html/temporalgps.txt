<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-temporalgps" class="anchor" aria-hidden="true" href="#temporalgps"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TemporalGPs</h1>
<p><a href="https://github.com/willtebbutt/TemporalGPs.jl/actions"><img src="https://github.com/willtebbutt/TemporalGPs.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/willtebbutt/TemporalGPs.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/c39dadf66c85ddabc3ab80fa0f8223ee2d69aaf568201cf4080811dc7bb390f8/68747470733a2f2f636f6465636f762e696f2f67682f77696c6c746562627574742f54656d706f72616c4750732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/willtebbutt/TemporalGPs.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>
<p>TemporalGPs.jl is a tool to make Gaussian processes (GPs) defined using <a href="https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/">AbstractGPs.jl</a> fast for time-series. It provides a single-function public API that lets you specify that this package should perform inference, rather than AbstractGPs.jl.</p>
<p><a href="https://www.youtube.com/watch?v=dysmEpX1QoE" rel="nofollow">JuliaCon 2020 Talk</a></p>
<h1><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h1>
<p>TemporalGPs.jl is registered, so simply type the following at the REPL:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="] add AbstractGPs KernelFunctions TemporalGPs
"><pre>] add AbstractGPs KernelFunctions TemporalGPs</pre></div>
<p>While you can install TemporalGPs without AbstractGPs and KernelFunctions, in practice the latter are needed for all common tasks in TemporalGPs.</p>
<h1><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example Usage</h1>
<p>This is a small problem by TemporalGPs' standard. See timing results below for expected performance on larger problems.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using AbstractGPs, KernelFunctions, TemporalGPs

# Specify a AbstractGPs.jl GP as usual
f_naive = GP(Matern32Kernel())

# Wrap it in an object that TemporalGPs knows how to handle.
f = to_sde(f_naive, SArrayStorage(Float64))

# Project onto finite-dimensional distribution as usual.
# x = range(-5.0; step=0.1, length=10_000)
x = RegularSpacing(0.0, 0.1, 10_000) # Hack for Zygote.
fx = f(x, 0.1)

# Sample from the prior as usual.
y = rand(fx)

# Compute the log marginal likelihood of the data as usual.
logpdf(fx, y)

# Construct the posterior distribution over `f` having observed `y` at `x`.
f_post = posterior(fx, y)

# Compute the posterior marginals.
marginals(f_post(x))

# Draw a sample from the posterior. Note: same API as prior.
rand(f_post(x))

# Compute posterior log predictive probability of `y`. Note: same API as prior.
logpdf(f_post(x), y)
"><pre><span class="pl-k">using</span> AbstractGPs, KernelFunctions, TemporalGPs

<span class="pl-c"><span class="pl-c">#</span> Specify a AbstractGPs.jl GP as usual</span>
f_naive <span class="pl-k">=</span> <span class="pl-c1">GP</span>(<span class="pl-c1">Matern32Kernel</span>())

<span class="pl-c"><span class="pl-c">#</span> Wrap it in an object that TemporalGPs knows how to handle.</span>
f <span class="pl-k">=</span> <span class="pl-c1">to_sde</span>(f_naive, <span class="pl-c1">SArrayStorage</span>(Float64))

<span class="pl-c"><span class="pl-c">#</span> Project onto finite-dimensional distribution as usual.</span>
<span class="pl-c"><span class="pl-c">#</span> x = range(-5.0; step=0.1, length=10_000)</span>
x <span class="pl-k">=</span> <span class="pl-c1">RegularSpacing</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">0.1</span>, <span class="pl-c1">10_000</span>) <span class="pl-c"><span class="pl-c">#</span> Hack for Zygote.</span>
fx <span class="pl-k">=</span> <span class="pl-c1">f</span>(x, <span class="pl-c1">0.1</span>)

<span class="pl-c"><span class="pl-c">#</span> Sample from the prior as usual.</span>
y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(fx)

<span class="pl-c"><span class="pl-c">#</span> Compute the log marginal likelihood of the data as usual.</span>
<span class="pl-c1">logpdf</span>(fx, y)

<span class="pl-c"><span class="pl-c">#</span> Construct the posterior distribution over `f` having observed `y` at `x`.</span>
f_post <span class="pl-k">=</span> <span class="pl-c1">posterior</span>(fx, y)

<span class="pl-c"><span class="pl-c">#</span> Compute the posterior marginals.</span>
<span class="pl-c1">marginals</span>(<span class="pl-c1">f_post</span>(x))

<span class="pl-c"><span class="pl-c">#</span> Draw a sample from the posterior. Note: same API as prior.</span>
<span class="pl-c1">rand</span>(<span class="pl-c1">f_post</span>(x))

<span class="pl-c"><span class="pl-c">#</span> Compute posterior log predictive probability of `y`. Note: same API as prior.</span>
<span class="pl-c1">logpdf</span>(<span class="pl-c1">f_post</span>(x), y)</pre></div>
<h2><a id="user-content-learning-kernel-parameters-with-optimjl-parameterhandlingjl-and-zygotejl" class="anchor" aria-hidden="true" href="#learning-kernel-parameters-with-optimjl-parameterhandlingjl-and-zygotejl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning kernel parameters with <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a>, <a href="https://github.com/invenia/ParameterHandling.jl">ParameterHandling.jl</a>, and <a href="https://github.com/FluxML/Zygote.jl/">Zygote.jl</a></h2>
<p>TemporalGPs.jl doesn't provide scikit-learn-like functionality to train your model (find good kernel parameter settings).
Instead, we offer the functionality needed to easily implement your own training functionality using standard tools from the Julia ecosystem, as shown below.</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# Load our GP-related packages.
using AbstractGPs
using KernelFunctions
using TemporalGPs

# Load standard packages from the Julia ecosystem
using Optim # Standard optimisation algorithms.
using ParameterHandling # Helper functionality for dealing with model parameters.
using Zygote # Algorithmic Differentiation

using ParameterHandling: value, flatten

# Declare model parameters using `ParameterHandling.jl` types.
params = (
    var_kernel = positive(0.6),
    λ = positive(2.5),
    var_noise = positive(0.1),
)

function build_gp(params)
    f_naive = GP(params.var_kernel * transform(Matern52Kernel(), params.λ))
    return to_sde(f_naive, SArrayStorage(Float64))
end

# Generate some synthetic data from the prior.
const x = RegularSpacing(0.0, 0.1, 10_000)
const y = rand(build_gp(value(params))(x, value(params.var_noise)))

# Construct mapping between structured and Vector representation of parameters.
flat_initial_params, unflatten = flatten(params)

# Specify an objective function for Optim to minimise in terms of x and y.
# We choose the usual negative log marginal likelihood (NLML).
function objective(flat_params)
    params = value(unflatten(flat_params))
    f = build_gp(params)
    return -logpdf(f(x, params.var_noise), y)
end

# Check that the objective function works:
objective(flat_initial_params)

# Optimise using Optim. This optimiser often works fairly well in practice,
# but it's not going to be the best choice in all situations. Consult
# Optim.jl for more info on available optimisers and their properties.
training_results = Optim.optimize(
    objective,
    θ -&gt; only(Zygote.gradient(objective, θ)),
    flat_initial_params + randn(3), # Add some noise to make learning non-trivial
    BFGS(
        alphaguess = Optim.LineSearches.InitialStatic(scaled=true),
        linesearch = Optim.LineSearches.BackTracking(),
    ),
    Optim.Options(show_trace = true);
    inplace=false,
)

# Extracting the final values of the parameters.
# Should be close to truth.
final_params = value(unflatten(training_results.minimizer))
"><pre><span class="pl-c"><span class="pl-c">#</span> Load our GP-related packages.</span>
<span class="pl-k">using</span> AbstractGPs
<span class="pl-k">using</span> KernelFunctions
<span class="pl-k">using</span> TemporalGPs

<span class="pl-c"><span class="pl-c">#</span> Load standard packages from the Julia ecosystem</span>
<span class="pl-k">using</span> Optim <span class="pl-c"><span class="pl-c">#</span> Standard optimisation algorithms.</span>
<span class="pl-k">using</span> ParameterHandling <span class="pl-c"><span class="pl-c">#</span> Helper functionality for dealing with model parameters.</span>
<span class="pl-k">using</span> Zygote <span class="pl-c"><span class="pl-c">#</span> Algorithmic Differentiation</span>

<span class="pl-k">using</span> ParameterHandling<span class="pl-k">:</span> value, flatten

<span class="pl-c"><span class="pl-c">#</span> Declare model parameters using `ParameterHandling.jl` types.</span>
params <span class="pl-k">=</span> (
    var_kernel <span class="pl-k">=</span> <span class="pl-c1">positive</span>(<span class="pl-c1">0.6</span>),
    λ <span class="pl-k">=</span> <span class="pl-c1">positive</span>(<span class="pl-c1">2.5</span>),
    var_noise <span class="pl-k">=</span> <span class="pl-c1">positive</span>(<span class="pl-c1">0.1</span>),
)

<span class="pl-k">function</span> <span class="pl-en">build_gp</span>(params)
    f_naive <span class="pl-k">=</span> <span class="pl-c1">GP</span>(params<span class="pl-k">.</span>var_kernel <span class="pl-k">*</span> <span class="pl-c1">transform</span>(<span class="pl-c1">Matern52Kernel</span>(), params<span class="pl-k">.</span>λ))
    <span class="pl-k">return</span> <span class="pl-c1">to_sde</span>(f_naive, <span class="pl-c1">SArrayStorage</span>(Float64))
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Generate some synthetic data from the prior.</span>
<span class="pl-k">const</span> x <span class="pl-k">=</span> <span class="pl-c1">RegularSpacing</span>(<span class="pl-c1">0.0</span>, <span class="pl-c1">0.1</span>, <span class="pl-c1">10_000</span>)
<span class="pl-k">const</span> y <span class="pl-k">=</span> <span class="pl-c1">rand</span>(<span class="pl-c1">build_gp</span>(<span class="pl-c1">value</span>(params))(x, <span class="pl-c1">value</span>(params<span class="pl-k">.</span>var_noise)))

<span class="pl-c"><span class="pl-c">#</span> Construct mapping between structured and Vector representation of parameters.</span>
flat_initial_params, unflatten <span class="pl-k">=</span> <span class="pl-c1">flatten</span>(params)

<span class="pl-c"><span class="pl-c">#</span> Specify an objective function for Optim to minimise in terms of x and y.</span>
<span class="pl-c"><span class="pl-c">#</span> We choose the usual negative log marginal likelihood (NLML).</span>
<span class="pl-k">function</span> <span class="pl-en">objective</span>(flat_params)
    params <span class="pl-k">=</span> <span class="pl-c1">value</span>(<span class="pl-c1">unflatten</span>(flat_params))
    f <span class="pl-k">=</span> <span class="pl-c1">build_gp</span>(params)
    <span class="pl-k">return</span> <span class="pl-k">-</span><span class="pl-c1">logpdf</span>(<span class="pl-c1">f</span>(x, params<span class="pl-k">.</span>var_noise), y)
<span class="pl-k">end</span>

<span class="pl-c"><span class="pl-c">#</span> Check that the objective function works:</span>
<span class="pl-c1">objective</span>(flat_initial_params)

<span class="pl-c"><span class="pl-c">#</span> Optimise using Optim. This optimiser often works fairly well in practice,</span>
<span class="pl-c"><span class="pl-c">#</span> but it's not going to be the best choice in all situations. Consult</span>
<span class="pl-c"><span class="pl-c">#</span> Optim.jl for more info on available optimisers and their properties.</span>
training_results <span class="pl-k">=</span> Optim<span class="pl-k">.</span><span class="pl-c1">optimize</span>(
    objective,
    θ <span class="pl-k">-&gt;</span> <span class="pl-c1">only</span>(Zygote<span class="pl-k">.</span><span class="pl-c1">gradient</span>(objective, θ)),
    flat_initial_params <span class="pl-k">+</span> <span class="pl-c1">randn</span>(<span class="pl-c1">3</span>), <span class="pl-c"><span class="pl-c">#</span> Add some noise to make learning non-trivial</span>
    <span class="pl-c1">BFGS</span>(
        alphaguess <span class="pl-k">=</span> Optim<span class="pl-k">.</span>LineSearches<span class="pl-k">.</span><span class="pl-c1">InitialStatic</span>(scaled<span class="pl-k">=</span><span class="pl-c1">true</span>),
        linesearch <span class="pl-k">=</span> Optim<span class="pl-k">.</span>LineSearches<span class="pl-k">.</span><span class="pl-c1">BackTracking</span>(),
    ),
    Optim<span class="pl-k">.</span><span class="pl-c1">Options</span>(show_trace <span class="pl-k">=</span> <span class="pl-c1">true</span>);
    inplace<span class="pl-k">=</span><span class="pl-c1">false</span>,
)

<span class="pl-c"><span class="pl-c">#</span> Extracting the final values of the parameters.</span>
<span class="pl-c"><span class="pl-c">#</span> Should be close to truth.</span>
final_params <span class="pl-k">=</span> <span class="pl-c1">value</span>(<span class="pl-c1">unflatten</span>(training_results<span class="pl-k">.</span>minimizer))</pre></div>
<p>Once you've learned the parameters, you can use <code>posterior</code>, <code>marginals</code>, and <code>rand</code> to make posterior-predictions with the optimal parameters.</p>
<p>In the above example we optimised the parameters, but we could just as easily have utilised e.g. <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a> in conjunction with a prior over the parameters to perform approximate Bayesian inference in them -- indeed, <a href="http://proceedings.mlr.press/v118/lalchand20a/lalchand20a.pdf" rel="nofollow">this is often a very good idea</a>. We leave this as an exercise for the interested user (see e.g. the examples in <a href="https://github.com/willtebbutt/Stheno.jl/">Stheno.jl</a> for inspiration).</p>
<p>Moreover, it should be possible to plug this into probabilistic programming framework such as <code>Turing</code> and <code>Soss</code> with minimal effort, since <code>f(x, params.var_noise)</code> is a plain old <code>Distributions.MultivariateDistribution</code>.</p>
<h1><a id="user-content-performance-optimisations" class="anchor" aria-hidden="true" href="#performance-optimisations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Performance Optimisations</h1>
<p>There are a couple of ways that <code>TemporalGPs.jl</code> can represent things internally. In particular, it can use regular Julia <code>Vector</code> and <code>Matrix</code> objects, or the <code>StaticArrays.jl</code> package to optimise in certain cases. The default is the former. To employ the latter, just add an extra argument to the <code>to_sde</code> function:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="f = to_sde(f_naive, SArrayStorage(Float64))
"><pre>f <span class="pl-k">=</span> <span class="pl-c1">to_sde</span>(f_naive, <span class="pl-c1">SArrayStorage</span>(Float64))</pre></div>
<p>This tells TemporalGPs that you want all parameters of <code>f</code> and anything derived from it to be a subtype of a <code>SArray</code> with element-type <code>Float64</code>, rather than (for example) a <code>Matrix{Float64}</code>s and <code>Vector{Float64}</code>. The decision made here can have quite a dramatic effect on performance, as shown in the graph below. For "larger" kernels (large sums, spatio-temporal problems), you might want to consider <code>ArrayStorage(Float64)</code> instead.</p>
<h1><a id="user-content-benchmarking-results" class="anchor" aria-hidden="true" href="#benchmarking-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmarking Results</h1>
<p><a target="_blank" rel="noopener noreferrer" href="/examples/benchmarks.png"><img src="/examples/benchmarks.png" alt="" style="max-width:100%;"></a></p>
<p>"naive" timings are with the usual <a href="https://https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/" rel="nofollow">AbstractGPs.jl</a> inference routines, and is the default implementation for GPs. "lgssm" timings are conducted using <code>to_sde</code> with no additional arguments. "static-lgssm" uses the <code>SArrayStorage(Float64)</code> option discussed above.</p>
<p>Gradient computations use Zygote. Custom adjoints have been implemented to achieve this level of performance.</p>
<h1><a id="user-content-on-going-work" class="anchor" aria-hidden="true" href="#on-going-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>On-going Work</h1>
<ul>
<li>Optimisation
<ul>
<li>in-place implementation with <code>ArrayStorage</code> to reduce allocations</li>
<li>input data types for posterior inference - the <code>RegularSpacing</code> type is great for expressing that the inputs are regularly spaced. A carefully constructed data type to let the user build regularly-spaced data when working with posteriors would also be very beneficial.</li>
</ul>
</li>
<li>Interfacing with other packages
<ul>
<li>When <a href="https://github.com/willtebbutt/Stheno.jl/">Stheno.jl</a> moves over to the AbstractGPs interface, it should be possible to get some interesting process decomposition functionality in this package.</li>
</ul>
</li>
<li>Approximate inference under non-Gaussian observation models</li>
</ul>
<p>If you're interested in helping out with this stuff, please get in touch by opening an issue, commenting on an open one, or messaging me on the Julia Slack.</p>
<h1><a id="user-content-relevant-literature" class="anchor" aria-hidden="true" href="#relevant-literature"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Relevant literature</h1>
<p>See chapter 12 of [1] for the basics.</p>
<p>[1] - Särkkä, Simo, and Arno Solin. Applied stochastic differential equations. Vol. 10. Cambridge University Press, 2019.</p>
<h1><a id="user-content-gotchas" class="anchor" aria-hidden="true" href="#gotchas"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Gotchas</h1>
<ul>
<li>And time-rescaling is assumed to be a strictly increasing function of time. If this is not the case, then your code will fail silently. Ideally an error would be thrown.</li>
</ul>
</article></div>