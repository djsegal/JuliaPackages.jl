<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-bayestestingjl" class="anchor" aria-hidden="true" href="#bayestestingjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>BayesTesting.jl</h1>
<p><strong>Bayesian Hypothesis Testing without Tears</strong></p>
<p>Objective Bayesian hypothesis testing that does not suffer from the problems inherent in the standard approach and so work well in practice:</p>
<ol>
<li>The JLB paradox does not arise.</li>
<li>Any prior can be employed, including uninformative priors, so the same prior employed for inference can be used for testing.</li>
<li>In standard problems when the posterior distribution matches (numerically) the frequentist sampling distribution or likelihood, there is a one-to-one correspondence with the frequentist test.</li>
<li>Provides posterior odds against the null hypothesis that are easy to interpret (unlike <em>p</em>-values), do not violate the likelihood principle, and result from minimizing a linear combination of type I and II errors rather than fixing the type I error before testing (as in Neyman-Pearson significance testing).</li>
</ol>
<p><strong>Installation</strong>
Currently unregistered, to install, enter the following (at the julia prompt):</p>
<pre><code>julia&gt; ]
(v1.0) pkg&gt; add https://github.com/tszanalytics/BayesTesting.jl.git
</code></pre>
<p><strong>Functions currently available (package is under development)</strong></p>
<p><strong>Functions added</strong> (see BayesTesting.jl_docs_2018.pdf for details)<strong>:
Bayesian_ttest, correlation_ttest, compare_means, compare_proportions, equiv_test</strong></p>
<p><strong>Hypothesis testing:</strong></p>
<p>Optional parameter in following functions: h0= value in hull hypothesis (default is h0 = 0)</p>
<p><strong>pdr_val(theta_draws)</strong> = returns posterior density ratio (PDR, aka posterior odds), tail_prob, 2xtail_prob (a "Bayesian p-value)</p>
<p><strong>todds(theta_hat,theta_hat_se,v)</strong> = returns Student-t posterior odds for theta</p>
<p><strong>mcodds(theta_draws)</strong> = returns posterior odds given MC sample for theta (any distribution).</p>
<p><strong>bayespval(theta_draws)</strong> = returns Bayesian p-value (tail area) give MC sample for theta</p>
<p><strong>Posterior inference:</strong></p>
<p><strong>update_mean(m1,m0,s1,s0,n1,n0)</strong> = For Gaussian posterior sample 1 (or prior) with mean = m0, sd = s0, number of obs. =n0, and Gaussian likelihood or posterior for sample 2 with mean = m1, SD = s1, number of obs. = n1, returns tuple of combined sample posterior mean = m2, SD = s2, number of obs. = n2</p>
<p><strong>marginal_posterior_mu(m,s, n, M)</strong> = return M draws from Student-t marginal posterior density with mean = m, SD = s, number of obs. = n.  M is an optional argument (default is M = 10000).</p>
<p><strong>blinreg(y,X)</strong> = estimate a linear model y=XÎ²+u (define X to contain vector of ones for an intercept)</p>
<p><strong>gsreg(y,X)</strong> = Gibbs sampler for linear regression with default uninformative prior, X must contain
vector of ones to include intercept.
Optional parameters:
tau = precision starting value (default = 1.0)
M = MCMC sample size (default = 10,000)</p>
<p><strong>gsreg(y,X, M=m, tau=t, b0=priorb, iB0 = invpriorcovb , d0=b, a0=a)</strong> = Gibbs sampler with NIG prior.
Note: iB0 = prior precision matrix = inv(prior variance matrix)
b0 must be a column vector,
a0 and b0 are prior parameters for tau ~ Gamma(a,b)</p>
<p><strong>Example 1: Testing if a sample mean equals zero</strong></p>
<pre><code>using BayesTesting
srand(1235)             # generate psuedo-data, n obs.
n = 50
x = randn(n)

v = n-1                 # degrees of freedom
mu_hat = mean(x)        # sample mean
se_mu = std(x)/sqrt(v)  # sample standard error of mean
todds(mu_hat,se_mu,v)   # posterior odds vs. zero

# Result: todds(mu_hat, se_mu, v, h0=0) = 1.016  =&gt; 1:1 odds against the null.


# with a nonzero mean - change the data generating process for x above to:
x = 0.5 + randn(n)
# Resulting posterior odds: todds(mu_hat, se_mu, v, h0=0) = 110.50  =&gt; 110:1 odds against the null
</code></pre>
<p><strong>More detailed help and examples in:</strong> BayesTesting.jl_docs_2018.pdf</p>
<p>ADDED: compare_means and compare_proportions functions</p>
<h3><a id="user-content-plot-function-for-use-with-compare-functions-mc-output" class="anchor" aria-hidden="true" href="#plot-function-for-use-with-compare-functions-mc-output"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>plot function for use with compare functions MC output</h3>
<p>Will be added as PlotRecipe to package functions soon.</p>
<pre><code>
function plot_mc_diff(draws_m1,draws_m2;  lbl=["mu 1" "mu 2"],lgd = :topright)
    diff_mean = draws_m1 - draws_m2
    l = @layout([a b])
    plt1 = plot(draws_m1,st=:density,fill=(0,0.4,:blue),alpha=0.4,label=lbl[1],legend=lgd,title="Posteriors from each mean")
    plot!(draws_m2,st=:density,fill=(0,0.4,:red),alpha=0.4,label=lbl[2])
    plt2 = plot(diff_mean,st=:density,fill=(0,0.4,:green),alpha=0.4,label="",title="Posterior difference")
    vline!([0.0],color=:black,label="")
    plt3 = plot(plt1, plt2, layout=l)
    return plt3
end
</code></pre>
<p>Example of use of plot_mc_diff function</p>
<pre><code>m1 = 1.0; s1 = 0.8; n1 = 10; m2 = 0.0; s2 = 1.0; n2 = 20
diff_mean, draws_m1, draws_m2, qs, tst = compare_means(m1, m2, s1, s2, n1, n2)
plt = plot_mc_diff(draws_m1,draws_m2)
</code></pre>
</article></div>