<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-gametheoryjl" class="anchor" aria-hidden="true" href="#gametheoryjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GameTheory.jl</h1>
<p dir="auto"><a href="https://github.com/QuantEcon/GameTheory.jl/actions/workflows/ci.yml"><img src="https://github.com/QuantEcon/GameTheory.jl/workflows/CI/badge.svg" alt="Build Status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/QuantEcon/GameTheory.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/2c0535dcd8a0831d62f172d59ddb1b24dde133e78f06c97b0bee0b1ee159593c/68747470733a2f2f636f6465636f762e696f2f67682f5175616e7445636f6e2f47616d655468656f72792e6a6c2f6272616e63682f6d61696e2f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/QuantEcon/GameTheory.jl/branch/main/graph/badge.svg" style="max-width: 100%;"></a>
<a href="https://QuantEcon.github.io/GameTheory.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width: 100%;"></a>
<a href="https://QuantEcon.github.io/GameTheory.jl/latest" rel="nofollow"><img src="https://camo.githubusercontent.com/56f8252ba8e9d3f0b810769543f77823d2fe031ce560d4c2d69fb1fcad800383/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-latest-blue.svg" style="max-width: 100%;"></a></p>
<p dir="auto">Algorithms and data structures for game theory in Julia</p>
<h2 dir="auto"><a id="user-content-example-usage" class="anchor" aria-hidden="true" href="#example-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example usage</h2>
<p dir="auto">Create a <code>NormalFormGame</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="using GameTheory
player1 = Player([3 3; 2 5; 0 6])
player2 = Player([3 2 3; 2 6 1])
g = NormalFormGame(player1, player2)
println(g)"><pre><span class="pl-k">using</span> GameTheory
player1 <span class="pl-k">=</span> <span class="pl-c1">Player</span>([<span class="pl-c1">3</span> <span class="pl-c1">3</span>; <span class="pl-c1">2</span> <span class="pl-c1">5</span>; <span class="pl-c1">0</span> <span class="pl-c1">6</span>])
player2 <span class="pl-k">=</span> <span class="pl-c1">Player</span>([<span class="pl-c1">3</span> <span class="pl-c1">2</span> <span class="pl-c1">3</span>; <span class="pl-c1">2</span> <span class="pl-c1">6</span> <span class="pl-c1">1</span>])
g <span class="pl-k">=</span> <span class="pl-c1">NormalFormGame</span>(player1, player2)
<span class="pl-c1">println</span>(g)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="3×2 NormalFormGame{2, Int64}:
 [3, 3]  [3, 2]
 [2, 2]  [5, 6]
 [0, 3]  [6, 1]"><pre class="notranslate"><code>3×2 NormalFormGame{2, Int64}:
 [3, 3]  [3, 2]
 [2, 2]  [5, 6]
 [0, 3]  [6, 1]
</code></pre></div>
<p dir="auto"><code>lrsnash</code> calls the Nash equilibrium computation routine in <a href="http://cgm.cs.mcgill.ca/~avis/C/lrs.html" rel="nofollow">lrslib</a>
(through its Julia wrapper <a href="https://github.com/JuliaPolyhedra/LRSLib.jl">LRSLib.jl</a>):</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="lrsnash(g)"><pre><span class="pl-c1">lrsnash</span>(g)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="3-element Vector{Tuple{Vector{Rational{BigInt}}, Vector{Rational{BigInt}}}}:
 ([4//5, 1//5, 0//1], [2//3, 1//3])
 ([0//1, 1//3, 2//3], [1//3, 2//3])
 ([1//1, 0//1, 0//1], [1//1, 0//1])"><pre class="notranslate"><code>3-element Vector{Tuple{Vector{Rational{BigInt}}, Vector{Rational{BigInt}}}}:
 ([4//5, 1//5, 0//1], [2//3, 1//3])
 ([0//1, 1//3, 2//3], [1//3, 2//3])
 ([1//1, 0//1, 0//1], [1//1, 0//1])
</code></pre></div>
<p dir="auto">A 2x2x2 <code>NormalFormGame</code>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="g = NormalFormGame((2, 2, 2))
g[1, 1, 1] = [9, 8, 12]
g[2, 2, 1] = [9, 8, 2]
g[1, 2, 2] = [3, 4, 6]
g[2, 1, 2] = [3, 4, 4]
println(g)"><pre>g <span class="pl-k">=</span> <span class="pl-c1">NormalFormGame</span>((<span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>))
g[<span class="pl-c1">1</span>, <span class="pl-c1">1</span>, <span class="pl-c1">1</span>] <span class="pl-k">=</span> [<span class="pl-c1">9</span>, <span class="pl-c1">8</span>, <span class="pl-c1">12</span>]
g[<span class="pl-c1">2</span>, <span class="pl-c1">2</span>, <span class="pl-c1">1</span>] <span class="pl-k">=</span> [<span class="pl-c1">9</span>, <span class="pl-c1">8</span>, <span class="pl-c1">2</span>]
g[<span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">2</span>] <span class="pl-k">=</span> [<span class="pl-c1">3</span>, <span class="pl-c1">4</span>, <span class="pl-c1">6</span>]
g[<span class="pl-c1">2</span>, <span class="pl-c1">1</span>, <span class="pl-c1">2</span>] <span class="pl-k">=</span> [<span class="pl-c1">3</span>, <span class="pl-c1">4</span>, <span class="pl-c1">4</span>]
<span class="pl-c1">println</span>(g)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="2×2×2 NormalFormGame{3, Float64}:
[:, :, 1] =
 [9.0, 8.0, 12.0]  [0.0, 0.0, 0.0]
 [0.0, 0.0, 0.0]   [9.0, 8.0, 2.0]

[:, :, 2] =
 [0.0, 0.0, 0.0]  [3.0, 4.0, 6.0]
 [3.0, 4.0, 4.0]  [0.0, 0.0, 0.0]"><pre class="notranslate"><code>2×2×2 NormalFormGame{3, Float64}:
[:, :, 1] =
 [9.0, 8.0, 12.0]  [0.0, 0.0, 0.0]
 [0.0, 0.0, 0.0]   [9.0, 8.0, 2.0]

[:, :, 2] =
 [0.0, 0.0, 0.0]  [3.0, 4.0, 6.0]
 [3.0, 4.0, 4.0]  [0.0, 0.0, 0.0]
</code></pre></div>
<p dir="auto"><code>hc_solve</code> computes all isolated Nash equilibria of an N-player game by using
<a href="https://github.com/JuliaHomotopyContinuation/HomotopyContinuation.jl">HomotopyContinuation.jl</a>:</p>
<div class="highlight highlight-source-julia notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="NEs = hc_solve(g)"><pre>NEs <span class="pl-k">=</span> <span class="pl-c1">hc_solve</span>(g)</pre></div>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="9-element Vector{Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}:
 ([2.63311e-36, 1.0], [0.333333, 0.666667], [0.333333, 0.666667])
 ([0.25, 0.75], [1.0, 0.0], [0.25, 0.75])
 ([0.0, 1.0], [0.0, 1.0], [1.0, 0.0])
 ([0.25, 0.75], [0.5, 0.5], [0.333333, 0.666667])
 ([0.5, 0.5], [0.5, 0.5], [1.0, 1.37753e-40])
 ([1.0, 0.0], [0.0, 1.0], [0.0, 1.0])
 ([0.5, 0.5], [0.333333, 0.666667], [0.25, 0.75])
 ([1.0, 0.0], [1.0, 9.40395e-38], [1.0, -9.40395e-38])
 ([0.0, 1.0], [1.0, 0.0], [0.0, 1.0])"><pre class="notranslate"><code>9-element Vector{Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}:
 ([2.63311e-36, 1.0], [0.333333, 0.666667], [0.333333, 0.666667])
 ([0.25, 0.75], [1.0, 0.0], [0.25, 0.75])
 ([0.0, 1.0], [0.0, 1.0], [1.0, 0.0])
 ([0.25, 0.75], [0.5, 0.5], [0.333333, 0.666667])
 ([0.5, 0.5], [0.5, 0.5], [1.0, 1.37753e-40])
 ([1.0, 0.0], [0.0, 1.0], [0.0, 1.0])
 ([0.5, 0.5], [0.333333, 0.666667], [0.25, 0.75])
 ([1.0, 0.0], [1.0, 9.40395e-38], [1.0, -9.40395e-38])
 ([0.0, 1.0], [1.0, 0.0], [0.0, 1.0])
</code></pre></div>
<p dir="auto">See the tutorials for further examples.</p>
<h2 dir="auto"><a id="user-content-implemented-algorithms" class="anchor" aria-hidden="true" href="#implemented-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Implemented algorithms</h2>
<h3 dir="auto"><a id="user-content-nash-equilibrium-computation" class="anchor" aria-hidden="true" href="#nash-equilibrium-computation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Nash equilibrium computation</h3>
<ul dir="auto">
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/computing_nash_equilibria.html#GameTheory.pure_nash-Tuple%7BNormalFormGame%7D" rel="nofollow"><code>pure_nash</code></a>:
Find all pure-action Nash equilibria of an N-player game (if any)</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/computing_nash_equilibria.html#GameTheory.support_enumeration-Union%7BTuple%7BNormalFormGame%7B2,%20T%7D%7D,%20Tuple%7BT%7D%7D%20where%20T" rel="nofollow"><code>support_enumeration</code></a>:
Find all mixed-action Nash equilibria of a two-player nondegenerate game</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/computing_nash_equilibria.html#GameTheory.lrsnash-Tuple%7BNormalFormGame%7B2,%20%3C:Union%7BInt64,%20Rational%7D%7D%7D" rel="nofollow"><code>lrsnash</code></a>:
Find all mixed-action Nash equilibria (or equilibrium components) of a two-player game</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/computing_nash_equilibria.html#GameTheory.hc_solve-Union%7BTuple%7BNormalFormGame%7BN%7D%7D,%20Tuple%7BN%7D%7D%20where%20N" rel="nofollow"><code>hc_solve</code></a>:
Find all isolated mixed-action Nash equilibria of an N-player game</li>
</ul>
<h3 dir="auto"><a id="user-content-learningevolutionary-dynamics" class="anchor" aria-hidden="true" href="#learningevolutionary-dynamics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Learning/evolutionary dynamics</h3>
<ul dir="auto">
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.BRD" rel="nofollow"><code>BRD</code></a>:
Best response dynamics</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.KMR" rel="nofollow"><code>KMR</code></a>:
Best response with mutations dynamics of Kandori-Mailath-Rob</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.SamplingBRD" rel="nofollow"><code>SamplingBRD</code></a>:
Sampling best response dynamics</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.FictitiousPlay" rel="nofollow"><code>FictitiousPlay</code></a>:
Fictitious play</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.StochasticFictitiousPlay" rel="nofollow"><code>StochasticFictitiousPlay</code></a>:
Stochastic fictitious play</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.LocalInteraction" rel="nofollow"><code>LocalInteraction</code></a>:
Local interaction dynamics</li>
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/learning_algorithms.html#GameTheory.LogitDynamics" rel="nofollow"><code>LogitDynamics</code></a>:
Logit dynamics</li>
</ul>
<h3 dir="auto"><a id="user-content-repeated-games" class="anchor" aria-hidden="true" href="#repeated-games"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Repeated games</h3>
<ul dir="auto">
<li><a href="https://quantecon.github.io/GameTheory.jl/stable/lib/repeated_games.html#GameTheory.outerapproximation-Tuple%7BRepeatedGame%7B2%7D%7D" rel="nofollow"><code>outerapproximation</code></a>:
Equilibrium payoff computation algorithm by Judd-Yeltekin-Conklin</li>
</ul>
<h2 dir="auto"><a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Tutorials</h2>
<ul dir="auto">
<li><a href="https://nbviewer.org/github/QuantEcon/game-theory-notebooks/blob/main/game_theory_jl.ipynb" rel="nofollow">Tools for Game Theory in GameTheory.jl</a></li>
<li><a href="https://nbviewer.org/github/QuantEcon/QuantEcon.notebooks/blob/master/recursive_repeated_games.ipynb" rel="nofollow">A Recursive Formulation of Repeated Games</a></li>
</ul>
<p dir="auto">See also the <a href="https://quanteconpy.readthedocs.io/en/latest/game_theory.html" rel="nofollow"><code>game_theory</code></a> submodule of
<a href="https://github.com/QuantEcon/QuantEcon.py"><code>QuantEcon.py</code></a>,
the Python counterpart of this package.</p>
</article></div>