<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-ciaoalgorithmsjl" class="anchor" aria-hidden="true" href="#ciaoalgorithmsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>CIAOAlgorithms.jl</h1>
<p dir="auto"><a href="https://github.com/kul-forbes/CIAOAlgorithms/actions?query=workflow%3ACI"><img src="https://github.com/kul-forbes/CIAOAlgorithms/workflows/CI/badge.svg" alt="Build status" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/kul-optec/CIAOAlgorithms.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/37a0cab286d7838d93f48f8c40a9d3a0dca5ff4305e42f07d759c18296d29a9c/68747470733a2f2f636f6465636f762e696f2f67682f6b756c2d6f707465632f4349414f416c676f726974686d732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d5751334556524648494a" alt="codecov" data-canonical-src="https://codecov.io/gh/kul-optec/CIAOAlgorithms.jl/branch/master/graph/badge.svg?token=WQ3EVRFHIJ" style="max-width: 100%;"></a></p>
<p dir="auto">CIAOAlgorithms implements Block-Coordinate and Incremental Aggregated Optimization Algorithms for minimizations of the form</p>
<math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="23501d39b2a0454aa5bcd5631108d19a">$$minimize    1/N sum_{i=1}^N f_i(x) + g(x)$$</math-renderer>
<p dir="auto">or</p>
<math-renderer class="js-display-math" style="display: block" data-static-url="https://github.githubassets.com/static" data-run-id="23501d39b2a0454aa5bcd5631108d19a">$$minimize    1/N sum_{i=1}^N f_i(x_i) + g(sum_{i=1}^N x_i)$$</math-renderer>
<p dir="auto">where f_i are smooth, and g is (possibly) nonsmooth with easy to compute proximal mapping. These functions can be defined using the <a href="https://github.com/kul-optec/ProximalOperators.jl">ProximalOperators.jl</a> package.</p>
<h3 dir="auto">
<a id="user-content-quick-guide" class="anchor" aria-hidden="true" href="#quick-guide"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick guide</h3>
<p dir="auto">You can add CIAOAlgorithms by pressing <code>]</code> to enter the package manager, then</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="pkg&gt; add CIAOAlgorithms"><pre class="notranslate"><code>pkg&gt; add CIAOAlgorithms
</code></pre></div>
<p dir="auto">Simple Lasso and logisitc regression test examples can be found <a href="test">here</a>.</p>
<h3 dir="auto">
<a id="user-content-implemented-algorithms" class="anchor" aria-hidden="true" href="#implemented-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Implemented Algorithms</h3>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Function</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finito/MISO/DIAG</td>
<td><a href="src/algorithms/Finito/Finito.jl"><code>Finito</code></a></td>
<td>
<a href="https://arxiv.org/pdf/1407.2710.pdf" rel="nofollow">[1]</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/140957639" rel="nofollow">[4]</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1101702" rel="nofollow">[8]</a>, <a href="https://arxiv.org/pdf/1906.10053.pdf" rel="nofollow">[9]</a>
</td>
</tr>
<tr>
<td>ProShI</td>
<td><a href="src/algorithms/ProShI/ProShI.jl"><code>Proshi</code></a></td>
<td><a href="https://arxiv.org/pdf/1906.10053.pdf" rel="nofollow">[9]</a></td>
</tr>
<tr>
<td>SAGA</td>
<td><a href="src/algorithms/SAGA_SAG/SAGA.jl"><code>SAGA</code></a></td>
<td>
<a href="https://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf" rel="nofollow">[3]</a>, <a href="https://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf" rel="nofollow">[6]</a>
</td>
</tr>
<tr>
<td>SAG</td>
<td><a href="src/algorithms/SAGA_SAG/SAGA.jl"><code>SAG</code></a></td>
<td><a href="https://link.springer.com/article/10.1007/s10107-016-1030-6" rel="nofollow">[7]</a></td>
</tr>
<tr>
<td>SVRG/SVRG++</td>
<td><a href="src/algorithms/SVRG/SVRG.jl"><code>SVRG</code></a></td>
<td>
<a href="https://epubs.siam.org/doi/pdf/10.1137/140961791" rel="nofollow">[2]</a>, <a href="https://arxiv.org/pdf/1506.01972.pdf" rel="nofollow">[4]</a>, <a href="https://papers.nips.cc/paper/6116-proximal-stochastic-methods-for-nonsmooth-nonconvex-finite-sum-optimization.pdf" rel="nofollow">[5]</a>
</td>
</tr>
</tbody>
</table>
<h3 dir="auto">
<a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h3>
<p dir="auto"><a href="https://arxiv.org/pdf/1407.2710.pdf" rel="nofollow">[1]</a> Defazio, Domke, <em>Finito: A faster, permutable incremental gradient method for big data problems</em>, In International Conference on Machine Learning pp. 1125-1133 (2014).</p>
<p dir="auto"><a href="https://epubs.siam.org/doi/pdf/10.1137/140961791" rel="nofollow">[2]</a> Xiao, Zhang, <em>A proximal stochastic gradient method  with progressive variance reduction</em>, SIAM Journal on Optimization 24(4):2057–2075 (2014).</p>
<p dir="auto"><a href="https://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf" rel="nofollow">[3]</a> Defazio, Bach, Lacoste-Julien, <em>SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</em>, In: Advances in neural information processing systems, pp. 1646–1654 (2014).</p>
<p dir="auto"><a href="https://epubs.siam.org/doi/pdf/10.1137/140957639" rel="nofollow">[4]</a> Mairal, <em>Incremental majorization-minimization optimization with application to large-scale machine learning</em>
SIAM Journal on Optimization 25(2), 829–855 (2015).</p>
<p dir="auto"><a href="https://arxiv.org/pdf/1506.01972.pdf" rel="nofollow">[5]</a> Allen-Zhu, Yuan, <em>Improved SVRG for non-strongly-convex or sum-of-non-convex objectives</em> In Proceedings of the 33rd International Conference on Machine Learning 1080–1089 (2016).</p>
<p dir="auto"><a href="https://papers.nips.cc/paper/6116-proximal-stochastic-methods-for-nonsmooth-nonconvex-finite-sum-optimization.pdf" rel="nofollow">[6]</a> Reddi, Sra, Poczos, and Smola, <em>Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization</em> In Advances in Neural Information Processing Systems, pp. 1145–1153 (2016).</p>
<p dir="auto"><a href="https://link.springer.com/article/10.1007/s10107-016-1030-6" rel="nofollow">[7]</a> Schmidt, Le Roux, Bach, <em>Minimizing finite sums with the stochastic average gradient</em> Mathematical Programming, 162(1-2), 83-112 (2017).</p>
<p dir="auto"><a href="https://epubs.siam.org/doi/pdf/10.1137/16M1101702" rel="nofollow">[8]</a> Mokhtari, Gürbüzbalaban, Ribeiro, <em>Surpassing gradient descent provably: A cyclic incremental method with linear convergence rate</em> SIAM Journal on Optimization 28(2), 1420–1447 (2018).</p>
<p dir="auto"><a href="https://arxiv.org/pdf/1906.10053.pdf" rel="nofollow">[9]</a> Latafat, Themelis, Patrinos, <em>Block-coordinate and incremental aggregated proximal gradient methods for nonsmooth nonconvex problems</em> arXiv:1906.10053 (2019).</p>
</article></div>