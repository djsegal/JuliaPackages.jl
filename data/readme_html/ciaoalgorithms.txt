<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-ciaoalgorithmsjl" class="anchor" aria-hidden="true" href="#ciaoalgorithmsjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CIAOAlgorithms.jl</h1>
<p><a href="https://github.com/kul-forbes/CIAOAlgorithms/actions?query=workflow%3ACI"><img src="https://github.com/kul-forbes/CIAOAlgorithms/workflows/CI/badge.svg" alt="Build status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/kul-forbes/CIAOAlgorithms.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/a24667523891ab604b7d2c4d15c8705996cf16712c48112b3bf461e0a55b85fe/68747470733a2f2f636f6465636f762e696f2f67682f6b756c2d666f726265732f4349414f416c676f726974686d732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d5751334556524648494a" alt="codecov" data-canonical-src="https://codecov.io/gh/kul-forbes/CIAOAlgorithms.jl/branch/master/graph/badge.svg?token=WQ3EVRFHIJ" style="max-width:100%;"></a></p>
<p>CIAOAlgorithms implements Block-Coordinate and Incremental Aggregated Optimization Algorithms for minimizations of the form</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="minimize    1/N sum_{i=1}^N f_i(x) + g(x)
"><pre lang="math"><code>minimize    1/N sum_{i=1}^N f_i(x) + g(x)
</code></pre></div>
<p>or</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="minimize    1/N sum_{i=1}^N f_i(x_i) + g(sum_{i=1}^N x_i)
"><pre lang="math"><code>minimize    1/N sum_{i=1}^N f_i(x_i) + g(sum_{i=1}^N x_i)
</code></pre></div>
<p>where f_i are smooth, and g is (possibly) nonsmooth with easy to compute proximal mapping. These functions can be defined using the <a href="https://github.com/kul-forbes/ProximalOperators.jl">ProximalOperators.jl</a> package.</p>
<h3><a id="user-content-quick-guide" class="anchor" aria-hidden="true" href="#quick-guide"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick guide</h3>
<p>You can add CIAOAlgorithms by pressing <code>]</code> to enter the package manager, then</p>
<div class="snippet-clipboard-content position-relative" data-snippet-clipboard-copy-content="pkg&gt; add CIAOAlgorithms
"><pre><code>pkg&gt; add CIAOAlgorithms
</code></pre></div>
<p>Simple Lasso and logisitc regression test examples can be found <a href="test">here</a>.</p>
<h3><a id="user-content-implemented-algorithms" class="anchor" aria-hidden="true" href="#implemented-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Implemented Algorithms</h3>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Function</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finito/MISO/DIAG</td>
<td><a href="src/algorithms/Finito/Finito.jl"><code>Finito</code></a></td>
<td><a href="https://arxiv.org/pdf/1407.2710.pdf" rel="nofollow">[1]</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/140957639" rel="nofollow">[4]</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/16M1101702" rel="nofollow">[8]</a>, <a href="https://arxiv.org/pdf/1906.10053.pdf" rel="nofollow">[9]</a></td>
</tr>
<tr>
<td>ProShI</td>
<td><a href="src/algorithms/ProShI/ProShI.jl"><code>Proshi</code></a></td>
<td><a href="https://arxiv.org/pdf/1906.10053.pdf" rel="nofollow">[9]</a></td>
</tr>
<tr>
<td>SAGA</td>
<td><a href="src/algorithms/SAGA_SAG/SAGA.jl"><code>SAGA</code></a></td>
<td><a href="https://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf" rel="nofollow">[3]</a>, <a href="https://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf" rel="nofollow">[6]</a></td>
</tr>
<tr>
<td>SAG</td>
<td><a href="src/algorithms/SAGA_SAG/SAGA.jl"><code>SAG</code></a></td>
<td><a href="https://link.springer.com/article/10.1007/s10107-016-1030-6" rel="nofollow">[7]</a></td>
</tr>
<tr>
<td>SVRG/SVRG++</td>
<td><a href="src/algorithms/SVRG/SVRG.jl"><code>SVRG</code></a></td>
<td><a href="https://epubs.siam.org/doi/pdf/10.1137/140961791" rel="nofollow">[2]</a>, <a href="https://arxiv.org/pdf/1506.01972.pdf" rel="nofollow">[4]</a>, <a href="https://papers.nips.cc/paper/6116-proximal-stochastic-methods-for-nonsmooth-nonconvex-finite-sum-optimization.pdf" rel="nofollow">[5]</a></td>
</tr>
</tbody>
</table>
<h3><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h3>
<p><a href="https://arxiv.org/pdf/1407.2710.pdf" rel="nofollow">[1]</a> Defazio, Domke, <em>Finito: A faster, permutable incremental gradient method for big data problems</em>, In International Conference on Machine Learning pp. 1125-1133 (2014).</p>
<p><a href="https://epubs.siam.org/doi/pdf/10.1137/140961791" rel="nofollow">[2]</a> Xiao, Zhang, <em>A proximal stochastic gradient method  with progressive variance reduction</em>, SIAM Journal on Optimization 24(4):2057–2075 (2014).</p>
<p><a href="https://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf" rel="nofollow">[3]</a> Defazio, Bach, Lacoste-Julien, <em>SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</em>, In: Advances in neural information processing systems, pp. 1646–1654 (2014).</p>
<p><a href="https://epubs.siam.org/doi/pdf/10.1137/140957639" rel="nofollow">[4]</a> Mairal, <em>Incremental majorization-minimization optimization with application to large-scale machine learning</em>
SIAM Journal on Optimization 25(2), 829–855 (2015).</p>
<p><a href="https://arxiv.org/pdf/1506.01972.pdf" rel="nofollow">[5]</a> Allen-Zhu, Yuan, <em>Improved SVRG for non-strongly-convex or sum-of-non-convex objectives</em> In Proceedings of the 33rd International Conference on Machine Learning 1080–1089 (2016).</p>
<p><a href="https://papers.nips.cc/paper/6116-proximal-stochastic-methods-for-nonsmooth-nonconvex-finite-sum-optimization.pdf" rel="nofollow">[6]</a> Reddi, Sra, Poczos, and Smola, <em>Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization</em> In Advances in Neural Information Processing Systems, pp. 1145–1153 (2016).</p>
<p><a href="https://link.springer.com/article/10.1007/s10107-016-1030-6" rel="nofollow">[7]</a> Schmidt, Le Roux, Bach, <em>Minimizing finite sums with the stochastic average gradient</em> Mathematical Programming, 162(1-2), 83-112 (2017).</p>
<p><a href="https://epubs.siam.org/doi/pdf/10.1137/16M1101702" rel="nofollow">[8]</a> Mokhtari, Gürbüzbalaban, Ribeiro, <em>Surpassing gradient descent provably: A cyclic incremental method with linear convergence rate</em> SIAM Journal on Optimization 28(2), 1420–1447 (2018).</p>
<p><a href="https://arxiv.org/pdf/1906.10053.pdf" rel="nofollow">[9]</a> Latafat, Themelis, Patrinos, <em>Block-coordinate and incremental aggregated proximal gradient methods for nonsmooth nonconvex problems</em> arXiv:1906.10053 (2019).</p>
</article></div>