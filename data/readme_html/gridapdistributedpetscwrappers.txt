<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text">
<p dir="auto"><a href="https://travis-ci.com/gridap/GridapDistributedPETScWrappers.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/6b4a5ff7ecc9ab59103fae99d1dee326a231d03f2694c09d7e9ccbc99c1a4680/68747470733a2f2f7472617669732d63692e636f6d2f6772696461702f4772696461704469737472696275746564504554536357726170706572732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/gridap/GridapDistributedPETScWrappers.svg?branch=master" style="max-width: 100%;"></a>
<a href="https://codecov.io/gh/gridap/GridapDistributedPETScWrappers.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/8963125073ecafd9594173a271903c681943d7b7a1d6007c1df336225e41039e/68747470733a2f2f636f6465636f762e696f2f67682f6772696461702f4772696461704469737472696275746564504554536357726170706572732e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Codecov" data-canonical-src="https://codecov.io/gh/gridap/GridapDistributedPETScWrappers.jl/branch/master/graph/badge.svg" style="max-width: 100%;"></a></p>
<h1 dir="auto"><a id="user-content-gridapdistributedpetscwrappers" class="anchor" aria-hidden="true" href="#gridapdistributedpetscwrappers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>GridapDistributedPETScWrappers</h1>
<p dir="auto"><a href="https://github.com/gridap/GridapDistributed.jl">GridapDistributed.jl</a> wrappers for the <a href="https://www.mcs.anl.gov/petsc/" rel="nofollow">PETSc</a> library. <g-emoji class="g-emoji" alias="construction" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a7.png">ðŸš§</g-emoji> work in progress <g-emoji class="g-emoji" alias="construction" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a7.png">ðŸš§</g-emoji></p>
<h2 dir="auto"><a id="user-content-purpose" class="anchor" aria-hidden="true" href="#purpose"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Purpose</h2>
<p dir="auto">This package is currently <strong>experimental, under development</strong>. In any case, we warn that the package is not though to be a fully-functional PETSc wrapper written in Julia (for that purpose we refer to the <a href="https://github.com/JuliaParallel/PETSc.jl">PETSc.jl</a> package, which is recently being revamped). Instead, it provides sufficient (although not necessarily necessary) functionality from PETSc as per-required by <code>GridapDistributed.jl</code>. Given that the latter is still under development, <code>GridapDistributedPETScWrappers.jl</code> may also vary accordingly to the changing requirements of <code>GridapDistributed.jl</code>. Once we have a more clear/definite understanding of what <code>GridapDistributed.jl</code> requires from <code>PETSc</code>, we may eventually significantly cut down the current code of  <code>GridapDistributedPETScWrappers.jl</code>.</p>
<p dir="auto">The development of this package started originally from JuliaParallel's org <code>PETSc.jl</code> (in particular, from commit <a href="https://github.com/JuliaParallel/PETSc.jl/commit/3d8c46a127821aa1ff20d5892f50ec75be11c77f">https://github.com/JuliaParallel/PETSc.jl/commit/3d8c46a127821aa1ff20d5892f50ec75be11c77f</a>, <code>uptodate</code> branch). More information can be found in the following issue: <a class="issue-link js-issue-link" data-error-text="Failed to load title" data-id="658803267" data-permission-text="Title is private" data-url="https://github.com/gridap/GridapDistributed.jl/issues/22" data-hovercard-type="issue" data-hovercard-url="/gridap/GridapDistributed.jl/issues/22/hovercard" href="https://github.com/gridap/GridapDistributed.jl/issues/22">gridap/GridapDistributed.jl#22</a>. Not all code which is currently in <code>GridapDistributedPETScWrappers.jl</code> is functional. In principle, one can safely use all the the machinery being tested in <code>test/runtests.jl</code>, although other parts may also be functional as well.</p>
<h2 dir="auto"><a id="user-content-requirements-limitations-warnings" class="anchor" aria-hidden="true" href="#requirements-limitations-warnings"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Requirements, limitations, warnings</h2>
<ul dir="auto">
<li>
<p dir="auto">PETSc version &gt;= v3.10.3 REQUIRED.</p>
<blockquote>
<p dir="auto">From this commit of PETSc (petsc/petsc@2ebc710#diff-d46e9870b0b2f6361c8563135bfdaa89eab41a56290d02afb6ca42f5463ea629), the value of PETSC_INT changed from 0 to 16. This has implications on the PETSc julia wrappers, that have to define the associated constant accordingly. Accordingly to PETSc release dates, this change is reflected from v3.10.3 on.</p>
</blockquote>
</li>
<li>
<p dir="auto">We currently only support PETSc compiled with <code>PetscScalar==double</code> and <code>PetscReal==double</code> (i.e., Julia's <code>Float64</code>). This is referred to as <code>RealDouble</code> within <code>GridapDistributedPETScWrappers.jl</code>. The version of <code>PETSc.jl</code> from which we started also supported <code>RealSingle</code> and <code>ComplexDouble</code>, although no efforts have been spent into supporting these back. On the other hand, either 32-bit or 64-bit integer compilations of PETSc are allowed. The package automatically detects during cache module pre-compilation which is the size of <code>PetscInt</code>.</p>
</li>
<li>
<p dir="auto">All <code>finalizer</code>s of Julia types wrapping PETSc ones are deactivated. Thus, the latter ones are not destroyed when the former ones are GC'ed. The user may explicitly destroy the latter ones calling <code>PetscDestroy</code>. The user may activate <code>finalizer</code>s setting the package-wide constant <code>deactivate_finalizers</code> to <code>false</code>, although this is not recommended because of two reasons, which, to be honest, I do not fully understand:</p>
<ol dir="auto">
<li>Tests fail when <code>finalizer</code>s are activated, because these cause an <code>MPI</code> call to be triggered after <code>MPI_Finalize</code> (could not understand why this is the case)</li>
<li>Quoting from a <code>PETSc.jl</code> dev doc file: "We can't attach finalizers for distributed objects (i.e. <code>VecMPI</code>), as <code>destroy</code> needs to be called collectively on all MPI ranks." I guess that the GC may not ensure the same order of execution for all MPI tasks, causing deadlocks and other sort of issues.</li>
</ol>
</li>
</ul>
<h2 dir="auto"><a id="user-content-installation-usage-instructions" class="anchor" aria-hidden="true" href="#installation-usage-instructions"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation, usage instructions</h2>
<p dir="auto"><code>GridapDistributedPETScWrappers.jl</code> uses, among others, the <a href="https://github.com/JuliaParallel/MPI.jl"><code>MPI.jl</code></a> Julia package; see configuration documentation for this package available <a href="https://juliaparallel.github.io/MPI.jl/stable/configuration/" rel="nofollow">here</a>.</p>
<p dir="auto">There are essentially two possible ways to build <code>GridapDistributedPETScWrappers.jl</code> (i.e., <code>pkg&gt; build GridapDistributedPETScWrappers</code>):</p>
<ol dir="auto">
<li>
<p dir="auto">One wants to use MPI+PETSc libraries pre-compiled in Julia registry packages (this is the typical case when one wants to use this package on your local computer). In this case one has to ensure that both <code>JULIA_MPI_BINARY</code> and <code>JULIA_PETSC_RealDouble_BINARY</code> are either unset or set to the empty string.</p>
</li>
<li>
<p dir="auto">One wants to use a PETSc library already installed on the system (typically this is the case one is on a HPC cluster). In this case one has to ensure that <code>MPI.jl</code> is built such that it uses the same MPI library this installation of the PETSc library is compiled/linked with (see <code>MPI.jl</code> instructions referred above). The following environment variables are used to configure how <code>GridapDistributedPETScWrappers.jl</code> is built:</p>
<ul dir="auto">
<li><code>JULIA_PETSC_RealDouble_BINARY</code> has to be set to <code>"system"</code>.</li>
<li><code>JULIA_PETSC_RealDouble_DIR</code> has to be set to PETSc's DIR.</li>
<li><code>JULIA_PETSC_RealDouble_ARCH</code> has to be set to PETSc's ARCH.</li>
<li><code>JULIA_PETSC_RealDouble_LIBNAME</code> may optionally be set to the name of PETSc's dynamic library file of the system installation of PETSc (<code>libpetsc</code> is used otherwise by default).</li>
</ul>
</li>
</ol>

</article></div>