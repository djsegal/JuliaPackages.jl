<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-accuratearithmeticjl" class="anchor" aria-hidden="true" href="#accuratearithmeticjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>AccurateArithmetic.jl</h1>
<h2><a id="user-content-floating-point-math-with-error-free-faithful-and-compensated-transforms" class="anchor" aria-hidden="true" href="#floating-point-math-with-error-free-faithful-and-compensated-transforms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Floating point math with error-free, faithful, and compensated transforms.</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/646b448bd38200d1c7f3cd622b3cb9aa79d5abe0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6966656379636c652d6d61747572696e672d626c75652e737667"><img src="https://camo.githubusercontent.com/646b448bd38200d1c7f3cd622b3cb9aa79d5abe0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6966656379636c652d6d61747572696e672d626c75652e737667" alt="Lifecycle" data-canonical-src="https://img.shields.io/badge/lifecycle-maturing-blue.svg" style="max-width:100%;"></a>
<a href="https://travis-ci.org/JuliaMath/AccurateArithmetic.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/99ff5970e6a8d78577eff3898fe79a2116d4e82f/68747470733a2f2f7472617669732d63692e6f72672f4a756c69614d6174682f416363757261746541726974686d657469632e6a6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/JuliaMath/AccurateArithmetic.jl.svg?branch=master" style="max-width:100%;"></a>
<a href="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/report.html" rel="nofollow"><img src="https://camo.githubusercontent.com/482f7293eab7cf25c9dc54308d2f68d2e3342df8/68747470733a2f2f6a756c696163692e6769746875622e696f2f4e616e6f736f6c646965725265706f7274732f706b676576616c5f6261646765732f412f416363757261746541726974686d657469632e737667" alt="PkgEval" data-canonical-src="https://juliaci.github.io/NanosoldierReports/pkgeval_badges/A/AccurateArithmetic.svg" style="max-width:100%;"></a></p>
<h3><a id="user-content-error-free-and-faithful-transforms" class="anchor" aria-hidden="true" href="#error-free-and-faithful-transforms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Error-free and faithful transforms</h3>
<p><code>AccurateArithmetic.jl</code> provides a set of error-free transforms (EFTs), which
allow getting not only the rounded result of a floating-point computation, but
also the accompanying rounding error:</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> AccurateArithmetic

<span class="pl-c"><span class="pl-c">#</span> WARNING: a is not really 1/10, as this value is not representable as a Float64</span>
<span class="pl-c"><span class="pl-c">#</span> (and similarly for b)</span>
julia<span class="pl-k">&gt;</span> (a, b) <span class="pl-k">=</span> (<span class="pl-c1">0.1</span>, <span class="pl-c1">0.2</span>)

julia<span class="pl-k">&gt;</span> (s, e) <span class="pl-k">=</span> AccurateArithmetic<span class="pl-k">.</span><span class="pl-c1">two_sum</span>(a, b)
(<span class="pl-c1">0.30000000000000004</span>, <span class="pl-k">-</span><span class="pl-c1">2.7755575615628914e-17</span>)</pre></div>
<p>In the above example, <code>s</code> is the result of the floating-point addition
<code>0.1+0.2</code>, rounded to the nearest representable floating-point number, exactly
what you would get from a standard addition. <code>e</code> is the rounding error
associated to <code>s</code>. In other words, it is guaranteed that a + b = s + e, in a
strict mathematical sense (i.e. when the <code>+</code> operate on real numbers and are not
rounded).</p>
<p>Similar EFTs are provided for the binary subtraction (<code>two_diff</code>) and
multiplication (<code>two_prod</code>). Some operations of higher arity are also supported,
such as <code>three_sum</code>, <code>four_sum</code> or <code>three_prod</code>.</p>
<h3><a id="user-content-compensated-algorithms" class="anchor" aria-hidden="true" href="#compensated-algorithms"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Compensated algorithms</h3>
<p>EFTs can be leveraged to build "compensated algorithms", which compute a result
as if the basic algorithm had been run using a higher precision.</p>
<div class="highlight highlight-source-julia"><pre><span class="pl-c"><span class="pl-c">#</span> By construction, this vector sums to 1</span>
julia<span class="pl-k">&gt;</span> x <span class="pl-k">=</span> <span class="pl-c1">5000</span> <span class="pl-k">|&gt;</span> N<span class="pl-k">-&gt;</span><span class="pl-c1">randn</span>(N) <span class="pl-k">.*</span> <span class="pl-c1">exp</span>.(<span class="pl-c1">10</span> <span class="pl-k">.*</span> <span class="pl-c1">randn</span>(N)) <span class="pl-k">|&gt;</span> x<span class="pl-k">-&gt;</span>[x;<span class="pl-k">-</span>x;<span class="pl-c1">1.0</span>] <span class="pl-k">|&gt;</span> x<span class="pl-k">-&gt;</span>x[<span class="pl-c1">sortperm</span>(<span class="pl-c1">rand</span>(<span class="pl-c1">length</span>(x)))];
julia<span class="pl-k">&gt;</span> <span class="pl-c1">sum</span>(<span class="pl-c1">big</span>.(x))
<span class="pl-c1">1.0</span>

<span class="pl-c"><span class="pl-c">#</span> But the standard summation algorithms computes this sum very inaccurately</span>
<span class="pl-c"><span class="pl-c">#</span> (not even the sign is correct)</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">sum</span>(x)
<span class="pl-k">-</span><span class="pl-c1">136.0</span>


<span class="pl-c"><span class="pl-c">#</span> Compensated summation algorithms should compute this more accurately</span>
julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> AccurateArithmetic

<span class="pl-c"><span class="pl-c">#</span> Algorithm by Ogita, Rump and Oishi</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">sum_oro</span>(x)
<span class="pl-c1">0.9999999999999716</span>

<span class="pl-c"><span class="pl-c">#</span> Algorithm by Kahan, Babuska and Neumaier</span>
julia<span class="pl-k">&gt;</span> <span class="pl-c1">sum_kbn</span>(x)
<span class="pl-c1">0.9999999999999716</span></pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="test/figs/qual.svg"><img src="test/figs/qual.svg" alt="" style="max-width:100%;"></a></p>
<p>In the graph above, we see the relative error vary as a function of the
condition number, in a log-log scale. Errors lower than ϵ are arbitrarily set to
ϵ; conversely, when the relative error is more than 100% (i.e no digit is
correctly computed anymore), the error is capped there in order to avoid
affecting the scale of the graph too much. What we see is that the pairwise
summation algorithm (as implemented in Base.sum) starts losing accuracy as soon
as the condition number increases, computing only noise when the condition
number exceeds 1/ϵ≃10¹⁶. In contrast, both compensated algorithms
(Kahan-Babuska-Neumaier and Ogita-Rump-Oishi) still accurately compute the
result at this point, and start losing accuracy there, computing meaningless
results when the condition nuber reaches 1/ϵ²≃10³². In effect these (simply)
compensated algorithms produce the same results as if a naive summation had been
performed with twice the working precision (128 bits in this case), and then
rounded to 64-bit floats.</p>
<br>
<p>Performancewise, compensated algorithms perform a lot better than alternatives
such as arbitrary precision (<code>BigFloat</code>) or rational arithmetic (<code>Rational</code>) :</p>
<div class="highlight highlight-source-julia"><pre>julia<span class="pl-k">&gt;</span> <span class="pl-k">using</span> BenchmarkTools

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">sum</span>(<span class="pl-k">$</span>x)
  <span class="pl-c1">1.305</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-k">-</span><span class="pl-c1">136.0</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">sum_oro</span>(<span class="pl-k">$</span>x)
  <span class="pl-c1">3.421</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">0.9999999999999716</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">sum_kbn</span>(<span class="pl-k">$</span>x)
  <span class="pl-c1">3.792</span> μs (<span class="pl-c1">0</span> allocations<span class="pl-k">:</span> <span class="pl-c1">0</span> bytes)
<span class="pl-c1">0.9999999999999716</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">sum</span>(<span class="pl-c1">big</span>.(<span class="pl-k">$</span>x))
  <span class="pl-c1">874.203</span> μs (<span class="pl-c1">20006</span> allocations<span class="pl-k">:</span> <span class="pl-c1">1.14</span> MiB)
<span class="pl-c1">1.0</span>

julia<span class="pl-k">&gt;</span> <span class="pl-c1">@btime</span> <span class="pl-c1">sum</span>(<span class="pl-c1">Rational</span><span class="pl-c1">{BigInt}</span>.(x))
  <span class="pl-c1">22.702</span> ms (<span class="pl-c1">582591</span> allocations<span class="pl-k">:</span> <span class="pl-c1">10.87</span> MiB)
<span class="pl-c1">1</span><span class="pl-k">//</span><span class="pl-c1">1</span></pre></div>
<p>However, compensated algorithms perform a larger number of elementary operations
than their naive floating-point counterparts. As such, they usually perform
worse. However, leveraging the power of modern architectures via vectorization,
the slow down can be kept to a small value.</p>
<p><a target="_blank" rel="noopener noreferrer" href="test/figs/perf.svg"><img src="test/figs/perf.svg" alt="" style="max-width:100%;"></a></p>
<p>In the graph above, the time spent in the summation (renormalized per element)
is plotted against the vector size (the units in the y-axis label should be
“ns/elem”). What we see with the standard summation is that, once vectors start
having significant sizes (say, more than 1000 elements), the implementation is
memory bound (as expected of a typical BLAS1 operation). Which is why we see
significant decreases in the performance when the vector can’t fit into the L2
cache (around 30k elements, or 256kB on my machine) or the L3 cache (around 400k
elements, or 3MB on y machine).</p>
<p>The Ogita-Rump-Oishi algorithm, when implemented with a suitable unrolling level
(ushift=2, i.e 2²=4 unrolled iterations), is CPU-bound when vectors fit inside
the cache. However, when vectors are to large to fit into the L3 cache, the
implementation becomes memory-bound again (on my system), which means we get the
same performance as the standard summation.</p>
<p>In other words, the improved accuracy is free for sufficiently large
vectors. For smaller vectors, the accuracy comes with a slow-down that can reach
values slightly above 3 for vectors which fit in the L2 cache.</p>
<h3><a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tests</h3>
<p>The graphs above can be reproduced using the <code>test/perftests.jl</code> script in this
repository. Before running them, be aware that it takes around one hour to
generate the performance graph, during which the benchmark machine should be as
low-loaded as possible in order to avoid perturbing performance measurements.</p>
<h3><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h3>
<ul>
<li>
<p>C. Elrod and F. Févotte, "Accurate and Efficient Sums and Dot Products in
Julia". <a href="https://hal.archives-ouvertes.fr/hal-02265534" rel="nofollow">preprint</a></p>
</li>
<li>
<p>T. Ogita, S. Rump and S. Oishi, "Accurate sum and dot product", SIAM Journal
on Scientific Computing, 6(26), 2005. DOI: 10.1137/030601818</p>
</li>
</ul>
</article></div>