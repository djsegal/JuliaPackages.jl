<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1 dir="auto"><a id="user-content-adagram" class="anchor" aria-hidden="true" href="#adagram"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>AdaGram</h1>
<p dir="auto">Adaptive Skip-gram (AdaGram) model is a nonparametric extension of famous Skip-gram model implemented in word2vec software which  is able to learn multiple representations per word capturing different word meanings. This projects implements AdaGram in Julia language.</p>
<h2 dir="auto"><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">AdaGram is not in the julia package repository yet, so it should be installed in the following way:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="using Pkg
Pkg.add(PackageSpec(url=&quot;https://github.com/sbos/AdaGram.jl.git&quot;))"><pre class="notranslate"><code>using Pkg
Pkg.add(PackageSpec(url="https://github.com/sbos/AdaGram.jl.git"))
</code></pre></div>
<h2 dir="auto"><a id="user-content-training-a-model" class="anchor" aria-hidden="true" href="#training-a-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training a model</h2>
<p dir="auto">The most straightforward way to train a model is to use <code>train.sh</code> script. If you run it with no parameters passed or with <code>--help</code> option, it will print usage information:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="usage: train.jl [--window WINDOW] [--workers WORKERS]
                [--min-freq MIN-FREQ] [--remove-top-k REMOVE-TOP-K]
                [--dim DIM] [--prototypes PROTOTYPES] [--alpha ALPHA]
                [--d D] [--subsample SUBSAMPLE] [--context-cut]
                [--epochs EPOCHS] [--init-count INIT-COUNT]
                [--stopwords STOPWORDS]
                [--sense-treshold SENSE-TRESHOLD] [--regex REGEX] [-h]
                train dict output"><pre class="notranslate"><code>usage: train.jl [--window WINDOW] [--workers WORKERS]
                [--min-freq MIN-FREQ] [--remove-top-k REMOVE-TOP-K]
                [--dim DIM] [--prototypes PROTOTYPES] [--alpha ALPHA]
                [--d D] [--subsample SUBSAMPLE] [--context-cut]
                [--epochs EPOCHS] [--init-count INIT-COUNT]
                [--stopwords STOPWORDS]
                [--sense-treshold SENSE-TRESHOLD] [--regex REGEX] [-h]
                train dict output
</code></pre></div>
<p dir="auto">Here is the description of all parameters:</p>
<ul dir="auto">
<li><code>WINDOW</code> is a half-context size. Useful values are 3-10.</li>
<li><code>WORKERS</code> is how much parallel processes will be used for training.</li>
<li><code>MIN-FREQ</code> specifies the minimum word frequency below which a word will be ignored. Useful values are 5-50 depending on the corpora.</li>
<li><code>REMOVE-TOP-K</code> allows to ignore K most frequent words as well.</li>
<li><code>DIM</code> is the dimensionality of learned representations</li>
<li><code>PROTOTYPES</code> sets the maximum number of learned prototypes. This is the truncating level used in truncated stick-breaking, so the actual amount of memory used depends on this number linearly.</li>
<li><code>ALPHA</code> is the parameter of underlying Dirichlet process. Larger values of <code>ALPHA</code> lead to more meanings discovered. Useful values are 0.05-0.2.</li>
<li><code>D</code> is used together with <code>ALPHA</code> in Pitman-Yor process and <code>D</code>=0 turns it into Dirichlet process. We couldn’t get reasonable results with PY, but left the option to change <code>D</code>.</li>
<li><code>SUBSAMPLE</code> is a threshold for subsampling frequent words, similarly to how this is done in word2vec.</li>
<li><code>—context-cut</code> option allows to randomly decrease <code>WINDOW</code> during the training, which increases training speed with almost no effects on model’s performance</li>
<li><code>EPOCHS</code> specifies the number of passes over training text, usually one epoch is enough, larger number of epochs is usually required on small corpora.</li>
<li><code>INIT-COUNT</code> is used for initialization of variational stick-breaking distribution. All prototypes are assigned with zero occurrences except first one which is assigned with <code>INIT-COUNT</code>. Zero value means that first prototype gets all occurrences.</li>
<li><code>STOPWORDS</code> is a path to newline-separated file with list of words that must be ignored during the training</li>
<li><code>SENSE-THRESHOLD</code> allows to sparse gradients and speed-up training. If the posterior probability of a prototype is blow that threshold then it won’t contribute to parameters’ gradients.</li>
<li><code>REGEX</code> will be used to filter out words not matching with from the <code>DICTIONARY</code> provided</li>
<li><code>train</code> — path to training text (see Format section below)</li>
<li><code>dict</code> — path to dictionary file (see Format section below)</li>
<li><code>output</code> — path for saving trained model.</li>
</ul>
<h2 dir="auto"><a id="user-content-input-format" class="anchor" aria-hidden="true" href="#input-format"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Input format</h2>
<p dir="auto">Training text should be formatted as for word2vec. Words are case-sensitive and are assumed to be separated by space characters. All punctuation should be removed unless specially intented to be preserved. You may use <code>utils/tokenize.sh INPUT_FILE OUTPUT_FILE</code> for simple tokenization with UNIX utils.</p>
<p dir="auto">In order to train a model you should also provide a dictionary file with word frequency statistics in the following format:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="word1   34
word2   456
...
wordN   83"><pre class="notranslate"><code>word1   34
word2   456
...
wordN   83
</code></pre></div>
<p dir="auto">AdaGram will assume that provided word frequencies are actually obtained from training file. You may build a dictionary file using <code>utils/dictionary.sh INPUT_FILE DICT_FILE</code>.</p>
<h2 dir="auto"><a id="user-content-playing-with-a-model" class="anchor" aria-hidden="true" href="#playing-with-a-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Playing with a model</h2>
<p dir="auto">After model is trained, you may use learned word vectors in the same way as ones learned by word2vec. However, since AdaGram learns several vectors for each word, you may need to <em>disambiguate</em> a word using its context first, in order to determine which vector should be used.</p>
<p dir="auto">First, load the model and the dictionary:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; using AdaGram
julia&gt; vm, dict = load_model(&quot;PATH_TO_THE_MODEL&quot;);"><pre class="notranslate"><code>julia&gt; using AdaGram
julia&gt; vm, dict = load_model("PATH_TO_THE_MODEL");
</code></pre></div>
<p dir="auto">To examine how many prototypes were learned for a word, use <code>expected_pi</code> function:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; expected_pi(vm, dict.word2id[&quot;apple&quot;])
30-element Array{Float64,1}:
 0.341832   
 0.658164   
 3.13843e-6
 2.84892e-7
 2.58649e-8
 2.34823e-9
 2.13192e-10
 1.93554e-11
 1.75725e-12
 ⋮          "><pre class="notranslate"><code>julia&gt; expected_pi(vm, dict.word2id["apple"])
30-element Array{Float64,1}:
 0.341832   
 0.658164   
 3.13843e-6
 2.84892e-7
 2.58649e-8
 2.34823e-9
 2.13192e-10
 1.93554e-11
 1.75725e-12
 ⋮          
</code></pre></div>
<p dir="auto">This function returns a <code>--prototypes</code>-sized array with prior probability of each prototype. As one may see, in this example only first two prototypes have probabilities significantly larger than zero, and thus we may conclude that only two meanings of word "apple" were discovered.
We may examine each prototype by looking at its 10 nearest neighbours:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; nearest_neighbors(vm, dict, &quot;apple&quot;, 1, 10)
10-element Array{(Any,Any,Any),1}:
 (&quot;almond&quot;,1,0.70396507f0)    
 (&quot;cherry&quot;,2,0.69193166f0)    
 (&quot;plum&quot;,1,0.690269f0)        
 (&quot;apricot&quot;,1,0.6882005f0)    
 (&quot;orange&quot;,4,0.6739181f0)     
 (&quot;pecan&quot;,1,0.6662803f0)      
 (&quot;pomegranate&quot;,1,0.6580653f0)
 (&quot;blueberry&quot;,1,0.6509351f0)  
 (&quot;pear&quot;,1,0.6484747f0)       
 (&quot;peach&quot;,1,0.6313036f0)   
julia&gt; nearest_neighbors(vm, dict, &quot;apple&quot;, 2, 10)
10-element Array{(Any,Any,Any),1}:
 (&quot;macintosh&quot;,1,0.79053026f0)     
 (&quot;iifx&quot;,1,0.71349466f0)          
 (&quot;iigs&quot;,1,0.7030192f0)           
 (&quot;computers&quot;,1,0.6952761f0)      
 (&quot;kaypro&quot;,1,0.6938647f0)         
 (&quot;ipad&quot;,1,0.6914306f0)           
 (&quot;pc&quot;,4,0.6801078f0)             
 (&quot;ibm&quot;,1,0.66797054f0)           
 (&quot;powerpc-based&quot;,1,0.66319686f0)
 (&quot;ibm-compatible&quot;,1,0.66120595f0)"><pre class="notranslate"><code>julia&gt; nearest_neighbors(vm, dict, "apple", 1, 10)
10-element Array{(Any,Any,Any),1}:
 ("almond",1,0.70396507f0)    
 ("cherry",2,0.69193166f0)    
 ("plum",1,0.690269f0)        
 ("apricot",1,0.6882005f0)    
 ("orange",4,0.6739181f0)     
 ("pecan",1,0.6662803f0)      
 ("pomegranate",1,0.6580653f0)
 ("blueberry",1,0.6509351f0)  
 ("pear",1,0.6484747f0)       
 ("peach",1,0.6313036f0)   
julia&gt; nearest_neighbors(vm, dict, "apple", 2, 10)
10-element Array{(Any,Any,Any),1}:
 ("macintosh",1,0.79053026f0)     
 ("iifx",1,0.71349466f0)          
 ("iigs",1,0.7030192f0)           
 ("computers",1,0.6952761f0)      
 ("kaypro",1,0.6938647f0)         
 ("ipad",1,0.6914306f0)           
 ("pc",4,0.6801078f0)             
 ("ibm",1,0.66797054f0)           
 ("powerpc-based",1,0.66319686f0)
 ("ibm-compatible",1,0.66120595f0)
</code></pre></div>
<p dir="auto">Now if we provide a context for word "apple" we may obtain posterior probability of each prototype:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="julia&gt; disambiguate(vm, dict, &quot;apple&quot;, split(&quot;new iphone was announced today&quot;))
30-element Array{Float64,1}:
 1.27888e-5
 0.999987  
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 ⋮     
julia&gt; disambiguate(vm, dict, &quot;apple&quot;, split(&quot;fresh tasty breakfast&quot;))
30-element Array{Float64,1}:
 0.999977  
 2.30527e-5
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 ⋮         "><pre class="notranslate"><code>julia&gt; disambiguate(vm, dict, "apple", split("new iphone was announced today"))
30-element Array{Float64,1}:
 1.27888e-5
 0.999987  
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 ⋮     
julia&gt; disambiguate(vm, dict, "apple", split("fresh tasty breakfast"))
30-element Array{Float64,1}:
 0.999977  
 2.30527e-5
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 0.0       
 ⋮         
</code></pre></div>
<p dir="auto">As one may see, model correctly estimated probabilities of each sense with quite large confidence. Vector corresponding to second prototype of word "apple" can be obtained from <code>vm.In[:, 2, dict.word2id["apple"]]</code> and then used as context-aware features of word "apple".</p>
<p dir="auto">Plase refer to <a href="https://github.com/sbos/AdaGram.jl/wiki/API">API documentation</a> for more detailed usage info.</p>
<h2 dir="auto"><a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Future work</h2>
<ul dir="auto">
<li>Full API documentation</li>
<li>C and python bindings</li>
<li>Disambiguation into user-provided sense inventory</li>
</ul>
<h2 dir="auto"><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<ol dir="auto">
<li>Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin, Dmitry Vetrov. Breaking Sticks and Ambiguities with Adaptive Skip-gram.  <a href="http://arxiv.org/abs/1502.07257" rel="nofollow">ArXiv preprint</a>, 2015</li>
<li>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.</li>
</ol>
</article></div>