<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-commonrlinterface" class="anchor" aria-hidden="true" href="#commonrlinterface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CommonRLInterface</h1>
<p><a href="https://JuliaReinforcementLearning.github.io/CommonRLInterface.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>
<a href="https://JuliaReinforcementLearning.github.io/CommonRLInterface.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="Dev" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a>
<a href="https://github.com/JuliaReinforcementLearning/CommonRLInterface.jl/actions"><img src="https://github.com/JuliaReinforcementLearning/CommonRLInterface.jl/workflows/CI/badge.svg" alt="Build status" style="max-width:100%;"></a>
<a href="https://codecov.io/gh/JuliaReinforcementLearning/CommonRLInterface.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/dddfc1ac04dd264a7c1c67bbd5afe5bbee6d8ef1e9f35669c8058aa0db6f1a25/68747470733a2f2f636f6465636f762e696f2f67682f4a756c69615265696e666f7263656d656e744c6561726e696e672f436f6d6d6f6e524c496e746572666163652e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage" data-canonical-src="https://codecov.io/gh/JuliaReinforcementLearning/CommonRLInterface.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a></p>

<h2><a id="user-content-purpose" class="anchor" aria-hidden="true" href="#purpose"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Purpose</h2>
<p>The CommonRLInterface package provides an interface for defining and interacting with <a href="http://incompleteideas.net/book/first/ebook/node28.html" rel="nofollow">Reinforcement Learning Environments</a>.</p>
<p>An important goal is to provide compatibility between different reinforcement learning (RL) environment interfaces - for example, an algorithm that uses <code>YourRLInterface</code> should be able to use an environment from <code>MyRLInterface</code> <em>without</em> depending on <code>MyRLInterface</code> as long as they both support <code>CommonRLInterface</code>.</p>
<p>By design, this package is only concerned with environments and <em>not</em> with policies or agents.</p>
<h2><a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Documentation</h2>
<p>A few simple examples can be found in the examples directory. Detailed documentation can be found here: <a href="https://JuliaReinforcementLearning.github.io/CommonRLInterface.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="Stable" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a>. A brief overview is given below:</p>
<h3><a id="user-content-required-interface" class="anchor" aria-hidden="true" href="#required-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Required Interface</h3>
<p><code>AbstractEnv</code> is a base type for all environments.</p>
<p>The interface has five required functions for all <code>AbstractEnv</code>s:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="reset!(env)     # returns nothing
actions(env)    # returns the set of all possible actions for the environment
observe(env)    # returns an observation
act!(env, a)    # steps the environment forward and returns a reward
terminated(env) # returns true or false indicating whether the environment has finished
"><pre><span class="pl-c1">reset!</span>(env)     <span class="pl-c"><span class="pl-c">#</span> returns nothing</span>
<span class="pl-c1">actions</span>(env)    <span class="pl-c"><span class="pl-c">#</span> returns the set of all possible actions for the environment</span>
<span class="pl-c1">observe</span>(env)    <span class="pl-c"><span class="pl-c">#</span> returns an observation</span>
<span class="pl-c1">act!</span>(env, a)    <span class="pl-c"><span class="pl-c">#</span> steps the environment forward and returns a reward</span>
<span class="pl-c1">terminated</span>(env) <span class="pl-c"><span class="pl-c">#</span> returns true or false indicating whether the environment has finished</span></pre></div>
<h3><a id="user-content-optional-interface" class="anchor" aria-hidden="true" href="#optional-interface"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Optional Interface</h3>
<p>Additional behavior for an environment can be specified with the optional interface outlined in the documentation. The <code>provided</code> function can be used to check whether optional behavior is provided by the environment.</p>
<h3><a id="user-content-multiplayer-environments" class="anchor" aria-hidden="true" href="#multiplayer-environments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multiplayer Environments</h3>
<p>Optional functions allow implementation of both sequential and simultaneous games and multi-agent (PO)MDPs</p>
<h3><a id="user-content-wrappers" class="anchor" aria-hidden="true" href="#wrappers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Wrappers</h3>
<p>A wrapper system described in the documentation allows for easy modification of environments.</p>
<h3><a id="user-content-compatible-packages" class="anchor" aria-hidden="true" href="#compatible-packages"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Compatible Packages</h3>
<p>These packages are compatible with CommonRLInterface:</p>
<ul>
<li><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">ReinforcementLearning.jl</a></li>
<li><a href="https://github.com/JuliaPOMDP/POMDPs.jl">POMDPs.jl</a></li>
<li><a href="https://github.com/jonathan-laurent/AlphaZero.jl">AlphaZero.jl</a></li>
</ul>
</article></div>