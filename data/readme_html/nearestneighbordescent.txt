<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><h1><a id="user-content-nearestneighbordescentjl" class="anchor" aria-hidden="true" href="#nearestneighbordescentjl"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NearestNeighborDescent.jl</h1>
<table>
<thead>
<tr>
<th align="center"><strong>Documentation</strong></th>
<th align="center"><strong>Build Status</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://dillondaudert.github.io/NearestNeighborDescent.jl/stable" rel="nofollow"><img src="https://camo.githubusercontent.com/c97f0a5f2ae95755f64a27f1aa8d9a17462941fd3d6c907c7630abd5d3e60acf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-stable-blue.svg" style="max-width:100%;"></a> <a href="https://dillondaudert.github.io/NearestNeighborDescent.jl/dev" rel="nofollow"><img src="https://camo.githubusercontent.com/7fcec4b2d3ab291529fce8ef6a4fcd4129a0683b2f5d5fe2f5c648f02db8b616/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6465762d626c75652e737667" alt="" data-canonical-src="https://img.shields.io/badge/docs-dev-blue.svg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://travis-ci.com/dillondaudert/NearestNeighborDescent.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/d71c56457360fdbad25d196ce9e78ec3849c7759838790130ca53c148f8976aa/68747470733a2f2f7472617669732d63692e636f6d2f64696c6c6f6e646175646572742f4e6561726573744e65696768626f7244657363656e742e6a6c2e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://travis-ci.com/dillondaudert/NearestNeighborDescent.jl.svg?branch=master" style="max-width:100%;"></a> <a href="https://ci.appveyor.com/project/dillondaudert/nearestneighbordescent-jl" rel="nofollow"><img src="https://camo.githubusercontent.com/f70b7328c71fafe9e545f10499cd69b8107ccf22bbd1410e75e8336d96b8d740/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6c723439703976786b723861337576303f7376673d74727565" alt="" data-canonical-src="https://ci.appveyor.com/api/projects/status/lr49p9vxkr8a3uv0?svg=true" style="max-width:100%;"></a> <a href="https://codecov.io/gh/dillondaudert/NearestNeighborDescent.jl" rel="nofollow"><img src="https://camo.githubusercontent.com/b792d171f8686f6ef85b00caad232935beed9df1439af712cc9a68eab1295412/68747470733a2f2f636f6465636f762e696f2f67682f64696c6c6f6e646175646572742f4e6561726573744e65696768626f7244657363656e742e6a6c2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="" data-canonical-src="https://codecov.io/gh/dillondaudert/NearestNeighborDescent.jl/branch/master/graph/badge.svg" style="max-width:100%;"></a> <a href="https://coveralls.io/github/dillondaudert/NearestNeighborDescent.jl?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/86abe5ede1ca80154f71c6fa732850b6f80e9df21719cb5767093b4db3efe05a/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f64696c6c6f6e646175646572742f4e6561726573744e65696768626f7244657363656e742e6a6c2f62616467652e7376673f6272616e63683d6d6173746572" alt="" data-canonical-src="https://coveralls.io/repos/github/dillondaudert/NearestNeighborDescent.jl/badge.svg?branch=master" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<p>A Julia implementation of Nearest Neighbor Descent.</p>
<blockquote>
<p>Dong, Wei <em>et al.</em> Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures. <em>WWW</em> (2011).</p>
</blockquote>
<h2><a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Overview</h2>
<p>Nearest Neighbor Descent (NNDescent) is an approximate K-nearest neighbor graph construction algorithm that has
several useful properties:</p>
<ul>
<li><strong>general</strong>: works with arbitrary dissimilarity functions</li>
<li><strong>scalable</strong>: has an empirical complexity of O(n^1.14) pairwise comparisons for a dataset of size n</li>
<li><strong>space efficient</strong>: the only data structure required is an approximate KNN graph which is operated on in-place and is also the final output</li>
<li><strong>accurate</strong>: converges to above 90% recall while only comparing each data point to a small percentage of the whole dataset on average</li>
</ul>
<p>NNDescent is based on the heuristic argument that <em>a neighbor of a neighbor is also likely to be a neighbor</em>. That is,
given a list of approximate nearest neighbors to a point, we can improve that list by exploring the neighbors of each
point in the list. The algorithm is in essence the repeated application of this principle.</p>
<h2><a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="]add NearestNeighborDescent
"><pre>]add NearestNeighborDescent</pre></div>
<h2><a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic Usage</h2>
<p>Approximate kNN graph construction on a dataset:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="using NearestNeighborDescent
using Distances
data = [rand(20) for _ in 1:1000]
n_neighbors = 10
metric = Euclidean()
graph = nndescent(data, n_neighbors, metric)
"><pre><span class="pl-k">using</span> NearestNeighborDescent
<span class="pl-k">using</span> Distances
data <span class="pl-k">=</span> [<span class="pl-c1">rand</span>(<span class="pl-c1">20</span>) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">1000</span>]
n_neighbors <span class="pl-k">=</span> <span class="pl-c1">10</span>
metric <span class="pl-k">=</span> <span class="pl-c1">Euclidean</span>()
graph <span class="pl-k">=</span> <span class="pl-c1">nndescent</span>(data, n_neighbors, metric)</pre></div>
<p>The approximate KNNs of the original dataset can be retrieved from the resulting graph with</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="# return the approximate knns as KxN matrices of indexes and distances, where
# indices[j, i] and distances[j, i] are the index of and distance to node i's jth
# nearest neighbor, respectively.
indices, distances = knn_matrices(graph)
"><pre><span class="pl-c"><span class="pl-c">#</span> return the approximate knns as KxN matrices of indexes and distances, where</span>
<span class="pl-c"><span class="pl-c">#</span> indices[j, i] and distances[j, i] are the index of and distance to node i's jth</span>
<span class="pl-c"><span class="pl-c">#</span> nearest neighbor, respectively.</span>
indices, distances <span class="pl-k">=</span> <span class="pl-c1">knn_matrices</span>(graph)</pre></div>
<p>To find the approximate neighbors for new points with respect to an already constructed graph:</p>
<div class="highlight highlight-source-julia position-relative" data-snippet-clipboard-copy-content="queries = [rand(20) for _ in 1:20]
n_neighbors = 5
indices, distances = search(graph, queries, n_neighbors)
"><pre>queries <span class="pl-k">=</span> [<span class="pl-c1">rand</span>(<span class="pl-c1">20</span>) <span class="pl-k">for</span> _ <span class="pl-k">in</span> <span class="pl-c1">1</span><span class="pl-k">:</span><span class="pl-c1">20</span>]
n_neighbors <span class="pl-k">=</span> <span class="pl-c1">5</span>
indices, distances <span class="pl-k">=</span> <span class="pl-c1">search</span>(graph, queries, n_neighbors)</pre></div>
</article></div>