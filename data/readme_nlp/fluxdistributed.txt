data parallel training fluxjl modern scale deep learning models increased size parameters substaintially aims provide tools mechanisms scale training fluxjl models multiple gpus supports task based process based parallelism former suited singlenode parallelism latter multinode training multinode training handled design multiple locally installed gpu clusters using process based parallelism setting dataset datatoml repo example datasets package imagenetlocal data set default update path system dataset available future requirement lifted favour api configure path code single node parallelism basic usage start julia environment package activated currently start julia threads available finally set environment via instantiate example simple task based parallel training demo julia using fluxdistributed metalhead flux cuda optimisers datasets julia classes julia model resnet julia key blobtree datasets dataset imagenet datatree fluxdistributed trainsolutions datatree path loctrainsolutioncsv classes julia val blobtree datasets dataset imagenet datatree fluxdistributed trainsolutions datatree path locvalsolutioncsv classes julia opt optimisers momentum optimisers momentum float f f julia setup buffer preparetraining model key cuda devices opt optimizer batchsize gpu epochs julia loss flux losses logitcrossentropy logitcrossentropy generic function method julia fluxdistributed train loss setup buffer opt val val sched identity model describes model train key describes table data accessed purposes demo loctrainsolutioncsv published imagenet alongside images look trainsolutions allow access training validation test sets loss typical loss function train neural network current system set supervised learning support semi supervised learning coming soon information found documentation process based parallelism multinode parallelism basic usage bin directory driverjl initial configuration level function designed start training loop notice function takes arguments keyword arguments sensible defaults worth mentioning ones start julia command threads logical cores available default driverjl starts workers implying training happen gpus tweaked according gpus available demo written heterogenous file loading mind data trained local filsystem hosted remotely amazon aws s bucket rcs updatedgradschannel remotechannel process child processes send gradients forth synchronise perform data parallel training syncgrads starts task main process continually monitors gradients coming available processes manual synchrnisation sends updated gradients processes gradients ultimately trains sent optimise model logging support hook favourite mlops backend set logging various backends default trace training metrics printed console replace julia logging infrastructure set logger wandbjl wandbjl unofficial bindings platform using fluxdistributed flux metalhead using wandb logging lg wandblogger project ddp name resnet config dictsaveweights false cycles stepincrement withloggerlg fluxdistributedtrain