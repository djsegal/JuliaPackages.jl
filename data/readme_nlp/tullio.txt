tulliojl tullio flexible einsum macro understands array operations written index notation matrix multiplication permutations convolutions stencils scattergather broadcasting example tullio sum create tullio log sum write tullio sum add values tullio ind exp product macro writes ordinary nested loops einsumeinsum difference parse expressions infer ranges indices multithreading via threadsspawn recursive tiling arrays cooperates various packages provided loaded macro called loopvectorizationavx speed disable keyword avxfalse day match speed openblas matrix multiplication kernelabstractionskernel plus cudakernels gpu version disable cudafalse somewhat experimental fast macro tries provide gradient tracker via chainrules zygote yota etc disable gradfalse nograd default takes symbolic derivative hand expression reductions min max functions typed diffrules option graddual instead forwarddiff differentiate hand reductions allows complicated expressions entire hand summed range indices appearing left pipe operators indicate functions performed outside sum example tullio lse log exp mat veclogsumexpmat dims option tullio verbosetrue cause print index ranges symbolic derivatives notices unable packages mentioned verbose print useful academic cite doi notation index notation simple functions using pkg pkg add tullio using tullio test rand tullio sum test sum dims tullio sqrt loop sum broadcasting test sqrt mult tullio sum matrix multiplication test mult transpose rand int tullio im nested loops concatenation test permutedims cat dims im tullio max abs reduce using max test dropdims maximum abs dims dims dbl tullio write existing matrix dbl test complicated examples using tullio abs downsample range allowed terms tullio intersect shifts range calculated terms tullio tullio automatic shift using offsetarrays convolve filter offsetarray tullio implies index values tullio extrema implies wrapped padded tullio mod wraps mod axes tullio clamp instead repeats tullio pad fills zeros using fftw functions indices ok fft tullio exp im pi axes finalisers applied sum equivalent tullio n sqrt n mapnorm eachcol tullio n n norm anon func reduction function tullio product tullio max maximum dims min ifelse findmin dims tullio min ts init typemax int access fields arrays eachindex fill tullio functions create arrays evaluated tullio abs rand int rand int using nameddims axiskeys dimension names plus pretty printing tullio row col tullio col row sum fast slow loopvectorization straightforward matrix multiplication real tullio tends fast openblas depending size computer speed comparison v race useful diagnostic goal little avoiding using blas libraries precisely optimised tullio fast weird tensor contractions otherwise permutedims using tullio loopvectorization nnlib benchmarktools batched matmul batch index bmmrev tullio sum randn randn bmmrev nnlib batchedmul permutedims true btime bmmrev s s speed unpermuted btime nnlib batchedmul permutedims ms mkl complex handled loopvectorization slower chained multiplication slow algorithm makes loops instead multiplying sequentially instead operations m m m randn randn randn btime m m m s btime tullio m m m m s slightly obviously randn randn tullio r begin operations instead tullio r r r tullio fast broadcast reductions avoid allocations loopvectorization speeding log tullio handling tiled memory access multithreading sumopp tullio log sumpart tullio log rand btime sumopp s allocations kib btime sum log transpose ms allocations mib btime sumpart ms computer btime sum log transpose dims ms indices using pad clamp mod slow result extra checks operations iteration edges conv tullio conv tullio conv tullio pad pad x rand k randn btime conv x k s btime conv x k s btime conv x k s using flux x reshape x k reshape k conv x k btime crosscor k false x s conv x k btime conv k false stride x s conv x k btime conv k false pad x s using dsp btime dsp conv x k s gradients gpu derivatives gpu using tullio mul tullio rand rand mul true using tracker zygote a tracker gradient sum mul a ones true using cuda cudakernels kernelabstractions defined gpu version mul tullio cu mul cu cu true cu a tracker gradient sum mul cu cu true reduction minmax tracker gradient tullio max res warnings complete reductions gpu extremely slow reorganisation multithreading cpu killed sorry gradients calculated scalars arrays example gradient tullio zero using graddual hand evaluated time backward pass avoids memory store partials function expensive slow larger expressions expression line example tullio inbounds begin sum mat mod mod axes mat axes mat grad dual nograd macro infer range output indices provided explicitly writing existing array begin ranges assignment attempt sum assumes indices bounds add inbounds mod mod axesmat safe able symbolic derivative dual fine examples using tullio offsetarrays convolution cyclic indices mat zeros mat mat tullio kern tullio begin xi mod axes mat xi means summed yj mod axesmat inbounds trunc int matxi mod kern disables automatic inbounds prevents range inferred stencil offsets vector tuples tullio begin offsets clamp extrema axes mat clamp extremaaxesmat written clamp inbounds mat clamp ranges read applying vector functions fs sin cos tan xs randn tullio ys fsxs using zygote forwarddiff rowmap fs xs tullio ys fsxs grad dual nograd fs zygote gradient sum rowmap fs ones fs agrees keyword options default setting tullio threadstrue fastmathtrue avxtrue tensortrue cuda gradbase verbosefalse threadsfalse threading threads sets threshold size divide replacing macro guess avxfalse loopvectorization avx inserts avx unroll gradfalse gradient calculation graddual switches forwarddiff loaded nograd gradient calculation nograd arrays tensorfalse tensoroperations assignment xi removes xi list indices range note calculated summed disables inbounds verbosetrue prints index ranges inferred gradient calculations verbose prints absolutely makes array write existing row col makes nameddimsarray tullio product tullio reductions tullio update init initial value reductions min min sensible defaults reductions zero implicit indices shifts range appear shifts run intersection ranges shifted output indices start unless offsetarrays visible calling module avx calculation gradients switched sufficiently complex syntax arrays arrays gradient hooks attached reversediff tracker zygote packages loaded macro run gradients defined reductions default min max gpu kernels constructed kernelabstractions cuda visible default cuda passed kernelcuda cpu kernels kernelabstractions called threadsfalse fast useful testing extras specify range indices inferred col fix index constant prevent col summed preferred include constants note gradient calculated indexing mod clamp maps lie axes disables inference ranges similarly pad extends range inserting zeros outside instead zero padnan value padding implementation mod clamp fast left array underscore inserts whatever shift based tullioprintgrad log prints symbolic derivatives macros tulliotensor macro tensoroperations evaluate expressions provides gradient definitions previously automatic behaviour tensoroperationsjl loaded expression suitable tullioeinsum variant changes allow running einsumjl tests following macros calling functions tensor tensoroperationsjl ein omeinsumjl matmul sum tensorcastjl writes loops einsum einsumjl expanding roughly promotetype eltype eltype array undef size size inbounds size size acc zero size acc acc tullio similar functions taking slightly complicated example tullio tanh expands roughly function act type abstractarray axi axj axk final true inbounds fastmath axi axj acc isnothing zero axk acc isnothing final acc tanh acc function axi axes axj axes axk axes check axes rhs tanh core compiler returntype rhs eltype plus fallback similar axi axj tullio threader act array axiaxj axk return tullio eval division allows dispatch methods act generated avx loopvectorization loaded cuarray kernelabstractions loaded allows threader divide calling act times threads tiles dividing axis axi half repeatedly divides axk sequentially true ranges except final except axi axj safe parallel finally eval exists zygote friends attach gradient calculation looks roughly adjoint function eval ab fwd ab c rev c ab function act type c a b axi axj axk axk axi axj ex c a ex b ex function c a similar b similar axi axk axes axj axes tullio threader act array axk axi axj return a b loop safely broken threads a b index a b filled notice derivative tanh written terms final result forward pass free result sum saved fail using graddual gradient kernel looks method handle finalisers tanh plain tullio read function act type c a b axi axj axk eps forwarddiff dual eps forwarddiff dual axk axi axj res eps eps a forwarddiff partials res c b forwarddiff partials res c writing tullio verbose print functions scalar reductions tullio log slightly act function simply returns sum variable acc elsewhere friends relatives loopvectorizationjl available gaiusjl paddedmatricesjl build gpuifyloopsjl kernelabstractionsjl generate gpucompatible kernels threadsxjl threaded reductions else stridedjl multithreaded broadcasting front nearlookalikes einsumjl makes simple loops testseinsumjl using tullio einsum seamless replacement tensoroperationsjl omeinsumjl identify patterns call various basic operations tensorrulesjl makes tensor differentiable tensorgradjl tensortrackjl earlier attempts tensorcastjl expresses julia array operations broadcasting reduction omeinsumjl treats special lazy broadcastreduction run tortillajl exist publicly nice talk arraymetajl julia tokamakjl readm