deepqlearning package provides implementation deep learning algorithm solving mdps information pomdpsjl fluxjl supports following innovations target network prioritized replay dueling double recurrent learning installation using pkg pkgregistryadd julia julia add registry throught pomdp package pkgaddpomdps using pomdps pomdpsaddregistry pkg add deepqlearning usage using deepqlearning using pomdps using flux using pomdpmodels using pomdpsimulators using pomdptools load mdp model pomdpmodels define mdp simplegridworld define network fluxjl documentation gridworld represented dimensional vector model chain dense dense length actions mdp exploration epsgreedypolicy mdp lineardecayschedule start stop steps solver deepqlearningsolver qnetwork model maxsteps explorationpolicy exploration learningrate logfreq recurrence false doubleq true dueling true prioritizedreplay true policy solve solver mdp sim rolloutsimulator maxsteps rtot simulate sim mdp policy println total discounted reward simulation rtot specifying exploration evaluation policy exploration policy evaluation policy specified solver parameters exploration policy provided form function return action function provided called follows policy env obs globalstep rng policy nn policy trained env environment obs observation action globalstep interaction step solver rng random generator package provides default epsilon greedy policy linear decrease epsilon globalstep evaluation policy provided similar manner function called follows policy env neval maxepisodelength verbose policy nn policy trained env environment neval evaluation episode maxepisodelength maximum steps episode verbose boolean enable printing evaluation function returns elements average total reward float average score episode average steps float average steps episode info dictionary mapping string float log custom scalar values network qnetwork options solver accept chain object expected multilayer perceptrons convolutional layers followed dense layer network dense layers dueling option split dense layers network observation multidimensional array image flattenbatch function flatten dimensions image useful connect convolutional layers dense layers example flattenbatch flatten dimensions batch size input size network dependent specified create network package exports type abstractnnpolicy represents neural network based policy addition functions pomdpsjl abstractnnpolicy objects supports following getnetworkpolicy returns value network policy resetstatepolicy reset hidden policy rnn savingreloading model fluxjl documentation saving loading models deepqlearning solver saves weights network bson file solverlogdirqnetworkbson logging logging tensorboardloggerjl log directory specified solver options disable logging set logdir option gpu support deepqlearningjl support running calculations gpus package cuarraysjl checkout branch gpusupport note tested thoroughly run solver gpu load cuarrays proceed usual using cuarrays using deepqlearning using pomdps using flux using pomdpmodels mdp simplegridworld model weights send gpu call solve model chain dense dense length actions mdp solver deepqlearningsolver qnetwork model maxsteps learningrate logfreq recurrence false doubleq true dueling true prioritizedreplay true policy solve solver mdp solver options fields learning solver qnetwork specify architecture network learningratefloat e learning rate maxstepsint total training step default targetupdatefreqint frequency target network updated default batchsizeint batch size sampled replay buffer default trainfreqint frequency active network updated default logfreqint frequency logg info default evalfreqint frequency eval network default numepevalint episodes evaluate policy default epsfractionfloat fraction training set explore default epsendfloat value epsilon exploration phase default doubleqbool double learning udpate default true duelingbool dueling structure network default true recurrencebool false set true drqn throw error set false pass recurrent model prioritizedreplaybool enable prioritized experience replay default true prioritizedreplayalphafloat default prioritizedreplayepsilonfloat default e prioritizedreplaybetafloat default buffersizeint size experience replay buffer default maxepisodelengthint maximum length training episode default trainstartint steps fill replay buffer initially default savefreqint save model savefreq steps default evaluationpolicyfunction basicevaluation function evaluate policy evalfreq steps default rollout return undiscounted average reward explorationpolicy linearepsilongreedymaxsteps epsfraction epsend exploration strategy default epsilon greedy linear decay rngabstractrng random generator default mersennetwister logdirstring folder save model verbosebool default tru