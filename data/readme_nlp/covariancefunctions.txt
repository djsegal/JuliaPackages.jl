covariancefunctionsjl covariancefunctionsjl primary goal efficient computations kernel matrices called gramian package implements lazy gramian type solve linear algebraic arising kernel methods efficiently running memory code automatically recognizes linear algebraic structures exploits computational efficiency particular fast matrixvector multiplications mvms feature highlights include lazy representation kernel matrices memory footprint efficient parallelized mvms gradient observations nd exact mvms kernel matrices arising gps gradient observations contrast nave nd complexity hessian observations nd exact mvms kernel matrices arising gps hessian observations contrast nave nd complexity toeplitz structure log exact mvms n exact linear solves isotropic kernel matrices uniform grid kronecker structure n exact mvms n exact linear solves separable product kernels matrices cartesian grid dimensions barneshut algorithm log approximate mvms isotropic kernel matrices sparsification nklog sparsification isotropic kernel matrices specified tolerance subsequent mvms zeros basic usage example construct kernel matrix using gramian function highlights memory footprint lazy representation matrixvector multiplication mul using covariancefunctions using linearalgebra covariancefunctions maternp matrn kernel generating data samples randn data vector vectors time gramian instantiating lazy gramian matrix allocation bytes size randn zero time mul multiplying allocates little memory allocations kib hand instantiating matrix densely consumes gib memory time matrix instantiating matrix costs gib allocations gib gc time kernels package implements popularly covariance kernels stationary kernels list stationary kernels implementations found srcstationaryjl exponentiatedquadratic eq rbf rationalquadratic rq exponential exp gammaexponential exp matern real valued parameters maternp integer cosinekernel cos spectralmixture sm stationary kernels following stationary kernels found srcmercerjl dot covariance functions linear function polynomial poly exponentialdot brownian covariance brownian motion finitebasis covariance corresponding finite set basis functions neuralnetwork nn implements mckay neural network kernel combining kernels covariancefunctionsjl implements transformations algebraic combinations kernel functions example using covariancefunctions smooth covariancefunctions rq rational quadratic kernel line covariancefunctions dot linear kernel kernel smooth line combination smooth kernel quadratic kernel assigns kernel linear combination smooth matrn kernel quadratic kernel resulting kernel evaluated base kernel classes randn randn kernel smooth line true using custom kernels simple custom kernel customrbf exp sum abs custom rbf implementation advantage specialized structureaware algorithms prudent covariancefunctionsjl input type covariancefunctions inputtrait typeof customrbf isotropicinput options include dotproductinput stationarylinearfunctionalinput enable efficient output type inference custom kernels parameters extend baseeltype custom kernel parameters set type bottom type union base eltype typeof customrbf union type output kernel inputs expected promotetypeeltype toeplitz structure kernel matrix corresponding isotropic kernel regular grid dimension exhibits special toeplitz structure covariancefunctionsjl automatically detects structure log multiplies n direct solves using covariancefunctions using linearalgebra covariancefunctions exponential exponential kernel range time gramian allocations kib typeof algorithm automatically recognized toeplitz structure toeplitzmatrices symmetrictoeplitzfloat randn zero time mul matrixvector multiplications fast log allocations mib contrast instantiating matrix slower memoryexpensive time matrix allocations gib gc time fast multiplications conjunction iterative solvers covariancefunctionsjl implements n direct solver called levinson using covariancefunctions levinson time xfast levinson allocations kib whereas navely magnitude julia time xnaive matrix allocations gib gc time notably results equal machine precision xnaive xfast true kronecker structure taking cartesian product vectors rise regular grid n dimensional kernel matrices constructed grids navely n mvm complexity n inversion complexity separable direct product kernels evaluated grids rise kronecker product structure allows faster n multiplies n solves exponential improvement exploit structure note covariancefunctionsjl lazily represent cartesian grid using lazygrid using linearalgebra using covariancefunctions using covariancefunctions lazygrid separableproduct randn time gx lazygrid allocations bytes length gx construct separable product kernel dimensions covariancefunctions exp separableproduct fill separable product kernel subsequently calling gramian grid gx automatically represents matrix lazy kroneckerproduct structure implemented kroneckerproductsjl time gramian gx allocations bytes using kroneckerproducts isa kroneckerproduct true allows efficient multiplies factorizations solves randn size time cholesky allocations kib time allocations mib factorizing solving takes fraction despite linear system million variables notably kronecker structure constituent kernel matrices lazily represented fast mvms required instantiate constituent matrices memory keeping kronecker product lazy using kroneckerproducts kronecker matrix instantiates constituent kernel matrices memory fast mvms randn zero time mul allocations kib note kronecker structure combined toeplitz structure yield quasilinear mvms resulting kernel matrix basis ski framework gradient kernels conditioning gaussian processes gradient information matrixvalued gradient kernels dimension input roos et al noted isotropic dot product kernels rise gradient kernel matrices datasparse structure proposed direct method nd n complexity lowdata regime observations covariancefunctionsjl implements automatic structure derivation engine range kernel functions including complex composite kernels mackay neural network kernel spectral mixture kernel permitting exact matrixvector product nd operations gradient kernel matrices contains generic fallback regular nd complexity special structure currently implemented example using covariancefunctions using linearalgebra covariancefunctions maternp matrn kernel covariancefunctions gradientkernel generating data samples randn data vector vectors time gramian instantiating lazy gradient kernel gramian matrix allocation bytes size despite million million matrix mvms fast randn zero time mul multiplying allocates little memory allocations kib impossible covariancefunctionsjl lazy structured representation gradient kernel matrix note gradientkernel computes covariances gradient observations covariance kernel includes value observations valuegradientkernel linear solves gradient kernel matrices via ldiv minimum residual method iterations converge matrn kernel rise extremely conditioned matrix dimensions time allocations mib true highlight scalability mvm algorithm compare implementation gpytorch fast approximate mvm provided skip plot mvm times exponentiated quadratic rbf gradient kernel function dimension notably gpytorch implementation scales nave complexity nd skip mvm scales linearly required preprocessing scales quadratically dominates total runtime skip restricted seperable product kernels rise low rank constituent kernel matrices accuracy plot compares skip covariancefunctionsjl mvm nave dense mvm covariancefunctionsjl mvm mathematically exact scales linearly dimensions time gpytorch skip support gradient kernels kernel supported exponentiated quadratic rbf gradient kernel contrast covariancefunctionsjl provides scalable nd mvms accurate machine precision class kernels including isotropic dotproduct kernels extends complex composite kernels mackay neural network kernel spectral mixture kernel achieved computing structured representation kernel matrix matrixstructureaware automatic differentiation following exemplifies combination matrn quadratic neural network kernels using dimensions matern covariancefunctions maternp matern quad covariancefunctions dot kernel component nn covariancefunctions nn neural network kernel matern quad nn covariancefunctions gradientkernel kernel time gramian allocations kib size time mul allocations mib gc time hook costum kernel covariancefunctionsjl automatic structure derivation engine specifying input type using inputtrait function basic input traits amenable specializations isotropicinput dotproductinput stationarylinearfunctionalinput transformations combinations kernels supported efficient d operations hessian kernels constrast nave d complexity main files containing implementation srcgradientjl srcgradientalgebrajl srchessianjl hessian kernels addition special structure gradient kernels covariancefunctionsjl implements similar structure kernels arising hessian observations reducing complexity multiplying hessian kernel matrices nd nd support hessian kernels comprehensive gradient kernels includes isotropic dot product kernels example using covariancefunctions using linearalgebra covariancefunctions eq exponentiated quadratic rbf kernel covariancefunctions hessiankernel keeping dimension moderate randn data vector vectors time gramian instantiating lazy gradient kernel gramian matrix allocations kib size d d mvms hessian kernel matrix fast randn zero time mul multiplying fast allocations mib gc time contrast nave approach instantiating hessian kernel matrix memory takes magnitude time memory time matrix allocations gib gc time notably multiplication hessian kernel matrix scales linearly d amount information gathered hessian observation implementation scales constants memory allocations optimized gradient kernel matrices feel free reach benefit improved implementation hessian kernel fasttrack sparsification depending data distribution kernel matrices associated exponentiallydecaying kernels approximately sparse particularly dimensions average distance uniformaly distributed unit hypercube grows dimension covariancefunctionsjl contains sparsification algorithm extends sparsearraysjl sparse function guaranteeing userdefined elementwise accuracy sparsification guarnatee positive definiteness resulting matrices special care method thank david bindel bottleneck computation sparse nearestneighborsjl inrange function principle nearest neighbors search based ball trees log operation complexity grows intrinsic dimension data consequently search fast data lowdimensional manifold bruteforce search efficient unstructured data dimensions accelerate search parallelization using covariancefunctions using sparsearrays covariancefunctions eq randn gramian time sparse e sparsification e tolerance allocations mib looking zeros highly sparse nnz zeros nnz entries sparse subsequent matrixvector multiplications fast randn zero time mul sparse multiply lazy dense multiply takes magnitude time mul lazy dense multiply allocations kib note sparsification takes time lazy dense multiplications break quickly iterative method barneshut barneshut algorithm origins accelerating gravity simulutations variant applied kernel matrices arising kernels allowing approximate matrixvector multiply log operations covariancefunctionsjl contains barneshutfactorization structure constructor called gramian matrix applies fast transform whenever mul called similar fast sparsification barneshut algorithm relies ball trees summarize interactions clusters data guaranteed fast lowdimensional data complexity approach increases intrinsic dimension data using covariancefunctions using linearalgebra covariancefunctions eq randn gramian time covariancefunctions barneshutfactorization allocations mib gc time notably constructing computes ball tree structure data allocates memory log operation parameter trades accuracy speed multiplying exact n subsequent multiplications fast randn zero time mul allocations mib whereas magnitude lazy dense multiply takes time mul allocations kib digits accuracy norm norm reducing increase accurary expense runtime covariancefunctions barneshutfactorization setting instead time mul allocations mib norm norm