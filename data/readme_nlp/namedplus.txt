namedplusjl package exists experiment arrays provided nameddimsjl package fairly minimal focused providing type performance defines lots useful functions defined packages loaded v convenient add names exports named named nameless pirate base rand int names keywords randtype ones ones zeros fill named parent adds names refines existing ones az named rand ellipsisnotation skip named read names generator variables rename renames nameless transpose named mt functions controlled split reshape size join copy adjacent size dropdims az defaults kills transpose az permutes hack lots code propagate names namedint size namedint exists zeros ones fill rand etc sqrt comprehensions propagate names reshape reshape propagate via sizes using einsum tensorcast packages dont names einsum mz arrayundef namedint cast tm reshape namedint automatic re dimensions align alignsum alignprod align lazy generalised permutedims named autopermuted broadcasting align manually fix alignsum int reduce including matrix multiplication mul contract batchmul mul matrix multiplication shared index typed tab using tensoroperations contract shared indices leaving infix version odot tab named tensor named inputs rearranged via strided using omeinsum contract sum shared leaving const batchmul batch index rename sum shared leaving using zygote gradient sumcontract contract defines gradient gradient sum omeinsum defines gradient bits moved axiskeysjl packages loaded using namedplus axiskeys plots named custom ranges scatter ans yaxis log labels axes series functions nameddimsjl try hard zerocost hard exploit constant propagation true particluar split join align rename cost s useful faster mul aligned broadcasting via named nearly free ns compared pytorch named tensors refinenames named except instead unflatten split exactly flatten join except dims consecutive permutes required alignto alignas align allows target subset superset neither input allows support einsum torchmatmul handles batched matrix multipl