wordtokenizers basic tokenizers natural language processing installation standard julia package installation pkg add wordtokenizers usage normal package call tokenizestr split string words splitsentencesstr split string sentences maybe tokenizesplitsentencesstr tokenize splitsentences configurable functions call tokenizers sentence splitters defined sensible defaults set override method calling settokenizerfunc setsentencesplitterfunc passing preferred function func list elsewhere configuring throw method overwritten warning trigger recompilation methods means using package wordtokenizersjl tokenizationsentence splitting via default methods changing tokenizersplitter change behavior package feature corpusloadersjl package author allow user change tokenizer tokenizer explicitly using tokenize method example setting tokenizer tinysegmenterjl example tinysegmenterjl tokenizer japanese text include tinysegmenter package wordtokenizersjl trivial import tinysegmenter settokenizertinysegmentertokenize example julia using wordtokenizers julia text julia tokenize text print default tokenizer julia import tinysegmenter julia settokenizer tinysegmenter tokenize julia tokenize text print tinysegmenter tokenizer substringstring word tokenizers word tokenizers basically assume sentence splitting poorman tokenizer poormanstokenize deletes punctuation splits spaces worse using split punctuation space tokenize punctuationspacetokenize marginally improved version poorman tokenizer deletes punctuation occurring outside words penn tokenizer penntokenize robert macintyre original tokenizer penn treebank splits contractions improved penn tokenizer improvedpenntokenize nltk improved penn treebank tokenizer similar original improvements punctuation contractions matches nltk nltktokenizetreebankwordtokenizertokenize nltk word tokenizer nltkwordtokenize nltk improved version penn tokenizer version unicode handling changes matches commonly nltkwordtokenize minus sentence tokenizing step weird historical nltk successive variations improving penn tokenizer matching nltk reversible tokenizer revtokenize revdetokenize tokenizer splits punctuations space special symbols generated tokens detokenized using revdetokenizer function tokenization toktok tokenizer toktoktokenize tokenizer simple tokenizer input sentence line final period tokenized enhanced version original toktok tokenizer tested reasonably results english persian russian czech french german vietnamese tajik default tokenizer tweet tokenizer tweettokenizer nltk casual tokenizer solely designed tweets apart twitter specific tokenizer handling emoticons web aspects support html entities closely matches nltk nltktokenizetweettokenizer sentence splitters currently sentence splitter rulebased sentence spitter rulebasedsplitsentences rule periods question marks exclamation marks followed whitespace sentences list exceptions splitsentences exported alias useful sentence splitter currently implemented atm sentence splitter rulebasedsplitsentences default sentencesplitter example julia tokenize package tokenizers range simple poorman complex penn print substringstring package tokenizers range simple poorman complex penn julia text leatherback sea turtle largest measuring six seven feet length maturity five feet width weighing pounds kg species feet length proportionally wide flatback turtle found solely northerncoast australia julia splitsentences text element arraysubstringstring leatherback sea turtle largest measuring six seven feet length maturity five feet width weighing pounds about kg species feet length proportionally wide flatback turtle found solely northern coast australia julia tokenize splitsentences text element arrayarraysubstringstring substringstring leatherback sea turtle largest measuring six pounds kg substringstring species proportionally wide substringstring flatback turtle found solely northern coast australia experimental api trying experimental api added dispatches basesplit splitfoo words tokenizefoo splitfoo sentences splitsentencesfoo using tokenbuffer api custom tokenizers offer tokenbuffer api supporting utility lexers speed tokenization writing tokenbuffer tokenizers tokenbuffer string readable stream building tokenizers utility lexers spaces span class span read characters stream array tokens lexers return true false indicate matched input stream combined easily spacesornumberts spacests ts skips whitespace parses token simplest useful tokenizer splits spaces using wordtokenizers tokenbuffer isdone spaces character function tokeniseinput ts tokenbufferinput isdonets spacests characterts return tstokens tokenisefoo bar baz foo bar baz prewritten components building custom tokenizers found srcwordsfastjl srcwordstweettokenizerjl components mixed matched create complex tokenizers complex example julia using wordtokenizers tokenbuffer isdone character spaces fastjl julia using wordtokenizers nltkurl nltkurl nltkphonenumbers tweettokenizerjl julia function tokeinze input urls ts nltkurl ts nltkurl ts ts tokenbuffer input isdone ts spaces ts continue urls ts nltkphonenumbers ts character ts return ts tokens tokeinze generic function method julia tokeinze url phonenumber element arraystring url url detected phonenumber phone detected tips writing custom tokenizers tokenbuffer lexer lexers written care example matches phone match split julia using wordtokenizers tokenbuffer isdone character spaces nltkphonenumbers julia order ts ts nltkphonenumbers ts order generic function method julia order ts nltkphonenumbers ts ts order generic function method julia function tokenize input ts tokenbuffer input isdone ts order ts character ts return ts tokens tokenize generic function method julia function tokenize input ts tokenbuffer input isdone ts order ts character ts return ts tokens tokenize generic function method julia tokenize ts nltkphonenumbersts element arraystring julia tokenize nltkphonenumbersts ts element arraystring boundserror errors handling edge common writing tokenbuffer lexers tokenbuffer ts flushts pushtstokens input characters buffer tsbuffer flushed separate tokens julia using wordtokenizers tokenbuffer flush spaces character isdone julia function tokenize input ts tokenbuffer input isdone ts spaces ts continue mypattern ts character ts return ts tokens julia function mypattern ts matches pattern continuous ts idx length ts input return false tsts idx tsts idx flush ts using flush ts idx return true return false mypattern generic function method julia tokenize hihello element arraystring hi hello julia function mypattern ts matches pattern continuous ts idx length ts input return false tsts idx tsts idx push ts tokens using flush ts idx return true return false mypattern generic function method julia tokenize hihello element arraystring hihello statistical tokenizer sentencepiece unigram encoder basically sentencepiece processor reimplementation julia vocab file generated sentencepiece library containing vocab log probability detail implementation refer blog post note sentencepiece escapes whitespace meta symbol pretrained wordtokenizer provides pretrained vocab file albert version version julia subtypes pretrainedtokenizer element array albertv albertv julia tokenizerfiles albertv element arraystring albertbasevkcleanvocab albertlargevkcleanvocab albertxlargevkcleanvocab albertxxlargevkcleanvocab datadeps handle downloading create issue pr pretrained models directly load providing path load function julia spm load albertversion loading default albertbase vocab sentencepiece wordtokenizers sentencepiecemodel dict shots dev silv doubtful pol chem disrespect julia tk tokenizer spm love julia language tk spm love julia language element arraystring love julia language julia subword tokenizer spm unfriendly element arraystring un friendly julia para spm julia level performance dynamic language technical computing element arraystring ulia level performance dynamic language technical computing indices usually deep learning models index special tokens albert pad unk cls sep mask julia idsfromtokens spm tk element arrayint sentences tokens julia sentencefromtokens tk love julia language julia sentencefromtoken subword unfriendly julia sentencefromtokens para julia level performance dynamic language technical computing contributing contributions form bugreports pullrequests additional documentation encouraged github repository follow colprac guide collaborative practices contributor read guide contributions communications abide julia community standards software contributions follow prevailing style codebase pull request issues getting responses days hesitate bump posting comment update status sometimes github notifications lost support feel free help julia discourse forum naturallanguage channel juliaslack join raise issues repository request improvements document