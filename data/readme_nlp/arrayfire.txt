arrayfirejl arrayfire library gpu accelerated computing arrayfirejl wraps arrayfire library julia provides julia interface installation install arrayfire library download binary official site build source julia add arrayfire simple usage congratulations installed arrayfirejl simple julia array cpu rand transfer array device calling afarray constructor using arrayfire forget load library ad afarray perform simple arithmetic bd ad course add divide check supported functions section information re device computation bring array cpu host array bd examples simple usage using arrayfire linearalgebra random generation rand afarrayfloat randn afarrayfloat transfer device cpu hosttodevice afarray rand transfer cpu devicetohost array hosttodevice basic arithmetic operations sin logical operations anytrues reduction operations totalmax maximum colwisemin min matrix operations determinant det bpositive abs product dotproduct transposer linear algebra lufact lu choleskyfact cholesky multiplied create positive definite matrix qrfact qr svdfact svd fft fastfourier fft execution model arrayfirejl introduces afarray type subtype abstractarray operations afarrays create afarrays data remains device unless specifically transferred wrapper provides simple julian interface aims mimic base julia versatility ease repl behaviour repl whenever create afarray repl displays values base julia happens showarray method overloaded ensure time display repl values transferred device host means single operation repl involves implicit memory transfer lead slowdown interactively depending size data memory bandwidth available semicolon statement disable displaying avoid memory transfer note script memory transfer unless display function explicitly called array constructor example async behaviour arrayfire asynchronous library essentially means whenever call particular function arrayfirejl return control host immediately julia continue executing device pretty useful mean host code independent device simply execute device computes resulting real world performance library performs kernel fusions elementary arithmetic operations arithmetic section supported functions arrayfire intelligent runtime jit compliation engine converts array expressions openclcuda kernels kernel fusion decreases kernel calls avoids extraneous global memory operations asynchronous behaviour jit operation called explicit synchronization barrier syncarray called note benchmarking julia time macro time execution times functions particular time simply time function call library execute asynchronously background lead misleading timings time individual operations run multiple times explicit synchronization barrier average multiple runs note affect user writes code users simply write normal julia code using arrayfirejl asynchronous behaviour abstracted whenever data onto cpu implicit barrier ensures computatation complete values transferred operations cpu device arrays consider following code return error rand float afarray throws error arrays reside regions memory host device coherent operation performed array transferred region memory arrayfirejl automatically performance considerations manually transfer arrays memory following operations array afarray note correctness sometimes arrayfirejl base julia return marginally values computation julia arrayfirejl sometimes lower level libraries blas fft etc example julia openblas blas operations arrayfirejl clblas opencl backend cublas cuda backend libraries exact values openblas decimal light users encouraged testing codes correctness note performance operations slow due base generic implementations intentional enable fast workflow re ready disable slow fallback methods julia allowslow afarray false julia xs error getindex disabled supported functions creating afarrays rand randn convert diagm eye range zeros ones trues falses constant getseed setseed iota arithmetic complex conj real imag max min abs round floor hypot sigmoid signbit vectorized form julia v ref issue linear algebra cholesky svd lu qr svdfact lufact qrfact matmul amulbt atmulb atmulbt acmulb amulbc acmulbc transpose transpose ctranspose ctranspose det inv rank norm dot diag islapackavailable chol solvelu upper lower signal processing fft ifft fft ifft conv conv fftcr fftrc conv convolve fir iir approx approx statistics mean median std var cov meanweighted varweighted corrcoef vector algorithms sum min max minimum maximum findmax findmin countnz sort union cumsum diff sortindex sortbykey diff minidx maxidx backend functions getactivebackend getbackendcount getavailablebackends setbackend getbackendid sync getactivebackendid device functions getdevice setdevice getdevicecount image processing scale hist loadimage saveimage isimageioavailable colorspace grayrgb rgbgray rgbhsv rgbycbcr ycbcrrgb hsvrgb regions sat bilateral maxfilt meanshift medfilt minfilt sobel histequal resize rotate skew transform transformcoordinates translate dilate erode dilated eroded gaussiankernel computer vision orb sift gloh diffofgaussians fast harris susan hammingmatcher nearestneighbour matchtemplate performance arrayfire benchmarked commonly operations benchmark negative matrix factorization cpu intel xeon cpu e ghz gpu grid k mb cuda compute arrayfire v benchmark scripts benchmark folder run include benchmarkjl include nmfbenchmarkjl backends backends arrayfirejl cuda backend opencl backend cpu backend backend essentially allows user switch backends runtime called unified backend arrayfirejl starts unified backend backend selected arrayfire default depends available drivers desired depending available hardware override default setting environment variable juliaarrayfirebackend starting julia specifically loading arrayfire module values juliaarrayfirebackend cpu cuda opencl change backend runtime via setbackendafbackendcpu resp afbackendcuda afbackendopencl unified backend computational backend represents interface switch backends runtime arrayfirejl starts unified backend getactivebackend return particular default backend depending installed library example built arrayfirejl cuda backend getactivebackend return afbackendcuda backend troubleshooting arrayfirejl error loading libaf try adding path libaf ldlibrarypath arrayfire error internal error whenever call rand re using cuda backend try checking libcudart libnvvm ldlibrarypath libafcuda try link libraries loads julia system install cuda platform arrayfirejl loads randafarrayfloat stuck cuda backend check installed cuda platform installed cuda simply downloaded binary doens try adding libnvvm libcudart path arrayfirejl atom create file home directory called juliarcjl write envldlibrarypath usrlocallib path libaf atom able load error arrayfire error double precision supported device error message pops devices support double precision example iris pro macbooks message single precision example re generating random directly device correct usage scenario randafarrayfloat instead randafarrayfloat