yaadjl automatic differentiation package julia installation press pkg mode julia repl type pkg add yaad introduction check blog post implement ad julia day project aims provide similar interface pytorch autograd keeping simple core implementation contains straightforward line julia highly inspired autogradjl pytorch operation directly return cachednode constructs computation graph dynamically using global tape note project self moment ad related experimental coding guarantee consistency stability versions version compatible practical usage suggest try fluxtracker zygote performance aimed experimental projects usage simple mark differentiate variable contains value grad time try backward evaluate gradient accumulated grad using linearalgebra x x variable rand variable rand tr x x tracked value backward backward propagation print x grad gradient goes register define create node computation graph sin abstractnode register sin define gradient gradient typeof sin grad output grad cos license apache license vers