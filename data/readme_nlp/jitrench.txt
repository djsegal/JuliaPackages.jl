jitrench dive deep trenches loss function jitrenchjl install add automatic gradient calculation julia using jitrench julia sin generic function method julia jitrench diff generic function method julia compute gradients deep functions visualize computational graphs julia scalar scalar float julia scalar scalar float julia goldstain goldstain generic function method julia goldstain scalar float e julia backward julia grad e julia grad e julia jitrench plotgraph tofile examplevisualizegoldstainpng compute gradients operations multidimensional arrays julia autodiff tensor tensormatrixint julia reshape tensormatrixint julia tensorvectorint julia sum scalar int julia backward julia grad matrixfloat gpu support cutensor cutensor type perform calculations gpu tensor julia using jitrench julia using benchmarktools julia tensor rand julia tensor rand julia xgpu cutensor rand julia wgpu cutensor rand julia benchmark benchmarktools trial samples evaluation range min max s ms gc min max time median s gc median time mean s s gc mean s histogram log frequency time ms memory estimate mib allocs estimate julia benchmark xgpu wgpu benchmarktools trial samples evaluations range min max s ms gc min max time median s gc median time mean s s gc mean s histogram log frequency time s memory estimate bytes allocs estimate example gradient descent rosenbrock x x x x x x scalar x scalar lr e iters iters rosenbrock x x jitrench autodiff cleargrad x jitrench autodiff cleargrad x backward x values lr x grad x values lr x grad exampleoptimizationgradientdescentjl details example newton method jitrenchjl compute derivatives using jitrench scalar iters iters jitrench autodiff cleargrad backward creategraph true gx grad jitrench autodiff cleargrad gx backward gx gx grad values gx values gx values exampleoptimizationnewtonmethodjl details example train neural network using jitrench using jitrench nn using printf niter rand sin rand function model nn linear outdim nn functions sigmoid nn linear outdim return nn result params nn init model nn initializer optimizer nn sgd params e tensor tensor iter niter pred nn apply model params loss nn functions meansquarederror pred nn cleargrads params backward loss nn optimize optimizer iter printf iters i loss f iter loss values nn saveweight params weight simple example training sine curve mlp saving weights weightsjtw progress training visualization code available examplennmlpjl