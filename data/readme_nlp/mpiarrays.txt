mpiarrays package provides distributed arrays based mpi sided communication primitives currently proofconcept functionality described readme implemented following simple example multiply matrix vector using mpi mpiarrays mpi init rank mpi commrank mpi commworld size matrix create uninitialized matrix vector mpiarray float mpiarray float set random values forlocalpart rand forlocalpart rand process finished initializing coefficients sync print process using slow elementelement communication ok print screen rank println matvec result clean free free mpi finalize construction constructors create uninitialized arrays basic form array size argument mpiarray int distribute array dimension processes mpicommworld communicator specify partitions direction create matrix partitions direction mpiarray int mpi commworld constructors automatically distribute total elements direction uniform manually specify elements partition direction mpiarray int mpi commworld create distributed array rows processor finally distributed array constructed predefined local mpiarray int produce distributed array partitions containing manipulation array interface implemented including normal indexing operations note lock unlock mpi window call expensive advantage operations defined mainly convenient access utility functions supported operations base linearalgebraamulb matrixvector product basefilter basefilter utility operations tuple containing locallyowned index range obtained using localindices rank optional argument range rank current rank calling free array destroy underlying mpi window manually guarantee garbagecollection happen time processes dangerous collective call destroy window finalizer processes wait call sync mpiarrays argument currently calls mpibarrier array communicator accessing local data local data accessed using forlocalpart function executes function argument local data array argument compatible block syntax fill local random forlocalpart rand sum local localsum forlocalpart lp result zero eltype lp lp result return result redistributing data data redistributed processes follows initial tiled distribution processes mpiarray int comm uniform redistribution column processor rest redistribute restore equal distribution columns redistribute examplesredistjl blocks block type helps accessing processor data individual element access expensive blocks created using indexing operators using range arguments ablock block contain data refers global index range track processes involved actually read data allocating array storage amat getblock ablock allocate matrix fill separate step amat allocate ablock getblock amat ablock modifying entries array associated block data sent processes amat allocate ablock rand amat putblock amat ablock global indexing block block refers regular julia array indexing local block index array linked block using global indices underlying mpiarray create globalblock gb globalblock amat ablock ghost nodes ghostedblock allows periodic reading processor data arbitrary locations array push method adds indices sort ensures fetching data using sync happens minimal mpi calls getglobal function array value local data ghost using global array indices ghosted ghostedblock push ghosted push ghosted sort ghosted fetch processor data sync ghosted ghosts ghosted entry using global indices local data ghosts getglobal ghosted benchmark simple test timings matrixvector product recorded range processes blas threads compared basearray distributedarraysjl performance compared effect distributing rows columns code tests testsmatmuljl results square matrix size using machines intel e v cpus cores machine using tcp gbit ethernet machines using openblasnumthreads mpi process machine yields following timings timings using mpi process machine openblasnumthreads singlethreaded compare library elemental using unmodified gemv example matrix size parameter openblas julia observations using single process darray mpiarray perform level basearray indicating overhead parallel structures ultimately wrap process array negligible reassuring using parallel structures slow serial code threading scaling breaks multiple machines play processes severe breakdown performance process attempts communicate machine process tcp pooling communications machines darray tolerate mpi using hybrid parallelism threads communicate machine mpi julia native parallelism machines faster mpi scaling ideal machines darray results erratic distribute matrix columns dense matrix matrixvector product basearray product openblasnumthreads completes ms mpi version cores machine completes ms suggests improvement threading implementation hand mpi processes faster mpi processes machine indicating memory bottleneck julia implementation simple compares favorably mature library apparent scaling starting processes elemental taking processes investigate cause performance tweaking elemental set