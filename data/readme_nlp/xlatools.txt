xla add irtools master add https github com mikeinnes mjolnir jl add https github com fluxml xla jl compile julia code xla package flux ml ecosystem designed packages including zygote automatic differentiation engine project alpha capability demos larger examples examples folder projectmanifest directory examples currently depend development versions supported features convert julia function xla julia using flux xla julia xadd xla julia xadd external orgtensorflow tensorflow compiler xla service service cc xla service xffee initialized platform host guarantee xla devices external orgtensorflow tensorflow compiler xla service service cc streamexecutor device host default version julia chain dense relu dense softmax f julia xm xla julia xm rand element xla xarrayfloat hello world julia function hello isxla println hello xla else println running xla julia hello running xla julia xla hello hello xla generic functions types julia square xla julia square julia square im im im else data structures loop support roadmap julia function updatezero env env env julia function myrelu env dict env updatezero env return env julia xrelu xla myrelu julia xrelu xrelu gradient julia randn julia gradient sum julia xla rand julia typeof ans tuplexla xarrayfloat arrayfloat gradient array xarray constant regardless input gradient computed compile time goes near xla computation xla code op julia gradient sum julia xla codexla rand float return support mutating array operations future hood results type inference julia xla codetyped softmax const softmax mjolnirshapearrayint const max int int max int return mjolnir kwfunc typeofmapreduce dims mapreduce identity int const int int int return broadcast mjolnirshapearrayint const exp int float float exp float return broadcast mjolnirshapearrayfloat const addsum float float float return mjolnir kwfunc typeofmapreduce dims mapreduce identity float const float float float return broadcast mjolnirshapearrayfloat return final xla code julia codexla softmax int int int xla max int return xla reduce int int xla sub int return xla map int xla convertelementtype float float xla exp float return xla map float float xla add float return xla reduce float float xla div float return xla map return xla internal text representation hlo text julia codehlo softmax hlomodule name name parameter sinvalid parameter parameter sinvalid parameter root maximum sinvalid maximum parameter parameter name parameter sinvalid parameter parameter sinvalid parameter root subtract sinvalid subtract parameter parameter name parameter sinvalid parameter convert finvalid convert parameter root exponential finvalid exponential convert name parameter finvalid parameter parameter finvalid parameter root add finvalid add parameter parameter name parameter finvalid parameter parameter finvalid parameter root divide finvalid divide parameter parameter entry name parameter s parameter constant s constant reduce s reduce parameter constant dimensions toapply name broadcast s broadcast reduce dimensions map s map parameter broadcast dimensions toapply name map f map map dimensions toapply name constant f constant reduce f reduce map constant dimensions toapply name broadcast f broadcast reduce dimensions root map f map map broadcast dimensions toapply name start simpler examples codexla codexla imim limitations notes xla specialised backend limitations primarily terms support dynamic memory allocation expect able support julia code vectorised array code fanciness including flux models expect xla huge julia codebase box error handling run errors please issues support add diagnostics explain code compiled xla reuses jax build xla via pip cpu build installed default gpu support python install gpuenabled jaxlib jax docs currently supported jaxlib version specified buildjl