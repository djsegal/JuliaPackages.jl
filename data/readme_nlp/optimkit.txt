optimkitjl blisfully ignorant julia package gradient optimization package gradient optimization tries assume little optimization gradient descent conjugate gradient bfgs method implemented starts defining algorithm creating instance algorithm gradientdescent params conjugategradient flavor params lbfgsint params parameters namely maxiter iterations defaults typemaxint essentially unbounded gradtol convergence criterion stop norm gradient gradtol default value gradtol e linesearch linesearch algorithm currently choice namely hagerzhanglinesearch verbosity verbosity level amount information printed default value information single stdout output algorithm line summary iteration step furthermore lbfgs takes single positional argument int previous steps account construction approximate inverse hessian takes keyword argument acceptfirst determines guess alpha line search accepted default value true typically leads function evaluations otherwise function evaluations iteration required despite erratic convergence gradient norm conjugategradient additional keyword argument flavor following hagerzhang real real default hestenesstiefel polakribiere daiyuan linesearch argument currently takes value hagerzhanglinesearch real parameter wolfe condition real paremeter wolfe condition real e parameter approximate wolfe condition accept fluctation function value real bisection real determines bisection step performed real expansion parameter initial bracket interval verbosity int linesearch independent verbosity flag control output information printed stdout default value equal verbosity optimization algorithm conjugategradient verbosity equivalent verbosity linesearch algorithm optimization algorithm applied calling fx gx numfg normgradhistory optimize fg x algorithm kwargs optimization objective function specified function fval gval fg returns function value gradient function value fval assumed real type real gradient gval type including tuples named tuples user specify following functions via keyword arguments p precondition apply preconditioner current gradient tangent vector position finalize numiter step completion linesearch allows modify position corresponding function value gradient printing statistics note step happens computing directions conjugate gradient lbfgs modified user risk wolfe conditions satisfied retractx step direction type gradients starting x step length returns r local tangent path position r informally dx d inner compute inner product gradients similar objects position dependence useful optimization manifolds function represents metric particular symmetric inner inner realvalued scale compute equivalent possibly return value scale compute negative gradient step direction add compute equivalent possibly overwriting return value transport transport tangent vector retraction direction type gradient step length return value transport receives retract final argument computed contain useful data recomputed note gradient objective function satisfy inner utility function optimtest facilitate testing compatibility relation choice fg retract inner gradientdescent algorithm requires conjugategradient lbfgs require five functions default values provided optimization algorithms standard optimization vector array retract inner linearalgebra norm linearalgebra dot transport add linearalgebra axpy scale linearalgebra rmul finally keyword argument isometrictransportbool indicate transport vectors preserves inner product inner inner retract transport transport default value false unless default transport transport inner product inner convergence conjugate gradient lbfgs robust theoretically proven isometric transport note isometric transport retraction transport particular transport isometric transport provided complement isometric rotation r transport parallel lbfgs called locking condition huang gallivan absil approach described sect