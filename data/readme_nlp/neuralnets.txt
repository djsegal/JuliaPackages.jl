neuralnetsjl implentation artificial neural networks julia neat features include poised deliver cuttingedge synergy business housecat realtime twitterready box hal skynet proof low calorie vegan homeopathic friendly excellent source vitamin exciting features flexible network topology combination activation functionlayer support common node activation functions addition support arbitrary activation functions automatic differentiation broad range training algorithms chose time hope develop library encompass modern types neural networks namely deep belief networks usage currently support multilayer perceptrons instantiated using mlpgenflayersizesact constructor describe network topology initialisation procedure follows genffunction function initialise weights commonly rand randn layersizesvectorint vector element input nodes element output nodes intermediary elements hidden nodes layer actvectorfunction vector activation functions corresponding layer actdvectorfunction vector corresponding derivatives respective functions act vector activation functions provided neuralnets derivatives seen dictionary neuralnetsderivs example mlprandn relulogisident reludlogisdidentd returns layer network input nodes output nodes hidden layers comprised nodes hidden layer relu activation function logis output nodes lack activation function specify ident function easily logis ensure convergence behaviour target vector classification neural network initialised trained predictions propmlpmlp command column vector node inputs course prop defined arrays inputting array data returns array predictions input nodes output nodes activation functions native support following activation functions define arbitrary activation function derivative calculated automatically using forwarddiffjl package natively supported activation derivatives bit twice fast evaluate compared derivatives calculated using forwarddiffjl ident identify function logis logistic sigmoid exp logissafe logistic sigmoid safe derivative collapse evaluating values relu rectified linear units log exp tanh hyperbolic tangent defined julia training methods mlp type constructed train using provided training functions trainnn trainx valx traint valt training method relies calling external optimjl package default gradientdescent algorithm setting trainmethod parameter following algorithms selected levenbergmarquardt momentumgradientdescent neldermead function accepts data sets training data set inputs outputs trainx traint validation set valx valt input data matrix data occuring column matrix optional parameters include maxiter default iterations giving tol default e convergence threshold affect levenbergmarquard epiterl default performance evaluated validation set epiter iterations slightly convergence iteration takes slightly time verbose default true print information training network gdmtrainnn nativelyimplemented gradient descent training algorithm momentum returns trained network optional list training losses time optional parameters include batchsize default randomly selected subset training extremely data sets feature stochastic gradient descent maxiter default iterations giving tol default e convergence threshold learningrate default learning rate gradient descent larger values converge faster using values result lack convergence typically happening weights infinity getting lots nans suggested start value increase improves learning momentumrate default amount momentum apply try momentum eval default network evaluated convergence eval iterations slightly convergence iteration takes slightly time storetrace default false store information training network information returned list calculated losses entire data set showtrace default false print information training network adatrain lmtrain exampl