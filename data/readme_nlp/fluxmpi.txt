fluxmpijl distributed data parallel training neural networks installation stable release add fluxmpi development version add fluxmpi main quick start using cuda fluxmpi lux optimisers random zygote fluxmpi init cuda allowscalar false model chain dense tanh dense tanh dense tanh dense rng random defaultrng random seed rng localrank ps st lux setup rng model gpu ps fluxmpi synchronize ps rootrank st fluxmpi synchronize st rootrank rand rng gpu opt distributedoptimizer adam f stopt optimisers setup opt ps loss sum abs model st stopt fluxmpi synchronize stopt rootrank gs gradient loss ps optimisers update stopt ps gs t time epoch global ps stopt zygote pullback loss ps fluxmpi fluxmpiprintln epoch epoch loss gs stopt ps optimisers update stopt ps gs fluxmpi fluxmpiprintln time t run code using mpiexecjl julia project filenamejl examples deep equilibrium models deep implicit neural networks infinite time neural odes imagenet training luxjl style guide follow lux style guide contributions adhere style guide changelog v dropped support mpi v fluxmpidisablecudampisupport instead fluxmpidisablecudampisupport setup localpreferencestoml file cleanprintprintln functions fluxmpiprintprintln v dropped support learnbase aka dataloadersjl distributeddatacontainer compatible mlutilsjl distributedoptimiser name changed distributedoptimizer v v introduces api gradient synchronization wrap distributedoptimiser instead add line allreducegradientsgsnamedtuple v internal mpiextensions functions renamed allreduce allreduce bcast bcast reduce reduce cudaunaware mpi bug resolved luxdlluxjl disable cudaaware mpi support fluxmpi using fluxmpidisablecudampisupporttrue temporarily readded dependencies mldatautils learnbase ensure dataloadersjl dropped future release v distributedoptimiser averages gradients instead values summed processes ensure averaging divide loss totalworkers rrule frule defined localrank totalworkers safely inside loss functions v fluxmpiprint fluxmpiprintln print current time fluxmpi initialized calling localrank totalworkers fluxmpiinit lead segfault throw error mldatautils learnbase dependencies dropped zygote flux dependencies removed dispatch fluxmpisynchronize available zygoteparams instead users manually broadcasting function zygoteparams v broadcastparameters renamed fluxmpisynchronize synchronizes lot trainable parameters distributedoptimiser tied flux essentially deal training compatible optimisersjl