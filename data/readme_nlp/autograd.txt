autograd autogradjl automatic differentiation package julia started port popular python autograd package forms foundation knet julia deep learning framework autograd differentiate regular julia code includes loops conditionals helper functions closures etc keeping track primitive operations using execution trace compute gradients reverse mode differentiation backpropagation efficiently handle functions array inputs scalar outputs compute gradients gradients handle derivatives installation install autograd julia using julia using pkg pkg add autograd code start using autograd interface param user declares parameters wrapped struct value original value sum abs act regular values outside differentiation diff sum abs gradients struct value represents value grad contains gradients params interface pre v autograd supported following grad interface supported sum abs grad example linear regression example using callable objects struct linear user defines model linear initialize model callable object parameters linear param randn param randn sgd training loop data loss diff sum abs params grad loss axpy examples directory examples extending autograd autograd handle function primitives gradients add primitives gradients using primitive zerograd macros macrosjl example primitive log dy dy primitive macro marks log method primitive expression defines gradient function wrt argument gradient expressions refer parameter return variable gradient dy optionally indicated argument list method declaration functions multiple inputs multiple gradient expressions existent zero gradients specified omitting gradient expression using default broadcasting version log defined primitive primitive macro note julia supports multipledispatch function multiple methods supporting argument types example logfloat logbigfloat log methods autogradjl method defined independently primitive specific gradient autograd defines gradients using argument types rules generic debugging profiling view contents computational graph differentiating function following julia autograd gcnode autogradnode values lost julia param rand param rand rand rand julia diff sum abs julia displaying array causes pretty printing array float array float array float array float array float array float array float array float array float broadcast array float array float array float array float array float array float array float sum abs array float julia collect list collect creates node array reverse julia dump maxdepth allowing look individual nodes values autograd node value autograd resultarrayfloat parents array autogradnode children array autogradnode outgrad array float cdr autograd node julia dump value maxdepth autograd resultarrayfloat value array float func function type typeof args tupleparamarrayfloat arrayfloat paramarrayfloat array float kwargs base iterators pairsunionuniontuplenamedtupletuple data namedtupletuple namedtuple itr tuple profile autograd using timeroutputsjl set environment variable envautogradtimertrue rebuild autograd pkgbuildautograd evaluating using autograd environment variable autogradtimer checked compile time run time performance reasons collect detailed timing information slows code forget deleteenvautogradtimer rebuild autograd example symbol sum indicates time spent forward pass sum function sum indicates time spent backward pass argument record sumoutgrads functions internal autograd julia env autogradtimer true julia using pkg pkg build autograd julia using autograd timeroutputs julia resettimer autograd julia param rand param rand rand rand julia diff sum abs julia autograd time allocations tot measured mib section ncalls time tot avg alloc tot avg ms ms mib mib sum ms ms mib mib ms ms kib kib ms ms kib kib ms ms mib mib ms ms mib mib record ms ms mib kib ms ms mib mib ms ms kib kib sum ms ms mib mib ms ms kib kib sumoutgrads ms s kib kib code structure corejl implements main functionality acts main documentation source macrosjl support functions define test primitives getindexjl iteratejl catjl set support common data structures including arrays tuples dictionaries numerical gradients defined files basejl mathjl current status future gradient coverage unit testing spotty adding gradients tests cover julia base documentation improved overwriting functions setindex supported efficiency improved reducing runtime compilation memoization support static computation acknowledgments references autogradjl written deniz yuret code initially ported python autograd package thank autograd author dougal maclaurin support baydin et al review automatic differentiation autograd tutorial python examples dougal phd thesis design principles juliadiff fluxml alternative differentiation tools julia thank current contributors carlo lucibello ekin akyrek emre yolcu jarrett revels mike innes ozan arkan rene donner suggested citation autograd inproceedingsknetmlsys authoryuret deniz titleknet beginning deep learning lines julia booktitlemachine learning systems workshop nip