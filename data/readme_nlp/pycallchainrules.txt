pycallchainrules julia lot existing useful differentiable python code pytorch jax etc pycalljl seamless wonder takes differentiate pycall library aims ideal thanks pabloferz cpu gpu array copies via dlpackjl basic usage pytorch cpu install python dependencies using pycall run pycallpyprogramname pip install torchcpu functorch example using pycallchainrules torch torchmodulewrapper torch using zygote indim outdim torchmodule torch nn linear indim outdim subclassing torchnnmodule jlwrap torchmodulewrapper torchmodule batchsize input randn float indim batchsize output jlwrap input target randn float outdim batchsize loss sum target grad zygote gradient loss input target jlwrap gpu install python dependencies using pycall cuda pytorch run pycallpyprogramname pip install torchcu functorch example using cuda using pycallchainrules torch torchmodulewrapper torch using zygote assert cuda functional indim outdim torchmodule torch nn linear indim outdim device torch device cuda subclassing torchnnmodule jlwrap torchmodulewrapper torchmodule batchsize input cuda cu randn float indim batchsize output jlwrap input target cuda cu randn float outdim batchsize loss sum grad zygote gradient loss input target jlwrap jax cpu install python dependencies using pycall run pycallpyprogramname pip install jax cpu cpu version example using pycallchainrules jax jaxfunctionwrapper jax stax pytodlpack batchsize indim outdim initlin applylin stax dense outdim params initlin jax random prngkey indim paramsjl map dlpack wrap pytodlpack params jlwrap jaxfunctionwrapper jax jit applylin input randn float indim batchsize output jlwrap paramsjl input target randn float outdim batchsize loss sum jlwrap grad zygote gradient loss input target paramsjl gpu install python dependencies using pycall run pycallpyprogramname pip install jax cuda example using pycallchainrules jax jaxfunctionwrapper jax stax using cuda using pycallchainrules jax jaxfunctionwrapper jax stax pytodlpack batchsize indim outdim initlin applylin stax dense outdim params initlin jax random prngkey indim paramsjl map dlpack wrap pytodlpack params jlwrap jaxfunctionwrapper jax jit applylin input cuda cu randn float indim batchsize output jlwrap paramsjl input target cuda cu randn float outdim batchsize loss sum jlwrap grad zygote gradient loss input target paramsjl mixing jax julia recommended disable jax preallocation setting environment variable xlapythonclientpreallocatefalse current limitations input output types wrapped python functions python tensors nested tuples python tensors keyword arguments arrays support differenti