using bayesflux flux using random distributions using statsplots random seed bayesflux bayesian extension flux bayesflux meant extension fluxjl machine learning library written entirely julia bayesflux meant fastest production ready library meant research experimentation easy bayesflux master thesis economic financial research specialisation econometrics likelily revisions coming months structure bayesian model broken probablistic model likelihood function prior parameters probabilistic model bayesflux somewhat follows splits bayesian network following network bnn network structure defined using flux currently supports dense rnn lstm layers network constructor bayesflux vectors parameters able vector network using networkconstructor likelihood likelihood function traditional estimation nns correspond negative loss function bayesflux twist nomenclature change twist likelihood contains additional parameters priors example gaussian likelihood likelihood object defines standard deviation prior standard deviation desing choice likelihood belonging separate network due potential confusion nomenclature change revisions prior network parameters prior network parameters currently rnn layers define priors initial initial sampled priors hyperpriors initialiser unless special initialisation values bayesflux draw initial values defined initialiser initialiser initialises network likelihood parameters reasonable values create bnn estimated using map sampled using mcmc methods implemented estimated using variational inference examples sections hopefully clarify questions remain please issue linear regression using bayesflux meant simple linear regression bayesflux section hopefully demonstrate basics sections examples idea data modelled via linear model form yi xibeta ei ei sim randn float randn float randn float standard linear model using stan turing due availability dense layer linear activation function implent bayesflux step define network mentioned network consists single dense layer linear activation function default activation flux hence explicitly shown net chain dense inputs output bayesflux vectors able transform vector network networkconstructor obtain return value destruct nc destruct net check creating random vector dimension calling networkconstructor using vector randn float nc numparamsnetwork nc indeed obtain network size structure define prior parameters network weight decay popular regularisation method standard ml estimation using gaussian prior bayesian weight decay prior gaussianprior nc f value standard deviation likelihood prior parameters likelihood introduces model gaussian likelihood introduces standard deviation model bayesflux currently implements gaussian student likelihoods feedforward seq easily implemented todo har link example feedforwardnormal nc gamma argument prior standard deviation lastly explicit initial value bayesflux draw initialiser currently type initialiser implemented bayesflux easily extended user init initialiseallsame normal f f prior argument dist draw parameters define bnn bnn bnn prior init map estimate idea map estimate serve purposes faster estimating model using mcmc vi serve quick check map estimate results bad predictions estimation results serve starting value mcmc samplers map estimate specify define optimiser bayesflux currently implements optimisers derived flux extended user opt fluxmodefinder bnn flux adam adam map findmode bnn opt batchsize epochs map estimate predictions calculate rmse nethat nc map yhat vec nethat sqrt mean abs yhat mcmc sgld map estimate starting sgld mcmc methods section simulations shown using relatively initial stepsize slow decaying stepsize schedule results mixing note usually samplers nuts linear regressions efficient sgld sampler sgld float stepsizea f stepsizeb f stepsize f ch mcmc bnn sampler ch ch obtain summary statistics trace density plots network parameters likelihood parameters transforming bayesflux chain mcmcchain using mcmcchains chain chains ch plot chain complicated networks usually hopeless goal obtain mixing parameter space focus output space network mixing parameter space hopeless due complicated topology posterior little helper function output values network function naiveprediction bnn draws array bnn bnn yhats array undef length size draws threads threads size draws net bnn nc draws yh vec net yhats yh return yhats yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat similarly obtain posterior predictive values evaluate quantiles obtained using percent actual data fall quantiles data fall quantile posterior predictive draws function getobservedquantiles posterioryhat targetq qs quantile yr targetq yr eachrow posterioryhat qs reduce hcat qs observedq mean reshape qs dims return observedq posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target mcmc sgnhts sgld sgnhts apply metropolishastings correction step contrary sgld sgnhts implementes thermostat task temperature dynamic system close sampling accurate thermostats goal achieved samples obtained using sgnhts outperform obtained using sgld sampler sgnhts f f xi f f ch mcmc bnn sampler ch ch chain chains ch yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target mcmc ggmc neither sgld sgnhts apply metropolishastings acceptance step difficult monitor indeed draws sgld sgnhts considered giving ensemble models draws posterior mh step unclear chain actually converge posterior bayesflux implements methods apply mh step easier monitor ggmc adaptivemh hmc ggmc hmc allow taking stochastic gradients ggmc allows delayed acceptance mh step applied couple steps step details ggmc hmc mh step provide measure mean acceptance rate tune stepsize using dual averaging stan details similarly mass matrices tuned bayesflux implements stepsize adapters mass adapters implement smart combining future experience naively combining helps complex models stepsize adapter sadapter dualaveragingstepsize f targetaccept f adaptsteps sampler ggmc float f f sadapter sadapter ch mcmc bnn sampler ch ch chain chains ch yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target mh correction step costly data environments evaluation likelihood costly applies delayed acceptance speed process sadapter dualaveragingstepsize f targetaccept f adaptsteps sampler ggmc float f f sadapter sadapter steps ch mcmc bnn sampler ch ch chain chains ch yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target mcmc hmc hmc mixing variables testing readme decided mass matrix adaptation simple sadapter dualaveragingstepsize f targetaccept f adaptsteps madapter diagcovmassadapter sampler hmc f sadapter sadapter ch mcmc bnn sampler ch ch chain chains ch yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target mcmc adaptive metropolishastings derivative free alternative bayesflux implements adaptive mh introduced currently costly method complex models evaluate mh ratio step plans exist parallelise calculation likelihood speed adaptive mh sampler adaptivemh diagm ones float bnn numtotalparams f f ch mcmc bnn sampler ch ch chain chains ch yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target variation inference mcmc method methods bayesflux currently implements bayesbackprop shortcoming current implementation variational family constrained diagonal multivariate gaussian correlations network parameters set zero cause situations plans exist allow felxible covariance specifications params losses bbb bnn mcsamples opt flux adam nsamplesconvergence ch rand posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target complicated fnn changes implement bnns using complicated feedforward structures truly changes network specify rest theory stay network complicated worth specify priors likelihoods example data reality coming linear model practice try simple models instead using network structure corresponding linear model following net chain dense relu dense relu dense prior likelihood initialiser change networkconstructor nc destruct net feedforwardnormal nc gamma prior gaussianprior nc f init initialiseallsame normal f f prior bnn bnn prior init rest example map opt fluxmodefinder bnn flux adam adam map findmode bnn opt batchsize epochs nethat nc map yhat vec nethat sqrt mean abs yhat mcmc vi method sgnhts option sampler sgnhts f f xi f f ch mcmc bnn sampler ch ch chain chains ch yhats naiveprediction bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target recurrent structures dense layers bayesflux implements rnn lstm layers require additional care layout data adjusted dimension dimension bayesflux batches seq setting seqseq implemented users implement custom likelihoods seqseq setting sequences dimension third demonstrate simulate sime ar data random seed gamma burnin zeros burnin burnin gamma randn float fnn network structure constructor prior network parameters likelihood prior additional parameters introduced likelihood initialiser net chain rnn dense layer linear output layer nc destruct net seqtoonenormal nc gamma prior gaussianprior nc f init initialiseallsame normal f f prior single sequence time series exploit batching feed sequence split single sequence overlapping subsequences length store tensor note add subsequence length observation subsequence training observation predict using fist five items subsequence makernntensor reshape vec ready create bnn map estimate map check overall network structure makes sense provide estimates bnn bnn prior init opt fluxmodefinder bnn flux rmsprop map findmode bnn opt checking performance feed sequences network observation observation nethat nc map yhat vec nethat xx xx eachslice dims sqrt mean abs yhat rest minor adjustments helper functions sampler sgnhts f f xi f f ch mcmc bnn sampler ch ch chain chains ch function naivepredictionrecurrent bnn draws array bnn bnn yhats array undef length size draws threads threads size draws net bnn nc draws yh vec net xx xx eachslice dims yhats yh return yhats yhats naivepredictionrecurrent bnn ch chainyhat chains yhats maximum summarystats chainyhat rhat posterioryhat sampleposteriorpredict bnn ch tq oq getobservedquantiles posterioryhat tq plot tq oq label posterior predictive legend topleft xlab target quantile ylab observed quantile plot tq label target customising bayesflux bayesflux coded user easily extend funcitonalities following easiest extend cover initialisers layers priors likelihoods customising initialisers initialiser implemented callable type extending abstract type bnninitialiser aready mentioned callable optional argument random generator return tupe vectors net hyper like latter vectors allowed length zero information read documentation bnninitialiser example code initialiseallsame customising layers bayesflux relies layers currently implemented flux step implementing layer bayesflux implement layer flux implement destruct method example dense layer following form function destruct cell fluxdense unpack weight bias cell vcat vec weight vec bias function re abstractvector pweight length weight newweight reshape pweight size weight pweight pbias length bias newbias reshape pbias size bias return flux dense newweight newbias return re destruct method takes input cell type cell newly implemented layer return vector containing network parameter trainedinferred function vector length restruct layer note flux implements destructure restructure method experience caused ad stable bayesflux stick manual setup care cells recurrent actual layer rnncell recurrent version fluxrecurrnncell destruct methos rnn cells takes following form function destruct cell fluxrecur fluxrnncell unpack wi wh state cell cell vcatvecwi vecwh vec vecstate vcat vec wi vec wh vec function re vector pwi length wi newwi reshape pwi size wi pwi pwh length wh newwh reshape pwh size wh pwh pb length newb reshape pb size pb pstate lengthstate newstate reshapepstate sizestate newstate zeros size state return flux recur flux rnncell newwi newwh newb newstate return re seen commented lines currently inferring initial theoretically bayesian setting cause bad mixing difficulties inferential process customising priors bayesflux implements priors subtypes abstract type networkprior happens calles loglikeprior bayesflux splits vector net hyper like calls prior net hyper hyperparameters prior type bayesflux theory allows simple highly complex multilevel priors hope provides flexibility encourage researchers try priors documentation please docs networkprior example mixture scale prior check code mixturescaleprior note prior defined network additional priors parameters likelihood handled likelihood sound odd nicely splits network specific likelihood specific bayesflux flexible customising likelihoods likelihoods implemented types extending abstract type bnnlikelihood extended implementing subtype traditionally likelihood truly refer likelihood decided somewhat unconventional decided design bayesflux likelihood types include prior parameters introduce network specification priors separate likelihood specification freely changed changing example implement simple gaussian likelihood feedforward structure implemented unless predifine standard deviation gaussian estimate prior traditional setting covered prior type covered likelihood type version implemented bayesflux takes construction prior distribution standard deviation prior domain real line constrained gamma distribution code likelihood type makes appropriately transform distribution real line implemented version using bijectorsjl documentation bnnlikelihood details exactly implemented page generated using literatejl