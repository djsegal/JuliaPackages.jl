smartboost julia implementation smartboost smooth additive regression trees described paper smartboost learning tabular data version via juliaconnector available currently support l loss extensions planned input features arrayfloatfloat dataframe installation pkg add parameters incomplete list smartparam documentation loss l currently l supported extensions planned depth tree depth default typically crossvalidated smartfit lambda learning rate loglikdivide panel data smartloglikdivide set parameter overlap overlaps typically overlap nfold fold cv set nfold single validation set sharevalidation share sample verbose verbosity float float faster float randomizecv false default purgedcv paper time series panel structure automatically detected smartdata subsamplesharevs row subssampling randomly drawn iteration share sample determining feature subsamplesharecolumns column subsampling example example example using smartboost single core numberworkers desired workers using distributed nprocsnumberworkers addprocs numberworkers nprocs addprocs using smartboost using random plots jld options smartboost cvdepth false false default depth true cv nfold nfold cv faster default slower accurate options generate data sum additive nonlinear functions gaussian noisestde ntest stde f f sin f f exp bbbb generate data xtest randn randn ntest f b f b f b f b ftest f xtest b f xtest b f xtest b f xtest b randn stde set smartparam smartdata fit predit param smartparam nfold nfoldverbose data smartdata paramfdgp cvdepth false output smartfit dataparam default depth else output smartfit dataparamparamfield depth cvgrid stopwhenlossup true starts depth stops soon loss increases yf smartpredict xtestoutput smarttrees predict println depth output bestvalue trees output ntrees println sample rmse truth sqrt sum yf ftest ntest save load fitted model save outputjld output load outputjld output note key load outputjld output keyerror feature importance partial dependence plots marginal effects fnamesfifnamessortedfisortedsortedindx smartrelevance output smarttreesdata pdp smartpartialplot dataoutput smarttrees qm smartmarginaleffect dataoutput smarttrees npoints compute marginal effect x grid set npoints otherxs x vector features qm smartmarginaleffectdataoutputsmarttreesotherxsxnpoints plot partial dependence figures example png example global equity data parallelization via distributed cv priors panel data refer examplesexample notice prepare panel data purgedcv calibrate loglikdivide effective sample size ess setting parallelization numberworkers desired workers using distributed nprocs numberworkers addprocs numberworkers nprocs addprocs using smartboost prepare data sorting dataframe date required purgedcv sort df date calibrate loglikdivide lldess smartloglikdivide df excessret date overlap param smartparam loglikdivide lldoverlap stopwhenlossup true cv stop soon loss increases refer examplesexample trainvalidationtest split smartboost particular notice default refit model crossvalidated tuning parameters entire data trainvalidation fold cv nfold wish skip step speed comparison methods set nofullsample true smartfit nfold default data validation test approximately purged validation change default set sharevalidation sample sharevalidation observations setting sharevalidation integer switches default nfold suggestions speeding smartboost example approximate computing time trees depth using workers amd epyc dgp linear dgp linear trees sufficient applications minutes trees k m m k m m smartboost runs faster particularly cores initial cost running cores consider limiting depth single validation sample instead default fold cv paramnfold additionally smartfit set nofullsample true reduces computing time roughly cost modest efficiency loss nofullsample true required trainvalidationtest split model fit train set default validation test cv retrain trainvalidation optimum parameter values computing time increases rapidly paramdepth smooth trees cv tree depth start low value stop soon sizable improvement set stopwhenlossup true smartfit workers rule thumb computing times double depth depth set stderulestop stop iterations loss decreasing sizably cost loss performance row column subssampling support